import numpy as np
import torch
import math
import torchvision
from torch import nn
from torch.nn import functional as F
from torchvision import datasets, transforms
import warnings
from functools import partial
import random
from models_util import *
from approx import *
# define customized functions with customized gradients
class STEFunction(torch.autograd.Function):
    """ define straight through estimator with overrided gradient (gate) """
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return (input > 0).float()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        return torch.mul(F.softplus(input), grad_output)

# class STEFunction(torch.autograd.Function):
#     """ define straight through estimator with overrided gradient (gate) """
#     @staticmethod
#     def forward(ctx, input):
#         ctx.save_for_backward(input)
#         return (input > 0).float()

#     @staticmethod
#     def backward(ctx, grad_output):
#         input, = ctx.saved_tensors
#         return torch.mul((input > 0).float(), grad_output)



# def x2act(x_input, scale_x2 = 0.1, scale_x = 0.5):
#     '''
#     Applies the x^2 Unit (x2act) function element-wise:
#         x2act(x) = scale*w0*x^2+w1*x
#     '''
#     return scale_x2 * torch.mul(x_input, x_input) + scale_x * x_input

# def xact_auto(x_input, scale_x = 0.5, bias = 0.4):
#     '''
#     Applies the x^2 Unit (x2act) function element-wise:
#         x2act(x) = scale*w0*x^2+w1*x+c
#     '''
#     return scale_x * x_input + bias


# # define customized functions with customized gradients
# class ReLU_Pruned(torch.autograd.Function):
#     """ define straight through estimator with overrided gradient (gate) """
#     @staticmethod
#     def forward(ctx, input):
#         ctx.save_for_backward(input)
#         return input

#     @staticmethod
#     def backward(ctx, grad_output):
#         input, = ctx.saved_tensors
#         return torch.mul((input > 0).float(), grad_output)
# define customized functions with customized gradients
class ReLU_Pruned(torch.autograd.Function):
    """ define straight through estimator with overrided gradient (gate) """
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input > 0)
        return input

    @staticmethod
    def backward(ctx, grad_output):
        relu_input, = ctx.saved_tensors
        return torch.mul(relu_input.float(), grad_output)

# Build model and load/initialize weights
def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    """ 
        function to help weight initialization
        Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    """
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

### ReLU with run time initialization method
# mask1: bitmap, 1 means has ReLU, 0 means direct pass.
# mask2: bitmap, 1 means direct pass, 0 means have ReLU
# a*mask2: passed element
# a*mask1: element need to be ReLU
# ReLU(a*mask1) + a*mask2
class ReLU_masked(nn.Module):
    def __init__(self, config, Num_mask = 1, dropRate=0):
        super().__init__()
        self.Num_mask = Num_mask
        self.num_feature = 0
        self.current_feature = 0
        self.sel_mask = 0
        self.init = 1
        self.dropout = nn.Dropout2d(p=dropRate, inplace=True)
    def init_w_aux(self, size):
        for i in range(self.Num_mask):
            setattr(self, "alpha_aux_{}_{}".format(self.num_feature, i), nn.Parameter(torch.Tensor(*size)))
            nn.init.uniform_(getattr(self, "alpha_aux_{}_{}".format(self.num_feature, i)), a = 0, b = 1) # weight init for aux parameter, can be truncated normal
    def mask_density_forward(self):
        l0_reg = 0
        sparse_list = []
        sparse_pert_list = []
        total_mask = 0
        for current_feature in range(self.num_feature):
            neuron_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(current_feature, self.sel_mask)))
            l0_reg += torch.sum(neuron_mask)
            sparse_list.append(torch.sum(neuron_mask).item())
            sparse_pert_list.append(sparse_list[-1]/neuron_mask.numel())
            total_mask += neuron_mask.numel()
        global_density = l0_reg/total_mask 
        return global_density, sparse_list, sparse_pert_list, total_mask

    def forward(self, x):
        ### Initialize the parameter at the beginning
        if self.init:
            x_size = list(x.size())[1:] ### Ignore batch size dimension
            self.init_w_aux(x_size)
            neuron_relu_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(self.num_feature, self.sel_mask))) ### Mask for element which applies ReLU
            self.num_feature += 1
        ### Conduct recurrently inference during normal inference and training
        else:
            # print("Current used: ", getattr(self, "alpha_aux_{}_{}".format(self.current_feature, self.sel_mask)))
            neuron_relu_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(self.current_feature, self.sel_mask))) ### Mask for element which applies ReLU
            self.current_feature = (self.current_feature + 1) % self.num_feature
        neuron_pass_mask = 1 - neuron_relu_mask  ### Mask for element which ignore ReLU
        # out = self.act(torch.mul(x, neuron_relu_mask)) + torch.mul(x, neuron_pass_mask)
        out = torch.mul(F.relu(x), neuron_relu_mask) + torch.mul(x, neuron_pass_mask)
        # out = self.dropout(out)
        return out

### ReLU with run time initialization method
# mask1: bitmap, 1 means has ReLU, 0 means direct pass.
# mask2: bitmap, 1 means direct pass, 0 means have ReLU
# a*mask2: passed element
# a*mask1: element need to be ReLU
# ReLU(a*mask1) + a*mask2
class ReLU_masked_spgrad(nn.Module):
    def __init__(self, config, Num_mask = 1, dropRate=0):
        super().__init__()
        self.Num_mask = Num_mask
        self.num_feature = 0
        self.current_feature = 0
        self.sel_mask = 0
        self.init = 1
        self.dropout = nn.Dropout2d(p=dropRate, inplace=True)
        self.p = dropRate
    def init_w_aux(self, size):
        for i in range(self.Num_mask):
            setattr(self, "alpha_aux_{}_{}".format(self.num_feature, i), nn.Parameter(torch.Tensor(*size)))
            nn.init.uniform_(getattr(self, "alpha_aux_{}_{}".format(self.num_feature, i)), a = 0, b = 1) # weight init for aux parameter, can be truncated normal
    def mask_density_forward(self):
        l0_reg = 0
        sparse_list = []
        sparse_pert_list = []
        total_mask = 0
        for current_feature in range(self.num_feature):
            neuron_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(current_feature, self.sel_mask)))
            l0_reg += torch.sum(neuron_mask)
            sparse_list.append(torch.sum(neuron_mask).item())
            sparse_pert_list.append(sparse_list[-1]/neuron_mask.numel())
            total_mask += neuron_mask.numel()
        global_density = l0_reg/total_mask 
        return global_density, sparse_list, sparse_pert_list, total_mask
    def forward(self, x):
        ### Initialize the parameter at the beginning
        if self.init:
            x_size = list(x.size())[1:] ### Ignore batch size dimension
            self.init_w_aux(x_size)
            neuron_relu_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(self.num_feature, self.sel_mask))) ### Mask for element which applies ReLU
            self.num_feature += 1
        ### Conduct recurrently inference during normal inference and training
        else:
            # print("Current used: ", getattr(self, "alpha_aux_{}_{}".format(self.current_feature, self.sel_mask)))
            neuron_relu_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(self.current_feature, self.sel_mask))) ### Mask for element which applies ReLU
            self.current_feature = (self.current_feature + 1) % self.num_feature
        neuron_pass_mask = 1 - neuron_relu_mask  ### Mask for element which ignore ReLU
        # out = self.act(torch.mul(x, neuron_relu_mask)) + ReLU_Pruned.apply(torch.mul(x, neuron_pass_mask))
        out = torch.mul(F.relu(x), neuron_relu_mask) + torch.mul(ReLU_Pruned.apply(x), neuron_pass_mask)
        # out = self.dropout(out)
        # out_relu = F.relu(x)
        # if (self.training and self.p > 0):
        #     sel = float(random.uniform(0, 1) < self.p)
        #     out_final = sel * out_relu + (1 - sel) * out
        #     return out_final
        # else:
        #     return out
        return out

### ReLU with run time initialization method
# mask1: bitmap, 1 means has ReLU, 0 means direct pass.
# mask2: bitmap, 1 means direct pass, 0 means have ReLU
# a*mask2: passed element
# a*mask1: element need to be ReLU
# ReLU(a*mask1) + a*mask2
class ReLU_masked_autopoly(nn.Module):
    def __init__(self, config, Num_mask = 1, dropRate=0):
        super().__init__()
        self.Num_mask = Num_mask
        self.num_feature = 0
        self.current_feature = 0
        self.sel_mask = 0
        self.init = 1
        self.degree = config.degree
        self.freezeact = config.freezeact
        self.scale_x2 = config.scale_x2
        self.out_act_rep = eval("x{}act_auto".format(self.degree))
        self.dropout = nn.Dropout2d(p=dropRate, inplace=True)
        self.p = dropRate
    def init_w_aux(self, size):
        for i in range(self.Num_mask):
            setattr(self, "alpha_aux_{}_{}".format(self.num_feature, i), nn.Parameter(torch.Tensor(*size)))
            nn.init.uniform_(getattr(self, "alpha_aux_{}_{}".format(self.num_feature, i)), a = 0, b = 1) # weight init for aux parameter, can be truncated normal

        ### Initialize the channel wise polynoimal activation parameter
        if len(size) == 4:
            para_size = [1, size[0], 1, 1]
        elif len(size) == 2:
            para_size = [1, size[1]]
        else:
            print("Operation with {} not supported".format(size))
            exit()
        exec("self.poly_para_{} = []".format(self.num_feature))
        for i in range(self.degree + 1):
            # print("channel size:", channel_size)
            # print(nn.Parameter(torch.Tensor(*channel_size)))
            setattr(self, "poly_para_{}_{}".format(self.num_feature, i), nn.Parameter(torch.Tensor(*para_size)))
            if i == 0:
                nn.init.uniform_(getattr(self, "poly_para_{}_{}".format(self.num_feature, i)), a = 0, b = 0.0001)
            elif i == 1:
                nn.init.uniform_(getattr(self, "poly_para_{}_{}".format(self.num_feature, i)), a = 1, b = 1.0001)
            elif i == 2:
                nn.init.uniform_(getattr(self, "poly_para_{}_{}".format(self.num_feature, i)), a = 0, b = 0.0001)
            else:
                print("we currently don't have support for degree higher than 2")
                exit()
            setattr(eval("self.poly_para_{}_{}".format(self.num_feature, i)), 'requires_grad', (not self.freezeact)) 
            exec("self.poly_para_{}.append(self.poly_para_{}_{})".format(self.num_feature, self.num_feature, i))
            # print(eval("self.poly_para_{}_{}".format(self.num_feature, i)))
            # print("Create poly_para_{}_{} successfully".format(self.num_feature, i))
        # print("Shape: \n", eval("self.poly_para_{}[0].shape".format(self.num_feature)))
        # print("Value: \n", eval("self.poly_para_{}".format(self.num_feature)))

    ## aggregate the polynomial parameter to a list
    def expand_aggr_poly(self,):
        for current_feature in range(self.num_feature):
            exec("self.poly_para_{} = []".format(current_feature))
            for i in range(self.degree + 1):
                exec("self.poly_para_{}.append(self.poly_para_{}_{})".format(current_feature, current_feature, i))

    def mask_density_forward(self):
        l0_reg = 0
        sparse_list = []
        sparse_pert_list = []
        total_mask = 0
        for current_feature in range(self.num_feature):
            neuron_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(current_feature, self.sel_mask)))
            l0_reg += torch.sum(neuron_mask)
            sparse_list.append(torch.sum(neuron_mask).item())
            sparse_pert_list.append(sparse_list[-1]/neuron_mask.numel())
            total_mask += neuron_mask.numel()
        global_density = l0_reg/total_mask 
        return global_density, sparse_list, sparse_pert_list, total_mask
    def forward(self, x):
        ### Initialize the parameter at the beginning
        if self.init:
            x_size = list(x.size())[1:] ### Ignore batch size dimension
            # var_map = x.var_map
            self.init_w_aux(x_size)
            neuron_relu_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(self.num_feature, self.sel_mask))) ### Mask for element which applies ReLU
            # act_choice = eval(f"self.var_map_{self.num_feature}")
            # out_act_rep = eval("self.act_d{}_var{}".format(self.degree, act_choice))
            out_act_rep = self.out_act_rep
            if self.degree == 2:
                out_act_rep = partial(out_act_rep, para = eval("self.poly_para_{}".format(self.num_feature)), scale_x2 = self.scale_x2)
            else:
                out_act_rep = partial(out_act_rep, para = eval("self.poly_para_{}".format(self.num_feature)))
            self.num_feature += 1
        ### Conduct recurrently inference during normal inference and training
        else:
            if self.current_feature == 0:
                self.expand_aggr_poly()
            # print("Current used: ", getattr(self, "alpha_aux_{}_{}".format(self.current_feature, self.sel_mask)))
            neuron_relu_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(self.current_feature, self.sel_mask))) ### Mask for element which applies ReLU
            # act_choice = eval(f"self.var_map_{self.current_feature}")
            # out_act_rep = eval("self.act_d{}_var{}".format(self.degree, act_choice))
            out_act_rep = self.out_act_rep
            if self.degree == 2:
                out_act_rep = partial(out_act_rep, para = eval("self.poly_para_{}".format(self.current_feature)), scale_x2 = self.scale_x2)
            else:
                out_act_rep = partial(out_act_rep, para = eval("self.poly_para_{}".format(self.current_feature)))
            self.current_feature = (self.current_feature + 1) % self.num_feature
        neuron_pass_mask = 1 - neuron_relu_mask  ### Mask for element which ignore ReLU
        # out = torch.mul(self.act(x), neuron_relu_mask) + torch.mul(out_act_rep(x), neuron_pass_mask)
        out = torch.mul(F.relu(x), neuron_relu_mask.expand_as(x)) + torch.mul(out_act_rep(x), neuron_pass_mask.expand_as(x))
        # out = self.dropout(out)
        
        # if (self.training and self.p > 0):
        #     out_relu = F.relu(x)
        #     sel = float(random.uniform(0, 1) < self.p)
        #     out_final = sel * out_relu + (1 - sel) * out
        #     return out_final
        # else:
        #     return out
        return out
