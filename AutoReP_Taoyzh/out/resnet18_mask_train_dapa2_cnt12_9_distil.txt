[HAMI-core Msg(465233:124733484747648:libvgpu.c:837)]: Initializing.....
06/01 02:05:59 AM | 
06/01 02:05:59 AM | Parameters:
06/01 02:05:59 AM | NUM_MASK=1
06/01 02:05:59 AM | RELU_COUNT=12.9
06/01 02:05:59 AM | ACT_TYPE=ReLU_masked_dapa_relay
06/01 02:05:59 AM | ALPHA_LR=0.0002
06/01 02:05:59 AM | ALPHA_WEIGHT_DECAY=0.001
06/01 02:05:59 AM | ARCH=resnet18
06/01 02:05:59 AM | BATCH_SIZE=128
06/01 02:05:59 AM | CHECKPOINT_PATH=None
06/01 02:05:59 AM | CLIP_X2=1.0
06/01 02:05:59 AM | CLIP_X2_BOOL=True
06/01 02:05:59 AM | DATA_PATH=./data/
06/01 02:05:59 AM | DATASET=cifar100
06/01 02:05:59 AM | DEGREE=2
06/01 02:05:59 AM | DISTIL=True
06/01 02:05:59 AM | DROPOUT=0
06/01 02:05:59 AM | ENABLE_GRAD_NORM=False
06/01 02:05:59 AM | ENABLE_LOOKAHEAD=True
06/01 02:05:59 AM | EPOCHS=200
06/01 02:05:59 AM | EVALUATE=None
06/01 02:05:59 AM | EXT=baseline
06/01 02:05:59 AM | FREEZEACT=False
06/01 02:05:59 AM | GPUS=[0]
06/01 02:05:59 AM | LAMDA=240.0
06/01 02:05:59 AM | MASK_DROPOUT=0
06/01 02:05:59 AM | MASK_EPOCHS=80
06/01 02:05:59 AM | NUM_CLASSES=100
06/01 02:05:59 AM | OPTIM=cosine
06/01 02:05:59 AM | PATH=train_cifar_dapa2_distil_relay/resnet18_resnet18_cifar100_relay_0.003/cosine_ReLUs12.9wm_lr0.001mep80_baseline
06/01 02:05:59 AM | PLOT_PATH=train_cifar_dapa2_distil_relay/resnet18_resnet18_cifar100_relay_0.003/cosine_ReLUs12.9wm_lr0.001mep80_baseline/plots
06/01 02:05:59 AM | PRECISION=full
06/01 02:05:59 AM | PRETRAINED=False
06/01 02:05:59 AM | PRETRAINED_PATH=./train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar
06/01 02:05:59 AM | PRINT_FREQ=100
06/01 02:05:59 AM | SCALE_X1=1.0
06/01 02:05:59 AM | SCALE_X2=2.0
06/01 02:05:59 AM | SEED=2
06/01 02:05:59 AM | START_EPOCH=0
06/01 02:05:59 AM | TEACHER_ARCH=resnet18
06/01 02:05:59 AM | TEACHER_PATH=./train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar
06/01 02:05:59 AM | THRESHOLD=0.003
06/01 02:05:59 AM | VAR_MIN=0.5
06/01 02:05:59 AM | W_DECAY_EPOCH=20
06/01 02:05:59 AM | W_GRAD_CLIP=5.0
06/01 02:05:59 AM | W_LR=0.0001
06/01 02:05:59 AM | W_LR_MIN=1e-05
06/01 02:05:59 AM | W_MASK_LR=0.001
06/01 02:05:59 AM | W_MOMENTUM=0.9
06/01 02:05:59 AM | W_WEIGHT_DECAY=0.0005
06/01 02:05:59 AM | WORKERS=4
06/01 02:05:59 AM | X_SIZE=[1, 3, 32, 32]
06/01 02:05:59 AM | 
06/01 02:05:59 AM | Logger is set - training start
[HAMI-core Msg(465233:124733484747648:libvgpu.c:856)]: Initialized
==> Load pretrained
=> loading checkpoint './train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar'
/mnt/ann25-22336216/AutoReP/models_util/model_util.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_path, map_location = "cpu")
=> loading checkpoint './train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar'
Files already downloaded and verified
Files already downloaded and verified
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 65536.0, 1.0]
['model.relu.alpha_mask_1_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_2_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_3_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_4_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_5_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_6_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_7_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_8_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_9_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_10_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_11_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_12_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_13_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_14_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_15_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_16_0', 2048, 2048.0, 1.0]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 188416.0, 1.0]
########## End ###########
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16601579520 total=17059545088 limit=4194304000 usage=415246848
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16366698496 total=17059545088 limit=4194304000 usage=650127872
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16106651648 total=17059545088 limit=4194304000 usage=910174720
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16085680128 total=17059545088 limit=4194304000 usage=931146240
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16066805760 total=17059545088 limit=4194304000 usage=950020608
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16045834240 total=17059545088 limit=4194304000 usage=970992128
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16039542784 total=17059545088 limit=4194304000 usage=977283584
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16037445632 total=17059545088 limit=4194304000 usage=979380736
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15760621568 total=17059545088 limit=4194304000 usage=1252285440
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15703998464 total=17059545088 limit=4194304000 usage=1308908544
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15703998464 total=17059545088 limit=4194304000 usage=1308908544
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15708192768 total=17059545088 limit=4194304000 usage=1304714240
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15651569664 total=17059545088 limit=4194304000 usage=1361337344
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15636889600 total=17059545088 limit=4194304000 usage=1376017408
06/01 02:06:04 AM | Train: [ 1/80] Step 000/390 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)
06/01 02:06:04 AM | layerwise density: [65536.0, 16384.0, 16384.0, 16384.0, 16384.0, 8192.0, 8192.0, 8192.0, 8192.0, 4096.0, 4096.0, 4096.0, 4096.0, 2048.0, 2048.0, 2048.0, 2048.0]
layerwise density percentage: ['1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000']
Global density: 1.0
06/01 02:06:15 AM | Train: [ 1/80] Step 100/390 Loss 0.024 Prec@(1,5) (99.9%, 100.0%)
06/01 02:06:15 AM | layerwise density: [62126.0, 15548.0, 15555.0, 15539.0, 15548.0, 7759.0, 7767.0, 7748.0, 7760.0, 3881.0, 3893.0, 3904.0, 3901.0, 1975.0, 1970.0, 1954.0, 1934.0]
layerwise density percentage: ['0.948', '0.949', '0.949', '0.948', '0.949', '0.947', '0.948', '0.946', '0.947', '0.948', '0.950', '0.953', '0.952', '0.964', '0.962', '0.954', '0.944']
Global density: 0.9487623572349548
06/01 02:06:26 AM | Train: [ 1/80] Step 200/390 Loss 0.032 Prec@(1,5) (99.8%, 100.0%)
06/01 02:06:26 AM | layerwise density: [58887.0, 14811.0, 14758.0, 14797.0, 14780.0, 7359.0, 7304.0, 7347.0, 7409.0, 3708.0, 3700.0, 3726.0, 3705.0, 1904.0, 1892.0, 1875.0, 1854.0]
layerwise density percentage: ['0.899', '0.904', '0.901', '0.903', '0.902', '0.898', '0.892', '0.897', '0.904', '0.905', '0.903', '0.910', '0.905', '0.930', '0.924', '0.916', '0.905']
Global density: 0.9012823104858398
06/01 02:06:37 AM | Train: [ 1/80] Step 300/390 Loss 0.043 Prec@(1,5) (99.7%, 100.0%)
06/01 02:06:37 AM | layerwise density: [55630.0, 13997.0, 13960.0, 13989.0, 14019.0, 6923.0, 6924.0, 6960.0, 7003.0, 3523.0, 3517.0, 3532.0, 3527.0, 1818.0, 1817.0, 1772.0, 1778.0]
layerwise density percentage: ['0.849', '0.854', '0.852', '0.854', '0.856', '0.845', '0.845', '0.850', '0.855', '0.860', '0.859', '0.862', '0.861', '0.888', '0.887', '0.865', '0.868']
Global density: 0.8528416156768799
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15464923136 total=17059545088 limit=4194304000 usage=1547983872
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16190537728 total=17059545088 limit=4194304000 usage=822369280
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16085680128 total=17059545088 limit=4194304000 usage=927226880
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16064708608 total=17059545088 limit=4194304000 usage=948198400
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16064708608 total=17059545088 limit=4194304000 usage=948198400
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16022765568 total=17059545088 limit=4194304000 usage=990141440
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16022765568 total=17059545088 limit=4194304000 usage=990141440
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16005988352 total=17059545088 limit=4194304000 usage=1006918656
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15989211136 total=17059545088 limit=4194304000 usage=1023695872
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15999696896 total=17059545088 limit=4194304000 usage=1013210112
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15999696896 total=17059545088 limit=4194304000 usage=1013210112
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15821438976 total=17059545088 limit=4194304000 usage=1191468032
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15792078848 total=17059545088 limit=4194304000 usage=1220828160
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15792078848 total=17059545088 limit=4194304000 usage=1220828160
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15794176000 total=17059545088 limit=4194304000 usage=1218731008
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15827730432 total=17059545088 limit=4194304000 usage=1185176576
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15827730432 total=17059545088 limit=4194304000 usage=1185176576
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15857090560 total=17059545088 limit=4194304000 usage=1155816448
06/01 02:06:48 AM | Train: [ 1/80] Step 390/390 Loss 0.058 Prec@(1,5) (99.4%, 100.0%)
06/01 02:06:48 AM | layerwise density: [52854.0, 13287.0, 13275.0, 13327.0, 13249.0, 6583.0, 6594.0, 6585.0, 6648.0, 3356.0, 3353.0, 3358.0, 3345.0, 1748.0, 1752.0, 1701.0, 1672.0]
layerwise density percentage: ['0.806', '0.811', '0.810', '0.813', '0.809', '0.804', '0.805', '0.804', '0.812', '0.819', '0.819', '0.820', '0.817', '0.854', '0.855', '0.831', '0.816']
Global density: 0.8103717565536499
06/01 02:06:48 AM | Train: [ 1/200] Final Prec@1 99.4400%
06/01 02:06:48 AM | Valid: [ 1/200] Step 000/078 Loss 1.133 Prec@(1,5) (72.7%, 91.4%)
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15781593088 total=17059545088 limit=4194304000 usage=1231313920
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16305881088 total=17059545088 limit=4194304000 usage=707025920
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16282812416 total=17059545088 limit=4194304000 usage=730094592
06/01 02:06:51 AM | Valid: [ 1/200] Step 078/078 Loss 1.328 Prec@(1,5) (68.7%, 89.0%)
06/01 02:06:51 AM | Valid: [ 1/200] Final Prec@1 68.6900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 52854.0, 0.806488037109375]
['model.relu.alpha_mask_1_0', 16384, 13287.0, 0.81097412109375]
['model.relu.alpha_mask_2_0', 16384, 13275.0, 0.81024169921875]
['model.relu.alpha_mask_3_0', 16384, 13327.0, 0.81341552734375]
['model.relu.alpha_mask_4_0', 16384, 13249.0, 0.80865478515625]
['model.relu.alpha_mask_5_0', 8192, 6583.0, 0.8035888671875]
['model.relu.alpha_mask_6_0', 8192, 6594.0, 0.804931640625]
['model.relu.alpha_mask_7_0', 8192, 6585.0, 0.8038330078125]
['model.relu.alpha_mask_8_0', 8192, 6648.0, 0.8115234375]
['model.relu.alpha_mask_9_0', 4096, 3356.0, 0.8193359375]
['model.relu.alpha_mask_10_0', 4096, 3353.0, 0.818603515625]
['model.relu.alpha_mask_11_0', 4096, 3358.0, 0.81982421875]
['model.relu.alpha_mask_12_0', 4096, 3345.0, 0.816650390625]
['model.relu.alpha_mask_13_0', 2048, 1748.0, 0.853515625]
['model.relu.alpha_mask_14_0', 2048, 1752.0, 0.85546875]
['model.relu.alpha_mask_15_0', 2048, 1701.0, 0.83056640625]
['model.relu.alpha_mask_16_0', 2048, 1672.0, 0.81640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 152687.0, 0.8103717306385869]
########## End ###########
06/01 02:06:52 AM | Train: [ 2/80] Step 000/390 Loss 0.071 Prec@(1,5) (99.2%, 100.0%)
06/01 02:06:52 AM | layerwise density: [52854.0, 13287.0, 13275.0, 13327.0, 13249.0, 6583.0, 6594.0, 6585.0, 6648.0, 3356.0, 3353.0, 3358.0, 3345.0, 1748.0, 1752.0, 1701.0, 1672.0]
layerwise density percentage: ['0.806', '0.811', '0.810', '0.813', '0.809', '0.804', '0.805', '0.804', '0.812', '0.819', '0.819', '0.820', '0.817', '0.854', '0.855', '0.831', '0.816']
Global density: 0.8103717565536499
06/01 02:07:03 AM | Train: [ 2/80] Step 100/390 Loss 0.112 Prec@(1,5) (98.2%, 100.0%)
06/01 02:07:03 AM | layerwise density: [49714.0, 12542.0, 12532.0, 12543.0, 12446.0, 6221.0, 6196.0, 6193.0, 6268.0, 3157.0, 3192.0, 3148.0, 3146.0, 1682.0, 1680.0, 1607.0, 1574.0]
layerwise density percentage: ['0.759', '0.766', '0.765', '0.766', '0.760', '0.759', '0.756', '0.756', '0.765', '0.771', '0.779', '0.769', '0.768', '0.821', '0.820', '0.785', '0.769']
Global density: 0.763422429561615
06/01 02:07:14 AM | Train: [ 2/80] Step 200/390 Loss 0.131 Prec@(1,5) (97.8%, 100.0%)
06/01 02:07:14 AM | layerwise density: [46766.0, 11769.0, 11800.0, 11831.0, 11705.0, 5859.0, 5824.0, 5855.0, 5911.0, 2984.0, 3011.0, 2984.0, 2961.0, 1609.0, 1605.0, 1528.0, 1491.0]
layerwise density percentage: ['0.714', '0.718', '0.720', '0.722', '0.714', '0.715', '0.711', '0.715', '0.722', '0.729', '0.735', '0.729', '0.723', '0.786', '0.784', '0.746', '0.728']
Global density: 0.7191162109375
06/01 02:07:24 AM | Train: [ 2/80] Step 300/390 Loss 0.157 Prec@(1,5) (97.1%, 100.0%)
06/01 02:07:24 AM | layerwise density: [43771.0, 11043.0, 11056.0, 11050.0, 10973.0, 5524.0, 5483.0, 5460.0, 5527.0, 2808.0, 2827.0, 2823.0, 2765.0, 1547.0, 1527.0, 1444.0, 1399.0]
layerwise density percentage: ['0.668', '0.674', '0.675', '0.674', '0.670', '0.674', '0.669', '0.667', '0.675', '0.686', '0.690', '0.689', '0.675', '0.755', '0.746', '0.705', '0.683']
Global density: 0.67418372631073
06/01 02:07:34 AM | Train: [ 2/80] Step 390/390 Loss 0.181 Prec@(1,5) (96.4%, 100.0%)
06/01 02:07:34 AM | layerwise density: [41124.0, 10381.0, 10387.0, 10350.0, 10264.0, 5194.0, 5155.0, 5102.0, 5225.0, 2673.0, 2661.0, 2665.0, 2623.0, 1493.0, 1483.0, 1376.0, 1314.0]
layerwise density percentage: ['0.628', '0.634', '0.634', '0.632', '0.626', '0.634', '0.629', '0.623', '0.638', '0.653', '0.650', '0.651', '0.640', '0.729', '0.724', '0.672', '0.642']
Global density: 0.6340757012367249
06/01 02:07:34 AM | Train: [ 2/200] Final Prec@1 96.4000%
06/01 02:07:35 AM | Valid: [ 2/200] Step 000/078 Loss 1.109 Prec@(1,5) (76.6%, 90.6%)
06/01 02:07:37 AM | Valid: [ 2/200] Step 078/078 Loss 1.383 Prec@(1,5) (67.3%, 88.2%)
06/01 02:07:37 AM | Valid: [ 2/200] Final Prec@1 67.3000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 41123.0, 0.6274871826171875]
['model.relu.alpha_mask_1_0', 16384, 10380.0, 0.633544921875]
['model.relu.alpha_mask_2_0', 16384, 10384.0, 0.6337890625]
['model.relu.alpha_mask_3_0', 16384, 10349.0, 0.63165283203125]
['model.relu.alpha_mask_4_0', 16384, 10264.0, 0.62646484375]
['model.relu.alpha_mask_5_0', 8192, 5194.0, 0.634033203125]
['model.relu.alpha_mask_6_0', 8192, 5155.0, 0.6292724609375]
['model.relu.alpha_mask_7_0', 8192, 5102.0, 0.622802734375]
['model.relu.alpha_mask_8_0', 8192, 5222.0, 0.637451171875]
['model.relu.alpha_mask_9_0', 4096, 2673.0, 0.652587890625]
['model.relu.alpha_mask_10_0', 4096, 2661.0, 0.649658203125]
['model.relu.alpha_mask_11_0', 4096, 2665.0, 0.650634765625]
['model.relu.alpha_mask_12_0', 4096, 2623.0, 0.640380859375]
['model.relu.alpha_mask_13_0', 2048, 1493.0, 0.72900390625]
['model.relu.alpha_mask_14_0', 2048, 1483.0, 0.72412109375]
['model.relu.alpha_mask_15_0', 2048, 1376.0, 0.671875]
['model.relu.alpha_mask_16_0', 2048, 1313.0, 0.64111328125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119460.0, 0.6340225883152174]
########## End ###########
06/01 02:07:38 AM | Train: [ 3/80] Step 000/390 Loss 0.269 Prec@(1,5) (94.5%, 100.0%)
06/01 02:07:38 AM | layerwise density: [41123.0, 10380.0, 10384.0, 10349.0, 10264.0, 5194.0, 5155.0, 5102.0, 5222.0, 2673.0, 2661.0, 2665.0, 2623.0, 1493.0, 1483.0, 1376.0, 1313.0]
layerwise density percentage: ['0.627', '0.634', '0.634', '0.632', '0.626', '0.634', '0.629', '0.623', '0.637', '0.653', '0.650', '0.651', '0.640', '0.729', '0.724', '0.672', '0.641']
Global density: 0.63402259349823
06/01 02:07:48 AM | Train: [ 3/80] Step 100/390 Loss 0.218 Prec@(1,5) (95.5%, 99.9%)
06/01 02:07:48 AM | layerwise density: [38195.0, 9626.0, 9640.0, 9641.0, 9552.0, 4831.0, 4787.0, 4735.0, 4871.0, 2494.0, 2476.0, 2490.0, 2447.0, 1431.0, 1420.0, 1312.0, 1244.0]
layerwise density percentage: ['0.583', '0.588', '0.588', '0.588', '0.583', '0.590', '0.584', '0.578', '0.595', '0.609', '0.604', '0.608', '0.597', '0.699', '0.693', '0.641', '0.607']
Global density: 0.5901409983634949
06/01 02:07:59 AM | Train: [ 3/80] Step 200/390 Loss 0.252 Prec@(1,5) (94.4%, 99.8%)
06/01 02:07:59 AM | layerwise density: [35414.0, 8923.0, 8960.0, 8933.0, 8801.0, 4487.0, 4453.0, 4366.0, 4502.0, 2319.0, 2307.0, 2327.0, 2316.0, 1377.0, 1356.0, 1235.0, 1162.0]
layerwise density percentage: ['0.540', '0.545', '0.547', '0.545', '0.537', '0.548', '0.544', '0.533', '0.550', '0.566', '0.563', '0.568', '0.565', '0.672', '0.662', '0.603', '0.567']
Global density: 0.5479258894920349
06/01 02:08:10 AM | Train: [ 3/80] Step 300/390 Loss 0.285 Prec@(1,5) (93.3%, 99.7%)
06/01 02:08:10 AM | layerwise density: [32656.0, 8189.0, 8254.0, 8230.0, 8178.0, 4189.0, 4110.0, 4054.0, 4171.0, 2156.0, 2151.0, 2147.0, 2154.0, 1316.0, 1307.0, 1162.0, 1080.0]
layerwise density percentage: ['0.498', '0.500', '0.504', '0.502', '0.499', '0.511', '0.502', '0.495', '0.509', '0.526', '0.525', '0.524', '0.526', '0.643', '0.638', '0.567', '0.527']
Global density: 0.5068784356117249
06/01 02:08:20 AM | Train: [ 3/80] Step 390/390 Loss 0.317 Prec@(1,5) (92.2%, 99.7%)
06/01 02:08:20 AM | layerwise density: [30126.0, 7568.0, 7654.0, 7636.0, 7547.0, 3875.0, 3843.0, 3750.0, 3821.0, 2011.0, 1998.0, 2008.0, 2007.0, 1279.0, 1257.0, 1091.0, 992.0]
layerwise density percentage: ['0.460', '0.462', '0.467', '0.466', '0.461', '0.473', '0.469', '0.458', '0.466', '0.491', '0.488', '0.490', '0.490', '0.625', '0.614', '0.533', '0.484']
Global density: 0.46950897574424744
06/01 02:08:20 AM | Train: [ 3/200] Final Prec@1 92.2300%
06/01 02:08:21 AM | Valid: [ 3/200] Step 000/078 Loss 1.267 Prec@(1,5) (71.9%, 90.6%)
06/01 02:08:23 AM | Valid: [ 3/200] Step 078/078 Loss 1.489 Prec@(1,5) (64.2%, 87.0%)
06/01 02:08:23 AM | Valid: [ 3/200] Final Prec@1 64.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 30072.0, 0.4588623046875]
['model.relu.alpha_mask_1_0', 16384, 7556.0, 0.461181640625]
['model.relu.alpha_mask_2_0', 16384, 7643.0, 0.46649169921875]
['model.relu.alpha_mask_3_0', 16384, 7630.0, 0.4656982421875]
['model.relu.alpha_mask_4_0', 16384, 7529.0, 0.45953369140625]
['model.relu.alpha_mask_5_0', 8192, 3869.0, 0.4722900390625]
['model.relu.alpha_mask_6_0', 8192, 3832.0, 0.4677734375]
['model.relu.alpha_mask_7_0', 8192, 3748.0, 0.45751953125]
['model.relu.alpha_mask_8_0', 8192, 3818.0, 0.466064453125]
['model.relu.alpha_mask_9_0', 4096, 2006.0, 0.48974609375]
['model.relu.alpha_mask_10_0', 4096, 1997.0, 0.487548828125]
['model.relu.alpha_mask_11_0', 4096, 2005.0, 0.489501953125]
['model.relu.alpha_mask_12_0', 4096, 2007.0, 0.489990234375]
['model.relu.alpha_mask_13_0', 2048, 1278.0, 0.6240234375]
['model.relu.alpha_mask_14_0', 2048, 1256.0, 0.61328125]
['model.relu.alpha_mask_15_0', 2048, 1087.0, 0.53076171875]
['model.relu.alpha_mask_16_0', 2048, 990.0, 0.4833984375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 88323.0, 0.4687659222146739]
########## End ###########
06/01 02:08:24 AM | Train: [ 4/80] Step 000/390 Loss 0.391 Prec@(1,5) (89.8%, 99.2%)
06/01 02:08:24 AM | layerwise density: [30072.0, 7556.0, 7643.0, 7630.0, 7529.0, 3869.0, 3832.0, 3748.0, 3818.0, 2006.0, 1997.0, 2005.0, 2007.0, 1278.0, 1256.0, 1087.0, 990.0]
layerwise density percentage: ['0.459', '0.461', '0.466', '0.466', '0.460', '0.472', '0.468', '0.458', '0.466', '0.490', '0.488', '0.490', '0.490', '0.624', '0.613', '0.531', '0.483']
Global density: 0.4687659442424774
06/01 02:08:35 AM | Train: [ 4/80] Step 100/390 Loss 0.359 Prec@(1,5) (91.0%, 99.7%)
06/01 02:08:35 AM | layerwise density: [27430.0, 6854.0, 6958.0, 6946.0, 6855.0, 3541.0, 3507.0, 3450.0, 3476.0, 1832.0, 1844.0, 1831.0, 1865.0, 1211.0, 1209.0, 1029.0, 915.0]
layerwise density percentage: ['0.419', '0.418', '0.425', '0.424', '0.418', '0.432', '0.428', '0.421', '0.424', '0.447', '0.450', '0.447', '0.455', '0.591', '0.590', '0.502', '0.447']
Global density: 0.4285888671875
06/01 02:08:45 AM | Train: [ 4/80] Step 200/390 Loss 0.400 Prec@(1,5) (89.7%, 99.5%)
06/01 02:08:45 AM | layerwise density: [24613.0, 6127.0, 6214.0, 6248.0, 6194.0, 3200.0, 3197.0, 3137.0, 3173.0, 1678.0, 1696.0, 1677.0, 1692.0, 1162.0, 1170.0, 970.0, 836.0]
layerwise density percentage: ['0.376', '0.374', '0.379', '0.381', '0.378', '0.391', '0.390', '0.383', '0.387', '0.410', '0.414', '0.409', '0.413', '0.567', '0.571', '0.474', '0.408']
Global density: 0.38735565543174744
06/01 02:08:56 AM | Train: [ 4/80] Step 300/390 Loss 0.445 Prec@(1,5) (88.3%, 99.3%)
06/01 02:08:56 AM | layerwise density: [22009.0, 5490.0, 5572.0, 5566.0, 5572.0, 2891.0, 2897.0, 2851.0, 2840.0, 1515.0, 1546.0, 1520.0, 1527.0, 1124.0, 1127.0, 904.0, 758.0]
layerwise density percentage: ['0.336', '0.335', '0.340', '0.340', '0.340', '0.353', '0.354', '0.348', '0.347', '0.370', '0.377', '0.371', '0.373', '0.549', '0.550', '0.441', '0.370']
Global density: 0.34874427318573
06/01 02:09:06 AM | Train: [ 4/80] Step 390/390 Loss 0.480 Prec@(1,5) (87.2%, 99.1%)
06/01 02:09:06 AM | layerwise density: [19647.0, 4871.0, 4971.0, 4917.0, 4974.0, 2578.0, 2619.0, 2544.0, 2568.0, 1354.0, 1435.0, 1394.0, 1390.0, 1086.0, 1099.0, 860.0, 667.0]
layerwise density percentage: ['0.300', '0.297', '0.303', '0.300', '0.304', '0.315', '0.320', '0.311', '0.313', '0.331', '0.350', '0.340', '0.339', '0.530', '0.537', '0.420', '0.326']
Global density: 0.31299889087677
06/01 02:09:06 AM | Train: [ 4/200] Final Prec@1 87.2140%
06/01 02:09:06 AM | Valid: [ 4/200] Step 000/078 Loss 1.286 Prec@(1,5) (68.0%, 90.6%)
06/01 02:09:09 AM | Valid: [ 4/200] Step 078/078 Loss 1.577 Prec@(1,5) (62.5%, 86.1%)
06/01 02:09:09 AM | Valid: [ 4/200] Final Prec@1 62.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 19590.0, 0.298919677734375]
['model.relu.alpha_mask_1_0', 16384, 4858.0, 0.2965087890625]
['model.relu.alpha_mask_2_0', 16384, 4955.0, 0.30242919921875]
['model.relu.alpha_mask_3_0', 16384, 4905.0, 0.29937744140625]
['model.relu.alpha_mask_4_0', 16384, 4962.0, 0.3028564453125]
['model.relu.alpha_mask_5_0', 8192, 2565.0, 0.3131103515625]
['model.relu.alpha_mask_6_0', 8192, 2615.0, 0.3192138671875]
['model.relu.alpha_mask_7_0', 8192, 2538.0, 0.309814453125]
['model.relu.alpha_mask_8_0', 8192, 2562.0, 0.312744140625]
['model.relu.alpha_mask_9_0', 4096, 1350.0, 0.32958984375]
['model.relu.alpha_mask_10_0', 4096, 1432.0, 0.349609375]
['model.relu.alpha_mask_11_0', 4096, 1392.0, 0.33984375]
['model.relu.alpha_mask_12_0', 4096, 1384.0, 0.337890625]
['model.relu.alpha_mask_13_0', 2048, 1086.0, 0.5302734375]
['model.relu.alpha_mask_14_0', 2048, 1100.0, 0.537109375]
['model.relu.alpha_mask_15_0', 2048, 858.0, 0.4189453125]
['model.relu.alpha_mask_16_0', 2048, 663.0, 0.32373046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 58815.0, 0.31215501868206524]
########## End ###########
06/01 02:09:10 AM | Train: [ 5/80] Step 000/390 Loss 0.442 Prec@(1,5) (85.9%, 100.0%)
06/01 02:09:10 AM | layerwise density: [19590.0, 4858.0, 4955.0, 4905.0, 4962.0, 2565.0, 2615.0, 2538.0, 2562.0, 1350.0, 1432.0, 1392.0, 1384.0, 1086.0, 1100.0, 858.0, 663.0]
layerwise density percentage: ['0.299', '0.297', '0.302', '0.299', '0.303', '0.313', '0.319', '0.310', '0.313', '0.330', '0.350', '0.340', '0.338', '0.530', '0.537', '0.419', '0.324']
Global density: 0.3121550381183624
06/01 02:09:21 AM | Train: [ 5/80] Step 100/390 Loss 0.535 Prec@(1,5) (85.7%, 98.8%)
06/01 02:09:21 AM | layerwise density: [17005.0, 4205.0, 4304.0, 4215.0, 4309.0, 2269.0, 2290.0, 2237.0, 2216.0, 1214.0, 1302.0, 1240.0, 1252.0, 1033.0, 1064.0, 807.0, 570.0]
layerwise density percentage: ['0.259', '0.257', '0.263', '0.257', '0.263', '0.277', '0.280', '0.273', '0.271', '0.296', '0.318', '0.303', '0.306', '0.504', '0.520', '0.394', '0.278']
Global density: 0.2735011875629425
06/01 02:09:32 AM | Train: [ 5/80] Step 200/390 Loss 0.589 Prec@(1,5) (84.1%, 98.4%)
06/01 02:09:32 AM | layerwise density: [14218.0, 3523.0, 3633.0, 3567.0, 3571.0, 1928.0, 1956.0, 1895.0, 1896.0, 1084.0, 1137.0, 1108.0, 1103.0, 987.0, 1024.0, 742.0, 475.0]
layerwise density percentage: ['0.217', '0.215', '0.222', '0.218', '0.218', '0.235', '0.239', '0.231', '0.231', '0.265', '0.278', '0.271', '0.269', '0.482', '0.500', '0.362', '0.232']
Global density: 0.23271378874778748
06/01 02:09:42 AM | Train: [ 5/80] Step 300/390 Loss 0.640 Prec@(1,5) (82.5%, 98.0%)
06/01 02:09:42 AM | layerwise density: [11608.0, 2861.0, 2997.0, 2937.0, 2907.0, 1651.0, 1645.0, 1596.0, 1580.0, 938.0, 977.0, 966.0, 972.0, 950.0, 991.0, 692.0, 400.0]
layerwise density percentage: ['0.177', '0.175', '0.183', '0.179', '0.177', '0.202', '0.201', '0.195', '0.193', '0.229', '0.239', '0.236', '0.237', '0.464', '0.484', '0.338', '0.195']
Global density: 0.19461192190647125
06/01 02:09:52 AM | Train: [ 5/80] Step 390/390 Loss 0.694 Prec@(1,5) (81.1%, 97.6%)
06/01 02:09:52 AM | layerwise density: [9291.0, 2243.0, 2401.0, 2366.0, 2320.0, 1391.0, 1380.0, 1295.0, 1309.0, 800.0, 867.0, 838.0, 842.0, 902.0, 963.0, 641.0, 327.0]
layerwise density percentage: ['0.142', '0.137', '0.147', '0.144', '0.142', '0.170', '0.168', '0.158', '0.160', '0.195', '0.212', '0.205', '0.206', '0.440', '0.470', '0.313', '0.160']
Global density: 0.16015625
06/01 02:09:52 AM | Train: [ 5/200] Final Prec@1 81.0720%
06/01 02:09:52 AM | Valid: [ 5/200] Step 000/078 Loss 1.625 Prec@(1,5) (65.6%, 82.8%)
06/01 02:09:55 AM | Valid: [ 5/200] Step 078/078 Loss 1.790 Prec@(1,5) (56.8%, 83.2%)
06/01 02:09:55 AM | Valid: [ 5/200] Final Prec@1 56.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9238.0, 0.140960693359375]
['model.relu.alpha_mask_1_0', 16384, 2230.0, 0.1361083984375]
['model.relu.alpha_mask_2_0', 16384, 2389.0, 0.14581298828125]
['model.relu.alpha_mask_3_0', 16384, 2349.0, 0.14337158203125]
['model.relu.alpha_mask_4_0', 16384, 2312.0, 0.14111328125]
['model.relu.alpha_mask_5_0', 8192, 1384.0, 0.1689453125]
['model.relu.alpha_mask_6_0', 8192, 1375.0, 0.1678466796875]
['model.relu.alpha_mask_7_0', 8192, 1294.0, 0.157958984375]
['model.relu.alpha_mask_8_0', 8192, 1301.0, 0.1588134765625]
['model.relu.alpha_mask_9_0', 4096, 797.0, 0.194580078125]
['model.relu.alpha_mask_10_0', 4096, 866.0, 0.21142578125]
['model.relu.alpha_mask_11_0', 4096, 835.0, 0.203857421875]
['model.relu.alpha_mask_12_0', 4096, 841.0, 0.205322265625]
['model.relu.alpha_mask_13_0', 2048, 899.0, 0.43896484375]
['model.relu.alpha_mask_14_0', 2048, 963.0, 0.47021484375]
['model.relu.alpha_mask_15_0', 2048, 641.0, 0.31298828125]
['model.relu.alpha_mask_16_0', 2048, 322.0, 0.1572265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 30036.0, 0.15941321331521738]
########## End ###########
06/01 02:09:56 AM | Train: [ 6/80] Step 000/390 Loss 0.701 Prec@(1,5) (83.6%, 98.4%)
06/01 02:09:56 AM | layerwise density: [9238.0, 2230.0, 2389.0, 2349.0, 2312.0, 1384.0, 1375.0, 1294.0, 1301.0, 797.0, 866.0, 835.0, 841.0, 899.0, 963.0, 641.0, 322.0]
layerwise density percentage: ['0.141', '0.136', '0.146', '0.143', '0.141', '0.169', '0.168', '0.158', '0.159', '0.195', '0.211', '0.204', '0.205', '0.439', '0.470', '0.313', '0.157']
Global density: 0.15941321849822998
06/01 02:10:06 AM | Train: [ 6/80] Step 100/390 Loss 0.794 Prec@(1,5) (78.1%, 96.8%)
06/01 02:10:06 AM | layerwise density: [6680.0, 1604.0, 1739.0, 1684.0, 1652.0, 1077.0, 1080.0, 966.0, 1024.0, 669.0, 722.0, 688.0, 685.0, 847.0, 929.0, 588.0, 240.0]
layerwise density percentage: ['0.102', '0.098', '0.106', '0.103', '0.101', '0.131', '0.132', '0.118', '0.125', '0.163', '0.176', '0.168', '0.167', '0.414', '0.454', '0.287', '0.117']
Global density: 0.1214015781879425
06/01 02:10:17 AM | Train: [ 6/80] Step 200/390 Loss 0.870 Prec@(1,5) (76.2%, 96.0%)
06/01 02:10:17 AM | layerwise density: [4100.0, 981.0, 1094.0, 1052.0, 1008.0, 780.0, 811.0, 686.0, 724.0, 523.0, 627.0, 542.0, 537.0, 815.0, 890.0, 552.0, 166.0]
layerwise density percentage: ['0.063', '0.060', '0.067', '0.064', '0.062', '0.095', '0.099', '0.084', '0.088', '0.128', '0.153', '0.132', '0.131', '0.398', '0.435', '0.270', '0.081']
Global density: 0.08432404696941376
06/01 02:10:28 AM | Train: [ 6/80] Step 300/390 Loss 0.923 Prec@(1,5) (74.7%, 95.4%)
06/01 02:10:28 AM | layerwise density: [2900.0, 669.0, 788.0, 773.0, 732.0, 647.0, 673.0, 534.0, 590.0, 448.0, 561.0, 467.0, 473.0, 839.0, 924.0, 551.0, 114.0]
layerwise density percentage: ['0.044', '0.041', '0.048', '0.047', '0.045', '0.079', '0.082', '0.065', '0.072', '0.109', '0.137', '0.114', '0.115', '0.410', '0.451', '0.269', '0.056']
Global density: 0.06731382012367249
06/01 02:10:39 AM | Train: [ 6/80] Step 390/390 Loss 0.930 Prec@(1,5) (74.5%, 95.3%)
06/01 02:10:39 AM | layerwise density: [2859.0, 672.0, 782.0, 769.0, 728.0, 663.0, 679.0, 541.0, 600.0, 465.0, 578.0, 489.0, 487.0, 890.0, 982.0, 600.0, 115.0]
layerwise density percentage: ['0.044', '0.041', '0.048', '0.047', '0.044', '0.081', '0.083', '0.066', '0.073', '0.114', '0.141', '0.119', '0.119', '0.435', '0.479', '0.293', '0.056']
Global density: 0.06846021860837936
06/01 02:10:39 AM | Train: [ 6/200] Final Prec@1 74.5020%
06/01 02:10:39 AM | Valid: [ 6/200] Step 000/078 Loss 1.375 Prec@(1,5) (70.3%, 88.3%)
06/01 02:10:41 AM | Valid: [ 6/200] Step 078/078 Loss 1.572 Prec@(1,5) (62.3%, 86.2%)
06/01 02:10:41 AM | Valid: [ 6/200] Final Prec@1 62.2500%
06/01 02:10:42 AM | Current mask training best Prec@1 = 62.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 2859.0, 0.0436248779296875]
['model.relu.alpha_mask_1_0', 16384, 672.0, 0.041015625]
['model.relu.alpha_mask_2_0', 16384, 782.0, 0.0477294921875]
['model.relu.alpha_mask_3_0', 16384, 769.0, 0.04693603515625]
['model.relu.alpha_mask_4_0', 16384, 728.0, 0.04443359375]
['model.relu.alpha_mask_5_0', 8192, 663.0, 0.0809326171875]
['model.relu.alpha_mask_6_0', 8192, 679.0, 0.0828857421875]
['model.relu.alpha_mask_7_0', 8192, 541.0, 0.0660400390625]
['model.relu.alpha_mask_8_0', 8192, 600.0, 0.0732421875]
['model.relu.alpha_mask_9_0', 4096, 465.0, 0.113525390625]
['model.relu.alpha_mask_10_0', 4096, 578.0, 0.14111328125]
['model.relu.alpha_mask_11_0', 4096, 489.0, 0.119384765625]
['model.relu.alpha_mask_12_0', 4096, 487.0, 0.118896484375]
['model.relu.alpha_mask_13_0', 2048, 890.0, 0.4345703125]
['model.relu.alpha_mask_14_0', 2048, 982.0, 0.4794921875]
['model.relu.alpha_mask_15_0', 2048, 600.0, 0.29296875]
['model.relu.alpha_mask_16_0', 2048, 115.0, 0.05615234375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12899.0, 0.06846021569293478]
########## End ###########
06/01 02:10:43 AM | Train: [ 7/80] Step 000/390 Loss 0.660 Prec@(1,5) (84.4%, 96.9%)
06/01 02:10:43 AM | layerwise density: [2859.0, 672.0, 782.0, 769.0, 728.0, 663.0, 679.0, 541.0, 600.0, 465.0, 578.0, 489.0, 487.0, 890.0, 982.0, 600.0, 115.0]
layerwise density percentage: ['0.044', '0.041', '0.048', '0.047', '0.044', '0.081', '0.083', '0.066', '0.073', '0.114', '0.141', '0.119', '0.119', '0.435', '0.479', '0.293', '0.056']
Global density: 0.06846021860837936
06/01 02:10:53 AM | Train: [ 7/80] Step 100/390 Loss 0.702 Prec@(1,5) (80.2%, 97.6%)
06/01 02:10:53 AM | layerwise density: [2726.0, 660.0, 764.0, 757.0, 703.0, 681.0, 688.0, 544.0, 607.0, 475.0, 596.0, 506.0, 501.0, 913.0, 1016.0, 634.0, 117.0]
layerwise density percentage: ['0.042', '0.040', '0.047', '0.046', '0.043', '0.083', '0.084', '0.066', '0.074', '0.116', '0.146', '0.124', '0.122', '0.446', '0.496', '0.310', '0.057']
Global density: 0.06840183585882187
06/01 02:11:04 AM | Train: [ 7/80] Step 200/390 Loss 0.690 Prec@(1,5) (80.6%, 97.7%)
06/01 02:11:04 AM | layerwise density: [2557.0, 646.0, 743.0, 731.0, 678.0, 690.0, 690.0, 548.0, 609.0, 486.0, 616.0, 520.0, 520.0, 948.0, 1044.0, 664.0, 118.0]
layerwise density percentage: ['0.039', '0.039', '0.045', '0.045', '0.041', '0.084', '0.084', '0.067', '0.074', '0.119', '0.150', '0.127', '0.127', '0.463', '0.510', '0.324', '0.058']
Global density: 0.06797724217176437
06/01 02:11:15 AM | Train: [ 7/80] Step 300/390 Loss 0.682 Prec@(1,5) (80.9%, 97.7%)
06/01 02:11:15 AM | layerwise density: [2445.0, 645.0, 729.0, 720.0, 673.0, 711.0, 701.0, 556.0, 615.0, 504.0, 638.0, 542.0, 545.0, 965.0, 1080.0, 692.0, 120.0]
layerwise density percentage: ['0.037', '0.039', '0.044', '0.044', '0.041', '0.087', '0.086', '0.068', '0.075', '0.123', '0.156', '0.132', '0.133', '0.471', '0.527', '0.338', '0.059']
Global density: 0.0683646872639656
06/01 02:11:25 AM | Train: [ 7/80] Step 390/390 Loss 0.666 Prec@(1,5) (81.4%, 97.8%)
06/01 02:11:25 AM | layerwise density: [2293.0, 631.0, 709.0, 706.0, 659.0, 707.0, 709.0, 558.0, 612.0, 516.0, 647.0, 549.0, 560.0, 989.0, 1101.0, 712.0, 122.0]
layerwise density percentage: ['0.035', '0.039', '0.043', '0.043', '0.040', '0.086', '0.087', '0.068', '0.075', '0.126', '0.158', '0.134', '0.137', '0.483', '0.538', '0.348', '0.060']
Global density: 0.06782863289117813
06/01 02:11:25 AM | Train: [ 7/200] Final Prec@1 81.3580%
06/01 02:11:25 AM | Valid: [ 7/200] Step 000/078 Loss 1.406 Prec@(1,5) (66.4%, 89.1%)
06/01 02:11:28 AM | Valid: [ 7/200] Step 078/078 Loss 1.552 Prec@(1,5) (63.2%, 86.4%)
06/01 02:11:28 AM | Valid: [ 7/200] Final Prec@1 63.1600%
06/01 02:11:28 AM | Current mask training best Prec@1 = 63.1600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 2293.0, 0.0349884033203125]
['model.relu.alpha_mask_1_0', 16384, 631.0, 0.03851318359375]
['model.relu.alpha_mask_2_0', 16384, 709.0, 0.04327392578125]
['model.relu.alpha_mask_3_0', 16384, 706.0, 0.0430908203125]
['model.relu.alpha_mask_4_0', 16384, 659.0, 0.04022216796875]
['model.relu.alpha_mask_5_0', 8192, 706.0, 0.086181640625]
['model.relu.alpha_mask_6_0', 8192, 709.0, 0.0865478515625]
['model.relu.alpha_mask_7_0', 8192, 558.0, 0.068115234375]
['model.relu.alpha_mask_8_0', 8192, 612.0, 0.07470703125]
['model.relu.alpha_mask_9_0', 4096, 516.0, 0.1259765625]
['model.relu.alpha_mask_10_0', 4096, 647.0, 0.157958984375]
['model.relu.alpha_mask_11_0', 4096, 549.0, 0.134033203125]
['model.relu.alpha_mask_12_0', 4096, 560.0, 0.13671875]
['model.relu.alpha_mask_13_0', 2048, 989.0, 0.48291015625]
['model.relu.alpha_mask_14_0', 2048, 1101.0, 0.53759765625]
['model.relu.alpha_mask_15_0', 2048, 712.0, 0.34765625]
['model.relu.alpha_mask_16_0', 2048, 122.0, 0.0595703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12779.0, 0.06782332710597826]
########## End ###########
06/01 02:11:29 AM | Train: [ 8/80] Step 000/390 Loss 0.473 Prec@(1,5) (85.9%, 98.4%)
06/01 02:11:29 AM | layerwise density: [2293.0, 631.0, 709.0, 706.0, 659.0, 706.0, 709.0, 558.0, 612.0, 516.0, 647.0, 549.0, 560.0, 989.0, 1101.0, 712.0, 122.0]
layerwise density percentage: ['0.035', '0.039', '0.043', '0.043', '0.040', '0.086', '0.087', '0.068', '0.075', '0.126', '0.158', '0.134', '0.137', '0.483', '0.538', '0.348', '0.060']
Global density: 0.06782332807779312
06/01 02:11:40 AM | Train: [ 8/80] Step 100/390 Loss 0.438 Prec@(1,5) (88.1%, 99.1%)
06/01 02:11:40 AM | layerwise density: [2232.0, 631.0, 692.0, 705.0, 654.0, 719.0, 718.0, 576.0, 636.0, 533.0, 662.0, 559.0, 572.0, 1009.0, 1127.0, 729.0, 131.0]
layerwise density percentage: ['0.034', '0.039', '0.042', '0.043', '0.040', '0.088', '0.088', '0.070', '0.078', '0.130', '0.162', '0.136', '0.140', '0.493', '0.550', '0.356', '0.064']
Global density: 0.06838591396808624
06/01 02:11:51 AM | Train: [ 8/80] Step 200/390 Loss 0.440 Prec@(1,5) (88.0%, 99.2%)
06/01 02:11:51 AM | layerwise density: [2145.0, 625.0, 678.0, 689.0, 634.0, 733.0, 717.0, 579.0, 647.0, 545.0, 680.0, 569.0, 593.0, 1031.0, 1146.0, 742.0, 138.0]
layerwise density percentage: ['0.033', '0.038', '0.041', '0.042', '0.039', '0.089', '0.088', '0.071', '0.079', '0.133', '0.166', '0.139', '0.145', '0.503', '0.560', '0.362', '0.067']
Global density: 0.0684177577495575
06/01 02:12:02 AM | Train: [ 8/80] Step 300/390 Loss 0.440 Prec@(1,5) (88.0%, 99.2%)
06/01 02:12:02 AM | layerwise density: [2047.0, 622.0, 661.0, 679.0, 611.0, 758.0, 722.0, 578.0, 653.0, 565.0, 694.0, 584.0, 604.0, 1042.0, 1154.0, 755.0, 147.0]
layerwise density percentage: ['0.031', '0.038', '0.040', '0.041', '0.037', '0.093', '0.088', '0.071', '0.080', '0.138', '0.169', '0.143', '0.147', '0.509', '0.563', '0.369', '0.072']
Global density: 0.06833814829587936
06/01 02:12:12 AM | Train: [ 8/80] Step 390/390 Loss 0.433 Prec@(1,5) (88.3%, 99.2%)
06/01 02:12:12 AM | layerwise density: [1981.0, 619.0, 650.0, 681.0, 600.0, 757.0, 732.0, 582.0, 648.0, 572.0, 706.0, 599.0, 614.0, 1060.0, 1167.0, 765.0, 151.0]
layerwise density percentage: ['0.030', '0.038', '0.040', '0.042', '0.037', '0.092', '0.089', '0.071', '0.079', '0.140', '0.172', '0.146', '0.150', '0.518', '0.570', '0.374', '0.074']
Global density: 0.06838060915470123
06/01 02:12:12 AM | Train: [ 8/200] Final Prec@1 88.2740%
06/01 02:12:12 AM | Valid: [ 8/200] Step 000/078 Loss 1.132 Prec@(1,5) (73.4%, 91.4%)
06/01 02:12:15 AM | Valid: [ 8/200] Step 078/078 Loss 1.566 Prec@(1,5) (63.6%, 85.9%)
06/01 02:12:15 AM | Valid: [ 8/200] Final Prec@1 63.6100%
06/01 02:12:15 AM | Current mask training best Prec@1 = 63.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 1981.0, 0.0302276611328125]
['model.relu.alpha_mask_1_0', 16384, 619.0, 0.03778076171875]
['model.relu.alpha_mask_2_0', 16384, 649.0, 0.03961181640625]
['model.relu.alpha_mask_3_0', 16384, 681.0, 0.04156494140625]
['model.relu.alpha_mask_4_0', 16384, 600.0, 0.03662109375]
['model.relu.alpha_mask_5_0', 8192, 756.0, 0.09228515625]
['model.relu.alpha_mask_6_0', 8192, 733.0, 0.0894775390625]
['model.relu.alpha_mask_7_0', 8192, 582.0, 0.071044921875]
['model.relu.alpha_mask_8_0', 8192, 647.0, 0.0789794921875]
['model.relu.alpha_mask_9_0', 4096, 572.0, 0.1396484375]
['model.relu.alpha_mask_10_0', 4096, 707.0, 0.172607421875]
['model.relu.alpha_mask_11_0', 4096, 599.0, 0.146240234375]
['model.relu.alpha_mask_12_0', 4096, 614.0, 0.14990234375]
['model.relu.alpha_mask_13_0', 2048, 1060.0, 0.517578125]
['model.relu.alpha_mask_14_0', 2048, 1167.0, 0.56982421875]
['model.relu.alpha_mask_15_0', 2048, 765.0, 0.37353515625]
['model.relu.alpha_mask_16_0', 2048, 151.0, 0.07373046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12883.0, 0.06837529721467392]
########## End ###########
06/01 02:12:16 AM | Train: [ 9/80] Step 000/390 Loss 0.216 Prec@(1,5) (96.1%, 100.0%)
06/01 02:12:16 AM | layerwise density: [1981.0, 619.0, 649.0, 681.0, 600.0, 756.0, 733.0, 582.0, 647.0, 572.0, 707.0, 599.0, 614.0, 1060.0, 1167.0, 765.0, 151.0]
layerwise density percentage: ['0.030', '0.038', '0.040', '0.042', '0.037', '0.092', '0.089', '0.071', '0.079', '0.140', '0.173', '0.146', '0.150', '0.518', '0.570', '0.374', '0.074']
Global density: 0.06837529689073563
06/01 02:12:27 AM | Train: [ 9/80] Step 100/390 Loss 0.305 Prec@(1,5) (92.3%, 99.6%)
06/01 02:12:27 AM | layerwise density: [1889.0, 603.0, 629.0, 671.0, 586.0, 779.0, 744.0, 583.0, 653.0, 583.0, 720.0, 612.0, 620.0, 1073.0, 1180.0, 778.0, 161.0]
layerwise density percentage: ['0.029', '0.037', '0.038', '0.041', '0.036', '0.095', '0.091', '0.071', '0.080', '0.142', '0.176', '0.149', '0.151', '0.524', '0.576', '0.380', '0.079']
Global density: 0.06827446073293686
06/01 02:12:38 AM | Train: [ 9/80] Step 200/390 Loss 0.307 Prec@(1,5) (92.1%, 99.6%)
06/01 02:12:38 AM | layerwise density: [1800.0, 606.0, 614.0, 659.0, 569.0, 777.0, 742.0, 588.0, 655.0, 597.0, 718.0, 627.0, 627.0, 1086.0, 1197.0, 791.0, 166.0]
layerwise density percentage: ['0.027', '0.037', '0.037', '0.040', '0.035', '0.095', '0.091', '0.072', '0.080', '0.146', '0.175', '0.153', '0.153', '0.530', '0.584', '0.386', '0.081']
Global density: 0.06803562492132187
06/01 02:12:49 AM | Train: [ 9/80] Step 300/390 Loss 0.311 Prec@(1,5) (91.9%, 99.6%)
06/01 02:12:49 AM | layerwise density: [1739.0, 608.0, 606.0, 659.0, 572.0, 789.0, 749.0, 601.0, 667.0, 604.0, 738.0, 641.0, 633.0, 1094.0, 1211.0, 805.0, 174.0]
layerwise density percentage: ['0.027', '0.037', '0.037', '0.040', '0.035', '0.096', '0.091', '0.073', '0.081', '0.147', '0.180', '0.156', '0.155', '0.534', '0.591', '0.393', '0.085']
Global density: 0.06841245293617249
06/01 02:12:59 AM | Train: [ 9/80] Step 390/390 Loss 0.309 Prec@(1,5) (91.9%, 99.6%)
06/01 02:12:59 AM | layerwise density: [1644.0, 596.0, 597.0, 656.0, 549.0, 793.0, 745.0, 604.0, 668.0, 611.0, 746.0, 647.0, 641.0, 1103.0, 1224.0, 819.0, 175.0]
layerwise density percentage: ['0.025', '0.036', '0.036', '0.040', '0.034', '0.097', '0.091', '0.074', '0.082', '0.149', '0.182', '0.158', '0.156', '0.539', '0.598', '0.400', '0.085']
Global density: 0.06803032010793686
06/01 02:12:59 AM | Train: [ 9/200] Final Prec@1 91.8880%
06/01 02:12:59 AM | Valid: [ 9/200] Step 000/078 Loss 1.155 Prec@(1,5) (75.0%, 89.8%)
06/01 02:13:01 AM | Valid: [ 9/200] Step 078/078 Loss 1.544 Prec@(1,5) (64.0%, 86.6%)
06/01 02:13:01 AM | Valid: [ 9/200] Final Prec@1 63.9700%
06/01 02:13:02 AM | Current mask training best Prec@1 = 63.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 1644.0, 0.02508544921875]
['model.relu.alpha_mask_1_0', 16384, 596.0, 0.036376953125]
['model.relu.alpha_mask_2_0', 16384, 597.0, 0.03643798828125]
['model.relu.alpha_mask_3_0', 16384, 657.0, 0.04010009765625]
['model.relu.alpha_mask_4_0', 16384, 550.0, 0.0335693359375]
['model.relu.alpha_mask_5_0', 8192, 789.0, 0.0963134765625]
['model.relu.alpha_mask_6_0', 8192, 745.0, 0.0909423828125]
['model.relu.alpha_mask_7_0', 8192, 603.0, 0.0736083984375]
['model.relu.alpha_mask_8_0', 8192, 667.0, 0.0814208984375]
['model.relu.alpha_mask_9_0', 4096, 611.0, 0.149169921875]
['model.relu.alpha_mask_10_0', 4096, 746.0, 0.18212890625]
['model.relu.alpha_mask_11_0', 4096, 647.0, 0.157958984375]
['model.relu.alpha_mask_12_0', 4096, 641.0, 0.156494140625]
['model.relu.alpha_mask_13_0', 2048, 1103.0, 0.53857421875]
['model.relu.alpha_mask_14_0', 2048, 1224.0, 0.59765625]
['model.relu.alpha_mask_15_0', 2048, 819.0, 0.39990234375]
['model.relu.alpha_mask_16_0', 2048, 175.0, 0.08544921875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12814.0, 0.06800908627717392]
########## End ###########
06/01 02:13:03 AM | Train: [10/80] Step 000/390 Loss 0.172 Prec@(1,5) (96.1%, 100.0%)
06/01 02:13:03 AM | layerwise density: [1644.0, 596.0, 597.0, 657.0, 550.0, 789.0, 745.0, 603.0, 667.0, 611.0, 746.0, 647.0, 641.0, 1103.0, 1224.0, 819.0, 175.0]
layerwise density percentage: ['0.025', '0.036', '0.036', '0.040', '0.034', '0.096', '0.091', '0.074', '0.081', '0.149', '0.182', '0.158', '0.156', '0.539', '0.598', '0.400', '0.085']
Global density: 0.06800908595323563
06/01 02:13:14 AM | Train: [10/80] Step 100/390 Loss 0.223 Prec@(1,5) (94.5%, 99.9%)
06/01 02:13:14 AM | layerwise density: [1552.0, 595.0, 584.0, 652.0, 538.0, 788.0, 750.0, 598.0, 669.0, 621.0, 747.0, 655.0, 657.0, 1118.0, 1236.0, 829.0, 182.0]
layerwise density percentage: ['0.024', '0.036', '0.036', '0.040', '0.033', '0.096', '0.092', '0.073', '0.082', '0.152', '0.182', '0.160', '0.160', '0.546', '0.604', '0.405', '0.089']
Global density: 0.06778086721897125
06/01 02:13:25 AM | Train: [10/80] Step 200/390 Loss 0.228 Prec@(1,5) (94.3%, 99.9%)
06/01 02:13:25 AM | layerwise density: [1503.0, 598.0, 574.0, 656.0, 537.0, 796.0, 764.0, 609.0, 684.0, 634.0, 770.0, 667.0, 668.0, 1136.0, 1246.0, 840.0, 196.0]
layerwise density percentage: ['0.023', '0.036', '0.035', '0.040', '0.033', '0.097', '0.093', '0.074', '0.083', '0.155', '0.188', '0.163', '0.163', '0.555', '0.608', '0.410', '0.096']
Global density: 0.06834875792264938
06/01 02:13:36 AM | Train: [10/80] Step 300/390 Loss 0.229 Prec@(1,5) (94.3%, 99.9%)
06/01 02:13:36 AM | layerwise density: [1407.0, 588.0, 545.0, 642.0, 524.0, 797.0, 767.0, 615.0, 676.0, 639.0, 779.0, 673.0, 672.0, 1145.0, 1270.0, 845.0, 199.0]
layerwise density percentage: ['0.021', '0.036', '0.033', '0.039', '0.032', '0.097', '0.094', '0.075', '0.083', '0.156', '0.190', '0.164', '0.164', '0.559', '0.620', '0.413', '0.097']
Global density: 0.06784455478191376
06/01 02:13:46 AM | Train: [10/80] Step 390/390 Loss 0.234 Prec@(1,5) (94.2%, 99.8%)
06/01 02:13:46 AM | layerwise density: [1356.0, 591.0, 531.0, 632.0, 508.0, 823.0, 776.0, 612.0, 687.0, 642.0, 784.0, 686.0, 677.0, 1149.0, 1281.0, 857.0, 207.0]
layerwise density percentage: ['0.021', '0.036', '0.032', '0.039', '0.031', '0.100', '0.095', '0.075', '0.084', '0.157', '0.191', '0.167', '0.165', '0.561', '0.625', '0.418', '0.101']
Global density: 0.0679294764995575
06/01 02:13:46 AM | Train: [10/200] Final Prec@1 94.1560%
06/01 02:13:46 AM | Valid: [10/200] Step 000/078 Loss 1.348 Prec@(1,5) (73.4%, 88.3%)
06/01 02:13:49 AM | Valid: [10/200] Step 078/078 Loss 1.590 Prec@(1,5) (63.5%, 86.1%)
06/01 02:13:49 AM | Valid: [10/200] Final Prec@1 63.4700%
06/01 02:13:49 AM | Current mask training best Prec@1 = 63.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 1356.0, 0.02069091796875]
['model.relu.alpha_mask_1_0', 16384, 591.0, 0.03607177734375]
['model.relu.alpha_mask_2_0', 16384, 531.0, 0.03240966796875]
['model.relu.alpha_mask_3_0', 16384, 633.0, 0.03863525390625]
['model.relu.alpha_mask_4_0', 16384, 507.0, 0.03094482421875]
['model.relu.alpha_mask_5_0', 8192, 824.0, 0.1005859375]
['model.relu.alpha_mask_6_0', 8192, 775.0, 0.0946044921875]
['model.relu.alpha_mask_7_0', 8192, 612.0, 0.07470703125]
['model.relu.alpha_mask_8_0', 8192, 687.0, 0.0838623046875]
['model.relu.alpha_mask_9_0', 4096, 642.0, 0.15673828125]
['model.relu.alpha_mask_10_0', 4096, 784.0, 0.19140625]
['model.relu.alpha_mask_11_0', 4096, 686.0, 0.16748046875]
['model.relu.alpha_mask_12_0', 4096, 678.0, 0.16552734375]
['model.relu.alpha_mask_13_0', 2048, 1149.0, 0.56103515625]
['model.relu.alpha_mask_14_0', 2048, 1281.0, 0.62548828125]
['model.relu.alpha_mask_15_0', 2048, 857.0, 0.41845703125]
['model.relu.alpha_mask_16_0', 2048, 207.0, 0.10107421875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12800.0, 0.06793478260869565]
########## End ###########
06/01 02:13:50 AM | Train: [11/80] Step 000/390 Loss 0.202 Prec@(1,5) (95.3%, 100.0%)
06/01 02:13:50 AM | layerwise density: [1356.0, 591.0, 531.0, 633.0, 507.0, 824.0, 775.0, 612.0, 687.0, 642.0, 784.0, 686.0, 678.0, 1149.0, 1281.0, 857.0, 207.0]
layerwise density percentage: ['0.021', '0.036', '0.032', '0.039', '0.031', '0.101', '0.095', '0.075', '0.084', '0.157', '0.191', '0.167', '0.166', '0.561', '0.625', '0.418', '0.101']
Global density: 0.0679347813129425
06/01 02:14:00 AM | Train: [11/80] Step 100/390 Loss 0.182 Prec@(1,5) (95.7%, 99.9%)
06/01 02:14:00 AM | layerwise density: [1270.0, 590.0, 521.0, 625.0, 484.0, 825.0, 772.0, 613.0, 684.0, 642.0, 798.0, 691.0, 688.0, 1163.0, 1295.0, 866.0, 212.0]
layerwise density percentage: ['0.019', '0.036', '0.032', '0.038', '0.030', '0.101', '0.094', '0.075', '0.083', '0.157', '0.195', '0.169', '0.168', '0.568', '0.632', '0.423', '0.104']
Global density: 0.06761103123426437
06/01 02:14:11 AM | Train: [11/80] Step 200/390 Loss 0.181 Prec@(1,5) (95.7%, 99.9%)
06/01 02:14:11 AM | layerwise density: [1227.0, 598.0, 516.0, 623.0, 475.0, 841.0, 774.0, 615.0, 688.0, 655.0, 801.0, 705.0, 700.0, 1175.0, 1303.0, 880.0, 220.0]
layerwise density percentage: ['0.019', '0.036', '0.031', '0.038', '0.029', '0.103', '0.094', '0.075', '0.084', '0.160', '0.196', '0.172', '0.171', '0.574', '0.636', '0.430', '0.107']
Global density: 0.06791355460882187
06/01 02:14:22 AM | Train: [11/80] Step 300/390 Loss 0.185 Prec@(1,5) (95.6%, 99.9%)
06/01 02:14:22 AM | layerwise density: [1158.0, 604.0, 515.0, 625.0, 477.0, 845.0, 783.0, 621.0, 699.0, 666.0, 804.0, 715.0, 721.0, 1195.0, 1316.0, 888.0, 228.0]
layerwise density percentage: ['0.018', '0.037', '0.031', '0.038', '0.029', '0.103', '0.096', '0.076', '0.085', '0.163', '0.196', '0.175', '0.176', '0.583', '0.643', '0.434', '0.111']
Global density: 0.06825322657823563
06/01 02:14:32 AM | Train: [11/80] Step 390/390 Loss 0.184 Prec@(1,5) (95.6%, 99.9%)
06/01 02:14:32 AM | layerwise density: [1095.0, 609.0, 514.0, 610.0, 466.0, 836.0, 783.0, 633.0, 699.0, 676.0, 809.0, 724.0, 745.0, 1203.0, 1319.0, 896.0, 234.0]
layerwise density percentage: ['0.017', '0.037', '0.031', '0.037', '0.028', '0.102', '0.096', '0.077', '0.085', '0.165', '0.198', '0.177', '0.182', '0.587', '0.644', '0.438', '0.114']
Global density: 0.06820546090602875
06/01 02:14:32 AM | Train: [11/200] Final Prec@1 95.6240%
06/01 02:14:33 AM | Valid: [11/200] Step 000/078 Loss 1.456 Prec@(1,5) (70.3%, 87.5%)
06/01 02:14:35 AM | Valid: [11/200] Step 078/078 Loss 1.559 Prec@(1,5) (64.4%, 86.8%)
06/01 02:14:35 AM | Valid: [11/200] Final Prec@1 64.3600%
06/01 02:14:36 AM | Current mask training best Prec@1 = 64.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 1094.0, 0.016693115234375]
['model.relu.alpha_mask_1_0', 16384, 609.0, 0.03717041015625]
['model.relu.alpha_mask_2_0', 16384, 514.0, 0.0313720703125]
['model.relu.alpha_mask_3_0', 16384, 611.0, 0.03729248046875]
['model.relu.alpha_mask_4_0', 16384, 466.0, 0.0284423828125]
['model.relu.alpha_mask_5_0', 8192, 836.0, 0.10205078125]
['model.relu.alpha_mask_6_0', 8192, 782.0, 0.095458984375]
['model.relu.alpha_mask_7_0', 8192, 633.0, 0.0772705078125]
['model.relu.alpha_mask_8_0', 8192, 699.0, 0.0853271484375]
['model.relu.alpha_mask_9_0', 4096, 676.0, 0.1650390625]
['model.relu.alpha_mask_10_0', 4096, 809.0, 0.197509765625]
['model.relu.alpha_mask_11_0', 4096, 724.0, 0.1767578125]
['model.relu.alpha_mask_12_0', 4096, 744.0, 0.181640625]
['model.relu.alpha_mask_13_0', 2048, 1202.0, 0.5869140625]
['model.relu.alpha_mask_14_0', 2048, 1319.0, 0.64404296875]
['model.relu.alpha_mask_15_0', 2048, 896.0, 0.4375]
['model.relu.alpha_mask_16_0', 2048, 234.0, 0.1142578125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12848.0, 0.06818953804347826]
########## End ###########
06/01 02:14:37 AM | Train: [12/80] Step 000/390 Loss 0.187 Prec@(1,5) (94.5%, 100.0%)
06/01 02:14:37 AM | layerwise density: [1094.0, 609.0, 514.0, 611.0, 466.0, 836.0, 782.0, 633.0, 699.0, 676.0, 809.0, 724.0, 744.0, 1202.0, 1319.0, 896.0, 234.0]
layerwise density percentage: ['0.017', '0.037', '0.031', '0.037', '0.028', '0.102', '0.095', '0.077', '0.085', '0.165', '0.198', '0.177', '0.182', '0.587', '0.644', '0.438', '0.114']
Global density: 0.06818953901529312
06/01 02:14:48 AM | Train: [12/80] Step 100/390 Loss 0.147 Prec@(1,5) (96.7%, 99.9%)
06/01 02:14:48 AM | layerwise density: [1036.0, 603.0, 500.0, 610.0, 454.0, 832.0, 784.0, 643.0, 704.0, 695.0, 815.0, 735.0, 747.0, 1203.0, 1340.0, 901.0, 242.0]
layerwise density percentage: ['0.016', '0.037', '0.031', '0.037', '0.028', '0.102', '0.096', '0.078', '0.086', '0.170', '0.199', '0.179', '0.182', '0.587', '0.654', '0.440', '0.118']
Global density: 0.06816831231117249
06/01 02:14:59 AM | Train: [12/80] Step 200/390 Loss 0.146 Prec@(1,5) (96.8%, 99.9%)
06/01 02:14:59 AM | layerwise density: [981.0, 601.0, 482.0, 608.0, 458.0, 840.0, 780.0, 643.0, 709.0, 704.0, 826.0, 747.0, 750.0, 1216.0, 1346.0, 913.0, 248.0]
layerwise density percentage: ['0.015', '0.037', '0.029', '0.037', '0.028', '0.103', '0.095', '0.078', '0.087', '0.172', '0.202', '0.182', '0.183', '0.594', '0.657', '0.446', '0.121']
Global density: 0.06821076571941376
06/01 02:15:10 AM | Train: [12/80] Step 300/390 Loss 0.149 Prec@(1,5) (96.7%, 99.9%)
06/01 02:15:10 AM | layerwise density: [892.0, 592.0, 444.0, 568.0, 436.0, 822.0, 773.0, 640.0, 693.0, 705.0, 838.0, 752.0, 755.0, 1225.0, 1350.0, 920.0, 256.0]
layerwise density percentage: ['0.014', '0.036', '0.027', '0.035', '0.027', '0.100', '0.094', '0.078', '0.085', '0.172', '0.205', '0.184', '0.184', '0.598', '0.659', '0.449', '0.125']
Global density: 0.0671970546245575
06/01 02:15:19 AM | Train: [12/80] Step 390/390 Loss 0.151 Prec@(1,5) (96.6%, 99.9%)
06/01 02:15:19 AM | layerwise density: [871.0, 598.0, 451.0, 574.0, 439.0, 850.0, 795.0, 635.0, 715.0, 713.0, 843.0, 761.0, 772.0, 1237.0, 1368.0, 927.0, 266.0]
layerwise density percentage: ['0.013', '0.036', '0.028', '0.035', '0.027', '0.104', '0.097', '0.078', '0.087', '0.174', '0.206', '0.186', '0.188', '0.604', '0.668', '0.453', '0.130']
Global density: 0.06801439821720123
06/01 02:15:19 AM | Train: [12/200] Final Prec@1 96.6340%
06/01 02:15:20 AM | Valid: [12/200] Step 000/078 Loss 1.158 Prec@(1,5) (75.8%, 89.8%)
06/01 02:15:22 AM | Valid: [12/200] Step 078/078 Loss 1.473 Prec@(1,5) (65.6%, 87.4%)
06/01 02:15:22 AM | Valid: [12/200] Final Prec@1 65.6200%
06/01 02:15:23 AM | Current mask training best Prec@1 = 65.6200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 871.0, 0.0132904052734375]
['model.relu.alpha_mask_1_0', 16384, 598.0, 0.0364990234375]
['model.relu.alpha_mask_2_0', 16384, 451.0, 0.02752685546875]
['model.relu.alpha_mask_3_0', 16384, 574.0, 0.0350341796875]
['model.relu.alpha_mask_4_0', 16384, 439.0, 0.02679443359375]
['model.relu.alpha_mask_5_0', 8192, 850.0, 0.103759765625]
['model.relu.alpha_mask_6_0', 8192, 795.0, 0.0970458984375]
['model.relu.alpha_mask_7_0', 8192, 635.0, 0.0775146484375]
['model.relu.alpha_mask_8_0', 8192, 715.0, 0.0872802734375]
['model.relu.alpha_mask_9_0', 4096, 713.0, 0.174072265625]
['model.relu.alpha_mask_10_0', 4096, 843.0, 0.205810546875]
['model.relu.alpha_mask_11_0', 4096, 761.0, 0.185791015625]
['model.relu.alpha_mask_12_0', 4096, 772.0, 0.1884765625]
['model.relu.alpha_mask_13_0', 2048, 1237.0, 0.60400390625]
['model.relu.alpha_mask_14_0', 2048, 1368.0, 0.66796875]
['model.relu.alpha_mask_15_0', 2048, 927.0, 0.45263671875]
['model.relu.alpha_mask_16_0', 2048, 266.0, 0.1298828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12815.0, 0.06801439368206522]
########## End ###########
06/01 02:15:24 AM | Train: [13/80] Step 000/390 Loss 0.125 Prec@(1,5) (98.4%, 100.0%)
06/01 02:15:24 AM | layerwise density: [871.0, 598.0, 451.0, 574.0, 439.0, 850.0, 795.0, 635.0, 715.0, 713.0, 843.0, 761.0, 772.0, 1237.0, 1368.0, 927.0, 266.0]
layerwise density percentage: ['0.013', '0.036', '0.028', '0.035', '0.027', '0.104', '0.097', '0.078', '0.087', '0.174', '0.206', '0.186', '0.188', '0.604', '0.668', '0.453', '0.130']
Global density: 0.06801439821720123
06/01 02:15:35 AM | Train: [13/80] Step 100/390 Loss 0.122 Prec@(1,5) (97.4%, 100.0%)
06/01 02:15:35 AM | layerwise density: [822.0, 587.0, 425.0, 560.0, 402.0, 842.0, 797.0, 630.0, 702.0, 712.0, 841.0, 764.0, 770.0, 1233.0, 1370.0, 931.0, 269.0]
layerwise density percentage: ['0.013', '0.036', '0.026', '0.034', '0.025', '0.103', '0.097', '0.077', '0.086', '0.174', '0.205', '0.187', '0.188', '0.602', '0.669', '0.455', '0.131']
Global density: 0.06717582792043686
06/01 02:15:46 AM | Train: [13/80] Step 200/390 Loss 0.123 Prec@(1,5) (97.4%, 100.0%)
06/01 02:15:46 AM | layerwise density: [787.0, 586.0, 413.0, 564.0, 397.0, 858.0, 798.0, 639.0, 717.0, 718.0, 843.0, 775.0, 780.0, 1243.0, 1395.0, 937.0, 280.0]
layerwise density percentage: ['0.012', '0.036', '0.025', '0.034', '0.024', '0.105', '0.097', '0.078', '0.088', '0.175', '0.206', '0.189', '0.190', '0.607', '0.681', '0.458', '0.137']
Global density: 0.0675632655620575
06/01 02:15:57 AM | Train: [13/80] Step 300/390 Loss 0.126 Prec@(1,5) (97.3%, 100.0%)
06/01 02:15:57 AM | layerwise density: [772.0, 598.0, 423.0, 574.0, 400.0, 872.0, 814.0, 651.0, 728.0, 736.0, 862.0, 781.0, 790.0, 1252.0, 1397.0, 943.0, 286.0]
layerwise density percentage: ['0.012', '0.036', '0.026', '0.035', '0.024', '0.106', '0.099', '0.079', '0.089', '0.180', '0.210', '0.191', '0.193', '0.611', '0.682', '0.460', '0.140']
Global density: 0.06835407018661499
06/01 02:16:07 AM | Train: [13/80] Step 390/390 Loss 0.128 Prec@(1,5) (97.2%, 100.0%)
06/01 02:16:07 AM | layerwise density: [735.0, 588.0, 415.0, 559.0, 398.0, 868.0, 804.0, 644.0, 723.0, 747.0, 865.0, 787.0, 803.0, 1259.0, 1404.0, 948.0, 292.0]
layerwise density percentage: ['0.011', '0.036', '0.025', '0.034', '0.024', '0.106', '0.098', '0.079', '0.088', '0.182', '0.211', '0.192', '0.196', '0.615', '0.686', '0.463', '0.143']
Global density: 0.06814177334308624
06/01 02:16:07 AM | Train: [13/200] Final Prec@1 97.2160%
06/01 02:16:07 AM | Valid: [13/200] Step 000/078 Loss 1.246 Prec@(1,5) (71.1%, 89.8%)
06/01 02:16:10 AM | Valid: [13/200] Step 078/078 Loss 1.462 Prec@(1,5) (66.0%, 87.4%)
06/01 02:16:10 AM | Valid: [13/200] Final Prec@1 66.0500%
06/01 02:16:10 AM | Current mask training best Prec@1 = 66.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 735.0, 0.0112152099609375]
['model.relu.alpha_mask_1_0', 16384, 588.0, 0.035888671875]
['model.relu.alpha_mask_2_0', 16384, 415.0, 0.02532958984375]
['model.relu.alpha_mask_3_0', 16384, 559.0, 0.03411865234375]
['model.relu.alpha_mask_4_0', 16384, 398.0, 0.0242919921875]
['model.relu.alpha_mask_5_0', 8192, 868.0, 0.10595703125]
['model.relu.alpha_mask_6_0', 8192, 804.0, 0.09814453125]
['model.relu.alpha_mask_7_0', 8192, 644.0, 0.07861328125]
['model.relu.alpha_mask_8_0', 8192, 723.0, 0.0882568359375]
['model.relu.alpha_mask_9_0', 4096, 747.0, 0.182373046875]
['model.relu.alpha_mask_10_0', 4096, 865.0, 0.211181640625]
['model.relu.alpha_mask_11_0', 4096, 787.0, 0.192138671875]
['model.relu.alpha_mask_12_0', 4096, 803.0, 0.196044921875]
['model.relu.alpha_mask_13_0', 2048, 1259.0, 0.61474609375]
['model.relu.alpha_mask_14_0', 2048, 1404.0, 0.685546875]
['model.relu.alpha_mask_15_0', 2048, 948.0, 0.462890625]
['model.relu.alpha_mask_16_0', 2048, 292.0, 0.142578125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12839.0, 0.06814177139945653]
########## End ###########
06/01 02:16:11 AM | Train: [14/80] Step 000/390 Loss 0.111 Prec@(1,5) (97.7%, 100.0%)
06/01 02:16:11 AM | layerwise density: [735.0, 588.0, 415.0, 559.0, 398.0, 868.0, 804.0, 644.0, 723.0, 747.0, 865.0, 787.0, 803.0, 1259.0, 1404.0, 948.0, 292.0]
layerwise density percentage: ['0.011', '0.036', '0.025', '0.034', '0.024', '0.106', '0.098', '0.079', '0.088', '0.182', '0.211', '0.192', '0.196', '0.615', '0.686', '0.463', '0.143']
Global density: 0.06814177334308624
06/01 02:16:22 AM | Train: [14/80] Step 100/390 Loss 0.104 Prec@(1,5) (98.0%, 100.0%)
06/01 02:16:22 AM | layerwise density: [712.0, 587.0, 413.0, 563.0, 400.0, 881.0, 808.0, 639.0, 710.0, 752.0, 874.0, 791.0, 806.0, 1261.0, 1412.0, 953.0, 313.0]
layerwise density percentage: ['0.011', '0.036', '0.025', '0.034', '0.024', '0.108', '0.099', '0.078', '0.087', '0.184', '0.213', '0.193', '0.197', '0.616', '0.689', '0.465', '0.153']
Global density: 0.06833283603191376
06/01 02:16:33 AM | Train: [14/80] Step 200/390 Loss 0.107 Prec@(1,5) (97.9%, 100.0%)
06/01 02:16:33 AM | layerwise density: [701.0, 579.0, 397.0, 546.0, 384.0, 887.0, 818.0, 644.0, 714.0, 754.0, 877.0, 801.0, 809.0, 1270.0, 1425.0, 953.0, 324.0]
layerwise density percentage: ['0.011', '0.035', '0.024', '0.033', '0.023', '0.108', '0.100', '0.079', '0.087', '0.184', '0.214', '0.196', '0.198', '0.620', '0.696', '0.465', '0.158']
Global density: 0.06837529689073563
06/01 02:16:45 AM | Train: [14/80] Step 300/390 Loss 0.107 Prec@(1,5) (97.9%, 100.0%)
06/01 02:16:45 AM | layerwise density: [672.0, 571.0, 380.0, 532.0, 377.0, 888.0, 815.0, 647.0, 718.0, 759.0, 878.0, 805.0, 816.0, 1274.0, 1432.0, 959.0, 338.0]
layerwise density percentage: ['0.010', '0.035', '0.023', '0.032', '0.023', '0.108', '0.099', '0.079', '0.088', '0.185', '0.214', '0.197', '0.199', '0.622', '0.699', '0.468', '0.165']
Global density: 0.06825853884220123
06/01 02:16:55 AM | Train: [14/80] Step 390/390 Loss 0.109 Prec@(1,5) (97.8%, 100.0%)
06/01 02:16:55 AM | layerwise density: [629.0, 568.0, 373.0, 529.0, 366.0, 887.0, 812.0, 644.0, 718.0, 759.0, 895.0, 811.0, 828.0, 1279.0, 1444.0, 964.0, 342.0]
layerwise density percentage: ['0.010', '0.035', '0.023', '0.032', '0.022', '0.108', '0.099', '0.079', '0.088', '0.185', '0.219', '0.198', '0.202', '0.625', '0.705', '0.471', '0.167']
Global density: 0.06818953901529312
06/01 02:16:55 AM | Train: [14/200] Final Prec@1 97.7960%
06/01 02:16:55 AM | Valid: [14/200] Step 000/078 Loss 1.294 Prec@(1,5) (69.5%, 89.1%)
06/01 02:16:57 AM | Valid: [14/200] Step 078/078 Loss 1.477 Prec@(1,5) (66.3%, 87.2%)
06/01 02:16:57 AM | Valid: [14/200] Final Prec@1 66.2600%
06/01 02:16:58 AM | Current mask training best Prec@1 = 66.2600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 630.0, 0.009613037109375]
['model.relu.alpha_mask_1_0', 16384, 568.0, 0.03466796875]
['model.relu.alpha_mask_2_0', 16384, 374.0, 0.0228271484375]
['model.relu.alpha_mask_3_0', 16384, 528.0, 0.0322265625]
['model.relu.alpha_mask_4_0', 16384, 368.0, 0.0224609375]
['model.relu.alpha_mask_5_0', 8192, 888.0, 0.1083984375]
['model.relu.alpha_mask_6_0', 8192, 811.0, 0.0989990234375]
['model.relu.alpha_mask_7_0', 8192, 644.0, 0.07861328125]
['model.relu.alpha_mask_8_0', 8192, 718.0, 0.087646484375]
['model.relu.alpha_mask_9_0', 4096, 759.0, 0.185302734375]
['model.relu.alpha_mask_10_0', 4096, 895.0, 0.218505859375]
['model.relu.alpha_mask_11_0', 4096, 811.0, 0.197998046875]
['model.relu.alpha_mask_12_0', 4096, 828.0, 0.2021484375]
['model.relu.alpha_mask_13_0', 2048, 1279.0, 0.62451171875]
['model.relu.alpha_mask_14_0', 2048, 1444.0, 0.705078125]
['model.relu.alpha_mask_15_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_mask_16_0', 2048, 342.0, 0.1669921875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12851.0, 0.06820546025815218]
########## End ###########
06/01 02:16:59 AM | Train: [15/80] Step 000/390 Loss 0.044 Prec@(1,5) (100.0%, 100.0%)
06/01 02:16:59 AM | layerwise density: [630.0, 568.0, 374.0, 528.0, 368.0, 888.0, 811.0, 644.0, 718.0, 759.0, 895.0, 811.0, 828.0, 1279.0, 1444.0, 964.0, 342.0]
layerwise density percentage: ['0.010', '0.035', '0.023', '0.032', '0.022', '0.108', '0.099', '0.079', '0.088', '0.185', '0.219', '0.198', '0.202', '0.625', '0.705', '0.471', '0.167']
Global density: 0.06820546090602875
06/01 02:17:09 AM | Train: [15/80] Step 100/390 Loss 0.092 Prec@(1,5) (98.2%, 100.0%)
06/01 02:17:09 AM | layerwise density: [619.0, 555.0, 373.0, 509.0, 350.0, 889.0, 806.0, 640.0, 716.0, 768.0, 895.0, 821.0, 823.0, 1275.0, 1451.0, 967.0, 359.0]
layerwise density percentage: ['0.009', '0.034', '0.023', '0.031', '0.021', '0.109', '0.098', '0.078', '0.087', '0.188', '0.219', '0.200', '0.201', '0.623', '0.708', '0.472', '0.175']
Global density: 0.06801970303058624
06/01 02:17:20 AM | Train: [15/80] Step 200/390 Loss 0.090 Prec@(1,5) (98.3%, 100.0%)
06/01 02:17:20 AM | layerwise density: [596.0, 555.0, 369.0, 506.0, 332.0, 889.0, 786.0, 640.0, 720.0, 777.0, 896.0, 830.0, 823.0, 1291.0, 1452.0, 967.0, 374.0]
layerwise density percentage: ['0.009', '0.034', '0.023', '0.031', '0.020', '0.109', '0.096', '0.078', '0.088', '0.190', '0.219', '0.203', '0.201', '0.630', '0.709', '0.472', '0.183']
Global density: 0.06795070320367813
06/01 02:17:31 AM | Train: [15/80] Step 300/390 Loss 0.092 Prec@(1,5) (98.2%, 100.0%)
06/01 02:17:31 AM | layerwise density: [572.0, 556.0, 359.0, 510.0, 336.0, 876.0, 791.0, 651.0, 721.0, 795.0, 905.0, 837.0, 822.0, 1290.0, 1455.0, 974.0, 388.0]
layerwise density percentage: ['0.009', '0.034', '0.022', '0.031', '0.021', '0.107', '0.097', '0.079', '0.088', '0.194', '0.221', '0.204', '0.201', '0.630', '0.710', '0.476', '0.189']
Global density: 0.06813646852970123
06/01 02:17:40 AM | Train: [15/80] Step 390/390 Loss 0.093 Prec@(1,5) (98.2%, 100.0%)
06/01 02:17:40 AM | layerwise density: [558.0, 550.0, 359.0, 510.0, 327.0, 884.0, 797.0, 651.0, 714.0, 793.0, 907.0, 846.0, 837.0, 1293.0, 1468.0, 980.0, 400.0]
layerwise density percentage: ['0.009', '0.034', '0.022', '0.031', '0.020', '0.108', '0.097', '0.079', '0.087', '0.194', '0.221', '0.207', '0.204', '0.631', '0.717', '0.479', '0.195']
Global density: 0.06832753121852875
06/01 02:17:40 AM | Train: [15/200] Final Prec@1 98.2120%
06/01 02:17:41 AM | Valid: [15/200] Step 000/078 Loss 1.107 Prec@(1,5) (71.9%, 90.6%)
06/01 02:17:43 AM | Valid: [15/200] Step 078/078 Loss 1.465 Prec@(1,5) (66.5%, 87.4%)
06/01 02:17:43 AM | Valid: [15/200] Final Prec@1 66.5500%
06/01 02:17:44 AM | Current mask training best Prec@1 = 66.5500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 558.0, 0.008514404296875]
['model.relu.alpha_mask_1_0', 16384, 550.0, 0.0335693359375]
['model.relu.alpha_mask_2_0', 16384, 360.0, 0.02197265625]
['model.relu.alpha_mask_3_0', 16384, 510.0, 0.0311279296875]
['model.relu.alpha_mask_4_0', 16384, 328.0, 0.02001953125]
['model.relu.alpha_mask_5_0', 8192, 885.0, 0.1080322265625]
['model.relu.alpha_mask_6_0', 8192, 796.0, 0.09716796875]
['model.relu.alpha_mask_7_0', 8192, 652.0, 0.07958984375]
['model.relu.alpha_mask_8_0', 8192, 715.0, 0.0872802734375]
['model.relu.alpha_mask_9_0', 4096, 792.0, 0.193359375]
['model.relu.alpha_mask_10_0', 4096, 908.0, 0.2216796875]
['model.relu.alpha_mask_11_0', 4096, 846.0, 0.20654296875]
['model.relu.alpha_mask_12_0', 4096, 837.0, 0.204345703125]
['model.relu.alpha_mask_13_0', 2048, 1294.0, 0.6318359375]
['model.relu.alpha_mask_14_0', 2048, 1470.0, 0.7177734375]
['model.relu.alpha_mask_15_0', 2048, 980.0, 0.478515625]
['model.relu.alpha_mask_16_0', 2048, 400.0, 0.1953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12881.0, 0.06836468240489131]
########## End ###########
06/01 02:17:45 AM | Train: [16/80] Step 000/390 Loss 0.100 Prec@(1,5) (97.7%, 100.0%)
06/01 02:17:45 AM | layerwise density: [558.0, 550.0, 360.0, 510.0, 328.0, 885.0, 796.0, 652.0, 715.0, 792.0, 908.0, 846.0, 837.0, 1294.0, 1470.0, 980.0, 400.0]
layerwise density percentage: ['0.009', '0.034', '0.022', '0.031', '0.020', '0.108', '0.097', '0.080', '0.087', '0.193', '0.222', '0.207', '0.204', '0.632', '0.718', '0.479', '0.195']
Global density: 0.0683646872639656
06/01 02:17:55 AM | Train: [16/80] Step 100/390 Loss 0.078 Prec@(1,5) (98.6%, 100.0%)
06/01 02:17:55 AM | layerwise density: [535.0, 540.0, 351.0, 495.0, 316.0, 880.0, 795.0, 643.0, 716.0, 792.0, 916.0, 850.0, 841.0, 1310.0, 1472.0, 986.0, 412.0]
layerwise density percentage: ['0.008', '0.033', '0.021', '0.030', '0.019', '0.107', '0.097', '0.078', '0.087', '0.193', '0.224', '0.208', '0.205', '0.640', '0.719', '0.481', '0.201']
Global density: 0.06820015609264374
06/01 02:18:06 AM | Train: [16/80] Step 200/390 Loss 0.080 Prec@(1,5) (98.5%, 100.0%)
06/01 02:18:06 AM | layerwise density: [516.0, 520.0, 347.0, 486.0, 316.0, 878.0, 799.0, 651.0, 732.0, 790.0, 920.0, 852.0, 835.0, 1315.0, 1468.0, 993.0, 422.0]
layerwise density percentage: ['0.008', '0.032', '0.021', '0.030', '0.019', '0.107', '0.098', '0.079', '0.089', '0.193', '0.225', '0.208', '0.204', '0.642', '0.717', '0.485', '0.206']
Global density: 0.06814707815647125
06/01 02:18:17 AM | Train: [16/80] Step 300/390 Loss 0.080 Prec@(1,5) (98.5%, 100.0%)
06/01 02:18:17 AM | layerwise density: [487.0, 470.0, 321.0, 427.0, 281.0, 823.0, 771.0, 624.0, 681.0, 781.0, 909.0, 847.0, 827.0, 1300.0, 1476.0, 994.0, 432.0]
layerwise density percentage: ['0.007', '0.029', '0.020', '0.026', '0.017', '0.100', '0.094', '0.076', '0.083', '0.191', '0.222', '0.207', '0.202', '0.635', '0.721', '0.485', '0.211']
Global density: 0.06608249992132187
06/01 02:18:27 AM | Train: [16/80] Step 390/390 Loss 0.082 Prec@(1,5) (98.5%, 100.0%)
06/01 02:18:27 AM | layerwise density: [486.0, 484.0, 323.0, 434.0, 279.0, 877.0, 784.0, 631.0, 696.0, 805.0, 917.0, 854.0, 836.0, 1310.0, 1490.0, 998.0, 445.0]
layerwise density percentage: ['0.007', '0.030', '0.020', '0.026', '0.017', '0.107', '0.096', '0.077', '0.085', '0.197', '0.224', '0.208', '0.204', '0.640', '0.728', '0.487', '0.217']
Global density: 0.06713336706161499
06/01 02:18:27 AM | Train: [16/200] Final Prec@1 98.4800%
06/01 02:18:27 AM | Valid: [16/200] Step 000/078 Loss 1.102 Prec@(1,5) (76.6%, 92.2%)
06/01 02:18:29 AM | Valid: [16/200] Step 078/078 Loss 1.459 Prec@(1,5) (66.0%, 87.3%)
06/01 02:18:29 AM | Valid: [16/200] Final Prec@1 66.0300%
06/01 02:18:29 AM | Current mask training best Prec@1 = 66.5500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 485.0, 0.0074005126953125]
['model.relu.alpha_mask_1_0', 16384, 484.0, 0.029541015625]
['model.relu.alpha_mask_2_0', 16384, 325.0, 0.01983642578125]
['model.relu.alpha_mask_3_0', 16384, 434.0, 0.0264892578125]
['model.relu.alpha_mask_4_0', 16384, 279.0, 0.01702880859375]
['model.relu.alpha_mask_5_0', 8192, 876.0, 0.10693359375]
['model.relu.alpha_mask_6_0', 8192, 782.0, 0.095458984375]
['model.relu.alpha_mask_7_0', 8192, 631.0, 0.0770263671875]
['model.relu.alpha_mask_8_0', 8192, 697.0, 0.0850830078125]
['model.relu.alpha_mask_9_0', 4096, 805.0, 0.196533203125]
['model.relu.alpha_mask_10_0', 4096, 918.0, 0.22412109375]
['model.relu.alpha_mask_11_0', 4096, 855.0, 0.208740234375]
['model.relu.alpha_mask_12_0', 4096, 835.0, 0.203857421875]
['model.relu.alpha_mask_13_0', 2048, 1311.0, 0.64013671875]
['model.relu.alpha_mask_14_0', 2048, 1492.0, 0.728515625]
['model.relu.alpha_mask_15_0', 2048, 998.0, 0.4873046875]
['model.relu.alpha_mask_16_0', 2048, 445.0, 0.21728515625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12652.0, 0.06714928668478261]
########## End ###########
06/01 02:18:30 AM | Train: [17/80] Step 000/390 Loss 0.065 Prec@(1,5) (98.4%, 100.0%)
06/01 02:18:30 AM | layerwise density: [485.0, 484.0, 325.0, 434.0, 279.0, 876.0, 782.0, 631.0, 697.0, 805.0, 918.0, 855.0, 835.0, 1311.0, 1492.0, 998.0, 445.0]
layerwise density percentage: ['0.007', '0.030', '0.020', '0.026', '0.017', '0.107', '0.095', '0.077', '0.085', '0.197', '0.224', '0.209', '0.204', '0.640', '0.729', '0.487', '0.217']
Global density: 0.06714928895235062
06/01 02:18:41 AM | Train: [17/80] Step 100/390 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)
06/01 02:18:41 AM | layerwise density: [463.0, 490.0, 312.0, 430.0, 267.0, 853.0, 780.0, 643.0, 704.0, 810.0, 929.0, 867.0, 840.0, 1316.0, 1491.0, 1003.0, 457.0]
layerwise density percentage: ['0.007', '0.030', '0.019', '0.026', '0.016', '0.104', '0.095', '0.078', '0.086', '0.198', '0.227', '0.212', '0.205', '0.643', '0.728', '0.490', '0.223']
Global density: 0.06716521084308624
06/01 02:18:52 AM | Train: [17/80] Step 200/390 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)
06/01 02:18:52 AM | layerwise density: [451.0, 485.0, 293.0, 418.0, 259.0, 835.0, 772.0, 635.0, 704.0, 815.0, 918.0, 869.0, 851.0, 1326.0, 1499.0, 1004.0, 468.0]
layerwise density percentage: ['0.007', '0.030', '0.018', '0.026', '0.016', '0.102', '0.094', '0.078', '0.086', '0.199', '0.224', '0.212', '0.208', '0.647', '0.732', '0.490', '0.229']
Global density: 0.06688391417264938
06/01 02:19:04 AM | Train: [17/80] Step 300/390 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)
06/01 02:19:04 AM | layerwise density: [454.0, 507.0, 304.0, 440.0, 271.0, 877.0, 801.0, 659.0, 728.0, 831.0, 937.0, 877.0, 865.0, 1329.0, 1508.0, 1018.0, 490.0]
layerwise density percentage: ['0.007', '0.031', '0.019', '0.027', '0.017', '0.107', '0.098', '0.080', '0.089', '0.203', '0.229', '0.214', '0.211', '0.649', '0.736', '0.497', '0.239']
Global density: 0.06844429671764374
06/01 02:19:13 AM | Train: [17/80] Step 390/390 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)
06/01 02:19:13 AM | layerwise density: [429.0, 469.0, 278.0, 413.0, 239.0, 826.0, 768.0, 616.0, 701.0, 821.0, 928.0, 878.0, 853.0, 1336.0, 1504.0, 1019.0, 493.0]
layerwise density percentage: ['0.007', '0.029', '0.017', '0.025', '0.015', '0.101', '0.094', '0.075', '0.086', '0.200', '0.227', '0.214', '0.208', '0.652', '0.734', '0.498', '0.241']
Global density: 0.06671939045190811
06/01 02:19:13 AM | Train: [17/200] Final Prec@1 98.7320%
06/01 02:19:14 AM | Valid: [17/200] Step 000/078 Loss 1.190 Prec@(1,5) (70.3%, 90.6%)
06/01 02:19:16 AM | Valid: [17/200] Step 078/078 Loss 1.481 Prec@(1,5) (65.8%, 87.2%)
06/01 02:19:16 AM | Valid: [17/200] Final Prec@1 65.7600%
06/01 02:19:16 AM | Current mask training best Prec@1 = 66.5500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 430.0, 0.006561279296875]
['model.relu.alpha_mask_1_0', 16384, 469.0, 0.02862548828125]
['model.relu.alpha_mask_2_0', 16384, 278.0, 0.0169677734375]
['model.relu.alpha_mask_3_0', 16384, 413.0, 0.02520751953125]
['model.relu.alpha_mask_4_0', 16384, 238.0, 0.0145263671875]
['model.relu.alpha_mask_5_0', 8192, 827.0, 0.1009521484375]
['model.relu.alpha_mask_6_0', 8192, 769.0, 0.0938720703125]
['model.relu.alpha_mask_7_0', 8192, 616.0, 0.0751953125]
['model.relu.alpha_mask_8_0', 8192, 700.0, 0.08544921875]
['model.relu.alpha_mask_9_0', 4096, 820.0, 0.2001953125]
['model.relu.alpha_mask_10_0', 4096, 928.0, 0.2265625]
['model.relu.alpha_mask_11_0', 4096, 878.0, 0.21435546875]
['model.relu.alpha_mask_12_0', 4096, 854.0, 0.20849609375]
['model.relu.alpha_mask_13_0', 2048, 1339.0, 0.65380859375]
['model.relu.alpha_mask_14_0', 2048, 1505.0, 0.73486328125]
['model.relu.alpha_mask_15_0', 2048, 1019.0, 0.49755859375]
['model.relu.alpha_mask_16_0', 2048, 494.0, 0.2412109375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12577.0, 0.06675123131793478]
########## End ###########
06/01 02:19:17 AM | Train: [18/80] Step 000/390 Loss 0.086 Prec@(1,5) (98.4%, 100.0%)
06/01 02:19:17 AM | layerwise density: [430.0, 469.0, 278.0, 413.0, 238.0, 827.0, 769.0, 616.0, 700.0, 820.0, 928.0, 878.0, 854.0, 1339.0, 1505.0, 1019.0, 494.0]
layerwise density percentage: ['0.007', '0.029', '0.017', '0.025', '0.015', '0.101', '0.094', '0.075', '0.085', '0.200', '0.227', '0.214', '0.208', '0.654', '0.735', '0.498', '0.241']
Global density: 0.06675123423337936
06/01 02:19:28 AM | Train: [18/80] Step 100/390 Loss 0.062 Prec@(1,5) (99.0%, 100.0%)
06/01 02:19:28 AM | layerwise density: [421.0, 475.0, 295.0, 422.0, 254.0, 881.0, 787.0, 635.0, 702.0, 826.0, 947.0, 877.0, 880.0, 1344.0, 1518.0, 1019.0, 505.0]
layerwise density percentage: ['0.006', '0.029', '0.018', '0.026', '0.016', '0.108', '0.096', '0.078', '0.086', '0.202', '0.231', '0.214', '0.215', '0.656', '0.741', '0.498', '0.247']
Global density: 0.06787109375
06/01 02:19:39 AM | Train: [18/80] Step 200/390 Loss 0.064 Prec@(1,5) (98.9%, 100.0%)
06/01 02:19:39 AM | layerwise density: [405.0, 491.0, 280.0, 419.0, 252.0, 880.0, 783.0, 638.0, 684.0, 819.0, 932.0, 883.0, 890.0, 1340.0, 1533.0, 1026.0, 518.0]
layerwise density percentage: ['0.006', '0.030', '0.017', '0.026', '0.015', '0.107', '0.096', '0.078', '0.083', '0.200', '0.228', '0.216', '0.217', '0.654', '0.749', '0.501', '0.253']
Global density: 0.06779148429632187
06/01 02:19:50 AM | Train: [18/80] Step 300/390 Loss 0.066 Prec@(1,5) (98.9%, 100.0%)
06/01 02:19:50 AM | layerwise density: [402.0, 492.0, 273.0, 401.0, 237.0, 851.0, 764.0, 643.0, 709.0, 820.0, 942.0, 894.0, 876.0, 1352.0, 1544.0, 1029.0, 534.0]
layerwise density percentage: ['0.006', '0.030', '0.017', '0.024', '0.014', '0.104', '0.093', '0.078', '0.087', '0.200', '0.230', '0.218', '0.214', '0.660', '0.754', '0.502', '0.261']
Global density: 0.06773840636014938
06/01 02:20:00 AM | Train: [18/80] Step 390/390 Loss 0.067 Prec@(1,5) (98.8%, 100.0%)
06/01 02:20:00 AM | layerwise density: [381.0, 455.0, 250.0, 398.0, 237.0, 839.0, 745.0, 644.0, 680.0, 818.0, 946.0, 901.0, 872.0, 1354.0, 1535.0, 1038.0, 540.0]
layerwise density percentage: ['0.006', '0.028', '0.015', '0.024', '0.014', '0.102', '0.091', '0.079', '0.083', '0.200', '0.231', '0.220', '0.213', '0.661', '0.750', '0.507', '0.264']
Global density: 0.06704844534397125
06/01 02:20:00 AM | Train: [18/200] Final Prec@1 98.8140%
06/01 02:20:00 AM | Valid: [18/200] Step 000/078 Loss 1.093 Prec@(1,5) (72.7%, 93.0%)
06/01 02:20:03 AM | Valid: [18/200] Step 078/078 Loss 1.423 Prec@(1,5) (67.3%, 88.0%)
06/01 02:20:03 AM | Valid: [18/200] Final Prec@1 67.3100%
06/01 02:20:03 AM | Current mask training best Prec@1 = 67.3100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 381.0, 0.0058135986328125]
['model.relu.alpha_mask_1_0', 16384, 455.0, 0.02777099609375]
['model.relu.alpha_mask_2_0', 16384, 250.0, 0.0152587890625]
['model.relu.alpha_mask_3_0', 16384, 398.0, 0.0242919921875]
['model.relu.alpha_mask_4_0', 16384, 237.0, 0.01446533203125]
['model.relu.alpha_mask_5_0', 8192, 839.0, 0.1024169921875]
['model.relu.alpha_mask_6_0', 8192, 745.0, 0.0909423828125]
['model.relu.alpha_mask_7_0', 8192, 644.0, 0.07861328125]
['model.relu.alpha_mask_8_0', 8192, 680.0, 0.0830078125]
['model.relu.alpha_mask_9_0', 4096, 818.0, 0.19970703125]
['model.relu.alpha_mask_10_0', 4096, 946.0, 0.23095703125]
['model.relu.alpha_mask_11_0', 4096, 901.0, 0.219970703125]
['model.relu.alpha_mask_12_0', 4096, 872.0, 0.212890625]
['model.relu.alpha_mask_13_0', 2048, 1354.0, 0.6611328125]
['model.relu.alpha_mask_14_0', 2048, 1535.0, 0.74951171875]
['model.relu.alpha_mask_15_0', 2048, 1038.0, 0.5068359375]
['model.relu.alpha_mask_16_0', 2048, 540.0, 0.263671875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12633.0, 0.06704844599184782]
########## End ###########
06/01 02:20:04 AM | Train: [19/80] Step 000/390 Loss 0.036 Prec@(1,5) (100.0%, 100.0%)
06/01 02:20:04 AM | layerwise density: [381.0, 455.0, 250.0, 398.0, 237.0, 839.0, 745.0, 644.0, 680.0, 818.0, 946.0, 901.0, 872.0, 1354.0, 1535.0, 1038.0, 540.0]
layerwise density percentage: ['0.006', '0.028', '0.015', '0.024', '0.014', '0.102', '0.091', '0.079', '0.083', '0.200', '0.231', '0.220', '0.213', '0.661', '0.750', '0.507', '0.264']
Global density: 0.06704844534397125
06/01 02:20:15 AM | Train: [19/80] Step 100/390 Loss 0.062 Prec@(1,5) (99.0%, 100.0%)
06/01 02:20:15 AM | layerwise density: [371.0, 432.0, 236.0, 369.0, 220.0, 811.0, 737.0, 630.0, 665.0, 815.0, 948.0, 903.0, 863.0, 1359.0, 1537.0, 1048.0, 556.0]
layerwise density percentage: ['0.006', '0.026', '0.014', '0.023', '0.013', '0.099', '0.090', '0.077', '0.081', '0.199', '0.231', '0.220', '0.211', '0.664', '0.750', '0.512', '0.271']
Global density: 0.0663425624370575
06/01 02:20:26 AM | Train: [19/80] Step 200/390 Loss 0.062 Prec@(1,5) (99.0%, 100.0%)
06/01 02:20:26 AM | layerwise density: [371.0, 460.0, 247.0, 381.0, 242.0, 857.0, 783.0, 654.0, 711.0, 842.0, 956.0, 907.0, 879.0, 1366.0, 1555.0, 1057.0, 576.0]
layerwise density percentage: ['0.006', '0.028', '0.015', '0.023', '0.015', '0.105', '0.096', '0.080', '0.087', '0.206', '0.233', '0.221', '0.215', '0.667', '0.759', '0.516', '0.281']
Global density: 0.06816831231117249
06/01 02:20:37 AM | Train: [19/80] Step 300/390 Loss 0.062 Prec@(1,5) (99.0%, 100.0%)
06/01 02:20:37 AM | layerwise density: [364.0, 457.0, 255.0, 389.0, 219.0, 862.0, 765.0, 645.0, 699.0, 851.0, 960.0, 908.0, 891.0, 1378.0, 1553.0, 1057.0, 584.0]
layerwise density percentage: ['0.006', '0.028', '0.016', '0.024', '0.013', '0.105', '0.093', '0.079', '0.085', '0.208', '0.234', '0.222', '0.218', '0.673', '0.758', '0.516', '0.285']
Global density: 0.06813115626573563
06/01 02:20:46 AM | Train: [19/80] Step 390/390 Loss 0.062 Prec@(1,5) (99.0%, 100.0%)
06/01 02:20:46 AM | layerwise density: [352.0, 454.0, 250.0, 381.0, 210.0, 839.0, 736.0, 627.0, 689.0, 844.0, 962.0, 912.0, 887.0, 1365.0, 1563.0, 1067.0, 600.0]
layerwise density percentage: ['0.005', '0.028', '0.015', '0.023', '0.013', '0.102', '0.090', '0.077', '0.084', '0.206', '0.235', '0.223', '0.217', '0.667', '0.763', '0.521', '0.293']
Global density: 0.06760572642087936
06/01 02:20:46 AM | Train: [19/200] Final Prec@1 98.9880%
06/01 02:20:47 AM | Valid: [19/200] Step 000/078 Loss 1.060 Prec@(1,5) (75.0%, 90.6%)
06/01 02:20:49 AM | Valid: [19/200] Step 078/078 Loss 1.428 Prec@(1,5) (67.2%, 88.0%)
06/01 02:20:49 AM | Valid: [19/200] Final Prec@1 67.1900%
06/01 02:20:49 AM | Current mask training best Prec@1 = 67.3100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 352.0, 0.00537109375]
['model.relu.alpha_mask_1_0', 16384, 454.0, 0.0277099609375]
['model.relu.alpha_mask_2_0', 16384, 250.0, 0.0152587890625]
['model.relu.alpha_mask_3_0', 16384, 381.0, 0.02325439453125]
['model.relu.alpha_mask_4_0', 16384, 210.0, 0.0128173828125]
['model.relu.alpha_mask_5_0', 8192, 840.0, 0.1025390625]
['model.relu.alpha_mask_6_0', 8192, 736.0, 0.08984375]
['model.relu.alpha_mask_7_0', 8192, 627.0, 0.0765380859375]
['model.relu.alpha_mask_8_0', 8192, 688.0, 0.083984375]
['model.relu.alpha_mask_9_0', 4096, 844.0, 0.2060546875]
['model.relu.alpha_mask_10_0', 4096, 962.0, 0.23486328125]
['model.relu.alpha_mask_11_0', 4096, 912.0, 0.22265625]
['model.relu.alpha_mask_12_0', 4096, 887.0, 0.216552734375]
['model.relu.alpha_mask_13_0', 2048, 1365.0, 0.66650390625]
['model.relu.alpha_mask_14_0', 2048, 1563.0, 0.76318359375]
['model.relu.alpha_mask_15_0', 2048, 1067.0, 0.52099609375]
['model.relu.alpha_mask_16_0', 2048, 600.0, 0.29296875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12738.0, 0.06760572350543478]
########## End ###########
06/01 02:20:50 AM | Train: [20/80] Step 000/390 Loss 0.042 Prec@(1,5) (100.0%, 100.0%)
06/01 02:20:50 AM | layerwise density: [352.0, 454.0, 250.0, 381.0, 210.0, 840.0, 736.0, 627.0, 688.0, 844.0, 962.0, 912.0, 887.0, 1365.0, 1563.0, 1067.0, 600.0]
layerwise density percentage: ['0.005', '0.028', '0.015', '0.023', '0.013', '0.103', '0.090', '0.077', '0.084', '0.206', '0.235', '0.223', '0.217', '0.667', '0.763', '0.521', '0.293']
Global density: 0.06760572642087936
06/01 02:21:00 AM | Train: [20/80] Step 100/390 Loss 0.052 Prec@(1,5) (99.1%, 100.0%)
06/01 02:21:00 AM | layerwise density: [345.0, 447.0, 240.0, 365.0, 206.0, 834.0, 750.0, 631.0, 686.0, 856.0, 963.0, 916.0, 898.0, 1376.0, 1561.0, 1073.0, 616.0]
layerwise density percentage: ['0.005', '0.027', '0.015', '0.022', '0.013', '0.102', '0.092', '0.077', '0.084', '0.209', '0.235', '0.224', '0.219', '0.672', '0.762', '0.524', '0.301']
Global density: 0.06773840636014938
06/01 02:21:11 AM | Train: [20/80] Step 200/390 Loss 0.052 Prec@(1,5) (99.2%, 100.0%)
06/01 02:21:11 AM | layerwise density: [342.0, 457.0, 242.0, 365.0, 210.0, 851.0, 755.0, 657.0, 689.0, 856.0, 954.0, 919.0, 911.0, 1386.0, 1574.0, 1076.0, 630.0]
layerwise density percentage: ['0.005', '0.028', '0.015', '0.022', '0.013', '0.104', '0.092', '0.080', '0.084', '0.209', '0.233', '0.224', '0.222', '0.677', '0.769', '0.525', '0.308']
Global density: 0.06832753121852875
06/01 02:21:21 AM | Train: [20/80] Step 300/390 Loss 0.053 Prec@(1,5) (99.2%, 100.0%)
06/01 02:21:21 AM | layerwise density: [324.0, 399.0, 220.0, 325.0, 174.0, 767.0, 691.0, 628.0, 645.0, 834.0, 939.0, 918.0, 885.0, 1383.0, 1579.0, 1073.0, 636.0]
layerwise density percentage: ['0.005', '0.024', '0.013', '0.020', '0.011', '0.094', '0.084', '0.077', '0.079', '0.204', '0.229', '0.224', '0.216', '0.675', '0.771', '0.524', '0.311']
Global density: 0.06591796875
06/01 02:21:31 AM | Train: [20/80] Step 390/390 Loss 0.054 Prec@(1,5) (99.1%, 100.0%)
06/01 02:21:31 AM | layerwise density: [321.0, 434.0, 234.0, 337.0, 186.0, 807.0, 736.0, 646.0, 679.0, 859.0, 958.0, 926.0, 910.0, 1386.0, 1587.0, 1083.0, 649.0]
layerwise density percentage: ['0.005', '0.026', '0.014', '0.021', '0.011', '0.099', '0.090', '0.079', '0.083', '0.210', '0.234', '0.226', '0.222', '0.677', '0.775', '0.529', '0.317']
Global density: 0.06760572642087936
06/01 02:21:31 AM | Train: [20/200] Final Prec@1 99.0960%
06/01 02:21:31 AM | Valid: [20/200] Step 000/078 Loss 1.166 Prec@(1,5) (74.2%, 89.8%)
06/01 02:21:34 AM | Valid: [20/200] Step 078/078 Loss 1.419 Prec@(1,5) (67.2%, 87.7%)
06/01 02:21:34 AM | Valid: [20/200] Final Prec@1 67.2400%
06/01 02:21:34 AM | Current mask training best Prec@1 = 67.3100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 321.0, 0.0048980712890625]
['model.relu.alpha_mask_1_0', 16384, 434.0, 0.0264892578125]
['model.relu.alpha_mask_2_0', 16384, 234.0, 0.0142822265625]
['model.relu.alpha_mask_3_0', 16384, 337.0, 0.02056884765625]
['model.relu.alpha_mask_4_0', 16384, 186.0, 0.0113525390625]
['model.relu.alpha_mask_5_0', 8192, 807.0, 0.0985107421875]
['model.relu.alpha_mask_6_0', 8192, 736.0, 0.08984375]
['model.relu.alpha_mask_7_0', 8192, 646.0, 0.078857421875]
['model.relu.alpha_mask_8_0', 8192, 678.0, 0.082763671875]
['model.relu.alpha_mask_9_0', 4096, 859.0, 0.209716796875]
['model.relu.alpha_mask_10_0', 4096, 960.0, 0.234375]
['model.relu.alpha_mask_11_0', 4096, 926.0, 0.22607421875]
['model.relu.alpha_mask_12_0', 4096, 909.0, 0.221923828125]
['model.relu.alpha_mask_13_0', 2048, 1386.0, 0.6767578125]
['model.relu.alpha_mask_14_0', 2048, 1586.0, 0.7744140625]
['model.relu.alpha_mask_15_0', 2048, 1083.0, 0.52880859375]
['model.relu.alpha_mask_16_0', 2048, 650.0, 0.3173828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12738.0, 0.06760572350543478]
########## End ###########
06/01 02:21:35 AM | Train: [21/80] Step 000/390 Loss 0.044 Prec@(1,5) (99.2%, 100.0%)
06/01 02:21:35 AM | layerwise density: [321.0, 434.0, 234.0, 337.0, 186.0, 807.0, 736.0, 646.0, 678.0, 859.0, 960.0, 926.0, 909.0, 1386.0, 1586.0, 1083.0, 650.0]
layerwise density percentage: ['0.005', '0.026', '0.014', '0.021', '0.011', '0.099', '0.090', '0.079', '0.083', '0.210', '0.234', '0.226', '0.222', '0.677', '0.774', '0.529', '0.317']
Global density: 0.06760572642087936
06/01 02:21:45 AM | Train: [21/80] Step 100/390 Loss 0.047 Prec@(1,5) (99.3%, 100.0%)
06/01 02:21:45 AM | layerwise density: [328.0, 429.0, 225.0, 335.0, 176.0, 831.0, 724.0, 646.0, 683.0, 874.0, 970.0, 930.0, 913.0, 1385.0, 1579.0, 1083.0, 663.0]
layerwise density percentage: ['0.005', '0.026', '0.014', '0.020', '0.011', '0.101', '0.088', '0.079', '0.083', '0.213', '0.237', '0.227', '0.223', '0.676', '0.771', '0.529', '0.324']
Global density: 0.06779678910970688
06/01 02:21:56 AM | Train: [21/80] Step 200/390 Loss 0.049 Prec@(1,5) (99.3%, 100.0%)
06/01 02:21:56 AM | layerwise density: [313.0, 383.0, 206.0, 309.0, 152.0, 771.0, 678.0, 596.0, 634.0, 853.0, 964.0, 926.0, 883.0, 1370.0, 1576.0, 1077.0, 674.0]
layerwise density percentage: ['0.005', '0.023', '0.013', '0.019', '0.009', '0.094', '0.083', '0.073', '0.077', '0.208', '0.235', '0.226', '0.216', '0.669', '0.770', '0.526', '0.329']
Global density: 0.06562606245279312
06/01 02:22:07 AM | Train: [21/80] Step 300/390 Loss 0.050 Prec@(1,5) (99.2%, 100.0%)
06/01 02:22:07 AM | layerwise density: [310.0, 387.0, 209.0, 313.0, 141.0, 780.0, 690.0, 596.0, 643.0, 856.0, 969.0, 931.0, 884.0, 1394.0, 1592.0, 1089.0, 688.0]
layerwise density percentage: ['0.005', '0.024', '0.013', '0.019', '0.009', '0.095', '0.084', '0.073', '0.078', '0.209', '0.237', '0.227', '0.216', '0.681', '0.777', '0.532', '0.336']
Global density: 0.06619395315647125
06/01 02:22:17 AM | Train: [21/80] Step 390/390 Loss 0.051 Prec@(1,5) (99.2%, 100.0%)
06/01 02:22:17 AM | layerwise density: [306.0, 426.0, 217.0, 334.0, 171.0, 850.0, 735.0, 642.0, 692.0, 884.0, 994.0, 938.0, 903.0, 1408.0, 1598.0, 1099.0, 705.0]
layerwise density percentage: ['0.005', '0.026', '0.013', '0.020', '0.010', '0.104', '0.090', '0.078', '0.084', '0.216', '0.243', '0.229', '0.220', '0.688', '0.780', '0.537', '0.344']
Global density: 0.06847614049911499
06/01 02:22:17 AM | Train: [21/200] Final Prec@1 99.1960%
06/01 02:22:17 AM | Valid: [21/200] Step 000/078 Loss 1.078 Prec@(1,5) (72.7%, 89.8%)
06/01 02:22:19 AM | Valid: [21/200] Step 078/078 Loss 1.405 Prec@(1,5) (67.8%, 88.1%)
06/01 02:22:19 AM | Valid: [21/200] Final Prec@1 67.7600%
06/01 02:22:20 AM | Current mask training best Prec@1 = 67.7600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 305.0, 0.0046539306640625]
['model.relu.alpha_mask_1_0', 16384, 427.0, 0.02606201171875]
['model.relu.alpha_mask_2_0', 16384, 217.0, 0.01324462890625]
['model.relu.alpha_mask_3_0', 16384, 333.0, 0.02032470703125]
['model.relu.alpha_mask_4_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_5_0', 8192, 849.0, 0.1036376953125]
['model.relu.alpha_mask_6_0', 8192, 734.0, 0.089599609375]
['model.relu.alpha_mask_7_0', 8192, 640.0, 0.078125]
['model.relu.alpha_mask_8_0', 8192, 691.0, 0.0843505859375]
['model.relu.alpha_mask_9_0', 4096, 883.0, 0.215576171875]
['model.relu.alpha_mask_10_0', 4096, 990.0, 0.24169921875]
['model.relu.alpha_mask_11_0', 4096, 938.0, 0.22900390625]
['model.relu.alpha_mask_12_0', 4096, 903.0, 0.220458984375]
['model.relu.alpha_mask_13_0', 2048, 1408.0, 0.6875]
['model.relu.alpha_mask_14_0', 2048, 1598.0, 0.7802734375]
['model.relu.alpha_mask_15_0', 2048, 1099.0, 0.53662109375]
['model.relu.alpha_mask_16_0', 2048, 705.0, 0.34423828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12890.0, 0.06841244904891304]
########## End ###########
06/01 02:22:21 AM | Train: [22/80] Step 000/390 Loss 0.038 Prec@(1,5) (99.2%, 100.0%)
06/01 02:22:21 AM | layerwise density: [305.0, 427.0, 217.0, 333.0, 170.0, 849.0, 734.0, 640.0, 691.0, 883.0, 990.0, 938.0, 903.0, 1408.0, 1598.0, 1099.0, 705.0]
layerwise density percentage: ['0.005', '0.026', '0.013', '0.020', '0.010', '0.104', '0.090', '0.078', '0.084', '0.216', '0.242', '0.229', '0.220', '0.688', '0.780', '0.537', '0.344']
Global density: 0.06841245293617249
06/01 02:22:32 AM | Train: [22/80] Step 100/390 Loss 0.044 Prec@(1,5) (99.4%, 100.0%)
06/01 02:22:32 AM | layerwise density: [302.0, 432.0, 214.0, 326.0, 159.0, 816.0, 725.0, 630.0, 682.0, 885.0, 966.0, 939.0, 918.0, 1414.0, 1606.0, 1102.0, 712.0]
layerwise density percentage: ['0.005', '0.026', '0.013', '0.020', '0.010', '0.100', '0.089', '0.077', '0.083', '0.216', '0.236', '0.229', '0.224', '0.690', '0.784', '0.538', '0.348']
Global density: 0.06808339059352875
06/01 02:22:43 AM | Train: [22/80] Step 200/390 Loss 0.044 Prec@(1,5) (99.3%, 100.0%)
06/01 02:22:43 AM | layerwise density: [293.0, 420.0, 205.0, 323.0, 151.0, 777.0, 715.0, 630.0, 663.0, 877.0, 974.0, 941.0, 919.0, 1406.0, 1615.0, 1104.0, 719.0]
layerwise density percentage: ['0.004', '0.026', '0.013', '0.020', '0.009', '0.095', '0.087', '0.077', '0.081', '0.214', '0.238', '0.230', '0.224', '0.687', '0.789', '0.539', '0.351']
Global density: 0.06757388263940811
06/01 02:22:53 AM | Train: [22/80] Step 300/390 Loss 0.045 Prec@(1,5) (99.3%, 100.0%)
06/01 02:22:53 AM | layerwise density: [290.0, 411.0, 206.0, 316.0, 148.0, 803.0, 710.0, 625.0, 677.0, 901.0, 982.0, 953.0, 927.0, 1417.0, 1613.0, 1107.0, 739.0]
layerwise density percentage: ['0.004', '0.025', '0.013', '0.019', '0.009', '0.098', '0.087', '0.076', '0.083', '0.220', '0.240', '0.233', '0.226', '0.692', '0.788', '0.541', '0.361']
Global density: 0.06806746870279312
06/01 02:23:03 AM | Train: [22/80] Step 390/390 Loss 0.046 Prec@(1,5) (99.3%, 100.0%)
06/01 02:23:03 AM | layerwise density: [284.0, 405.0, 206.0, 303.0, 145.0, 804.0, 714.0, 612.0, 659.0, 897.0, 979.0, 957.0, 913.0, 1417.0, 1618.0, 1116.0, 745.0]
layerwise density percentage: ['0.004', '0.025', '0.013', '0.018', '0.009', '0.098', '0.087', '0.075', '0.080', '0.219', '0.239', '0.234', '0.223', '0.692', '0.790', '0.545', '0.364']
Global density: 0.06779678910970688
06/01 02:23:03 AM | Train: [22/200] Final Prec@1 99.3180%
06/01 02:23:03 AM | Valid: [22/200] Step 000/078 Loss 0.998 Prec@(1,5) (77.3%, 91.4%)
06/01 02:23:06 AM | Valid: [22/200] Step 078/078 Loss 1.426 Prec@(1,5) (67.4%, 87.9%)
06/01 02:23:06 AM | Valid: [22/200] Final Prec@1 67.3900%
06/01 02:23:06 AM | Current mask training best Prec@1 = 67.7600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 283.0, 0.0043182373046875]
['model.relu.alpha_mask_1_0', 16384, 406.0, 0.0247802734375]
['model.relu.alpha_mask_2_0', 16384, 206.0, 0.0125732421875]
['model.relu.alpha_mask_3_0', 16384, 304.0, 0.0185546875]
['model.relu.alpha_mask_4_0', 16384, 146.0, 0.0089111328125]
['model.relu.alpha_mask_5_0', 8192, 805.0, 0.0982666015625]
['model.relu.alpha_mask_6_0', 8192, 714.0, 0.087158203125]
['model.relu.alpha_mask_7_0', 8192, 612.0, 0.07470703125]
['model.relu.alpha_mask_8_0', 8192, 661.0, 0.0806884765625]
['model.relu.alpha_mask_9_0', 4096, 898.0, 0.21923828125]
['model.relu.alpha_mask_10_0', 4096, 981.0, 0.239501953125]
['model.relu.alpha_mask_11_0', 4096, 957.0, 0.233642578125]
['model.relu.alpha_mask_12_0', 4096, 915.0, 0.223388671875]
['model.relu.alpha_mask_13_0', 2048, 1417.0, 0.69189453125]
['model.relu.alpha_mask_14_0', 2048, 1618.0, 0.7900390625]
['model.relu.alpha_mask_15_0', 2048, 1116.0, 0.544921875]
['model.relu.alpha_mask_16_0', 2048, 745.0, 0.36376953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12784.0, 0.06784986413043478]
########## End ###########
06/01 02:23:07 AM | Train: [23/80] Step 000/390 Loss 0.056 Prec@(1,5) (98.4%, 100.0%)
06/01 02:23:07 AM | layerwise density: [283.0, 406.0, 206.0, 304.0, 146.0, 805.0, 714.0, 612.0, 661.0, 898.0, 981.0, 957.0, 915.0, 1417.0, 1618.0, 1116.0, 745.0]
layerwise density percentage: ['0.004', '0.025', '0.013', '0.019', '0.009', '0.098', '0.087', '0.075', '0.081', '0.219', '0.240', '0.234', '0.223', '0.692', '0.790', '0.545', '0.364']
Global density: 0.06784986704587936
06/01 02:23:17 AM | Train: [23/80] Step 100/390 Loss 0.040 Prec@(1,5) (99.5%, 100.0%)
06/01 02:23:17 AM | layerwise density: [283.0, 402.0, 205.0, 302.0, 147.0, 784.0, 693.0, 610.0, 653.0, 873.0, 980.0, 953.0, 921.0, 1409.0, 1617.0, 1111.0, 750.0]
layerwise density percentage: ['0.004', '0.025', '0.013', '0.018', '0.009', '0.096', '0.085', '0.074', '0.080', '0.213', '0.239', '0.233', '0.225', '0.688', '0.790', '0.542', '0.366']
Global density: 0.06736689060926437
06/01 02:23:28 AM | Train: [23/80] Step 200/390 Loss 0.039 Prec@(1,5) (99.5%, 100.0%)
06/01 02:23:28 AM | layerwise density: [278.0, 395.0, 210.0, 304.0, 150.0, 795.0, 715.0, 609.0, 658.0, 888.0, 995.0, 953.0, 911.0, 1412.0, 1642.0, 1115.0, 767.0]
layerwise density percentage: ['0.004', '0.024', '0.013', '0.019', '0.009', '0.097', '0.087', '0.074', '0.080', '0.217', '0.243', '0.233', '0.222', '0.689', '0.802', '0.544', '0.375']
Global density: 0.06791885942220688
06/01 02:23:39 AM | Train: [23/80] Step 300/390 Loss 0.040 Prec@(1,5) (99.4%, 100.0%)
06/01 02:23:39 AM | layerwise density: [274.0, 388.0, 206.0, 297.0, 141.0, 793.0, 715.0, 613.0, 647.0, 898.0, 983.0, 956.0, 922.0, 1423.0, 1639.0, 1124.0, 781.0]
layerwise density percentage: ['0.004', '0.024', '0.013', '0.018', '0.009', '0.097', '0.087', '0.075', '0.079', '0.219', '0.240', '0.233', '0.225', '0.695', '0.800', '0.549', '0.381']
Global density: 0.0679347813129425
06/01 02:23:49 AM | Train: [23/80] Step 390/390 Loss 0.041 Prec@(1,5) (99.4%, 100.0%)
06/01 02:23:49 AM | layerwise density: [272.0, 376.0, 201.0, 277.0, 141.0, 764.0, 703.0, 599.0, 636.0, 899.0, 979.0, 961.0, 921.0, 1419.0, 1626.0, 1122.0, 786.0]
layerwise density percentage: ['0.004', '0.023', '0.012', '0.017', '0.009', '0.093', '0.086', '0.073', '0.078', '0.219', '0.239', '0.235', '0.225', '0.693', '0.794', '0.548', '0.384']
Global density: 0.06730850785970688
06/01 02:23:49 AM | Train: [23/200] Final Prec@1 99.3720%
06/01 02:23:49 AM | Valid: [23/200] Step 000/078 Loss 1.275 Prec@(1,5) (72.7%, 86.7%)
06/01 02:23:51 AM | Valid: [23/200] Step 078/078 Loss 1.438 Prec@(1,5) (66.6%, 87.8%)
06/01 02:23:51 AM | Valid: [23/200] Final Prec@1 66.5800%
06/01 02:23:51 AM | Current mask training best Prec@1 = 67.7600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 272.0, 0.004150390625]
['model.relu.alpha_mask_1_0', 16384, 375.0, 0.02288818359375]
['model.relu.alpha_mask_2_0', 16384, 201.0, 0.01226806640625]
['model.relu.alpha_mask_3_0', 16384, 275.0, 0.01678466796875]
['model.relu.alpha_mask_4_0', 16384, 141.0, 0.00860595703125]
['model.relu.alpha_mask_5_0', 8192, 768.0, 0.09375]
['model.relu.alpha_mask_6_0', 8192, 704.0, 0.0859375]
['model.relu.alpha_mask_7_0', 8192, 599.0, 0.0731201171875]
['model.relu.alpha_mask_8_0', 8192, 636.0, 0.07763671875]
['model.relu.alpha_mask_9_0', 4096, 899.0, 0.219482421875]
['model.relu.alpha_mask_10_0', 4096, 979.0, 0.239013671875]
['model.relu.alpha_mask_11_0', 4096, 960.0, 0.234375]
['model.relu.alpha_mask_12_0', 4096, 921.0, 0.224853515625]
['model.relu.alpha_mask_13_0', 2048, 1417.0, 0.69189453125]
['model.relu.alpha_mask_14_0', 2048, 1624.0, 0.79296875]
['model.relu.alpha_mask_15_0', 2048, 1121.0, 0.54736328125]
['model.relu.alpha_mask_16_0', 2048, 786.0, 0.3837890625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12678.0, 0.06728727921195653]
########## End ###########
06/01 02:23:52 AM | Train: [24/80] Step 000/390 Loss 0.034 Prec@(1,5) (99.2%, 100.0%)
06/01 02:23:52 AM | layerwise density: [272.0, 375.0, 201.0, 275.0, 141.0, 768.0, 704.0, 599.0, 636.0, 899.0, 979.0, 960.0, 921.0, 1417.0, 1624.0, 1121.0, 786.0]
layerwise density percentage: ['0.004', '0.023', '0.012', '0.017', '0.009', '0.094', '0.086', '0.073', '0.078', '0.219', '0.239', '0.234', '0.225', '0.692', '0.793', '0.547', '0.384']
Global density: 0.06728728115558624
06/01 02:24:03 AM | Train: [24/80] Step 100/390 Loss 0.036 Prec@(1,5) (99.6%, 100.0%)
06/01 02:24:03 AM | layerwise density: [269.0, 368.0, 192.0, 279.0, 149.0, 788.0, 699.0, 623.0, 646.0, 907.0, 995.0, 961.0, 933.0, 1420.0, 1639.0, 1124.0, 796.0]
layerwise density percentage: ['0.004', '0.022', '0.012', '0.017', '0.009', '0.096', '0.085', '0.076', '0.079', '0.221', '0.243', '0.235', '0.228', '0.693', '0.800', '0.549', '0.389']
Global density: 0.06787109375
06/01 02:24:14 AM | Train: [24/80] Step 200/390 Loss 0.037 Prec@(1,5) (99.5%, 100.0%)
06/01 02:24:14 AM | layerwise density: [262.0, 363.0, 192.0, 278.0, 139.0, 777.0, 697.0, 625.0, 644.0, 909.0, 1002.0, 964.0, 923.0, 1428.0, 1645.0, 1127.0, 804.0]
layerwise density percentage: ['0.004', '0.022', '0.012', '0.017', '0.008', '0.095', '0.085', '0.076', '0.079', '0.222', '0.245', '0.235', '0.225', '0.697', '0.803', '0.550', '0.393']
Global density: 0.06782332807779312
06/01 02:24:25 AM | Train: [24/80] Step 300/390 Loss 0.038 Prec@(1,5) (99.5%, 100.0%)
06/01 02:24:25 AM | layerwise density: [260.0, 330.0, 178.0, 262.0, 121.0, 706.0, 631.0, 576.0, 588.0, 890.0, 968.0, 958.0, 898.0, 1436.0, 1628.0, 1122.0, 808.0]
layerwise density percentage: ['0.004', '0.020', '0.011', '0.016', '0.007', '0.086', '0.077', '0.070', '0.072', '0.217', '0.236', '0.234', '0.219', '0.701', '0.795', '0.548', '0.395']
Global density: 0.06559952348470688
06/01 02:24:35 AM | Train: [24/80] Step 390/390 Loss 0.039 Prec@(1,5) (99.5%, 100.0%)
06/01 02:24:35 AM | layerwise density: [255.0, 340.0, 184.0, 255.0, 120.0, 737.0, 665.0, 568.0, 605.0, 900.0, 975.0, 963.0, 915.0, 1439.0, 1658.0, 1131.0, 821.0]
layerwise density percentage: ['0.004', '0.021', '0.011', '0.016', '0.007', '0.090', '0.081', '0.069', '0.074', '0.220', '0.238', '0.235', '0.223', '0.703', '0.810', '0.552', '0.401']
Global density: 0.06650709360837936
06/01 02:24:35 AM | Train: [24/200] Final Prec@1 99.4700%
06/01 02:24:35 AM | Valid: [24/200] Step 000/078 Loss 1.115 Prec@(1,5) (73.4%, 89.1%)
06/01 02:24:38 AM | Valid: [24/200] Step 078/078 Loss 1.407 Prec@(1,5) (67.4%, 88.5%)
06/01 02:24:38 AM | Valid: [24/200] Final Prec@1 67.4400%
06/01 02:24:38 AM | Current mask training best Prec@1 = 67.7600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 255.0, 0.0038909912109375]
['model.relu.alpha_mask_1_0', 16384, 340.0, 0.020751953125]
['model.relu.alpha_mask_2_0', 16384, 184.0, 0.01123046875]
['model.relu.alpha_mask_3_0', 16384, 255.0, 0.01556396484375]
['model.relu.alpha_mask_4_0', 16384, 120.0, 0.00732421875]
['model.relu.alpha_mask_5_0', 8192, 737.0, 0.0899658203125]
['model.relu.alpha_mask_6_0', 8192, 665.0, 0.0811767578125]
['model.relu.alpha_mask_7_0', 8192, 568.0, 0.0693359375]
['model.relu.alpha_mask_8_0', 8192, 605.0, 0.0738525390625]
['model.relu.alpha_mask_9_0', 4096, 900.0, 0.2197265625]
['model.relu.alpha_mask_10_0', 4096, 975.0, 0.238037109375]
['model.relu.alpha_mask_11_0', 4096, 963.0, 0.235107421875]
['model.relu.alpha_mask_12_0', 4096, 915.0, 0.223388671875]
['model.relu.alpha_mask_13_0', 2048, 1439.0, 0.70263671875]
['model.relu.alpha_mask_14_0', 2048, 1658.0, 0.8095703125]
['model.relu.alpha_mask_15_0', 2048, 1131.0, 0.55224609375]
['model.relu.alpha_mask_16_0', 2048, 821.0, 0.40087890625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12531.0, 0.06650709069293478]
########## End ###########
06/01 02:24:39 AM | Train: [25/80] Step 000/390 Loss 0.026 Prec@(1,5) (100.0%, 100.0%)
06/01 02:24:39 AM | layerwise density: [255.0, 340.0, 184.0, 255.0, 120.0, 737.0, 665.0, 568.0, 605.0, 900.0, 975.0, 963.0, 915.0, 1439.0, 1658.0, 1131.0, 821.0]
layerwise density percentage: ['0.004', '0.021', '0.011', '0.016', '0.007', '0.090', '0.081', '0.069', '0.074', '0.220', '0.238', '0.235', '0.223', '0.703', '0.810', '0.552', '0.401']
Global density: 0.06650709360837936
06/01 02:24:49 AM | Train: [25/80] Step 100/390 Loss 0.036 Prec@(1,5) (99.5%, 100.0%)
06/01 02:24:49 AM | layerwise density: [252.0, 378.0, 192.0, 273.0, 130.0, 797.0, 689.0, 603.0, 643.0, 937.0, 995.0, 972.0, 936.0, 1443.0, 1672.0, 1132.0, 835.0]
layerwise density percentage: ['0.004', '0.023', '0.012', '0.017', '0.008', '0.097', '0.084', '0.074', '0.078', '0.229', '0.243', '0.237', '0.229', '0.705', '0.816', '0.553', '0.408']
Global density: 0.06835407018661499
06/01 02:25:00 AM | Train: [25/80] Step 200/390 Loss 0.037 Prec@(1,5) (99.5%, 100.0%)
06/01 02:25:00 AM | layerwise density: [245.0, 321.0, 174.0, 239.0, 115.0, 705.0, 618.0, 546.0, 576.0, 888.0, 957.0, 959.0, 903.0, 1438.0, 1645.0, 1129.0, 839.0]
layerwise density percentage: ['0.004', '0.020', '0.011', '0.015', '0.007', '0.086', '0.075', '0.067', '0.070', '0.217', '0.234', '0.234', '0.220', '0.702', '0.803', '0.551', '0.410']
Global density: 0.06526515632867813
06/01 02:25:11 AM | Train: [25/80] Step 300/390 Loss 0.037 Prec@(1,5) (99.5%, 100.0%)
06/01 02:25:11 AM | layerwise density: [252.0, 340.0, 169.0, 249.0, 118.0, 756.0, 659.0, 572.0, 611.0, 937.0, 978.0, 964.0, 924.0, 1453.0, 1673.0, 1139.0, 854.0]
layerwise density percentage: ['0.004', '0.021', '0.010', '0.015', '0.007', '0.092', '0.080', '0.070', '0.075', '0.229', '0.239', '0.235', '0.226', '0.709', '0.817', '0.556', '0.417']
Global density: 0.06712805479764938
06/01 02:25:20 AM | Train: [25/80] Step 390/390 Loss 0.036 Prec@(1,5) (99.5%, 100.0%)
06/01 02:25:20 AM | layerwise density: [244.0, 342.0, 175.0, 260.0, 125.0, 776.0, 675.0, 596.0, 631.0, 925.0, 987.0, 978.0, 926.0, 1462.0, 1670.0, 1147.0, 864.0]
layerwise density percentage: ['0.004', '0.021', '0.011', '0.016', '0.008', '0.095', '0.082', '0.073', '0.077', '0.226', '0.241', '0.239', '0.226', '0.714', '0.815', '0.560', '0.422']
Global density: 0.06784455478191376
06/01 02:25:20 AM | Train: [25/200] Final Prec@1 99.5340%
06/01 02:25:21 AM | Valid: [25/200] Step 000/078 Loss 1.005 Prec@(1,5) (76.6%, 90.6%)
06/01 02:25:23 AM | Valid: [25/200] Step 078/078 Loss 1.384 Prec@(1,5) (68.2%, 88.4%)
06/01 02:25:23 AM | Valid: [25/200] Final Prec@1 68.2500%
06/01 02:25:23 AM | Current mask training best Prec@1 = 68.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 244.0, 0.00372314453125]
['model.relu.alpha_mask_1_0', 16384, 342.0, 0.0208740234375]
['model.relu.alpha_mask_2_0', 16384, 175.0, 0.01068115234375]
['model.relu.alpha_mask_3_0', 16384, 260.0, 0.015869140625]
['model.relu.alpha_mask_4_0', 16384, 125.0, 0.00762939453125]
['model.relu.alpha_mask_5_0', 8192, 776.0, 0.0947265625]
['model.relu.alpha_mask_6_0', 8192, 675.0, 0.0823974609375]
['model.relu.alpha_mask_7_0', 8192, 596.0, 0.07275390625]
['model.relu.alpha_mask_8_0', 8192, 631.0, 0.0770263671875]
['model.relu.alpha_mask_9_0', 4096, 925.0, 0.225830078125]
['model.relu.alpha_mask_10_0', 4096, 987.0, 0.240966796875]
['model.relu.alpha_mask_11_0', 4096, 978.0, 0.23876953125]
['model.relu.alpha_mask_12_0', 4096, 926.0, 0.22607421875]
['model.relu.alpha_mask_13_0', 2048, 1462.0, 0.7138671875]
['model.relu.alpha_mask_14_0', 2048, 1671.0, 0.81591796875]
['model.relu.alpha_mask_15_0', 2048, 1147.0, 0.56005859375]
['model.relu.alpha_mask_16_0', 2048, 864.0, 0.421875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12784.0, 0.06784986413043478]
########## End ###########
06/01 02:25:24 AM | Train: [26/80] Step 000/390 Loss 0.017 Prec@(1,5) (100.0%, 100.0%)
06/01 02:25:24 AM | layerwise density: [244.0, 342.0, 175.0, 260.0, 125.0, 776.0, 675.0, 596.0, 631.0, 925.0, 987.0, 978.0, 926.0, 1462.0, 1671.0, 1147.0, 864.0]
layerwise density percentage: ['0.004', '0.021', '0.011', '0.016', '0.008', '0.095', '0.082', '0.073', '0.077', '0.226', '0.241', '0.239', '0.226', '0.714', '0.816', '0.560', '0.422']
Global density: 0.06784986704587936
06/01 02:25:35 AM | Train: [26/80] Step 100/390 Loss 0.032 Prec@(1,5) (99.6%, 100.0%)
06/01 02:25:35 AM | layerwise density: [243.0, 340.0, 180.0, 245.0, 117.0, 792.0, 670.0, 599.0, 625.0, 914.0, 979.0, 978.0, 937.0, 1456.0, 1677.0, 1144.0, 868.0]
layerwise density percentage: ['0.004', '0.021', '0.011', '0.015', '0.007', '0.097', '0.082', '0.073', '0.076', '0.223', '0.239', '0.239', '0.229', '0.711', '0.819', '0.559', '0.424']
Global density: 0.06774371862411499
06/01 02:25:46 AM | Train: [26/80] Step 200/390 Loss 0.032 Prec@(1,5) (99.6%, 100.0%)
06/01 02:25:46 AM | layerwise density: [236.0, 343.0, 173.0, 232.0, 111.0, 773.0, 650.0, 587.0, 621.0, 896.0, 972.0, 976.0, 935.0, 1453.0, 1673.0, 1141.0, 880.0]
layerwise density percentage: ['0.004', '0.021', '0.011', '0.014', '0.007', '0.094', '0.079', '0.072', '0.076', '0.219', '0.237', '0.238', '0.228', '0.709', '0.817', '0.557', '0.430']
Global density: 0.06714928895235062
06/01 02:25:57 AM | Train: [26/80] Step 300/390 Loss 0.033 Prec@(1,5) (99.6%, 100.0%)
06/01 02:25:57 AM | layerwise density: [240.0, 344.0, 171.0, 241.0, 99.0, 779.0, 645.0, 598.0, 630.0, 915.0, 985.0, 984.0, 940.0, 1458.0, 1688.0, 1144.0, 888.0]
layerwise density percentage: ['0.004', '0.021', '0.010', '0.015', '0.006', '0.095', '0.079', '0.073', '0.077', '0.223', '0.240', '0.240', '0.229', '0.712', '0.824', '0.559', '0.434']
Global density: 0.06766410917043686
06/01 02:26:06 AM | Train: [26/80] Step 390/390 Loss 0.033 Prec@(1,5) (99.6%, 100.0%)
06/01 02:26:06 AM | layerwise density: [227.0, 314.0, 164.0, 214.0, 94.0, 689.0, 598.0, 548.0, 576.0, 895.0, 975.0, 986.0, 907.0, 1458.0, 1663.0, 1140.0, 891.0]
layerwise density percentage: ['0.003', '0.019', '0.010', '0.013', '0.006', '0.084', '0.073', '0.067', '0.070', '0.219', '0.238', '0.241', '0.221', '0.712', '0.812', '0.557', '0.435']
Global density: 0.0654880702495575
06/01 02:26:06 AM | Train: [26/200] Final Prec@1 99.5600%
06/01 02:26:07 AM | Valid: [26/200] Step 000/078 Loss 1.151 Prec@(1,5) (69.5%, 89.1%)
06/01 02:26:09 AM | Valid: [26/200] Step 078/078 Loss 1.394 Prec@(1,5) (68.2%, 88.3%)
06/01 02:26:09 AM | Valid: [26/200] Final Prec@1 68.1600%
06/01 02:26:09 AM | Current mask training best Prec@1 = 68.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 227.0, 0.0034637451171875]
['model.relu.alpha_mask_1_0', 16384, 314.0, 0.0191650390625]
['model.relu.alpha_mask_2_0', 16384, 164.0, 0.010009765625]
['model.relu.alpha_mask_3_0', 16384, 213.0, 0.01300048828125]
['model.relu.alpha_mask_4_0', 16384, 94.0, 0.0057373046875]
['model.relu.alpha_mask_5_0', 8192, 688.0, 0.083984375]
['model.relu.alpha_mask_6_0', 8192, 598.0, 0.072998046875]
['model.relu.alpha_mask_7_0', 8192, 546.0, 0.066650390625]
['model.relu.alpha_mask_8_0', 8192, 575.0, 0.0701904296875]
['model.relu.alpha_mask_9_0', 4096, 891.0, 0.217529296875]
['model.relu.alpha_mask_10_0', 4096, 974.0, 0.23779296875]
['model.relu.alpha_mask_11_0', 4096, 986.0, 0.24072265625]
['model.relu.alpha_mask_12_0', 4096, 907.0, 0.221435546875]
['model.relu.alpha_mask_13_0', 2048, 1458.0, 0.7119140625]
['model.relu.alpha_mask_14_0', 2048, 1663.0, 0.81201171875]
['model.relu.alpha_mask_15_0', 2048, 1140.0, 0.556640625]
['model.relu.alpha_mask_16_0', 2048, 891.0, 0.43505859375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12329.0, 0.06543499490489131]
########## End ###########
06/01 02:26:10 AM | Train: [27/80] Step 000/390 Loss 0.026 Prec@(1,5) (99.2%, 100.0%)
06/01 02:26:10 AM | layerwise density: [227.0, 314.0, 164.0, 213.0, 94.0, 688.0, 598.0, 546.0, 575.0, 891.0, 974.0, 986.0, 907.0, 1458.0, 1663.0, 1140.0, 891.0]
layerwise density percentage: ['0.003', '0.019', '0.010', '0.013', '0.006', '0.084', '0.073', '0.067', '0.070', '0.218', '0.238', '0.241', '0.221', '0.712', '0.812', '0.557', '0.435']
Global density: 0.0654349997639656
06/01 02:26:21 AM | Train: [27/80] Step 100/390 Loss 0.031 Prec@(1,5) (99.6%, 100.0%)
06/01 02:26:21 AM | layerwise density: [226.0, 306.0, 162.0, 209.0, 97.0, 711.0, 592.0, 546.0, 572.0, 909.0, 985.0, 988.0, 923.0, 1465.0, 1689.0, 1153.0, 903.0]
layerwise density percentage: ['0.003', '0.019', '0.010', '0.013', '0.006', '0.087', '0.072', '0.067', '0.070', '0.222', '0.240', '0.241', '0.225', '0.715', '0.825', '0.563', '0.441']
Global density: 0.06600289046764374
06/01 02:26:31 AM | Train: [27/80] Step 200/390 Loss 0.030 Prec@(1,5) (99.6%, 100.0%)
06/01 02:26:31 AM | layerwise density: [222.0, 335.0, 170.0, 225.0, 110.0, 767.0, 630.0, 577.0, 588.0, 931.0, 1013.0, 995.0, 954.0, 1474.0, 1694.0, 1158.0, 919.0]
layerwise density percentage: ['0.003', '0.020', '0.010', '0.014', '0.007', '0.094', '0.077', '0.070', '0.072', '0.227', '0.247', '0.243', '0.233', '0.720', '0.827', '0.565', '0.449']
Global density: 0.06773310154676437
06/01 02:26:42 AM | Train: [27/80] Step 300/390 Loss 0.030 Prec@(1,5) (99.6%, 100.0%)
06/01 02:26:42 AM | layerwise density: [220.0, 311.0, 167.0, 225.0, 108.0, 746.0, 644.0, 568.0, 582.0, 919.0, 1004.0, 1003.0, 938.0, 1477.0, 1687.0, 1160.0, 928.0]
layerwise density percentage: ['0.003', '0.019', '0.010', '0.014', '0.007', '0.091', '0.079', '0.069', '0.071', '0.224', '0.245', '0.245', '0.229', '0.721', '0.824', '0.566', '0.453']
Global density: 0.06733504682779312
06/01 02:26:52 AM | Train: [27/80] Step 390/390 Loss 0.031 Prec@(1,5) (99.6%, 100.0%)
06/01 02:26:52 AM | layerwise density: [218.0, 329.0, 170.0, 228.0, 116.0, 762.0, 656.0, 584.0, 598.0, 933.0, 1010.0, 1005.0, 948.0, 1480.0, 1703.0, 1171.0, 941.0]
layerwise density percentage: ['0.003', '0.020', '0.010', '0.014', '0.007', '0.093', '0.080', '0.071', '0.073', '0.228', '0.247', '0.245', '0.231', '0.723', '0.832', '0.572', '0.459']
Global density: 0.06821076571941376
06/01 02:26:52 AM | Train: [27/200] Final Prec@1 99.5940%
06/01 02:26:52 AM | Valid: [27/200] Step 000/078 Loss 1.093 Prec@(1,5) (76.6%, 91.4%)
06/01 02:26:54 AM | Valid: [27/200] Step 078/078 Loss 1.378 Prec@(1,5) (68.2%, 88.2%)
06/01 02:26:54 AM | Valid: [27/200] Final Prec@1 68.2100%
06/01 02:26:54 AM | Current mask training best Prec@1 = 68.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 218.0, 0.003326416015625]
['model.relu.alpha_mask_1_0', 16384, 331.0, 0.02020263671875]
['model.relu.alpha_mask_2_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_3_0', 16384, 228.0, 0.013916015625]
['model.relu.alpha_mask_4_0', 16384, 115.0, 0.00701904296875]
['model.relu.alpha_mask_5_0', 8192, 763.0, 0.0931396484375]
['model.relu.alpha_mask_6_0', 8192, 656.0, 0.080078125]
['model.relu.alpha_mask_7_0', 8192, 584.0, 0.0712890625]
['model.relu.alpha_mask_8_0', 8192, 599.0, 0.0731201171875]
['model.relu.alpha_mask_9_0', 4096, 931.0, 0.227294921875]
['model.relu.alpha_mask_10_0', 4096, 1009.0, 0.246337890625]
['model.relu.alpha_mask_11_0', 4096, 1005.0, 0.245361328125]
['model.relu.alpha_mask_12_0', 4096, 948.0, 0.2314453125]
['model.relu.alpha_mask_13_0', 2048, 1483.0, 0.72412109375]
['model.relu.alpha_mask_14_0', 2048, 1701.0, 0.83056640625]
['model.relu.alpha_mask_15_0', 2048, 1171.0, 0.57177734375]
['model.relu.alpha_mask_16_0', 2048, 941.0, 0.45947265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12853.0, 0.06821607506793478]
########## End ###########
06/01 02:26:55 AM | Train: [28/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/01 02:26:55 AM | layerwise density: [218.0, 331.0, 170.0, 228.0, 115.0, 763.0, 656.0, 584.0, 599.0, 931.0, 1009.0, 1005.0, 948.0, 1483.0, 1701.0, 1171.0, 941.0]
layerwise density percentage: ['0.003', '0.020', '0.010', '0.014', '0.007', '0.093', '0.080', '0.071', '0.073', '0.227', '0.246', '0.245', '0.231', '0.724', '0.831', '0.572', '0.459']
Global density: 0.06821607798337936
06/01 02:27:06 AM | Train: [28/80] Step 100/390 Loss 0.027 Prec@(1,5) (99.7%, 100.0%)
06/01 02:27:06 AM | layerwise density: [210.0, 318.0, 165.0, 221.0, 103.0, 718.0, 631.0, 573.0, 593.0, 936.0, 1000.0, 999.0, 935.0, 1467.0, 1689.0, 1166.0, 946.0]
layerwise density percentage: ['0.003', '0.019', '0.010', '0.013', '0.006', '0.088', '0.077', '0.070', '0.072', '0.229', '0.244', '0.244', '0.228', '0.716', '0.825', '0.569', '0.462']
Global density: 0.06724482029676437
06/01 02:27:16 AM | Train: [28/80] Step 200/390 Loss 0.027 Prec@(1,5) (99.7%, 100.0%)
06/01 02:27:16 AM | layerwise density: [212.0, 325.0, 163.0, 232.0, 104.0, 744.0, 661.0, 589.0, 616.0, 952.0, 1006.0, 1007.0, 962.0, 1479.0, 1708.0, 1170.0, 958.0]
layerwise density percentage: ['0.003', '0.020', '0.010', '0.014', '0.006', '0.091', '0.081', '0.072', '0.075', '0.232', '0.246', '0.246', '0.235', '0.722', '0.834', '0.571', '0.468']
Global density: 0.06840183585882187
06/01 02:27:26 AM | Train: [28/80] Step 300/390 Loss 0.028 Prec@(1,5) (99.7%, 100.0%)
06/01 02:27:26 AM | layerwise density: [205.0, 310.0, 159.0, 215.0, 100.0, 713.0, 650.0, 569.0, 579.0, 931.0, 992.0, 1009.0, 951.0, 1486.0, 1707.0, 1168.0, 963.0]
layerwise density percentage: ['0.003', '0.019', '0.010', '0.013', '0.006', '0.087', '0.079', '0.069', '0.071', '0.227', '0.242', '0.246', '0.232', '0.726', '0.833', '0.570', '0.470']
Global density: 0.0674411952495575
06/01 02:27:35 AM | Train: [28/80] Step 390/390 Loss 0.029 Prec@(1,5) (99.6%, 100.0%)
06/01 02:27:35 AM | layerwise density: [206.0, 303.0, 161.0, 229.0, 103.0, 748.0, 656.0, 582.0, 606.0, 945.0, 1004.0, 1020.0, 969.0, 1485.0, 1709.0, 1172.0, 972.0]
layerwise density percentage: ['0.003', '0.018', '0.010', '0.014', '0.006', '0.091', '0.080', '0.071', '0.074', '0.231', '0.245', '0.249', '0.237', '0.725', '0.834', '0.572', '0.475']
Global density: 0.06830630451440811
06/01 02:27:35 AM | Train: [28/200] Final Prec@1 99.6360%
06/01 02:27:35 AM | Valid: [28/200] Step 000/078 Loss 1.108 Prec@(1,5) (78.9%, 92.2%)
06/01 02:27:38 AM | Valid: [28/200] Step 078/078 Loss 1.420 Prec@(1,5) (68.0%, 88.3%)
06/01 02:27:38 AM | Valid: [28/200] Final Prec@1 68.0300%
06/01 02:27:38 AM | Current mask training best Prec@1 = 68.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 206.0, 0.003143310546875]
['model.relu.alpha_mask_1_0', 16384, 303.0, 0.01849365234375]
['model.relu.alpha_mask_2_0', 16384, 161.0, 0.00982666015625]
['model.relu.alpha_mask_3_0', 16384, 230.0, 0.0140380859375]
['model.relu.alpha_mask_4_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_5_0', 8192, 749.0, 0.0914306640625]
['model.relu.alpha_mask_6_0', 8192, 655.0, 0.0799560546875]
['model.relu.alpha_mask_7_0', 8192, 582.0, 0.071044921875]
['model.relu.alpha_mask_8_0', 8192, 607.0, 0.0740966796875]
['model.relu.alpha_mask_9_0', 4096, 946.0, 0.23095703125]
['model.relu.alpha_mask_10_0', 4096, 1003.0, 0.244873046875]
['model.relu.alpha_mask_11_0', 4096, 1020.0, 0.2490234375]
['model.relu.alpha_mask_12_0', 4096, 970.0, 0.23681640625]
['model.relu.alpha_mask_13_0', 2048, 1486.0, 0.7255859375]
['model.relu.alpha_mask_14_0', 2048, 1709.0, 0.83447265625]
['model.relu.alpha_mask_15_0', 2048, 1172.0, 0.572265625]
['model.relu.alpha_mask_16_0', 2048, 972.0, 0.474609375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12874.0, 0.06832753057065218]
########## End ###########
06/01 02:27:39 AM | Train: [29/80] Step 000/390 Loss 0.064 Prec@(1,5) (99.2%, 100.0%)
06/01 02:27:39 AM | layerwise density: [206.0, 303.0, 161.0, 230.0, 103.0, 749.0, 655.0, 582.0, 607.0, 946.0, 1003.0, 1020.0, 970.0, 1486.0, 1709.0, 1172.0, 972.0]
layerwise density percentage: ['0.003', '0.018', '0.010', '0.014', '0.006', '0.091', '0.080', '0.071', '0.074', '0.231', '0.245', '0.249', '0.237', '0.726', '0.834', '0.572', '0.475']
Global density: 0.06832753121852875
06/01 02:27:49 AM | Train: [29/80] Step 100/390 Loss 0.028 Prec@(1,5) (99.7%, 100.0%)
06/01 02:27:49 AM | layerwise density: [204.0, 294.0, 155.0, 220.0, 107.0, 740.0, 619.0, 567.0, 562.0, 933.0, 994.0, 1015.0, 956.0, 1470.0, 1702.0, 1162.0, 976.0]
layerwise density percentage: ['0.003', '0.018', '0.009', '0.013', '0.007', '0.090', '0.076', '0.069', '0.069', '0.228', '0.243', '0.248', '0.233', '0.718', '0.831', '0.567', '0.477']
Global density: 0.06727666407823563
06/01 02:27:59 AM | Train: [29/80] Step 200/390 Loss 0.028 Prec@(1,5) (99.7%, 100.0%)
06/01 02:27:59 AM | layerwise density: [209.0, 304.0, 158.0, 223.0, 109.0, 761.0, 634.0, 579.0, 587.0, 946.0, 1000.0, 1025.0, 961.0, 1491.0, 1721.0, 1170.0, 983.0]
layerwise density percentage: ['0.003', '0.019', '0.010', '0.014', '0.007', '0.093', '0.077', '0.071', '0.072', '0.231', '0.244', '0.250', '0.235', '0.728', '0.840', '0.571', '0.480']
Global density: 0.06825853884220123
06/01 02:28:09 AM | Train: [29/80] Step 300/390 Loss 0.028 Prec@(1,5) (99.6%, 100.0%)
06/01 02:28:09 AM | layerwise density: [203.0, 291.0, 141.0, 214.0, 99.0, 724.0, 610.0, 562.0, 567.0, 938.0, 992.0, 1020.0, 942.0, 1467.0, 1719.0, 1164.0, 990.0]
layerwise density percentage: ['0.003', '0.018', '0.009', '0.013', '0.006', '0.088', '0.074', '0.069', '0.069', '0.229', '0.242', '0.249', '0.230', '0.716', '0.839', '0.568', '0.483']
Global density: 0.06710152328014374
06/01 02:28:18 AM | Train: [29/80] Step 390/390 Loss 0.028 Prec@(1,5) (99.6%, 100.0%)
06/01 02:28:18 AM | layerwise density: [204.0, 292.0, 152.0, 223.0, 96.0, 722.0, 597.0, 563.0, 573.0, 948.0, 1014.0, 1032.0, 956.0, 1491.0, 1721.0, 1170.0, 1004.0]
layerwise density percentage: ['0.003', '0.018', '0.009', '0.014', '0.006', '0.088', '0.073', '0.069', '0.070', '0.231', '0.248', '0.252', '0.233', '0.728', '0.840', '0.571', '0.490']
Global density: 0.06771187484264374
06/01 02:28:18 AM | Train: [29/200] Final Prec@1 99.6080%
06/01 02:28:18 AM | Valid: [29/200] Step 000/078 Loss 1.113 Prec@(1,5) (75.8%, 90.6%)
06/01 02:28:21 AM | Valid: [29/200] Step 078/078 Loss 1.425 Prec@(1,5) (67.3%, 88.1%)
06/01 02:28:21 AM | Valid: [29/200] Final Prec@1 67.2800%
06/01 02:28:21 AM | Current mask training best Prec@1 = 68.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 204.0, 0.00311279296875]
['model.relu.alpha_mask_1_0', 16384, 290.0, 0.0177001953125]
['model.relu.alpha_mask_2_0', 16384, 151.0, 0.00921630859375]
['model.relu.alpha_mask_3_0', 16384, 223.0, 0.01361083984375]
['model.relu.alpha_mask_4_0', 16384, 96.0, 0.005859375]
['model.relu.alpha_mask_5_0', 8192, 721.0, 0.0880126953125]
['model.relu.alpha_mask_6_0', 8192, 595.0, 0.0726318359375]
['model.relu.alpha_mask_7_0', 8192, 562.0, 0.068603515625]
['model.relu.alpha_mask_8_0', 8192, 570.0, 0.069580078125]
['model.relu.alpha_mask_9_0', 4096, 946.0, 0.23095703125]
['model.relu.alpha_mask_10_0', 4096, 1010.0, 0.24658203125]
['model.relu.alpha_mask_11_0', 4096, 1032.0, 0.251953125]
['model.relu.alpha_mask_12_0', 4096, 956.0, 0.2333984375]
['model.relu.alpha_mask_13_0', 2048, 1491.0, 0.72802734375]
['model.relu.alpha_mask_14_0', 2048, 1722.0, 0.8408203125]
['model.relu.alpha_mask_15_0', 2048, 1170.0, 0.5712890625]
['model.relu.alpha_mask_16_0', 2048, 1004.0, 0.490234375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12743.0, 0.06763226052989131]
########## End ###########
06/01 02:28:22 AM | Train: [30/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/01 02:28:22 AM | layerwise density: [204.0, 290.0, 151.0, 223.0, 96.0, 721.0, 595.0, 562.0, 570.0, 946.0, 1010.0, 1032.0, 956.0, 1491.0, 1722.0, 1170.0, 1004.0]
layerwise density percentage: ['0.003', '0.018', '0.009', '0.014', '0.006', '0.088', '0.073', '0.069', '0.070', '0.231', '0.247', '0.252', '0.233', '0.728', '0.841', '0.571', '0.490']
Global density: 0.0676322653889656
06/01 02:28:33 AM | Train: [30/80] Step 100/390 Loss 0.023 Prec@(1,5) (99.8%, 100.0%)
06/01 02:28:33 AM | layerwise density: [200.0, 282.0, 156.0, 214.0, 96.0, 713.0, 595.0, 561.0, 567.0, 941.0, 1006.0, 1030.0, 955.0, 1490.0, 1718.0, 1178.0, 1010.0]
layerwise density percentage: ['0.003', '0.017', '0.010', '0.013', '0.006', '0.087', '0.073', '0.068', '0.069', '0.230', '0.246', '0.251', '0.233', '0.728', '0.839', '0.575', '0.493']
Global density: 0.06746773421764374
06/01 02:28:44 AM | Train: [30/80] Step 200/390 Loss 0.022 Prec@(1,5) (99.8%, 100.0%)
06/01 02:28:44 AM | layerwise density: [197.0, 276.0, 156.0, 211.0, 99.0, 714.0, 571.0, 561.0, 570.0, 956.0, 1000.0, 1029.0, 948.0, 1495.0, 1727.0, 1177.0, 1017.0]
layerwise density percentage: ['0.003', '0.017', '0.010', '0.013', '0.006', '0.087', '0.070', '0.068', '0.070', '0.233', '0.244', '0.251', '0.231', '0.730', '0.843', '0.575', '0.497']
Global density: 0.06742527335882187
06/01 02:28:55 AM | Train: [30/80] Step 300/390 Loss 0.023 Prec@(1,5) (99.8%, 100.0%)
06/01 02:28:55 AM | layerwise density: [190.0, 278.0, 151.0, 207.0, 97.0, 693.0, 585.0, 554.0, 568.0, 942.0, 1000.0, 1039.0, 951.0, 1499.0, 1731.0, 1179.0, 1024.0]
layerwise density percentage: ['0.003', '0.017', '0.009', '0.013', '0.006', '0.085', '0.071', '0.068', '0.069', '0.230', '0.244', '0.254', '0.232', '0.732', '0.845', '0.576', '0.500']
Global density: 0.06734035164117813
06/01 02:29:04 AM | Train: [30/80] Step 390/390 Loss 0.023 Prec@(1,5) (99.8%, 100.0%)
06/01 02:29:04 AM | layerwise density: [192.0, 282.0, 149.0, 210.0, 97.0, 720.0, 603.0, 567.0, 561.0, 934.0, 1004.0, 1038.0, 955.0, 1505.0, 1731.0, 1179.0, 1034.0]
layerwise density percentage: ['0.003', '0.017', '0.009', '0.013', '0.006', '0.088', '0.074', '0.069', '0.068', '0.228', '0.245', '0.253', '0.233', '0.735', '0.845', '0.576', '0.505']
Global density: 0.06772779673337936
06/01 02:29:04 AM | Train: [30/200] Final Prec@1 99.7800%
06/01 02:29:05 AM | Valid: [30/200] Step 000/078 Loss 1.022 Prec@(1,5) (75.8%, 90.6%)
06/01 02:29:07 AM | Valid: [30/200] Step 078/078 Loss 1.378 Prec@(1,5) (68.4%, 88.7%)
06/01 02:29:07 AM | Valid: [30/200] Final Prec@1 68.3600%
06/01 02:29:08 AM | Current mask training best Prec@1 = 68.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 192.0, 0.0029296875]
['model.relu.alpha_mask_1_0', 16384, 282.0, 0.0172119140625]
['model.relu.alpha_mask_2_0', 16384, 149.0, 0.00909423828125]
['model.relu.alpha_mask_3_0', 16384, 210.0, 0.0128173828125]
['model.relu.alpha_mask_4_0', 16384, 97.0, 0.00592041015625]
['model.relu.alpha_mask_5_0', 8192, 720.0, 0.087890625]
['model.relu.alpha_mask_6_0', 8192, 603.0, 0.0736083984375]
['model.relu.alpha_mask_7_0', 8192, 567.0, 0.0692138671875]
['model.relu.alpha_mask_8_0', 8192, 561.0, 0.0684814453125]
['model.relu.alpha_mask_9_0', 4096, 934.0, 0.22802734375]
['model.relu.alpha_mask_10_0', 4096, 1004.0, 0.2451171875]
['model.relu.alpha_mask_11_0', 4096, 1038.0, 0.25341796875]
['model.relu.alpha_mask_12_0', 4096, 955.0, 0.233154296875]
['model.relu.alpha_mask_13_0', 2048, 1505.0, 0.73486328125]
['model.relu.alpha_mask_14_0', 2048, 1731.0, 0.84521484375]
['model.relu.alpha_mask_15_0', 2048, 1179.0, 0.57568359375]
['model.relu.alpha_mask_16_0', 2048, 1034.0, 0.5048828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12761.0, 0.06772779381793478]
########## End ###########
06/01 02:29:09 AM | Train: [31/80] Step 000/390 Loss 0.019 Prec@(1,5) (100.0%, 100.0%)
06/01 02:29:09 AM | layerwise density: [192.0, 282.0, 149.0, 210.0, 97.0, 720.0, 603.0, 567.0, 561.0, 934.0, 1004.0, 1038.0, 955.0, 1505.0, 1731.0, 1179.0, 1034.0]
layerwise density percentage: ['0.003', '0.017', '0.009', '0.013', '0.006', '0.088', '0.074', '0.069', '0.068', '0.228', '0.245', '0.253', '0.233', '0.735', '0.845', '0.576', '0.505']
Global density: 0.06772779673337936
06/01 02:29:19 AM | Train: [31/80] Step 100/390 Loss 0.022 Prec@(1,5) (99.8%, 100.0%)
06/01 02:29:19 AM | layerwise density: [189.0, 280.0, 144.0, 212.0, 97.0, 694.0, 599.0, 571.0, 550.0, 923.0, 998.0, 1041.0, 947.0, 1502.0, 1734.0, 1182.0, 1038.0]
layerwise density percentage: ['0.003', '0.017', '0.009', '0.013', '0.006', '0.085', '0.073', '0.070', '0.067', '0.225', '0.244', '0.254', '0.231', '0.733', '0.847', '0.577', '0.507']
Global density: 0.06740935146808624
06/01 02:29:30 AM | Train: [31/80] Step 200/390 Loss 0.022 Prec@(1,5) (99.8%, 100.0%)
06/01 02:29:30 AM | layerwise density: [190.0, 280.0, 145.0, 208.0, 95.0, 713.0, 612.0, 570.0, 574.0, 944.0, 1016.0, 1041.0, 959.0, 1511.0, 1732.0, 1185.0, 1052.0]
layerwise density percentage: ['0.003', '0.017', '0.009', '0.013', '0.006', '0.087', '0.075', '0.070', '0.070', '0.230', '0.248', '0.254', '0.234', '0.738', '0.846', '0.579', '0.514']
Global density: 0.06807808578014374
06/01 02:29:41 AM | Train: [31/80] Step 300/390 Loss 0.022 Prec@(1,5) (99.8%, 100.0%)
06/01 02:29:41 AM | layerwise density: [189.0, 274.0, 144.0, 197.0, 84.0, 684.0, 567.0, 558.0, 548.0, 948.0, 1008.0, 1034.0, 954.0, 1508.0, 1741.0, 1187.0, 1055.0]
layerwise density percentage: ['0.003', '0.017', '0.009', '0.012', '0.005', '0.083', '0.069', '0.068', '0.067', '0.231', '0.246', '0.252', '0.233', '0.736', '0.850', '0.580', '0.515']
Global density: 0.06729789823293686
06/01 02:29:51 AM | Train: [31/80] Step 390/390 Loss 0.022 Prec@(1,5) (99.8%, 100.0%)
06/01 02:29:51 AM | layerwise density: [190.0, 289.0, 145.0, 209.0, 93.0, 732.0, 572.0, 575.0, 565.0, 958.0, 1013.0, 1039.0, 976.0, 1507.0, 1747.0, 1188.0, 1060.0]
layerwise density percentage: ['0.003', '0.018', '0.009', '0.013', '0.006', '0.089', '0.070', '0.070', '0.069', '0.234', '0.247', '0.254', '0.238', '0.736', '0.853', '0.580', '0.518']
Global density: 0.0682426169514656
06/01 02:29:51 AM | Train: [31/200] Final Prec@1 99.7580%
06/01 02:29:51 AM | Valid: [31/200] Step 000/078 Loss 1.057 Prec@(1,5) (77.3%, 92.2%)
06/01 02:29:53 AM | Valid: [31/200] Step 078/078 Loss 1.370 Prec@(1,5) (68.2%, 88.7%)
06/01 02:29:53 AM | Valid: [31/200] Final Prec@1 68.1900%
06/01 02:29:53 AM | Current mask training best Prec@1 = 68.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 190.0, 0.002899169921875]
['model.relu.alpha_mask_1_0', 16384, 289.0, 0.01763916015625]
['model.relu.alpha_mask_2_0', 16384, 145.0, 0.00885009765625]
['model.relu.alpha_mask_3_0', 16384, 209.0, 0.01275634765625]
['model.relu.alpha_mask_4_0', 16384, 93.0, 0.00567626953125]
['model.relu.alpha_mask_5_0', 8192, 733.0, 0.0894775390625]
['model.relu.alpha_mask_6_0', 8192, 572.0, 0.06982421875]
['model.relu.alpha_mask_7_0', 8192, 575.0, 0.0701904296875]
['model.relu.alpha_mask_8_0', 8192, 565.0, 0.0689697265625]
['model.relu.alpha_mask_9_0', 4096, 958.0, 0.23388671875]
['model.relu.alpha_mask_10_0', 4096, 1013.0, 0.247314453125]
['model.relu.alpha_mask_11_0', 4096, 1039.0, 0.253662109375]
['model.relu.alpha_mask_12_0', 4096, 976.0, 0.23828125]
['model.relu.alpha_mask_13_0', 2048, 1507.0, 0.73583984375]
['model.relu.alpha_mask_14_0', 2048, 1747.0, 0.85302734375]
['model.relu.alpha_mask_15_0', 2048, 1188.0, 0.580078125]
['model.relu.alpha_mask_16_0', 2048, 1060.0, 0.517578125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12859.0, 0.06824791949728261]
########## End ###########
06/01 02:29:54 AM | Train: [32/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 02:29:54 AM | layerwise density: [190.0, 289.0, 145.0, 209.0, 93.0, 733.0, 572.0, 575.0, 565.0, 958.0, 1013.0, 1039.0, 976.0, 1507.0, 1747.0, 1188.0, 1060.0]
layerwise density percentage: ['0.003', '0.018', '0.009', '0.013', '0.006', '0.089', '0.070', '0.070', '0.069', '0.234', '0.247', '0.254', '0.238', '0.736', '0.853', '0.580', '0.518']
Global density: 0.06824792176485062
06/01 02:30:05 AM | Train: [32/80] Step 100/390 Loss 0.021 Prec@(1,5) (99.8%, 100.0%)
06/01 02:30:05 AM | layerwise density: [186.0, 268.0, 140.0, 201.0, 88.0, 699.0, 557.0, 559.0, 539.0, 937.0, 996.0, 1031.0, 968.0, 1494.0, 1743.0, 1182.0, 1064.0]
layerwise density percentage: ['0.003', '0.016', '0.009', '0.012', '0.005', '0.085', '0.068', '0.068', '0.066', '0.229', '0.243', '0.252', '0.236', '0.729', '0.851', '0.577', '0.520']
Global density: 0.06714928895235062
06/01 02:30:16 AM | Train: [32/80] Step 200/390 Loss 0.021 Prec@(1,5) (99.8%, 100.0%)
06/01 02:30:16 AM | layerwise density: [190.0, 266.0, 143.0, 205.0, 91.0, 735.0, 567.0, 563.0, 557.0, 967.0, 1004.0, 1037.0, 981.0, 1514.0, 1758.0, 1186.0, 1068.0]
layerwise density percentage: ['0.003', '0.016', '0.009', '0.013', '0.006', '0.090', '0.069', '0.069', '0.068', '0.236', '0.245', '0.253', '0.240', '0.739', '0.858', '0.579', '0.521']
Global density: 0.06810461729764938
06/01 02:30:27 AM | Train: [32/80] Step 300/390 Loss 0.021 Prec@(1,5) (99.8%, 100.0%)
06/01 02:30:27 AM | layerwise density: [178.0, 211.0, 126.0, 170.0, 66.0, 600.0, 498.0, 486.0, 471.0, 890.0, 970.0, 1023.0, 918.0, 1500.0, 1732.0, 1174.0, 1068.0]
layerwise density percentage: ['0.003', '0.013', '0.008', '0.010', '0.004', '0.073', '0.061', '0.059', '0.057', '0.217', '0.237', '0.250', '0.224', '0.732', '0.846', '0.573', '0.521']
Global density: 0.06411875784397125
06/01 02:30:36 AM | Train: [32/80] Step 390/390 Loss 0.021 Prec@(1,5) (99.8%, 100.0%)
06/01 02:30:36 AM | layerwise density: [177.0, 223.0, 124.0, 173.0, 68.0, 627.0, 517.0, 495.0, 489.0, 905.0, 987.0, 1023.0, 935.0, 1494.0, 1756.0, 1189.0, 1076.0]
layerwise density percentage: ['0.003', '0.014', '0.008', '0.011', '0.004', '0.077', '0.063', '0.060', '0.060', '0.221', '0.241', '0.250', '0.228', '0.729', '0.857', '0.581', '0.525']
Global density: 0.06505817174911499
06/01 02:30:36 AM | Train: [32/200] Final Prec@1 99.7840%
06/01 02:30:36 AM | Valid: [32/200] Step 000/078 Loss 1.053 Prec@(1,5) (78.1%, 90.6%)
06/01 02:30:39 AM | Valid: [32/200] Step 078/078 Loss 1.365 Prec@(1,5) (68.8%, 88.7%)
06/01 02:30:39 AM | Valid: [32/200] Final Prec@1 68.8100%
06/01 02:30:39 AM | Current mask training best Prec@1 = 68.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 177.0, 0.0027008056640625]
['model.relu.alpha_mask_1_0', 16384, 222.0, 0.0135498046875]
['model.relu.alpha_mask_2_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_3_0', 16384, 173.0, 0.01055908203125]
['model.relu.alpha_mask_4_0', 16384, 68.0, 0.004150390625]
['model.relu.alpha_mask_5_0', 8192, 627.0, 0.0765380859375]
['model.relu.alpha_mask_6_0', 8192, 517.0, 0.0631103515625]
['model.relu.alpha_mask_7_0', 8192, 495.0, 0.0604248046875]
['model.relu.alpha_mask_8_0', 8192, 489.0, 0.0596923828125]
['model.relu.alpha_mask_9_0', 4096, 905.0, 0.220947265625]
['model.relu.alpha_mask_10_0', 4096, 989.0, 0.241455078125]
['model.relu.alpha_mask_11_0', 4096, 1023.0, 0.249755859375]
['model.relu.alpha_mask_12_0', 4096, 935.0, 0.228271484375]
['model.relu.alpha_mask_13_0', 2048, 1493.0, 0.72900390625]
['model.relu.alpha_mask_14_0', 2048, 1756.0, 0.857421875]
['model.relu.alpha_mask_15_0', 2048, 1189.0, 0.58056640625]
['model.relu.alpha_mask_16_0', 2048, 1076.0, 0.525390625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12258.0, 0.06505816915760869]
########## End ###########
06/01 02:30:40 AM | Train: [33/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/01 02:30:40 AM | layerwise density: [177.0, 222.0, 124.0, 173.0, 68.0, 627.0, 517.0, 495.0, 489.0, 905.0, 989.0, 1023.0, 935.0, 1493.0, 1756.0, 1189.0, 1076.0]
layerwise density percentage: ['0.003', '0.014', '0.008', '0.011', '0.004', '0.077', '0.063', '0.060', '0.060', '0.221', '0.241', '0.250', '0.228', '0.729', '0.857', '0.581', '0.525']
Global density: 0.06505817174911499
06/01 02:30:51 AM | Train: [33/80] Step 100/390 Loss 0.019 Prec@(1,5) (99.8%, 100.0%)
06/01 02:30:51 AM | layerwise density: [173.0, 244.0, 131.0, 180.0, 78.0, 690.0, 551.0, 524.0, 531.0, 933.0, 1015.0, 1034.0, 949.0, 1517.0, 1760.0, 1197.0, 1084.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.011', '0.005', '0.084', '0.067', '0.064', '0.065', '0.228', '0.248', '0.252', '0.232', '0.741', '0.859', '0.584', '0.529']
Global density: 0.06682553887367249
06/01 02:31:02 AM | Train: [33/80] Step 200/390 Loss 0.019 Prec@(1,5) (99.8%, 100.0%)
06/01 02:31:02 AM | layerwise density: [174.0, 269.0, 136.0, 182.0, 90.0, 719.0, 572.0, 548.0, 541.0, 962.0, 1008.0, 1039.0, 960.0, 1509.0, 1750.0, 1200.0, 1090.0]
layerwise density percentage: ['0.003', '0.016', '0.008', '0.011', '0.005', '0.088', '0.070', '0.067', '0.066', '0.235', '0.246', '0.254', '0.234', '0.737', '0.854', '0.586', '0.532']
Global density: 0.06766410917043686
06/01 02:31:13 AM | Train: [33/80] Step 300/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/01 02:31:13 AM | layerwise density: [176.0, 255.0, 137.0, 181.0, 85.0, 706.0, 563.0, 532.0, 523.0, 962.0, 995.0, 1039.0, 957.0, 1510.0, 1750.0, 1206.0, 1093.0]
layerwise density percentage: ['0.003', '0.016', '0.008', '0.011', '0.005', '0.086', '0.069', '0.065', '0.064', '0.235', '0.243', '0.254', '0.234', '0.737', '0.854', '0.589', '0.534']
Global density: 0.06724482029676437
06/01 02:31:23 AM | Train: [33/80] Step 390/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/01 02:31:23 AM | layerwise density: [183.0, 269.0, 147.0, 194.0, 91.0, 725.0, 575.0, 532.0, 550.0, 973.0, 1016.0, 1051.0, 981.0, 1516.0, 1751.0, 1208.0, 1107.0]
layerwise density percentage: ['0.003', '0.016', '0.009', '0.012', '0.006', '0.089', '0.070', '0.065', '0.067', '0.238', '0.248', '0.257', '0.240', '0.740', '0.855', '0.590', '0.541']
Global density: 0.0683009922504425
06/01 02:31:23 AM | Train: [33/200] Final Prec@1 99.7860%
06/01 02:31:23 AM | Valid: [33/200] Step 000/078 Loss 1.053 Prec@(1,5) (78.1%, 93.0%)
06/01 02:31:25 AM | Valid: [33/200] Step 078/078 Loss 1.361 Prec@(1,5) (69.0%, 88.8%)
06/01 02:31:25 AM | Valid: [33/200] Final Prec@1 69.0100%
06/01 02:31:26 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 183.0, 0.0027923583984375]
['model.relu.alpha_mask_1_0', 16384, 269.0, 0.01641845703125]
['model.relu.alpha_mask_2_0', 16384, 148.0, 0.009033203125]
['model.relu.alpha_mask_3_0', 16384, 194.0, 0.0118408203125]
['model.relu.alpha_mask_4_0', 16384, 91.0, 0.00555419921875]
['model.relu.alpha_mask_5_0', 8192, 728.0, 0.0888671875]
['model.relu.alpha_mask_6_0', 8192, 575.0, 0.0701904296875]
['model.relu.alpha_mask_7_0', 8192, 532.0, 0.06494140625]
['model.relu.alpha_mask_8_0', 8192, 550.0, 0.067138671875]
['model.relu.alpha_mask_9_0', 4096, 973.0, 0.237548828125]
['model.relu.alpha_mask_10_0', 4096, 1017.0, 0.248291015625]
['model.relu.alpha_mask_11_0', 4096, 1052.0, 0.2568359375]
['model.relu.alpha_mask_12_0', 4096, 981.0, 0.239501953125]
['model.relu.alpha_mask_13_0', 2048, 1515.0, 0.73974609375]
['model.relu.alpha_mask_14_0', 2048, 1749.0, 0.85400390625]
['model.relu.alpha_mask_15_0', 2048, 1209.0, 0.59033203125]
['model.relu.alpha_mask_16_0', 2048, 1107.0, 0.54052734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12873.0, 0.06832222316576086]
########## End ###########
06/01 02:31:27 AM | Train: [34/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 02:31:27 AM | layerwise density: [183.0, 269.0, 148.0, 194.0, 91.0, 728.0, 575.0, 532.0, 550.0, 973.0, 1017.0, 1052.0, 981.0, 1515.0, 1749.0, 1209.0, 1107.0]
layerwise density percentage: ['0.003', '0.016', '0.009', '0.012', '0.006', '0.089', '0.070', '0.065', '0.067', '0.238', '0.248', '0.257', '0.240', '0.740', '0.854', '0.590', '0.541']
Global density: 0.06832222640514374
06/01 02:31:38 AM | Train: [34/80] Step 100/390 Loss 0.017 Prec@(1,5) (99.9%, 100.0%)
06/01 02:31:38 AM | layerwise density: [176.0, 241.0, 146.0, 180.0, 83.0, 674.0, 534.0, 508.0, 506.0, 941.0, 996.0, 1045.0, 964.0, 1515.0, 1757.0, 1204.0, 1109.0]
layerwise density percentage: ['0.003', '0.015', '0.009', '0.011', '0.005', '0.082', '0.065', '0.062', '0.062', '0.230', '0.243', '0.255', '0.235', '0.740', '0.858', '0.588', '0.542']
Global density: 0.06676184386014938
06/01 02:31:49 AM | Train: [34/80] Step 200/390 Loss 0.017 Prec@(1,5) (99.9%, 100.0%)
06/01 02:31:49 AM | layerwise density: [176.0, 260.0, 143.0, 179.0, 85.0, 697.0, 546.0, 521.0, 532.0, 967.0, 1003.0, 1051.0, 973.0, 1514.0, 1770.0, 1199.0, 1117.0]
layerwise density percentage: ['0.003', '0.016', '0.009', '0.011', '0.005', '0.085', '0.067', '0.064', '0.065', '0.236', '0.245', '0.257', '0.238', '0.739', '0.864', '0.585', '0.545']
Global density: 0.06757918745279312
06/01 02:31:59 AM | Train: [34/80] Step 300/390 Loss 0.018 Prec@(1,5) (99.8%, 100.0%)
06/01 02:31:59 AM | layerwise density: [173.0, 259.0, 134.0, 179.0, 73.0, 684.0, 547.0, 515.0, 521.0, 957.0, 1006.0, 1051.0, 965.0, 1515.0, 1759.0, 1199.0, 1123.0]
layerwise density percentage: ['0.003', '0.016', '0.008', '0.011', '0.004', '0.083', '0.067', '0.063', '0.064', '0.234', '0.246', '0.257', '0.236', '0.740', '0.859', '0.585', '0.548']
Global density: 0.06719174981117249
06/01 02:32:09 AM | Train: [34/80] Step 390/390 Loss 0.018 Prec@(1,5) (99.8%, 100.0%)
06/01 02:32:09 AM | layerwise density: [167.0, 252.0, 129.0, 173.0, 76.0, 674.0, 549.0, 516.0, 522.0, 946.0, 1002.0, 1043.0, 964.0, 1517.0, 1757.0, 1203.0, 1130.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.011', '0.005', '0.082', '0.067', '0.063', '0.064', '0.231', '0.245', '0.255', '0.235', '0.741', '0.858', '0.587', '0.552']
Global density: 0.06697945296764374
06/01 02:32:09 AM | Train: [34/200] Final Prec@1 99.8400%
06/01 02:32:09 AM | Valid: [34/200] Step 000/078 Loss 1.110 Prec@(1,5) (75.0%, 90.6%)
06/01 02:32:12 AM | Valid: [34/200] Step 078/078 Loss 1.370 Prec@(1,5) (68.4%, 89.0%)
06/01 02:32:12 AM | Valid: [34/200] Final Prec@1 68.4000%
06/01 02:32:12 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 168.0, 0.0025634765625]
['model.relu.alpha_mask_1_0', 16384, 250.0, 0.0152587890625]
['model.relu.alpha_mask_2_0', 16384, 129.0, 0.00787353515625]
['model.relu.alpha_mask_3_0', 16384, 173.0, 0.01055908203125]
['model.relu.alpha_mask_4_0', 16384, 76.0, 0.004638671875]
['model.relu.alpha_mask_5_0', 8192, 676.0, 0.08251953125]
['model.relu.alpha_mask_6_0', 8192, 549.0, 0.0670166015625]
['model.relu.alpha_mask_7_0', 8192, 516.0, 0.06298828125]
['model.relu.alpha_mask_8_0', 8192, 522.0, 0.063720703125]
['model.relu.alpha_mask_9_0', 4096, 947.0, 0.231201171875]
['model.relu.alpha_mask_10_0', 4096, 1003.0, 0.244873046875]
['model.relu.alpha_mask_11_0', 4096, 1043.0, 0.254638671875]
['model.relu.alpha_mask_12_0', 4096, 964.0, 0.2353515625]
['model.relu.alpha_mask_13_0', 2048, 1520.0, 0.7421875]
['model.relu.alpha_mask_14_0', 2048, 1758.0, 0.8583984375]
['model.relu.alpha_mask_15_0', 2048, 1203.0, 0.58740234375]
['model.relu.alpha_mask_16_0', 2048, 1130.0, 0.5517578125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12627.0, 0.0670166015625]
########## End ###########
06/01 02:32:13 AM | Train: [35/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 02:32:13 AM | layerwise density: [168.0, 250.0, 129.0, 173.0, 76.0, 676.0, 549.0, 516.0, 522.0, 947.0, 1003.0, 1043.0, 964.0, 1520.0, 1758.0, 1203.0, 1130.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.011', '0.005', '0.083', '0.067', '0.063', '0.064', '0.231', '0.245', '0.255', '0.235', '0.742', '0.858', '0.587', '0.552']
Global density: 0.0670166015625
06/01 02:32:23 AM | Train: [35/80] Step 100/390 Loss 0.017 Prec@(1,5) (99.9%, 100.0%)
06/01 02:32:23 AM | layerwise density: [169.0, 254.0, 136.0, 179.0, 74.0, 704.0, 569.0, 540.0, 555.0, 974.0, 1013.0, 1049.0, 987.0, 1532.0, 1773.0, 1211.0, 1135.0]
layerwise density percentage: ['0.003', '0.016', '0.008', '0.011', '0.005', '0.086', '0.069', '0.066', '0.068', '0.238', '0.247', '0.256', '0.241', '0.748', '0.866', '0.591', '0.554']
Global density: 0.06822138279676437
06/01 02:32:33 AM | Train: [35/80] Step 200/390 Loss 0.018 Prec@(1,5) (99.8%, 100.0%)
06/01 02:32:33 AM | layerwise density: [170.0, 239.0, 132.0, 166.0, 65.0, 653.0, 530.0, 515.0, 513.0, 949.0, 994.0, 1047.0, 966.0, 1527.0, 1757.0, 1201.0, 1137.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.010', '0.004', '0.080', '0.065', '0.063', '0.063', '0.232', '0.243', '0.256', '0.236', '0.746', '0.858', '0.586', '0.555']
Global density: 0.06666631251573563
06/01 02:32:44 AM | Train: [35/80] Step 300/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/01 02:32:44 AM | layerwise density: [169.0, 241.0, 130.0, 167.0, 69.0, 648.0, 534.0, 520.0, 544.0, 969.0, 1023.0, 1048.0, 978.0, 1524.0, 1770.0, 1209.0, 1141.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.010', '0.004', '0.079', '0.065', '0.063', '0.066', '0.237', '0.250', '0.256', '0.239', '0.744', '0.864', '0.590', '0.557']
Global density: 0.0673191249370575
06/01 02:32:53 AM | Train: [35/80] Step 390/390 Loss 0.018 Prec@(1,5) (99.8%, 100.0%)
06/01 02:32:53 AM | layerwise density: [169.0, 242.0, 137.0, 174.0, 71.0, 671.0, 531.0, 523.0, 536.0, 986.0, 1014.0, 1057.0, 975.0, 1520.0, 1778.0, 1204.0, 1147.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.011', '0.004', '0.082', '0.065', '0.064', '0.065', '0.241', '0.248', '0.258', '0.238', '0.742', '0.868', '0.588', '0.560']
Global density: 0.06758980453014374
06/01 02:32:53 AM | Train: [35/200] Final Prec@1 99.8140%
06/01 02:32:53 AM | Valid: [35/200] Step 000/078 Loss 1.122 Prec@(1,5) (74.2%, 89.8%)
06/01 02:32:56 AM | Valid: [35/200] Step 078/078 Loss 1.374 Prec@(1,5) (68.3%, 88.8%)
06/01 02:32:56 AM | Valid: [35/200] Final Prec@1 68.2700%
06/01 02:32:56 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 167.0, 0.0025482177734375]
['model.relu.alpha_mask_1_0', 16384, 242.0, 0.0147705078125]
['model.relu.alpha_mask_2_0', 16384, 137.0, 0.00836181640625]
['model.relu.alpha_mask_3_0', 16384, 174.0, 0.0106201171875]
['model.relu.alpha_mask_4_0', 16384, 70.0, 0.0042724609375]
['model.relu.alpha_mask_5_0', 8192, 668.0, 0.08154296875]
['model.relu.alpha_mask_6_0', 8192, 531.0, 0.0648193359375]
['model.relu.alpha_mask_7_0', 8192, 522.0, 0.063720703125]
['model.relu.alpha_mask_8_0', 8192, 534.0, 0.065185546875]
['model.relu.alpha_mask_9_0', 4096, 985.0, 0.240478515625]
['model.relu.alpha_mask_10_0', 4096, 1015.0, 0.247802734375]
['model.relu.alpha_mask_11_0', 4096, 1057.0, 0.258056640625]
['model.relu.alpha_mask_12_0', 4096, 971.0, 0.237060546875]
['model.relu.alpha_mask_13_0', 2048, 1520.0, 0.7421875]
['model.relu.alpha_mask_14_0', 2048, 1777.0, 0.86767578125]
['model.relu.alpha_mask_15_0', 2048, 1203.0, 0.58740234375]
['model.relu.alpha_mask_16_0', 2048, 1147.0, 0.56005859375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12720.0, 0.06751019021739131]
########## End ###########
06/01 02:32:57 AM | Train: [36/80] Step 000/390 Loss 0.015 Prec@(1,5) (100.0%, 100.0%)
06/01 02:32:57 AM | layerwise density: [167.0, 242.0, 137.0, 174.0, 70.0, 668.0, 531.0, 522.0, 534.0, 985.0, 1015.0, 1057.0, 971.0, 1520.0, 1777.0, 1203.0, 1147.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.011', '0.004', '0.082', '0.065', '0.064', '0.065', '0.240', '0.248', '0.258', '0.237', '0.742', '0.868', '0.587', '0.560']
Global density: 0.0675101950764656
06/01 02:33:07 AM | Train: [36/80] Step 100/390 Loss 0.015 Prec@(1,5) (99.9%, 100.0%)
06/01 02:33:07 AM | layerwise density: [167.0, 233.0, 134.0, 171.0, 66.0, 645.0, 509.0, 510.0, 506.0, 964.0, 1014.0, 1060.0, 953.0, 1513.0, 1782.0, 1204.0, 1149.0]
layerwise density percentage: ['0.003', '0.014', '0.008', '0.010', '0.004', '0.079', '0.062', '0.062', '0.062', '0.235', '0.248', '0.259', '0.233', '0.739', '0.870', '0.588', '0.561']
Global density: 0.06676715612411499
06/01 02:33:18 AM | Train: [36/80] Step 200/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/01 02:33:18 AM | layerwise density: [167.0, 242.0, 138.0, 177.0, 69.0, 686.0, 534.0, 517.0, 531.0, 974.0, 1030.0, 1060.0, 976.0, 1538.0, 1779.0, 1212.0, 1153.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.011', '0.004', '0.084', '0.065', '0.063', '0.065', '0.238', '0.251', '0.259', '0.238', '0.751', '0.869', '0.592', '0.563']
Global density: 0.06784455478191376
06/01 02:33:28 AM | Train: [36/80] Step 300/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/01 02:33:28 AM | layerwise density: [164.0, 219.0, 135.0, 159.0, 75.0, 647.0, 522.0, 489.0, 498.0, 950.0, 1012.0, 1056.0, 974.0, 1520.0, 1762.0, 1198.0, 1157.0]
layerwise density percentage: ['0.003', '0.013', '0.008', '0.010', '0.005', '0.079', '0.064', '0.060', '0.061', '0.232', '0.247', '0.258', '0.238', '0.742', '0.860', '0.585', '0.565']
Global density: 0.06653893738985062
06/01 02:33:38 AM | Train: [36/80] Step 390/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/01 02:33:38 AM | layerwise density: [168.0, 229.0, 136.0, 155.0, 77.0, 649.0, 528.0, 497.0, 499.0, 960.0, 1026.0, 1058.0, 970.0, 1532.0, 1769.0, 1215.0, 1163.0]
layerwise density percentage: ['0.003', '0.014', '0.008', '0.009', '0.005', '0.079', '0.064', '0.061', '0.061', '0.234', '0.250', '0.258', '0.237', '0.748', '0.864', '0.593', '0.568']
Global density: 0.06703783571720123
06/01 02:33:38 AM | Train: [36/200] Final Prec@1 99.8360%
06/01 02:33:38 AM | Valid: [36/200] Step 000/078 Loss 1.062 Prec@(1,5) (75.8%, 92.2%)
06/01 02:33:40 AM | Valid: [36/200] Step 078/078 Loss 1.354 Prec@(1,5) (69.0%, 89.1%)
06/01 02:33:41 AM | Valid: [36/200] Final Prec@1 68.9500%
06/01 02:33:41 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 168.0, 0.0025634765625]
['model.relu.alpha_mask_1_0', 16384, 229.0, 0.01397705078125]
['model.relu.alpha_mask_2_0', 16384, 136.0, 0.00830078125]
['model.relu.alpha_mask_3_0', 16384, 155.0, 0.00946044921875]
['model.relu.alpha_mask_4_0', 16384, 77.0, 0.00469970703125]
['model.relu.alpha_mask_5_0', 8192, 649.0, 0.0792236328125]
['model.relu.alpha_mask_6_0', 8192, 528.0, 0.064453125]
['model.relu.alpha_mask_7_0', 8192, 497.0, 0.0606689453125]
['model.relu.alpha_mask_8_0', 8192, 499.0, 0.0609130859375]
['model.relu.alpha_mask_9_0', 4096, 960.0, 0.234375]
['model.relu.alpha_mask_10_0', 4096, 1026.0, 0.25048828125]
['model.relu.alpha_mask_11_0', 4096, 1058.0, 0.25830078125]
['model.relu.alpha_mask_12_0', 4096, 970.0, 0.23681640625]
['model.relu.alpha_mask_13_0', 2048, 1532.0, 0.748046875]
['model.relu.alpha_mask_14_0', 2048, 1769.0, 0.86376953125]
['model.relu.alpha_mask_15_0', 2048, 1215.0, 0.59326171875]
['model.relu.alpha_mask_16_0', 2048, 1163.0, 0.56787109375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12631.0, 0.06703783118206522]
########## End ###########
06/01 02:33:42 AM | Train: [37/80] Step 000/390 Loss 0.014 Prec@(1,5) (99.2%, 100.0%)
06/01 02:33:42 AM | layerwise density: [168.0, 229.0, 136.0, 155.0, 77.0, 649.0, 528.0, 497.0, 499.0, 960.0, 1026.0, 1058.0, 970.0, 1532.0, 1769.0, 1215.0, 1163.0]
layerwise density percentage: ['0.003', '0.014', '0.008', '0.009', '0.005', '0.079', '0.064', '0.061', '0.061', '0.234', '0.250', '0.258', '0.237', '0.748', '0.864', '0.593', '0.568']
Global density: 0.06703783571720123
06/01 02:33:52 AM | Train: [37/80] Step 100/390 Loss 0.014 Prec@(1,5) (99.9%, 100.0%)
06/01 02:33:52 AM | layerwise density: [173.0, 247.0, 137.0, 165.0, 82.0, 687.0, 559.0, 523.0, 521.0, 979.0, 1043.0, 1066.0, 987.0, 1538.0, 1785.0, 1224.0, 1169.0]
layerwise density percentage: ['0.003', '0.015', '0.008', '0.010', '0.005', '0.084', '0.068', '0.064', '0.064', '0.239', '0.255', '0.260', '0.241', '0.751', '0.872', '0.598', '0.571']
Global density: 0.06838591396808624
06/01 02:34:03 AM | Train: [37/80] Step 200/390 Loss 0.015 Prec@(1,5) (99.9%, 100.0%)
06/01 02:34:03 AM | layerwise density: [171.0, 233.0, 131.0, 160.0, 77.0, 638.0, 510.0, 492.0, 502.0, 944.0, 1021.0, 1059.0, 973.0, 1530.0, 1764.0, 1207.0, 1169.0]
layerwise density percentage: ['0.003', '0.014', '0.008', '0.010', '0.005', '0.078', '0.062', '0.060', '0.061', '0.230', '0.249', '0.259', '0.238', '0.747', '0.861', '0.589', '0.571']
Global density: 0.0667724609375
06/01 02:34:15 AM | Train: [37/80] Step 300/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/01 02:34:15 AM | layerwise density: [163.0, 240.0, 129.0, 167.0, 72.0, 643.0, 526.0, 490.0, 528.0, 962.0, 1019.0, 1057.0, 964.0, 1534.0, 1784.0, 1219.0, 1172.0]
layerwise density percentage: ['0.002', '0.015', '0.008', '0.010', '0.004', '0.078', '0.064', '0.060', '0.064', '0.235', '0.249', '0.258', '0.235', '0.749', '0.871', '0.595', '0.572']
Global density: 0.06723951548337936
06/01 02:34:24 AM | Train: [37/80] Step 390/390 Loss 0.015 Prec@(1,5) (99.9%, 100.0%)
06/01 02:34:24 AM | layerwise density: [159.0, 220.0, 119.0, 156.0, 69.0, 612.0, 505.0, 482.0, 499.0, 958.0, 1020.0, 1057.0, 965.0, 1535.0, 1777.0, 1204.0, 1177.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.010', '0.004', '0.075', '0.062', '0.059', '0.061', '0.234', '0.249', '0.258', '0.236', '0.750', '0.868', '0.588', '0.575']
Global density: 0.06641686707735062
06/01 02:34:24 AM | Train: [37/200] Final Prec@1 99.8500%
06/01 02:34:25 AM | Valid: [37/200] Step 000/078 Loss 1.123 Prec@(1,5) (72.7%, 90.6%)
06/01 02:34:27 AM | Valid: [37/200] Step 078/078 Loss 1.364 Prec@(1,5) (68.6%, 88.7%)
06/01 02:34:27 AM | Valid: [37/200] Final Prec@1 68.5800%
06/01 02:34:27 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 159.0, 0.0024261474609375]
['model.relu.alpha_mask_1_0', 16384, 220.0, 0.013427734375]
['model.relu.alpha_mask_2_0', 16384, 119.0, 0.00726318359375]
['model.relu.alpha_mask_3_0', 16384, 156.0, 0.009521484375]
['model.relu.alpha_mask_4_0', 16384, 69.0, 0.00421142578125]
['model.relu.alpha_mask_5_0', 8192, 612.0, 0.07470703125]
['model.relu.alpha_mask_6_0', 8192, 505.0, 0.0616455078125]
['model.relu.alpha_mask_7_0', 8192, 482.0, 0.058837890625]
['model.relu.alpha_mask_8_0', 8192, 499.0, 0.0609130859375]
['model.relu.alpha_mask_9_0', 4096, 958.0, 0.23388671875]
['model.relu.alpha_mask_10_0', 4096, 1020.0, 0.2490234375]
['model.relu.alpha_mask_11_0', 4096, 1057.0, 0.258056640625]
['model.relu.alpha_mask_12_0', 4096, 965.0, 0.235595703125]
['model.relu.alpha_mask_13_0', 2048, 1535.0, 0.74951171875]
['model.relu.alpha_mask_14_0', 2048, 1777.0, 0.86767578125]
['model.relu.alpha_mask_15_0', 2048, 1204.0, 0.587890625]
['model.relu.alpha_mask_16_0', 2048, 1177.0, 0.57470703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12514.0, 0.06641686480978261]
########## End ###########
06/01 02:34:28 AM | Train: [38/80] Step 000/390 Loss 0.021 Prec@(1,5) (100.0%, 100.0%)
06/01 02:34:28 AM | layerwise density: [159.0, 220.0, 119.0, 156.0, 69.0, 612.0, 505.0, 482.0, 499.0, 958.0, 1020.0, 1057.0, 965.0, 1535.0, 1777.0, 1204.0, 1177.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.010', '0.004', '0.075', '0.062', '0.059', '0.061', '0.234', '0.249', '0.258', '0.236', '0.750', '0.868', '0.588', '0.575']
Global density: 0.06641686707735062
06/01 02:34:38 AM | Train: [38/80] Step 100/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/01 02:34:38 AM | layerwise density: [155.0, 183.0, 105.0, 140.0, 60.0, 515.0, 441.0, 444.0, 433.0, 918.0, 978.0, 1044.0, 919.0, 1521.0, 1759.0, 1185.0, 1177.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.009', '0.004', '0.063', '0.054', '0.054', '0.053', '0.224', '0.239', '0.255', '0.224', '0.743', '0.859', '0.579', '0.575']
Global density: 0.06356678903102875
06/01 02:34:49 AM | Train: [38/80] Step 200/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/01 02:34:49 AM | layerwise density: [156.0, 189.0, 103.0, 140.0, 53.0, 538.0, 445.0, 449.0, 438.0, 934.0, 983.0, 1046.0, 935.0, 1531.0, 1765.0, 1206.0, 1178.0]
layerwise density percentage: ['0.002', '0.012', '0.006', '0.009', '0.003', '0.066', '0.054', '0.055', '0.053', '0.228', '0.240', '0.255', '0.228', '0.748', '0.862', '0.589', '0.575']
Global density: 0.06416121870279312
06/01 02:35:00 AM | Train: [38/80] Step 300/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/01 02:35:00 AM | layerwise density: [162.0, 201.0, 115.0, 145.0, 60.0, 601.0, 473.0, 461.0, 465.0, 967.0, 1017.0, 1053.0, 977.0, 1537.0, 1788.0, 1223.0, 1181.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.004', '0.073', '0.058', '0.056', '0.057', '0.236', '0.248', '0.257', '0.239', '0.750', '0.873', '0.597', '0.577']
Global density: 0.06594981253147125
06/01 02:35:10 AM | Train: [38/80] Step 390/390 Loss 0.015 Prec@(1,5) (99.9%, 100.0%)
06/01 02:35:10 AM | layerwise density: [164.0, 205.0, 123.0, 155.0, 63.0, 654.0, 491.0, 490.0, 493.0, 1000.0, 1030.0, 1058.0, 999.0, 1548.0, 1791.0, 1229.0, 1189.0]
layerwise density percentage: ['0.003', '0.013', '0.008', '0.009', '0.004', '0.080', '0.060', '0.060', '0.060', '0.244', '0.251', '0.258', '0.244', '0.756', '0.875', '0.600', '0.581']
Global density: 0.06730850785970688
06/01 02:35:10 AM | Train: [38/200] Final Prec@1 99.8520%
06/01 02:35:10 AM | Valid: [38/200] Step 000/078 Loss 1.242 Prec@(1,5) (71.9%, 88.3%)
06/01 02:35:12 AM | Valid: [38/200] Step 078/078 Loss 1.360 Prec@(1,5) (68.4%, 89.0%)
06/01 02:35:12 AM | Valid: [38/200] Final Prec@1 68.4100%
06/01 02:35:12 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 164.0, 0.00250244140625]
['model.relu.alpha_mask_1_0', 16384, 205.0, 0.01251220703125]
['model.relu.alpha_mask_2_0', 16384, 123.0, 0.00750732421875]
['model.relu.alpha_mask_3_0', 16384, 155.0, 0.00946044921875]
['model.relu.alpha_mask_4_0', 16384, 63.0, 0.00384521484375]
['model.relu.alpha_mask_5_0', 8192, 656.0, 0.080078125]
['model.relu.alpha_mask_6_0', 8192, 491.0, 0.0599365234375]
['model.relu.alpha_mask_7_0', 8192, 490.0, 0.059814453125]
['model.relu.alpha_mask_8_0', 8192, 493.0, 0.0601806640625]
['model.relu.alpha_mask_9_0', 4096, 1001.0, 0.244384765625]
['model.relu.alpha_mask_10_0', 4096, 1030.0, 0.25146484375]
['model.relu.alpha_mask_11_0', 4096, 1058.0, 0.25830078125]
['model.relu.alpha_mask_12_0', 4096, 1000.0, 0.244140625]
['model.relu.alpha_mask_13_0', 2048, 1548.0, 0.755859375]
['model.relu.alpha_mask_14_0', 2048, 1790.0, 0.8740234375]
['model.relu.alpha_mask_15_0', 2048, 1229.0, 0.60009765625]
['model.relu.alpha_mask_16_0', 2048, 1189.0, 0.58056640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12685.0, 0.06732443104619565]
########## End ###########
06/01 02:35:13 AM | Train: [39/80] Step 000/390 Loss 0.017 Prec@(1,5) (100.0%, 100.0%)
06/01 02:35:13 AM | layerwise density: [164.0, 205.0, 123.0, 155.0, 63.0, 656.0, 491.0, 490.0, 493.0, 1001.0, 1030.0, 1058.0, 1000.0, 1548.0, 1790.0, 1229.0, 1189.0]
layerwise density percentage: ['0.003', '0.013', '0.008', '0.009', '0.004', '0.080', '0.060', '0.060', '0.060', '0.244', '0.251', '0.258', '0.244', '0.756', '0.874', '0.600', '0.581']
Global density: 0.0673244297504425
06/01 02:35:24 AM | Train: [39/80] Step 100/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/01 02:35:24 AM | layerwise density: [165.0, 202.0, 120.0, 159.0, 63.0, 654.0, 479.0, 495.0, 495.0, 991.0, 1036.0, 1058.0, 999.0, 1541.0, 1788.0, 1224.0, 1194.0]
layerwise density percentage: ['0.003', '0.012', '0.007', '0.010', '0.004', '0.080', '0.058', '0.060', '0.060', '0.242', '0.253', '0.258', '0.244', '0.752', '0.873', '0.598', '0.583']
Global density: 0.06720767170190811
06/01 02:35:35 AM | Train: [39/80] Step 200/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/01 02:35:35 AM | layerwise density: [161.0, 202.0, 127.0, 153.0, 56.0, 662.0, 477.0, 495.0, 501.0, 988.0, 1019.0, 1060.0, 983.0, 1540.0, 1789.0, 1221.0, 1196.0]
layerwise density percentage: ['0.002', '0.012', '0.008', '0.009', '0.003', '0.081', '0.058', '0.060', '0.061', '0.241', '0.249', '0.259', '0.240', '0.752', '0.874', '0.596', '0.584']
Global density: 0.06703252345323563
06/01 02:35:46 AM | Train: [39/80] Step 300/390 Loss 0.014 Prec@(1,5) (99.9%, 100.0%)
06/01 02:35:46 AM | layerwise density: [157.0, 213.0, 127.0, 157.0, 66.0, 670.0, 504.0, 510.0, 506.0, 988.0, 1032.0, 1065.0, 982.0, 1556.0, 1795.0, 1226.0, 1200.0]
layerwise density percentage: ['0.002', '0.013', '0.008', '0.010', '0.004', '0.082', '0.062', '0.062', '0.062', '0.241', '0.252', '0.260', '0.240', '0.760', '0.876', '0.599', '0.586']
Global density: 0.0676906406879425
06/01 02:35:56 AM | Train: [39/80] Step 390/390 Loss 0.014 Prec@(1,5) (99.8%, 100.0%)
06/01 02:35:56 AM | layerwise density: [156.0, 217.0, 131.0, 162.0, 77.0, 681.0, 530.0, 525.0, 494.0, 990.0, 1044.0, 1068.0, 989.0, 1561.0, 1785.0, 1228.0, 1201.0]
layerwise density percentage: ['0.002', '0.013', '0.008', '0.010', '0.005', '0.083', '0.065', '0.064', '0.060', '0.242', '0.255', '0.261', '0.241', '0.762', '0.872', '0.600', '0.586']
Global density: 0.06814177334308624
06/01 02:35:56 AM | Train: [39/200] Final Prec@1 99.8480%
06/01 02:35:56 AM | Valid: [39/200] Step 000/078 Loss 1.104 Prec@(1,5) (72.7%, 91.4%)
06/01 02:35:58 AM | Valid: [39/200] Step 078/078 Loss 1.359 Prec@(1,5) (68.8%, 89.1%)
06/01 02:35:58 AM | Valid: [39/200] Final Prec@1 68.7900%
06/01 02:35:58 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 156.0, 0.00238037109375]
['model.relu.alpha_mask_1_0', 16384, 216.0, 0.01318359375]
['model.relu.alpha_mask_2_0', 16384, 130.0, 0.0079345703125]
['model.relu.alpha_mask_3_0', 16384, 160.0, 0.009765625]
['model.relu.alpha_mask_4_0', 16384, 76.0, 0.004638671875]
['model.relu.alpha_mask_5_0', 8192, 678.0, 0.082763671875]
['model.relu.alpha_mask_6_0', 8192, 527.0, 0.0643310546875]
['model.relu.alpha_mask_7_0', 8192, 523.0, 0.0638427734375]
['model.relu.alpha_mask_8_0', 8192, 490.0, 0.059814453125]
['model.relu.alpha_mask_9_0', 4096, 988.0, 0.2412109375]
['model.relu.alpha_mask_10_0', 4096, 1039.0, 0.253662109375]
['model.relu.alpha_mask_11_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_12_0', 4096, 986.0, 0.24072265625]
['model.relu.alpha_mask_13_0', 2048, 1561.0, 0.76220703125]
['model.relu.alpha_mask_14_0', 2048, 1781.0, 0.86962890625]
['model.relu.alpha_mask_15_0', 2048, 1228.0, 0.599609375]
['model.relu.alpha_mask_16_0', 2048, 1201.0, 0.58642578125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12808.0, 0.06797724184782608]
########## End ###########
06/01 02:35:59 AM | Train: [40/80] Step 000/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
06/01 02:35:59 AM | layerwise density: [156.0, 216.0, 130.0, 160.0, 76.0, 678.0, 527.0, 523.0, 490.0, 988.0, 1039.0, 1068.0, 986.0, 1561.0, 1781.0, 1228.0, 1201.0]
layerwise density percentage: ['0.002', '0.013', '0.008', '0.010', '0.005', '0.083', '0.064', '0.064', '0.060', '0.241', '0.254', '0.261', '0.241', '0.762', '0.870', '0.600', '0.586']
Global density: 0.06797724217176437
06/01 02:36:10 AM | Train: [40/80] Step 100/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/01 02:36:10 AM | layerwise density: [158.0, 201.0, 119.0, 155.0, 78.0, 645.0, 491.0, 495.0, 456.0, 962.0, 1029.0, 1064.0, 961.0, 1539.0, 1782.0, 1214.0, 1201.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.005', '0.079', '0.060', '0.060', '0.056', '0.235', '0.251', '0.260', '0.235', '0.751', '0.870', '0.593', '0.586']
Global density: 0.06660792976617813
06/01 02:36:21 AM | Train: [40/80] Step 200/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/01 02:36:21 AM | layerwise density: [162.0, 216.0, 118.0, 157.0, 70.0, 659.0, 496.0, 498.0, 472.0, 956.0, 1026.0, 1071.0, 974.0, 1555.0, 1792.0, 1224.0, 1210.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.010', '0.004', '0.080', '0.061', '0.061', '0.058', '0.233', '0.250', '0.261', '0.238', '0.759', '0.875', '0.598', '0.591']
Global density: 0.06717051565647125
06/01 02:36:32 AM | Train: [40/80] Step 300/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/01 02:36:32 AM | layerwise density: [157.0, 229.0, 123.0, 159.0, 66.0, 670.0, 511.0, 500.0, 489.0, 974.0, 1043.0, 1071.0, 1005.0, 1561.0, 1799.0, 1228.0, 1212.0]
layerwise density percentage: ['0.002', '0.014', '0.008', '0.010', '0.004', '0.082', '0.062', '0.061', '0.060', '0.238', '0.255', '0.261', '0.245', '0.762', '0.878', '0.600', '0.592']
Global density: 0.06791885942220688
06/01 02:36:41 AM | Train: [40/80] Step 390/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/01 02:36:41 AM | layerwise density: [154.0, 217.0, 125.0, 150.0, 58.0, 644.0, 498.0, 488.0, 467.0, 948.0, 1028.0, 1065.0, 977.0, 1545.0, 1782.0, 1218.0, 1215.0]
layerwise density percentage: ['0.002', '0.013', '0.008', '0.009', '0.004', '0.079', '0.061', '0.060', '0.057', '0.231', '0.251', '0.260', '0.239', '0.754', '0.870', '0.595', '0.593']
Global density: 0.06676184386014938
06/01 02:36:41 AM | Train: [40/200] Final Prec@1 99.8960%
06/01 02:36:42 AM | Valid: [40/200] Step 000/078 Loss 1.145 Prec@(1,5) (76.6%, 89.1%)
06/01 02:36:44 AM | Valid: [40/200] Step 078/078 Loss 1.361 Prec@(1,5) (68.5%, 89.1%)
06/01 02:36:44 AM | Valid: [40/200] Final Prec@1 68.4800%
06/01 02:36:44 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 154.0, 0.002349853515625]
['model.relu.alpha_mask_1_0', 16384, 216.0, 0.01318359375]
['model.relu.alpha_mask_2_0', 16384, 125.0, 0.00762939453125]
['model.relu.alpha_mask_3_0', 16384, 150.0, 0.0091552734375]
['model.relu.alpha_mask_4_0', 16384, 58.0, 0.0035400390625]
['model.relu.alpha_mask_5_0', 8192, 644.0, 0.07861328125]
['model.relu.alpha_mask_6_0', 8192, 498.0, 0.060791015625]
['model.relu.alpha_mask_7_0', 8192, 489.0, 0.0596923828125]
['model.relu.alpha_mask_8_0', 8192, 466.0, 0.056884765625]
['model.relu.alpha_mask_9_0', 4096, 948.0, 0.2314453125]
['model.relu.alpha_mask_10_0', 4096, 1027.0, 0.250732421875]
['model.relu.alpha_mask_11_0', 4096, 1065.0, 0.260009765625]
['model.relu.alpha_mask_12_0', 4096, 977.0, 0.238525390625]
['model.relu.alpha_mask_13_0', 2048, 1545.0, 0.75439453125]
['model.relu.alpha_mask_14_0', 2048, 1782.0, 0.8701171875]
['model.relu.alpha_mask_15_0', 2048, 1217.0, 0.59423828125]
['model.relu.alpha_mask_16_0', 2048, 1215.0, 0.59326171875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12576.0, 0.06674592391304347]
########## End ###########
06/01 02:36:45 AM | Train: [41/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 02:36:45 AM | layerwise density: [154.0, 216.0, 125.0, 150.0, 58.0, 644.0, 498.0, 489.0, 466.0, 948.0, 1027.0, 1065.0, 977.0, 1545.0, 1782.0, 1217.0, 1215.0]
layerwise density percentage: ['0.002', '0.013', '0.008', '0.009', '0.004', '0.079', '0.061', '0.060', '0.057', '0.231', '0.251', '0.260', '0.239', '0.754', '0.870', '0.594', '0.593']
Global density: 0.06674592196941376
06/01 02:36:55 AM | Train: [41/80] Step 100/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/01 02:36:55 AM | layerwise density: [156.0, 213.0, 125.0, 151.0, 62.0, 647.0, 488.0, 485.0, 473.0, 964.0, 1026.0, 1068.0, 970.0, 1539.0, 1777.0, 1220.0, 1218.0]
layerwise density percentage: ['0.002', '0.013', '0.008', '0.009', '0.004', '0.079', '0.060', '0.059', '0.058', '0.235', '0.250', '0.261', '0.237', '0.751', '0.868', '0.596', '0.595']
Global density: 0.0667777732014656
06/01 02:37:06 AM | Train: [41/80] Step 200/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/01 02:37:06 AM | layerwise density: [152.0, 211.0, 123.0, 155.0, 62.0, 668.0, 496.0, 501.0, 475.0, 995.0, 1046.0, 1068.0, 993.0, 1562.0, 1794.0, 1232.0, 1220.0]
layerwise density percentage: ['0.002', '0.013', '0.008', '0.009', '0.004', '0.082', '0.061', '0.061', '0.058', '0.243', '0.255', '0.261', '0.242', '0.763', '0.876', '0.602', '0.596']
Global density: 0.0676853358745575
06/01 02:37:16 AM | Train: [41/80] Step 300/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/01 02:37:16 AM | layerwise density: [149.0, 216.0, 122.0, 149.0, 65.0, 646.0, 482.0, 487.0, 457.0, 979.0, 1021.0, 1070.0, 987.0, 1570.0, 1790.0, 1222.0, 1225.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.009', '0.004', '0.079', '0.059', '0.059', '0.056', '0.239', '0.249', '0.261', '0.241', '0.767', '0.874', '0.597', '0.598']
Global density: 0.06706967949867249
06/01 02:37:26 AM | Train: [41/80] Step 390/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/01 02:37:26 AM | layerwise density: [151.0, 204.0, 117.0, 148.0, 57.0, 623.0, 454.0, 484.0, 457.0, 951.0, 1015.0, 1072.0, 981.0, 1569.0, 1794.0, 1218.0, 1227.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.003', '0.076', '0.055', '0.059', '0.056', '0.232', '0.248', '0.262', '0.240', '0.766', '0.876', '0.595', '0.599']
Global density: 0.06645932793617249
06/01 02:37:26 AM | Train: [41/200] Final Prec@1 99.9080%
06/01 02:37:26 AM | Valid: [41/200] Step 000/078 Loss 1.036 Prec@(1,5) (77.3%, 89.8%)
06/01 02:37:28 AM | Valid: [41/200] Step 078/078 Loss 1.369 Prec@(1,5) (68.9%, 88.6%)
06/01 02:37:28 AM | Valid: [41/200] Final Prec@1 68.9300%
06/01 02:37:28 AM | Current mask training best Prec@1 = 69.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 151.0, 0.0023040771484375]
['model.relu.alpha_mask_1_0', 16384, 206.0, 0.0125732421875]
['model.relu.alpha_mask_2_0', 16384, 117.0, 0.00714111328125]
['model.relu.alpha_mask_3_0', 16384, 148.0, 0.009033203125]
['model.relu.alpha_mask_4_0', 16384, 57.0, 0.00347900390625]
['model.relu.alpha_mask_5_0', 8192, 622.0, 0.075927734375]
['model.relu.alpha_mask_6_0', 8192, 454.0, 0.055419921875]
['model.relu.alpha_mask_7_0', 8192, 484.0, 0.05908203125]
['model.relu.alpha_mask_8_0', 8192, 457.0, 0.0557861328125]
['model.relu.alpha_mask_9_0', 4096, 951.0, 0.232177734375]
['model.relu.alpha_mask_10_0', 4096, 1015.0, 0.247802734375]
['model.relu.alpha_mask_11_0', 4096, 1072.0, 0.26171875]
['model.relu.alpha_mask_12_0', 4096, 980.0, 0.2392578125]
['model.relu.alpha_mask_13_0', 2048, 1568.0, 0.765625]
['model.relu.alpha_mask_14_0', 2048, 1797.0, 0.87744140625]
['model.relu.alpha_mask_15_0', 2048, 1217.0, 0.59423828125]
['model.relu.alpha_mask_16_0', 2048, 1227.0, 0.59912109375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12523.0, 0.06646463145380435]
########## End ###########
06/01 02:37:30 AM | Train: [42/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/01 02:37:30 AM | layerwise density: [151.0, 206.0, 117.0, 148.0, 57.0, 622.0, 454.0, 484.0, 457.0, 951.0, 1015.0, 1072.0, 980.0, 1568.0, 1797.0, 1217.0, 1227.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.009', '0.003', '0.076', '0.055', '0.059', '0.056', '0.232', '0.248', '0.262', '0.239', '0.766', '0.877', '0.594', '0.599']
Global density: 0.0664646327495575
06/01 02:37:40 AM | Train: [42/80] Step 100/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/01 02:37:40 AM | layerwise density: [148.0, 206.0, 113.0, 150.0, 67.0, 626.0, 470.0, 481.0, 481.0, 975.0, 1024.0, 1072.0, 994.0, 1563.0, 1808.0, 1221.0, 1233.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.009', '0.004', '0.076', '0.057', '0.059', '0.059', '0.238', '0.250', '0.262', '0.243', '0.763', '0.883', '0.596', '0.602']
Global density: 0.06704314053058624
06/01 02:37:51 AM | Train: [42/80] Step 200/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/01 02:37:51 AM | layerwise density: [149.0, 218.0, 117.0, 156.0, 77.0, 636.0, 494.0, 497.0, 493.0, 989.0, 1040.0, 1075.0, 1016.0, 1574.0, 1805.0, 1237.0, 1236.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.010', '0.005', '0.078', '0.060', '0.061', '0.060', '0.241', '0.254', '0.262', '0.248', '0.769', '0.881', '0.604', '0.604']
Global density: 0.06798254698514938
06/01 02:38:02 AM | Train: [42/80] Step 300/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/01 02:38:02 AM | layerwise density: [147.0, 209.0, 110.0, 146.0, 64.0, 606.0, 467.0, 478.0, 458.0, 973.0, 1029.0, 1077.0, 996.0, 1546.0, 1786.0, 1223.0, 1236.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.009', '0.004', '0.074', '0.057', '0.058', '0.056', '0.238', '0.251', '0.263', '0.243', '0.755', '0.872', '0.597', '0.604']
Global density: 0.06661324203014374
06/01 02:38:11 AM | Train: [42/80] Step 390/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/01 02:38:11 AM | layerwise density: [149.0, 212.0, 113.0, 138.0, 60.0, 611.0, 469.0, 478.0, 465.0, 982.0, 1018.0, 1080.0, 986.0, 1550.0, 1793.0, 1230.0, 1237.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.008', '0.004', '0.075', '0.057', '0.058', '0.057', '0.240', '0.249', '0.264', '0.241', '0.757', '0.875', '0.601', '0.604']
Global density: 0.06671939045190811
06/01 02:38:11 AM | Train: [42/200] Final Prec@1 99.8740%
06/01 02:38:12 AM | Valid: [42/200] Step 000/078 Loss 1.141 Prec@(1,5) (74.2%, 92.2%)
06/01 02:38:14 AM | Valid: [42/200] Step 078/078 Loss 1.347 Prec@(1,5) (69.1%, 89.1%)
06/01 02:38:14 AM | Valid: [42/200] Final Prec@1 69.1400%
06/01 02:38:15 AM | Current mask training best Prec@1 = 69.1400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 149.0, 0.0022735595703125]
['model.relu.alpha_mask_1_0', 16384, 212.0, 0.012939453125]
['model.relu.alpha_mask_2_0', 16384, 113.0, 0.00689697265625]
['model.relu.alpha_mask_3_0', 16384, 138.0, 0.0084228515625]
['model.relu.alpha_mask_4_0', 16384, 60.0, 0.003662109375]
['model.relu.alpha_mask_5_0', 8192, 611.0, 0.0745849609375]
['model.relu.alpha_mask_6_0', 8192, 469.0, 0.0572509765625]
['model.relu.alpha_mask_7_0', 8192, 478.0, 0.058349609375]
['model.relu.alpha_mask_8_0', 8192, 465.0, 0.0567626953125]
['model.relu.alpha_mask_9_0', 4096, 982.0, 0.23974609375]
['model.relu.alpha_mask_10_0', 4096, 1018.0, 0.24853515625]
['model.relu.alpha_mask_11_0', 4096, 1080.0, 0.263671875]
['model.relu.alpha_mask_12_0', 4096, 986.0, 0.24072265625]
['model.relu.alpha_mask_13_0', 2048, 1550.0, 0.7568359375]
['model.relu.alpha_mask_14_0', 2048, 1793.0, 0.87548828125]
['model.relu.alpha_mask_15_0', 2048, 1230.0, 0.6005859375]
['model.relu.alpha_mask_16_0', 2048, 1237.0, 0.60400390625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12571.0, 0.06671938688858696]
########## End ###########
06/01 02:38:16 AM | Train: [43/80] Step 000/390 Loss 0.017 Prec@(1,5) (100.0%, 100.0%)
06/01 02:38:16 AM | layerwise density: [149.0, 212.0, 113.0, 138.0, 60.0, 611.0, 469.0, 478.0, 465.0, 982.0, 1018.0, 1080.0, 986.0, 1550.0, 1793.0, 1230.0, 1237.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.008', '0.004', '0.075', '0.057', '0.058', '0.057', '0.240', '0.249', '0.264', '0.241', '0.757', '0.875', '0.601', '0.604']
Global density: 0.06671939045190811
06/01 02:38:26 AM | Train: [43/80] Step 100/390 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)
06/01 02:38:26 AM | layerwise density: [146.0, 209.0, 120.0, 144.0, 57.0, 630.0, 472.0, 496.0, 491.0, 1013.0, 1023.0, 1085.0, 998.0, 1561.0, 1806.0, 1233.0, 1240.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.009', '0.003', '0.077', '0.058', '0.061', '0.060', '0.247', '0.250', '0.265', '0.244', '0.762', '0.882', '0.602', '0.605']
Global density: 0.06753142178058624
06/01 02:38:37 AM | Train: [43/80] Step 200/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/01 02:38:37 AM | layerwise density: [146.0, 210.0, 122.0, 148.0, 61.0, 665.0, 506.0, 507.0, 500.0, 1011.0, 1045.0, 1087.0, 1015.0, 1573.0, 1821.0, 1237.0, 1244.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.009', '0.004', '0.081', '0.062', '0.062', '0.061', '0.247', '0.255', '0.265', '0.248', '0.768', '0.889', '0.604', '0.607']
Global density: 0.06845490634441376
06/01 02:38:48 AM | Train: [43/80] Step 300/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/01 02:38:48 AM | layerwise density: [139.0, 193.0, 110.0, 135.0, 60.0, 601.0, 463.0, 481.0, 462.0, 972.0, 1022.0, 1081.0, 986.0, 1563.0, 1799.0, 1230.0, 1244.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.008', '0.004', '0.073', '0.057', '0.059', '0.056', '0.237', '0.250', '0.264', '0.241', '0.763', '0.878', '0.601', '0.607']
Global density: 0.06656016409397125
06/01 02:38:58 AM | Train: [43/80] Step 390/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/01 02:38:58 AM | layerwise density: [135.0, 194.0, 112.0, 140.0, 58.0, 594.0, 465.0, 477.0, 464.0, 967.0, 1020.0, 1083.0, 988.0, 1562.0, 1799.0, 1233.0, 1246.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.004', '0.073', '0.057', '0.058', '0.057', '0.236', '0.249', '0.264', '0.241', '0.763', '0.878', '0.602', '0.608']
Global density: 0.06653893738985062
06/01 02:38:58 AM | Train: [43/200] Final Prec@1 99.9200%
06/01 02:38:58 AM | Valid: [43/200] Step 000/078 Loss 1.078 Prec@(1,5) (77.3%, 89.8%)
06/01 02:39:00 AM | Valid: [43/200] Step 078/078 Loss 1.342 Prec@(1,5) (69.2%, 89.2%)
06/01 02:39:01 AM | Valid: [43/200] Final Prec@1 69.1800%
06/01 02:39:01 AM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 135.0, 0.0020599365234375]
['model.relu.alpha_mask_1_0', 16384, 194.0, 0.0118408203125]
['model.relu.alpha_mask_2_0', 16384, 112.0, 0.0068359375]
['model.relu.alpha_mask_3_0', 16384, 140.0, 0.008544921875]
['model.relu.alpha_mask_4_0', 16384, 58.0, 0.0035400390625]
['model.relu.alpha_mask_5_0', 8192, 594.0, 0.072509765625]
['model.relu.alpha_mask_6_0', 8192, 465.0, 0.0567626953125]
['model.relu.alpha_mask_7_0', 8192, 477.0, 0.0582275390625]
['model.relu.alpha_mask_8_0', 8192, 464.0, 0.056640625]
['model.relu.alpha_mask_9_0', 4096, 967.0, 0.236083984375]
['model.relu.alpha_mask_10_0', 4096, 1020.0, 0.2490234375]
['model.relu.alpha_mask_11_0', 4096, 1083.0, 0.264404296875]
['model.relu.alpha_mask_12_0', 4096, 988.0, 0.2412109375]
['model.relu.alpha_mask_13_0', 2048, 1562.0, 0.7626953125]
['model.relu.alpha_mask_14_0', 2048, 1799.0, 0.87841796875]
['model.relu.alpha_mask_15_0', 2048, 1233.0, 0.60205078125]
['model.relu.alpha_mask_16_0', 2048, 1246.0, 0.6083984375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12537.0, 0.06653893512228261]
########## End ###########
06/01 02:39:02 AM | Train: [44/80] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 02:39:02 AM | layerwise density: [135.0, 194.0, 112.0, 140.0, 58.0, 594.0, 465.0, 477.0, 464.0, 967.0, 1020.0, 1083.0, 988.0, 1562.0, 1799.0, 1233.0, 1246.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.004', '0.073', '0.057', '0.058', '0.057', '0.236', '0.249', '0.264', '0.241', '0.763', '0.878', '0.602', '0.608']
Global density: 0.06653893738985062
06/01 02:39:13 AM | Train: [44/80] Step 100/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/01 02:39:13 AM | layerwise density: [135.0, 200.0, 113.0, 148.0, 66.0, 633.0, 475.0, 486.0, 471.0, 995.0, 1029.0, 1087.0, 1004.0, 1577.0, 1799.0, 1237.0, 1247.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.004', '0.077', '0.058', '0.059', '0.057', '0.243', '0.251', '0.265', '0.245', '0.770', '0.878', '0.604', '0.609']
Global density: 0.06741465628147125
06/01 02:39:24 AM | Train: [44/80] Step 200/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/01 02:39:24 AM | layerwise density: [138.0, 208.0, 118.0, 149.0, 71.0, 633.0, 488.0, 493.0, 498.0, 1017.0, 1048.0, 1087.0, 1012.0, 1573.0, 1812.0, 1244.0, 1258.0]
layerwise density percentage: ['0.002', '0.013', '0.007', '0.009', '0.004', '0.077', '0.060', '0.060', '0.061', '0.248', '0.256', '0.265', '0.247', '0.768', '0.885', '0.607', '0.614']
Global density: 0.06818423420190811
06/01 02:39:34 AM | Train: [44/80] Step 300/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/01 02:39:34 AM | layerwise density: [135.0, 186.0, 112.0, 137.0, 61.0, 572.0, 464.0, 464.0, 467.0, 993.0, 1020.0, 1088.0, 1000.0, 1563.0, 1806.0, 1227.0, 1264.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.004', '0.070', '0.057', '0.057', '0.057', '0.242', '0.249', '0.266', '0.244', '0.763', '0.882', '0.599', '0.617']
Global density: 0.0666557028889656
06/01 02:39:44 AM | Train: [44/80] Step 390/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/01 02:39:44 AM | layerwise density: [134.0, 186.0, 108.0, 139.0, 57.0, 592.0, 465.0, 466.0, 453.0, 989.0, 1039.0, 1086.0, 1011.0, 1564.0, 1808.0, 1228.0, 1266.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.003', '0.072', '0.057', '0.057', '0.055', '0.241', '0.254', '0.265', '0.247', '0.764', '0.883', '0.600', '0.618']
Global density: 0.06682553887367249
06/01 02:39:44 AM | Train: [44/200] Final Prec@1 99.8980%
06/01 02:39:44 AM | Valid: [44/200] Step 000/078 Loss 1.031 Prec@(1,5) (75.8%, 91.4%)
06/01 02:39:46 AM | Valid: [44/200] Step 078/078 Loss 1.344 Prec@(1,5) (68.9%, 89.2%)
06/01 02:39:46 AM | Valid: [44/200] Final Prec@1 68.9100%
06/01 02:39:46 AM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 134.0, 0.002044677734375]
['model.relu.alpha_mask_1_0', 16384, 186.0, 0.0113525390625]
['model.relu.alpha_mask_2_0', 16384, 108.0, 0.006591796875]
['model.relu.alpha_mask_3_0', 16384, 139.0, 0.00848388671875]
['model.relu.alpha_mask_4_0', 16384, 57.0, 0.00347900390625]
['model.relu.alpha_mask_5_0', 8192, 592.0, 0.072265625]
['model.relu.alpha_mask_6_0', 8192, 465.0, 0.0567626953125]
['model.relu.alpha_mask_7_0', 8192, 468.0, 0.05712890625]
['model.relu.alpha_mask_8_0', 8192, 454.0, 0.055419921875]
['model.relu.alpha_mask_9_0', 4096, 989.0, 0.241455078125]
['model.relu.alpha_mask_10_0', 4096, 1039.0, 0.253662109375]
['model.relu.alpha_mask_11_0', 4096, 1086.0, 0.26513671875]
['model.relu.alpha_mask_12_0', 4096, 1010.0, 0.24658203125]
['model.relu.alpha_mask_13_0', 2048, 1564.0, 0.763671875]
['model.relu.alpha_mask_14_0', 2048, 1808.0, 0.8828125]
['model.relu.alpha_mask_15_0', 2048, 1228.0, 0.599609375]
['model.relu.alpha_mask_16_0', 2048, 1266.0, 0.6181640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12593.0, 0.06683614979619565]
########## End ###########
06/01 02:39:47 AM | Train: [45/80] Step 000/390 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)
06/01 02:39:47 AM | layerwise density: [134.0, 186.0, 108.0, 139.0, 57.0, 592.0, 465.0, 468.0, 454.0, 989.0, 1039.0, 1086.0, 1010.0, 1564.0, 1808.0, 1228.0, 1266.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.003', '0.072', '0.057', '0.057', '0.055', '0.241', '0.254', '0.265', '0.247', '0.764', '0.883', '0.600', '0.618']
Global density: 0.0668361485004425
06/01 02:39:58 AM | Train: [45/80] Step 100/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/01 02:39:58 AM | layerwise density: [134.0, 198.0, 110.0, 145.0, 59.0, 623.0, 477.0, 473.0, 457.0, 1001.0, 1051.0, 1089.0, 1030.0, 1568.0, 1815.0, 1236.0, 1270.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.004', '0.076', '0.058', '0.058', '0.056', '0.244', '0.257', '0.266', '0.251', '0.766', '0.886', '0.604', '0.620']
Global density: 0.06759510934352875
06/01 02:40:08 AM | Train: [45/80] Step 200/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/01 02:40:08 AM | layerwise density: [136.0, 194.0, 106.0, 139.0, 58.0, 612.0, 494.0, 472.0, 475.0, 1000.0, 1049.0, 1086.0, 1021.0, 1561.0, 1807.0, 1238.0, 1276.0]
layerwise density percentage: ['0.002', '0.012', '0.006', '0.008', '0.004', '0.075', '0.060', '0.058', '0.058', '0.244', '0.256', '0.265', '0.249', '0.762', '0.882', '0.604', '0.623']
Global density: 0.06753142178058624
06/01 02:40:19 AM | Train: [45/80] Step 300/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/01 02:40:19 AM | layerwise density: [132.0, 179.0, 105.0, 135.0, 48.0, 584.0, 477.0, 459.0, 458.0, 977.0, 1031.0, 1083.0, 991.0, 1556.0, 1799.0, 1229.0, 1276.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.008', '0.003', '0.071', '0.058', '0.056', '0.056', '0.239', '0.252', '0.264', '0.242', '0.760', '0.878', '0.600', '0.623']
Global density: 0.06644340604543686
06/01 02:40:28 AM | Train: [45/80] Step 390/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/01 02:40:28 AM | layerwise density: [130.0, 177.0, 106.0, 131.0, 52.0, 585.0, 475.0, 471.0, 452.0, 975.0, 1038.0, 1082.0, 983.0, 1562.0, 1804.0, 1236.0, 1280.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.008', '0.003', '0.071', '0.058', '0.057', '0.055', '0.238', '0.253', '0.264', '0.240', '0.763', '0.881', '0.604', '0.625']
Global density: 0.06654955446720123
06/01 02:40:29 AM | Train: [45/200] Final Prec@1 99.9000%
06/01 02:40:29 AM | Valid: [45/200] Step 000/078 Loss 1.081 Prec@(1,5) (77.3%, 90.6%)
06/01 02:40:31 AM | Valid: [45/200] Step 078/078 Loss 1.346 Prec@(1,5) (69.5%, 89.4%)
06/01 02:40:31 AM | Valid: [45/200] Final Prec@1 69.5200%
06/01 02:40:32 AM | Current mask training best Prec@1 = 69.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 130.0, 0.001983642578125]
['model.relu.alpha_mask_1_0', 16384, 178.0, 0.0108642578125]
['model.relu.alpha_mask_2_0', 16384, 105.0, 0.00640869140625]
['model.relu.alpha_mask_3_0', 16384, 131.0, 0.00799560546875]
['model.relu.alpha_mask_4_0', 16384, 52.0, 0.003173828125]
['model.relu.alpha_mask_5_0', 8192, 587.0, 0.0716552734375]
['model.relu.alpha_mask_6_0', 8192, 476.0, 0.05810546875]
['model.relu.alpha_mask_7_0', 8192, 472.0, 0.0576171875]
['model.relu.alpha_mask_8_0', 8192, 453.0, 0.0552978515625]
['model.relu.alpha_mask_9_0', 4096, 976.0, 0.23828125]
['model.relu.alpha_mask_10_0', 4096, 1039.0, 0.253662109375]
['model.relu.alpha_mask_11_0', 4096, 1083.0, 0.264404296875]
['model.relu.alpha_mask_12_0', 4096, 983.0, 0.239990234375]
['model.relu.alpha_mask_13_0', 2048, 1563.0, 0.76318359375]
['model.relu.alpha_mask_14_0', 2048, 1804.0, 0.880859375]
['model.relu.alpha_mask_15_0', 2048, 1236.0, 0.603515625]
['model.relu.alpha_mask_16_0', 2048, 1280.0, 0.625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12548.0, 0.06659731657608696]
########## End ###########
06/01 02:40:33 AM | Train: [46/80] Step 000/390 Loss 0.011 Prec@(1,5) (99.2%, 100.0%)
06/01 02:40:33 AM | layerwise density: [130.0, 178.0, 105.0, 131.0, 52.0, 587.0, 476.0, 472.0, 453.0, 976.0, 1039.0, 1083.0, 983.0, 1563.0, 1804.0, 1236.0, 1280.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.008', '0.003', '0.072', '0.058', '0.058', '0.055', '0.238', '0.254', '0.264', '0.240', '0.763', '0.881', '0.604', '0.625']
Global density: 0.06659732013940811
06/01 02:40:44 AM | Train: [46/80] Step 100/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 02:40:44 AM | layerwise density: [129.0, 187.0, 109.0, 137.0, 57.0, 607.0, 479.0, 481.0, 451.0, 990.0, 1045.0, 1090.0, 989.0, 1569.0, 1813.0, 1244.0, 1281.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.003', '0.074', '0.058', '0.059', '0.055', '0.242', '0.255', '0.266', '0.241', '0.766', '0.885', '0.607', '0.625']
Global density: 0.06718113273382187
06/01 02:40:54 AM | Train: [46/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:40:54 AM | layerwise density: [131.0, 200.0, 110.0, 151.0, 59.0, 649.0, 488.0, 497.0, 467.0, 1009.0, 1064.0, 1095.0, 998.0, 1573.0, 1815.0, 1244.0, 1283.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.004', '0.079', '0.060', '0.061', '0.057', '0.246', '0.260', '0.267', '0.244', '0.768', '0.886', '0.607', '0.626']
Global density: 0.06810992956161499
06/01 02:41:05 AM | Train: [46/80] Step 300/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:41:05 AM | layerwise density: [128.0, 185.0, 106.0, 144.0, 58.0, 589.0, 468.0, 463.0, 439.0, 980.0, 1033.0, 1092.0, 981.0, 1560.0, 1803.0, 1227.0, 1286.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.009', '0.004', '0.072', '0.057', '0.057', '0.054', '0.239', '0.252', '0.267', '0.240', '0.762', '0.880', '0.599', '0.628']
Global density: 0.06656547635793686
06/01 02:41:15 AM | Train: [46/80] Step 390/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:41:15 AM | layerwise density: [129.0, 188.0, 110.0, 141.0, 57.0, 600.0, 456.0, 454.0, 445.0, 972.0, 1037.0, 1087.0, 980.0, 1561.0, 1810.0, 1233.0, 1286.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.009', '0.003', '0.073', '0.056', '0.055', '0.054', '0.237', '0.253', '0.265', '0.239', '0.762', '0.884', '0.602', '0.628']
Global density: 0.0665867030620575
06/01 02:41:15 AM | Train: [46/200] Final Prec@1 99.9220%
06/01 02:41:15 AM | Valid: [46/200] Step 000/078 Loss 1.093 Prec@(1,5) (75.0%, 90.6%)
06/01 02:41:17 AM | Valid: [46/200] Step 078/078 Loss 1.335 Prec@(1,5) (69.3%, 89.4%)
06/01 02:41:17 AM | Valid: [46/200] Final Prec@1 69.2700%
06/01 02:41:17 AM | Current mask training best Prec@1 = 69.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 128.0, 0.001953125]
['model.relu.alpha_mask_1_0', 16384, 187.0, 0.01141357421875]
['model.relu.alpha_mask_2_0', 16384, 111.0, 0.00677490234375]
['model.relu.alpha_mask_3_0', 16384, 142.0, 0.0086669921875]
['model.relu.alpha_mask_4_0', 16384, 58.0, 0.0035400390625]
['model.relu.alpha_mask_5_0', 8192, 599.0, 0.0731201171875]
['model.relu.alpha_mask_6_0', 8192, 455.0, 0.0555419921875]
['model.relu.alpha_mask_7_0', 8192, 454.0, 0.055419921875]
['model.relu.alpha_mask_8_0', 8192, 447.0, 0.0545654296875]
['model.relu.alpha_mask_9_0', 4096, 972.0, 0.2373046875]
['model.relu.alpha_mask_10_0', 4096, 1037.0, 0.253173828125]
['model.relu.alpha_mask_11_0', 4096, 1087.0, 0.265380859375]
['model.relu.alpha_mask_12_0', 4096, 980.0, 0.2392578125]
['model.relu.alpha_mask_13_0', 2048, 1561.0, 0.76220703125]
['model.relu.alpha_mask_14_0', 2048, 1811.0, 0.88427734375]
['model.relu.alpha_mask_15_0', 2048, 1234.0, 0.6025390625]
['model.relu.alpha_mask_16_0', 2048, 1286.0, 0.6279296875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12549.0, 0.06660262398097826]
########## End ###########
06/01 02:41:18 AM | Train: [47/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 02:41:18 AM | layerwise density: [128.0, 187.0, 111.0, 142.0, 58.0, 599.0, 455.0, 454.0, 447.0, 972.0, 1037.0, 1087.0, 980.0, 1561.0, 1811.0, 1234.0, 1286.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.009', '0.004', '0.073', '0.056', '0.055', '0.055', '0.237', '0.253', '0.265', '0.239', '0.762', '0.884', '0.603', '0.628']
Global density: 0.06660262495279312
06/01 02:41:29 AM | Train: [47/80] Step 100/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 02:41:29 AM | layerwise density: [127.0, 186.0, 109.0, 148.0, 60.0, 597.0, 460.0, 454.0, 461.0, 980.0, 1041.0, 1091.0, 1005.0, 1567.0, 1809.0, 1236.0, 1288.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.009', '0.004', '0.073', '0.056', '0.055', '0.056', '0.239', '0.254', '0.266', '0.245', '0.765', '0.883', '0.604', '0.629']
Global density: 0.06697414070367813
06/01 02:41:39 AM | Train: [47/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:41:39 AM | layerwise density: [127.0, 194.0, 110.0, 151.0, 56.0, 623.0, 471.0, 478.0, 468.0, 1012.0, 1053.0, 1096.0, 1028.0, 1579.0, 1809.0, 1241.0, 1293.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.003', '0.076', '0.057', '0.058', '0.057', '0.247', '0.257', '0.268', '0.251', '0.771', '0.883', '0.606', '0.631']
Global density: 0.0678764060139656
06/01 02:41:50 AM | Train: [47/80] Step 300/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:41:50 AM | layerwise density: [124.0, 188.0, 103.0, 140.0, 63.0, 585.0, 454.0, 472.0, 455.0, 994.0, 1044.0, 1097.0, 1013.0, 1569.0, 1809.0, 1233.0, 1299.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.009', '0.004', '0.071', '0.055', '0.058', '0.056', '0.243', '0.255', '0.268', '0.247', '0.766', '0.883', '0.602', '0.634']
Global density: 0.06709621101617813
06/01 02:42:00 AM | Train: [47/80] Step 390/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/01 02:42:00 AM | layerwise density: [126.0, 186.0, 100.0, 129.0, 63.0, 548.0, 431.0, 469.0, 438.0, 980.0, 1014.0, 1092.0, 1001.0, 1565.0, 1806.0, 1230.0, 1300.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.008', '0.004', '0.067', '0.053', '0.057', '0.053', '0.239', '0.248', '0.267', '0.244', '0.764', '0.882', '0.601', '0.635']
Global density: 0.0662257969379425
06/01 02:42:00 AM | Train: [47/200] Final Prec@1 99.9100%
06/01 02:42:00 AM | Valid: [47/200] Step 000/078 Loss 1.095 Prec@(1,5) (72.7%, 91.4%)
06/01 02:42:03 AM | Valid: [47/200] Step 078/078 Loss 1.336 Prec@(1,5) (69.4%, 89.2%)
06/01 02:42:03 AM | Valid: [47/200] Final Prec@1 69.4200%
06/01 02:42:03 AM | Current mask training best Prec@1 = 69.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 126.0, 0.001922607421875]
['model.relu.alpha_mask_1_0', 16384, 184.0, 0.01123046875]
['model.relu.alpha_mask_2_0', 16384, 102.0, 0.0062255859375]
['model.relu.alpha_mask_3_0', 16384, 129.0, 0.00787353515625]
['model.relu.alpha_mask_4_0', 16384, 63.0, 0.00384521484375]
['model.relu.alpha_mask_5_0', 8192, 549.0, 0.0670166015625]
['model.relu.alpha_mask_6_0', 8192, 431.0, 0.0526123046875]
['model.relu.alpha_mask_7_0', 8192, 469.0, 0.0572509765625]
['model.relu.alpha_mask_8_0', 8192, 439.0, 0.0535888671875]
['model.relu.alpha_mask_9_0', 4096, 980.0, 0.2392578125]
['model.relu.alpha_mask_10_0', 4096, 1015.0, 0.247802734375]
['model.relu.alpha_mask_11_0', 4096, 1092.0, 0.2666015625]
['model.relu.alpha_mask_12_0', 4096, 1002.0, 0.24462890625]
['model.relu.alpha_mask_13_0', 2048, 1565.0, 0.76416015625]
['model.relu.alpha_mask_14_0', 2048, 1806.0, 0.8818359375]
['model.relu.alpha_mask_15_0', 2048, 1231.0, 0.60107421875]
['model.relu.alpha_mask_16_0', 2048, 1300.0, 0.634765625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12483.0, 0.06625233525815218]
########## End ###########
06/01 02:42:04 AM | Train: [48/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:42:04 AM | layerwise density: [126.0, 184.0, 102.0, 129.0, 63.0, 549.0, 431.0, 469.0, 439.0, 980.0, 1015.0, 1092.0, 1002.0, 1565.0, 1806.0, 1231.0, 1300.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.008', '0.004', '0.067', '0.053', '0.057', '0.054', '0.239', '0.248', '0.267', '0.245', '0.764', '0.882', '0.601', '0.635']
Global density: 0.06625233590602875
06/01 02:42:14 AM | Train: [48/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:42:14 AM | layerwise density: [126.0, 182.0, 105.0, 127.0, 66.0, 552.0, 438.0, 483.0, 436.0, 987.0, 1000.0, 1093.0, 994.0, 1572.0, 1820.0, 1234.0, 1302.0]
layerwise density percentage: ['0.002', '0.011', '0.006', '0.008', '0.004', '0.067', '0.053', '0.059', '0.053', '0.241', '0.244', '0.267', '0.243', '0.768', '0.889', '0.603', '0.636']
Global density: 0.06643278896808624
06/01 02:42:25 AM | Train: [48/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:42:25 AM | layerwise density: [121.0, 188.0, 110.0, 134.0, 62.0, 577.0, 441.0, 489.0, 441.0, 1001.0, 1009.0, 1095.0, 1002.0, 1580.0, 1832.0, 1238.0, 1302.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.004', '0.070', '0.054', '0.060', '0.054', '0.244', '0.246', '0.267', '0.245', '0.771', '0.895', '0.604', '0.636']
Global density: 0.06699006259441376
06/01 02:42:36 AM | Train: [48/80] Step 300/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:42:36 AM | layerwise density: [123.0, 200.0, 125.0, 139.0, 61.0, 615.0, 461.0, 489.0, 457.0, 1030.0, 1023.0, 1098.0, 1021.0, 1576.0, 1830.0, 1245.0, 1303.0]
layerwise density percentage: ['0.002', '0.012', '0.008', '0.008', '0.004', '0.075', '0.056', '0.060', '0.056', '0.251', '0.250', '0.268', '0.249', '0.770', '0.894', '0.608', '0.636']
Global density: 0.06791355460882187
06/01 02:42:46 AM | Train: [48/80] Step 390/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:42:46 AM | layerwise density: [120.0, 191.0, 116.0, 145.0, 56.0, 599.0, 462.0, 473.0, 449.0, 1018.0, 1027.0, 1096.0, 1017.0, 1572.0, 1815.0, 1233.0, 1304.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.003', '0.073', '0.056', '0.058', '0.055', '0.249', '0.251', '0.268', '0.248', '0.768', '0.886', '0.602', '0.637']
Global density: 0.06736689060926437
06/01 02:42:46 AM | Train: [48/200] Final Prec@1 99.9180%
06/01 02:42:46 AM | Valid: [48/200] Step 000/078 Loss 1.085 Prec@(1,5) (75.8%, 91.4%)
06/01 02:42:48 AM | Valid: [48/200] Step 078/078 Loss 1.335 Prec@(1,5) (69.4%, 89.6%)
06/01 02:42:48 AM | Valid: [48/200] Final Prec@1 69.3700%
06/01 02:42:48 AM | Current mask training best Prec@1 = 69.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 120.0, 0.0018310546875]
['model.relu.alpha_mask_1_0', 16384, 191.0, 0.01165771484375]
['model.relu.alpha_mask_2_0', 16384, 116.0, 0.007080078125]
['model.relu.alpha_mask_3_0', 16384, 145.0, 0.00885009765625]
['model.relu.alpha_mask_4_0', 16384, 56.0, 0.00341796875]
['model.relu.alpha_mask_5_0', 8192, 599.0, 0.0731201171875]
['model.relu.alpha_mask_6_0', 8192, 462.0, 0.056396484375]
['model.relu.alpha_mask_7_0', 8192, 473.0, 0.0577392578125]
['model.relu.alpha_mask_8_0', 8192, 449.0, 0.0548095703125]
['model.relu.alpha_mask_9_0', 4096, 1018.0, 0.24853515625]
['model.relu.alpha_mask_10_0', 4096, 1027.0, 0.250732421875]
['model.relu.alpha_mask_11_0', 4096, 1096.0, 0.267578125]
['model.relu.alpha_mask_12_0', 4096, 1017.0, 0.248291015625]
['model.relu.alpha_mask_13_0', 2048, 1572.0, 0.767578125]
['model.relu.alpha_mask_14_0', 2048, 1815.0, 0.88623046875]
['model.relu.alpha_mask_15_0', 2048, 1233.0, 0.60205078125]
['model.relu.alpha_mask_16_0', 2048, 1304.0, 0.63671875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12693.0, 0.06736689028532608]
########## End ###########
06/01 02:42:49 AM | Train: [49/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 02:42:49 AM | layerwise density: [120.0, 191.0, 116.0, 145.0, 56.0, 599.0, 462.0, 473.0, 449.0, 1018.0, 1027.0, 1096.0, 1017.0, 1572.0, 1815.0, 1233.0, 1304.0]
layerwise density percentage: ['0.002', '0.012', '0.007', '0.009', '0.003', '0.073', '0.056', '0.058', '0.055', '0.249', '0.251', '0.268', '0.248', '0.768', '0.886', '0.602', '0.637']
Global density: 0.06736689060926437
06/01 02:43:00 AM | Train: [49/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:43:00 AM | layerwise density: [118.0, 180.0, 110.0, 144.0, 52.0, 564.0, 448.0, 457.0, 424.0, 1002.0, 1020.0, 1094.0, 992.0, 1558.0, 1814.0, 1233.0, 1304.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.009', '0.003', '0.069', '0.055', '0.056', '0.052', '0.245', '0.249', '0.267', '0.242', '0.761', '0.886', '0.602', '0.637']
Global density: 0.06641686707735062
06/01 02:43:11 AM | Train: [49/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:43:11 AM | layerwise density: [117.0, 177.0, 110.0, 138.0, 55.0, 570.0, 438.0, 455.0, 432.0, 1013.0, 1021.0, 1095.0, 1000.0, 1575.0, 1814.0, 1232.0, 1306.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.003', '0.070', '0.053', '0.056', '0.053', '0.247', '0.249', '0.267', '0.244', '0.769', '0.886', '0.602', '0.638']
Global density: 0.06659732013940811
06/01 02:43:22 AM | Train: [49/80] Step 300/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/01 02:43:22 AM | layerwise density: [116.0, 183.0, 111.0, 137.0, 55.0, 585.0, 450.0, 464.0, 446.0, 1028.0, 1031.0, 1093.0, 1008.0, 1585.0, 1820.0, 1241.0, 1307.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.003', '0.071', '0.055', '0.057', '0.054', '0.251', '0.252', '0.267', '0.246', '0.774', '0.889', '0.606', '0.638']
Global density: 0.06719174981117249
06/01 02:43:31 AM | Train: [49/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:43:31 AM | layerwise density: [123.0, 183.0, 116.0, 133.0, 60.0, 603.0, 466.0, 476.0, 466.0, 1034.0, 1059.0, 1093.0, 1020.0, 1588.0, 1830.0, 1249.0, 1310.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.004', '0.074', '0.057', '0.058', '0.057', '0.252', '0.259', '0.267', '0.249', '0.775', '0.894', '0.610', '0.640']
Global density: 0.06798254698514938
06/01 02:43:31 AM | Train: [49/200] Final Prec@1 99.9360%
06/01 02:43:31 AM | Valid: [49/200] Step 000/078 Loss 1.102 Prec@(1,5) (72.7%, 89.8%)
06/01 02:43:34 AM | Valid: [49/200] Step 078/078 Loss 1.327 Prec@(1,5) (69.6%, 89.5%)
06/01 02:43:34 AM | Valid: [49/200] Final Prec@1 69.6100%
06/01 02:43:35 AM | Current mask training best Prec@1 = 69.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 123.0, 0.0018768310546875]
['model.relu.alpha_mask_1_0', 16384, 183.0, 0.01116943359375]
['model.relu.alpha_mask_2_0', 16384, 116.0, 0.007080078125]
['model.relu.alpha_mask_3_0', 16384, 133.0, 0.00811767578125]
['model.relu.alpha_mask_4_0', 16384, 60.0, 0.003662109375]
['model.relu.alpha_mask_5_0', 8192, 603.0, 0.0736083984375]
['model.relu.alpha_mask_6_0', 8192, 466.0, 0.056884765625]
['model.relu.alpha_mask_7_0', 8192, 476.0, 0.05810546875]
['model.relu.alpha_mask_8_0', 8192, 466.0, 0.056884765625]
['model.relu.alpha_mask_9_0', 4096, 1034.0, 0.25244140625]
['model.relu.alpha_mask_10_0', 4096, 1059.0, 0.258544921875]
['model.relu.alpha_mask_11_0', 4096, 1093.0, 0.266845703125]
['model.relu.alpha_mask_12_0', 4096, 1020.0, 0.2490234375]
['model.relu.alpha_mask_13_0', 2048, 1588.0, 0.775390625]
['model.relu.alpha_mask_14_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_15_0', 2048, 1249.0, 0.60986328125]
['model.relu.alpha_mask_16_0', 2048, 1310.0, 0.6396484375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12809.0, 0.06798254925271739]
########## End ###########
06/01 02:43:36 AM | Train: [50/80] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 02:43:36 AM | layerwise density: [123.0, 183.0, 116.0, 133.0, 60.0, 603.0, 466.0, 476.0, 466.0, 1034.0, 1059.0, 1093.0, 1020.0, 1588.0, 1830.0, 1249.0, 1310.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.004', '0.074', '0.057', '0.058', '0.057', '0.252', '0.259', '0.267', '0.249', '0.775', '0.894', '0.610', '0.640']
Global density: 0.06798254698514938
06/01 02:43:46 AM | Train: [50/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:43:46 AM | layerwise density: [113.0, 159.0, 111.0, 122.0, 55.0, 529.0, 435.0, 446.0, 430.0, 985.0, 1038.0, 1088.0, 993.0, 1581.0, 1816.0, 1229.0, 1311.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.065', '0.053', '0.054', '0.052', '0.240', '0.253', '0.266', '0.242', '0.772', '0.887', '0.600', '0.640']
Global density: 0.06602942198514938
06/01 02:43:57 AM | Train: [50/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:43:57 AM | layerwise density: [112.0, 140.0, 102.0, 118.0, 50.0, 452.0, 392.0, 409.0, 383.0, 934.0, 1003.0, 1080.0, 954.0, 1570.0, 1801.0, 1211.0, 1311.0]
layerwise density percentage: ['0.002', '0.009', '0.006', '0.007', '0.003', '0.055', '0.048', '0.050', '0.047', '0.228', '0.245', '0.264', '0.233', '0.767', '0.879', '0.591', '0.640']
Global density: 0.06380562484264374
06/01 02:44:08 AM | Train: [50/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:44:08 AM | layerwise density: [111.0, 131.0, 104.0, 115.0, 48.0, 454.0, 386.0, 392.0, 364.0, 919.0, 990.0, 1074.0, 946.0, 1565.0, 1798.0, 1219.0, 1311.0]
layerwise density percentage: ['0.002', '0.008', '0.006', '0.007', '0.003', '0.055', '0.047', '0.048', '0.044', '0.224', '0.242', '0.262', '0.231', '0.764', '0.878', '0.595', '0.640']
Global density: 0.06330142170190811
06/01 02:44:17 AM | Train: [50/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:44:17 AM | layerwise density: [109.0, 130.0, 102.0, 113.0, 47.0, 459.0, 384.0, 390.0, 359.0, 931.0, 992.0, 1074.0, 953.0, 1575.0, 1805.0, 1226.0, 1311.0]
layerwise density percentage: ['0.002', '0.008', '0.006', '0.007', '0.003', '0.056', '0.047', '0.048', '0.044', '0.227', '0.242', '0.262', '0.233', '0.769', '0.881', '0.599', '0.640']
Global density: 0.0634765625
06/01 02:44:18 AM | Train: [50/200] Final Prec@1 99.9240%
06/01 02:44:18 AM | Valid: [50/200] Step 000/078 Loss 1.021 Prec@(1,5) (75.0%, 90.6%)
06/01 02:44:20 AM | Valid: [50/200] Step 078/078 Loss 1.333 Prec@(1,5) (69.6%, 89.4%)
06/01 02:44:20 AM | Valid: [50/200] Final Prec@1 69.5800%
06/01 02:44:20 AM | Current mask training best Prec@1 = 69.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 110.0, 0.001678466796875]
['model.relu.alpha_mask_1_0', 16384, 130.0, 0.0079345703125]
['model.relu.alpha_mask_2_0', 16384, 102.0, 0.0062255859375]
['model.relu.alpha_mask_3_0', 16384, 113.0, 0.00689697265625]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 459.0, 0.0560302734375]
['model.relu.alpha_mask_6_0', 8192, 384.0, 0.046875]
['model.relu.alpha_mask_7_0', 8192, 390.0, 0.047607421875]
['model.relu.alpha_mask_8_0', 8192, 359.0, 0.0438232421875]
['model.relu.alpha_mask_9_0', 4096, 931.0, 0.227294921875]
['model.relu.alpha_mask_10_0', 4096, 992.0, 0.2421875]
['model.relu.alpha_mask_11_0', 4096, 1074.0, 0.26220703125]
['model.relu.alpha_mask_12_0', 4096, 952.0, 0.232421875]
['model.relu.alpha_mask_13_0', 2048, 1574.0, 0.7685546875]
['model.relu.alpha_mask_14_0', 2048, 1804.0, 0.880859375]
['model.relu.alpha_mask_15_0', 2048, 1226.0, 0.5986328125]
['model.relu.alpha_mask_16_0', 2048, 1311.0, 0.64013671875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 11958.0, 0.06346594769021739]
########## End ###########
06/01 02:44:21 AM | Train: [51/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:44:21 AM | layerwise density: [110.0, 130.0, 102.0, 113.0, 47.0, 459.0, 384.0, 390.0, 359.0, 931.0, 992.0, 1074.0, 952.0, 1574.0, 1804.0, 1226.0, 1311.0]
layerwise density percentage: ['0.002', '0.008', '0.006', '0.007', '0.003', '0.056', '0.047', '0.048', '0.044', '0.227', '0.242', '0.262', '0.232', '0.769', '0.881', '0.599', '0.640']
Global density: 0.06346594542264938
06/01 02:44:32 AM | Train: [51/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:44:32 AM | layerwise density: [113.0, 135.0, 105.0, 112.0, 44.0, 488.0, 389.0, 391.0, 372.0, 957.0, 1000.0, 1079.0, 978.0, 1574.0, 1811.0, 1235.0, 1312.0]
layerwise density percentage: ['0.002', '0.008', '0.006', '0.007', '0.003', '0.060', '0.047', '0.048', '0.045', '0.234', '0.244', '0.263', '0.239', '0.769', '0.884', '0.603', '0.641']
Global density: 0.06419306248426437
06/01 02:44:43 AM | Train: [51/80] Step 200/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 02:44:43 AM | layerwise density: [112.0, 149.0, 109.0, 109.0, 47.0, 515.0, 405.0, 402.0, 381.0, 982.0, 1015.0, 1086.0, 994.0, 1587.0, 1816.0, 1239.0, 1315.0]
layerwise density percentage: ['0.002', '0.009', '0.007', '0.007', '0.003', '0.063', '0.049', '0.049', '0.047', '0.240', '0.248', '0.265', '0.243', '0.775', '0.887', '0.605', '0.642']
Global density: 0.06508471071720123
06/01 02:44:54 AM | Train: [51/80] Step 300/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 02:44:54 AM | layerwise density: [111.0, 162.0, 111.0, 112.0, 54.0, 538.0, 432.0, 423.0, 396.0, 1001.0, 1029.0, 1090.0, 1018.0, 1599.0, 1832.0, 1248.0, 1317.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.066', '0.053', '0.052', '0.048', '0.244', '0.251', '0.266', '0.249', '0.781', '0.895', '0.609', '0.643']
Global density: 0.06619926542043686
06/01 02:45:05 AM | Train: [51/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:45:05 AM | layerwise density: [114.0, 169.0, 113.0, 114.0, 55.0, 579.0, 447.0, 437.0, 404.0, 1028.0, 1051.0, 1095.0, 1039.0, 1599.0, 1845.0, 1248.0, 1320.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.071', '0.055', '0.053', '0.049', '0.251', '0.257', '0.267', '0.254', '0.781', '0.901', '0.609', '0.645']
Global density: 0.06717582792043686
06/01 02:45:05 AM | Train: [51/200] Final Prec@1 99.9480%
06/01 02:45:05 AM | Valid: [51/200] Step 000/078 Loss 1.116 Prec@(1,5) (75.8%, 89.8%)
06/01 02:45:07 AM | Valid: [51/200] Step 078/078 Loss 1.329 Prec@(1,5) (69.6%, 89.5%)
06/01 02:45:07 AM | Valid: [51/200] Final Prec@1 69.5700%
06/01 02:45:07 AM | Current mask training best Prec@1 = 69.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 114.0, 0.001739501953125]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 113.0, 0.00689697265625]
['model.relu.alpha_mask_3_0', 16384, 114.0, 0.0069580078125]
['model.relu.alpha_mask_4_0', 16384, 55.0, 0.00335693359375]
['model.relu.alpha_mask_5_0', 8192, 578.0, 0.070556640625]
['model.relu.alpha_mask_6_0', 8192, 448.0, 0.0546875]
['model.relu.alpha_mask_7_0', 8192, 438.0, 0.053466796875]
['model.relu.alpha_mask_8_0', 8192, 404.0, 0.04931640625]
['model.relu.alpha_mask_9_0', 4096, 1028.0, 0.2509765625]
['model.relu.alpha_mask_10_0', 4096, 1051.0, 0.256591796875]
['model.relu.alpha_mask_11_0', 4096, 1094.0, 0.26708984375]
['model.relu.alpha_mask_12_0', 4096, 1040.0, 0.25390625]
['model.relu.alpha_mask_13_0', 2048, 1599.0, 0.78076171875]
['model.relu.alpha_mask_14_0', 2048, 1845.0, 0.90087890625]
['model.relu.alpha_mask_15_0', 2048, 1248.0, 0.609375]
['model.relu.alpha_mask_16_0', 2048, 1320.0, 0.64453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12659.0, 0.06718643851902174]
########## End ###########
06/01 02:45:08 AM | Train: [52/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:45:08 AM | layerwise density: [114.0, 170.0, 113.0, 114.0, 55.0, 578.0, 448.0, 438.0, 404.0, 1028.0, 1051.0, 1094.0, 1040.0, 1599.0, 1845.0, 1248.0, 1320.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.071', '0.055', '0.053', '0.049', '0.251', '0.257', '0.267', '0.254', '0.781', '0.901', '0.609', '0.645']
Global density: 0.06718643754720688
06/01 02:45:19 AM | Train: [52/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:45:19 AM | layerwise density: [117.0, 179.0, 117.0, 120.0, 59.0, 594.0, 440.0, 463.0, 426.0, 1049.0, 1065.0, 1100.0, 1044.0, 1597.0, 1842.0, 1245.0, 1322.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.073', '0.054', '0.057', '0.052', '0.256', '0.260', '0.269', '0.255', '0.780', '0.899', '0.608', '0.646']
Global density: 0.06782332807779312
06/01 02:45:30 AM | Train: [52/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:45:30 AM | layerwise density: [118.0, 181.0, 118.0, 123.0, 62.0, 640.0, 448.0, 485.0, 454.0, 1050.0, 1070.0, 1100.0, 1046.0, 1595.0, 1840.0, 1250.0, 1322.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.008', '0.004', '0.078', '0.055', '0.059', '0.055', '0.256', '0.261', '0.269', '0.255', '0.779', '0.898', '0.610', '0.646']
Global density: 0.06847614049911499
06/01 02:45:41 AM | Train: [52/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:45:41 AM | layerwise density: [110.0, 142.0, 98.0, 109.0, 45.0, 501.0, 378.0, 428.0, 375.0, 962.0, 1010.0, 1085.0, 982.0, 1565.0, 1798.0, 1218.0, 1322.0]
layerwise density percentage: ['0.002', '0.009', '0.006', '0.007', '0.003', '0.061', '0.046', '0.052', '0.046', '0.235', '0.247', '0.265', '0.240', '0.764', '0.878', '0.595', '0.646']
Global density: 0.06436821073293686
06/01 02:45:51 AM | Train: [52/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:45:51 AM | layerwise density: [108.0, 137.0, 94.0, 107.0, 46.0, 478.0, 362.0, 417.0, 365.0, 941.0, 994.0, 1082.0, 963.0, 1561.0, 1788.0, 1219.0, 1322.0]
layerwise density percentage: ['0.002', '0.008', '0.006', '0.007', '0.003', '0.058', '0.044', '0.051', '0.045', '0.230', '0.243', '0.264', '0.235', '0.762', '0.873', '0.595', '0.646']
Global density: 0.0636039450764656
06/01 02:45:51 AM | Train: [52/200] Final Prec@1 99.9340%
06/01 02:45:51 AM | Valid: [52/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.8%, 90.6%)
06/01 02:45:53 AM | Valid: [52/200] Step 078/078 Loss 1.337 Prec@(1,5) (69.5%, 89.4%)
06/01 02:45:53 AM | Valid: [52/200] Final Prec@1 69.4700%
06/01 02:45:53 AM | Current mask training best Prec@1 = 69.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 108.0, 0.00164794921875]
['model.relu.alpha_mask_1_0', 16384, 137.0, 0.00836181640625]
['model.relu.alpha_mask_2_0', 16384, 94.0, 0.0057373046875]
['model.relu.alpha_mask_3_0', 16384, 107.0, 0.00653076171875]
['model.relu.alpha_mask_4_0', 16384, 46.0, 0.0028076171875]
['model.relu.alpha_mask_5_0', 8192, 479.0, 0.0584716796875]
['model.relu.alpha_mask_6_0', 8192, 362.0, 0.044189453125]
['model.relu.alpha_mask_7_0', 8192, 417.0, 0.0509033203125]
['model.relu.alpha_mask_8_0', 8192, 366.0, 0.044677734375]
['model.relu.alpha_mask_9_0', 4096, 941.0, 0.229736328125]
['model.relu.alpha_mask_10_0', 4096, 995.0, 0.242919921875]
['model.relu.alpha_mask_11_0', 4096, 1081.0, 0.263916015625]
['model.relu.alpha_mask_12_0', 4096, 963.0, 0.235107421875]
['model.relu.alpha_mask_13_0', 2048, 1561.0, 0.76220703125]
['model.relu.alpha_mask_14_0', 2048, 1788.0, 0.873046875]
['model.relu.alpha_mask_15_0', 2048, 1220.0, 0.595703125]
['model.relu.alpha_mask_16_0', 2048, 1322.0, 0.6455078125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 11987.0, 0.06361986243206522]
########## End ###########
06/01 02:45:54 AM | Train: [53/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/01 02:45:54 AM | layerwise density: [108.0, 137.0, 94.0, 107.0, 46.0, 479.0, 362.0, 417.0, 366.0, 941.0, 995.0, 1081.0, 963.0, 1561.0, 1788.0, 1220.0, 1322.0]
layerwise density percentage: ['0.002', '0.008', '0.006', '0.007', '0.003', '0.058', '0.044', '0.051', '0.045', '0.230', '0.243', '0.264', '0.235', '0.762', '0.873', '0.596', '0.646']
Global density: 0.06361986696720123
06/01 02:46:05 AM | Train: [53/80] Step 100/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/01 02:46:05 AM | layerwise density: [109.0, 132.0, 93.0, 105.0, 49.0, 484.0, 349.0, 416.0, 361.0, 932.0, 993.0, 1081.0, 955.0, 1569.0, 1806.0, 1231.0, 1324.0]
layerwise density percentage: ['0.002', '0.008', '0.006', '0.006', '0.003', '0.059', '0.043', '0.051', '0.044', '0.228', '0.242', '0.264', '0.233', '0.766', '0.882', '0.601', '0.646']
Global density: 0.06363047659397125
06/01 02:46:16 AM | Train: [53/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:46:16 AM | layerwise density: [109.0, 131.0, 96.0, 108.0, 46.0, 495.0, 350.0, 418.0, 367.0, 936.0, 1004.0, 1082.0, 968.0, 1569.0, 1817.0, 1241.0, 1324.0]
layerwise density percentage: ['0.002', '0.008', '0.006', '0.007', '0.003', '0.060', '0.043', '0.051', '0.045', '0.229', '0.245', '0.264', '0.236', '0.766', '0.887', '0.606', '0.646']
Global density: 0.06401260942220688
06/01 02:46:26 AM | Train: [53/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:46:26 AM | layerwise density: [107.0, 136.0, 107.0, 109.0, 47.0, 516.0, 367.0, 418.0, 372.0, 953.0, 1015.0, 1084.0, 987.0, 1569.0, 1826.0, 1239.0, 1325.0]
layerwise density percentage: ['0.002', '0.008', '0.007', '0.007', '0.003', '0.063', '0.045', '0.051', '0.045', '0.233', '0.248', '0.265', '0.241', '0.766', '0.892', '0.605', '0.647']
Global density: 0.06462827324867249
06/01 02:46:36 AM | Train: [53/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:46:36 AM | layerwise density: [108.0, 140.0, 111.0, 110.0, 51.0, 529.0, 386.0, 426.0, 378.0, 974.0, 1026.0, 1084.0, 997.0, 1579.0, 1825.0, 1244.0, 1327.0]
layerwise density percentage: ['0.002', '0.009', '0.007', '0.007', '0.003', '0.065', '0.047', '0.052', '0.046', '0.238', '0.250', '0.265', '0.243', '0.771', '0.891', '0.607', '0.648']
Global density: 0.06525454670190811
06/01 02:46:36 AM | Train: [53/200] Final Prec@1 99.9400%
06/01 02:46:36 AM | Valid: [53/200] Step 000/078 Loss 1.034 Prec@(1,5) (78.9%, 89.1%)
06/01 02:46:39 AM | Valid: [53/200] Step 078/078 Loss 1.317 Prec@(1,5) (70.1%, 89.6%)
06/01 02:46:39 AM | Valid: [53/200] Final Prec@1 70.1000%
06/01 02:46:39 AM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 108.0, 0.00164794921875]
['model.relu.alpha_mask_1_0', 16384, 140.0, 0.008544921875]
['model.relu.alpha_mask_2_0', 16384, 111.0, 0.00677490234375]
['model.relu.alpha_mask_3_0', 16384, 110.0, 0.0067138671875]
['model.relu.alpha_mask_4_0', 16384, 51.0, 0.00311279296875]
['model.relu.alpha_mask_5_0', 8192, 529.0, 0.0645751953125]
['model.relu.alpha_mask_6_0', 8192, 387.0, 0.0472412109375]
['model.relu.alpha_mask_7_0', 8192, 426.0, 0.052001953125]
['model.relu.alpha_mask_8_0', 8192, 380.0, 0.04638671875]
['model.relu.alpha_mask_9_0', 4096, 975.0, 0.238037109375]
['model.relu.alpha_mask_10_0', 4096, 1027.0, 0.250732421875]
['model.relu.alpha_mask_11_0', 4096, 1084.0, 0.2646484375]
['model.relu.alpha_mask_12_0', 4096, 999.0, 0.243896484375]
['model.relu.alpha_mask_13_0', 2048, 1581.0, 0.77197265625]
['model.relu.alpha_mask_14_0', 2048, 1825.0, 0.89111328125]
['model.relu.alpha_mask_15_0', 2048, 1244.0, 0.607421875]
['model.relu.alpha_mask_16_0', 2048, 1327.0, 0.64794921875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12304.0, 0.06530230978260869]
########## End ###########
06/01 02:46:40 AM | Train: [54/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:46:40 AM | layerwise density: [108.0, 140.0, 111.0, 110.0, 51.0, 529.0, 387.0, 426.0, 380.0, 975.0, 1027.0, 1084.0, 999.0, 1581.0, 1825.0, 1244.0, 1327.0]
layerwise density percentage: ['0.002', '0.009', '0.007', '0.007', '0.003', '0.065', '0.047', '0.052', '0.046', '0.238', '0.251', '0.265', '0.244', '0.772', '0.891', '0.607', '0.648']
Global density: 0.06530231237411499
06/01 02:46:51 AM | Train: [54/80] Step 100/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:46:51 AM | layerwise density: [110.0, 147.0, 114.0, 111.0, 53.0, 538.0, 401.0, 437.0, 388.0, 1004.0, 1036.0, 1088.0, 1010.0, 1587.0, 1831.0, 1253.0, 1332.0]
layerwise density percentage: ['0.002', '0.009', '0.007', '0.007', '0.003', '0.066', '0.049', '0.053', '0.047', '0.245', '0.253', '0.266', '0.247', '0.775', '0.894', '0.612', '0.650']
Global density: 0.06602411717176437
06/01 02:47:02 AM | Train: [54/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:47:02 AM | layerwise density: [114.0, 159.0, 115.0, 110.0, 59.0, 565.0, 417.0, 448.0, 391.0, 1019.0, 1040.0, 1094.0, 1040.0, 1594.0, 1842.0, 1253.0, 1334.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.069', '0.051', '0.055', '0.048', '0.249', '0.254', '0.267', '0.254', '0.778', '0.899', '0.612', '0.651']
Global density: 0.06684146076440811
06/01 02:47:13 AM | Train: [54/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:47:13 AM | layerwise density: [112.0, 164.0, 113.0, 118.0, 60.0, 576.0, 432.0, 454.0, 408.0, 1041.0, 1054.0, 1098.0, 1052.0, 1601.0, 1843.0, 1250.0, 1337.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.070', '0.053', '0.055', '0.050', '0.254', '0.257', '0.268', '0.257', '0.782', '0.900', '0.610', '0.653']
Global density: 0.06747303903102875
06/01 02:47:22 AM | Train: [54/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:47:22 AM | layerwise density: [113.0, 170.0, 119.0, 121.0, 61.0, 599.0, 449.0, 460.0, 425.0, 1046.0, 1062.0, 1100.0, 1047.0, 1602.0, 1848.0, 1252.0, 1337.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.073', '0.055', '0.056', '0.052', '0.255', '0.259', '0.269', '0.256', '0.782', '0.902', '0.611', '0.653']
Global density: 0.0679931640625
06/01 02:47:22 AM | Train: [54/200] Final Prec@1 99.9320%
06/01 02:47:23 AM | Valid: [54/200] Step 000/078 Loss 1.050 Prec@(1,5) (78.1%, 89.8%)
06/01 02:47:25 AM | Valid: [54/200] Step 078/078 Loss 1.323 Prec@(1,5) (70.0%, 89.3%)
06/01 02:47:25 AM | Valid: [54/200] Final Prec@1 69.9900%
06/01 02:47:25 AM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 113.0, 0.0017242431640625]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 119.0, 0.00726318359375]
['model.relu.alpha_mask_3_0', 16384, 121.0, 0.00738525390625]
['model.relu.alpha_mask_4_0', 16384, 61.0, 0.00372314453125]
['model.relu.alpha_mask_5_0', 8192, 599.0, 0.0731201171875]
['model.relu.alpha_mask_6_0', 8192, 449.0, 0.0548095703125]
['model.relu.alpha_mask_7_0', 8192, 460.0, 0.05615234375]
['model.relu.alpha_mask_8_0', 8192, 425.0, 0.0518798828125]
['model.relu.alpha_mask_9_0', 4096, 1046.0, 0.25537109375]
['model.relu.alpha_mask_10_0', 4096, 1062.0, 0.25927734375]
['model.relu.alpha_mask_11_0', 4096, 1100.0, 0.2685546875]
['model.relu.alpha_mask_12_0', 4096, 1047.0, 0.255615234375]
['model.relu.alpha_mask_13_0', 2048, 1602.0, 0.7822265625]
['model.relu.alpha_mask_14_0', 2048, 1848.0, 0.90234375]
['model.relu.alpha_mask_15_0', 2048, 1252.0, 0.611328125]
['model.relu.alpha_mask_16_0', 2048, 1337.0, 0.65283203125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12811.0, 0.0679931640625]
########## End ###########
06/01 02:47:26 AM | Train: [55/80] Step 000/390 Loss 0.013 Prec@(1,5) (99.2%, 100.0%)
06/01 02:47:26 AM | layerwise density: [113.0, 170.0, 119.0, 121.0, 61.0, 599.0, 449.0, 460.0, 425.0, 1046.0, 1062.0, 1100.0, 1047.0, 1602.0, 1848.0, 1252.0, 1337.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.073', '0.055', '0.056', '0.052', '0.255', '0.259', '0.269', '0.256', '0.782', '0.902', '0.611', '0.653']
Global density: 0.0679931640625
06/01 02:47:37 AM | Train: [55/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/01 02:47:37 AM | layerwise density: [110.0, 170.0, 120.0, 127.0, 62.0, 604.0, 450.0, 462.0, 420.0, 1048.0, 1068.0, 1099.0, 1034.0, 1602.0, 1846.0, 1244.0, 1342.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.008', '0.004', '0.074', '0.055', '0.056', '0.051', '0.256', '0.261', '0.268', '0.252', '0.782', '0.901', '0.607', '0.655']
Global density: 0.06797724217176437
06/01 02:47:47 AM | Train: [55/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:47:47 AM | layerwise density: [106.0, 166.0, 117.0, 124.0, 59.0, 574.0, 425.0, 451.0, 406.0, 1018.0, 1053.0, 1096.0, 1020.0, 1582.0, 1831.0, 1240.0, 1342.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.008', '0.004', '0.070', '0.052', '0.055', '0.050', '0.249', '0.257', '0.268', '0.249', '0.772', '0.894', '0.605', '0.655']
Global density: 0.06692637503147125
06/01 02:47:58 AM | Train: [55/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:47:58 AM | layerwise density: [108.0, 167.0, 115.0, 122.0, 58.0, 578.0, 418.0, 446.0, 410.0, 1016.0, 1048.0, 1097.0, 1025.0, 1574.0, 1822.0, 1240.0, 1343.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.071', '0.051', '0.054', '0.050', '0.248', '0.256', '0.268', '0.250', '0.769', '0.890', '0.605', '0.656']
Global density: 0.06680430471897125
06/01 02:48:08 AM | Train: [55/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:48:08 AM | layerwise density: [107.0, 164.0, 117.0, 123.0, 64.0, 572.0, 424.0, 447.0, 399.0, 1016.0, 1051.0, 1098.0, 1013.0, 1576.0, 1821.0, 1244.0, 1343.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.008', '0.004', '0.070', '0.052', '0.055', '0.049', '0.248', '0.257', '0.268', '0.247', '0.770', '0.889', '0.607', '0.656']
Global density: 0.06676184386014938
06/01 02:48:08 AM | Train: [55/200] Final Prec@1 99.9300%
06/01 02:48:08 AM | Valid: [55/200] Step 000/078 Loss 1.116 Prec@(1,5) (75.8%, 89.8%)
06/01 02:48:10 AM | Valid: [55/200] Step 078/078 Loss 1.316 Prec@(1,5) (70.0%, 89.6%)
06/01 02:48:10 AM | Valid: [55/200] Final Prec@1 69.9500%
06/01 02:48:10 AM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 107.0, 0.0016326904296875]
['model.relu.alpha_mask_1_0', 16384, 164.0, 0.010009765625]
['model.relu.alpha_mask_2_0', 16384, 117.0, 0.00714111328125]
['model.relu.alpha_mask_3_0', 16384, 123.0, 0.00750732421875]
['model.relu.alpha_mask_4_0', 16384, 64.0, 0.00390625]
['model.relu.alpha_mask_5_0', 8192, 571.0, 0.0697021484375]
['model.relu.alpha_mask_6_0', 8192, 424.0, 0.0517578125]
['model.relu.alpha_mask_7_0', 8192, 447.0, 0.0545654296875]
['model.relu.alpha_mask_8_0', 8192, 399.0, 0.0487060546875]
['model.relu.alpha_mask_9_0', 4096, 1016.0, 0.248046875]
['model.relu.alpha_mask_10_0', 4096, 1051.0, 0.256591796875]
['model.relu.alpha_mask_11_0', 4096, 1098.0, 0.26806640625]
['model.relu.alpha_mask_12_0', 4096, 1013.0, 0.247314453125]
['model.relu.alpha_mask_13_0', 2048, 1576.0, 0.76953125]
['model.relu.alpha_mask_14_0', 2048, 1821.0, 0.88916015625]
['model.relu.alpha_mask_15_0', 2048, 1244.0, 0.607421875]
['model.relu.alpha_mask_16_0', 2048, 1343.0, 0.65576171875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12578.0, 0.06675653872282608]
########## End ###########
06/01 02:48:11 AM | Train: [56/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:48:11 AM | layerwise density: [107.0, 164.0, 117.0, 123.0, 64.0, 571.0, 424.0, 447.0, 399.0, 1016.0, 1051.0, 1098.0, 1013.0, 1576.0, 1821.0, 1244.0, 1343.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.008', '0.004', '0.070', '0.052', '0.055', '0.049', '0.248', '0.257', '0.268', '0.247', '0.770', '0.889', '0.607', '0.656']
Global density: 0.06675653904676437
06/01 02:48:23 AM | Train: [56/80] Step 100/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:48:23 AM | layerwise density: [105.0, 164.0, 116.0, 122.0, 66.0, 562.0, 424.0, 442.0, 406.0, 1024.0, 1058.0, 1099.0, 1026.0, 1587.0, 1834.0, 1244.0, 1344.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.069', '0.052', '0.054', '0.050', '0.250', '0.258', '0.268', '0.250', '0.775', '0.896', '0.607', '0.656']
Global density: 0.06699537485837936
06/01 02:48:34 AM | Train: [56/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:48:34 AM | layerwise density: [107.0, 166.0, 114.0, 120.0, 69.0, 560.0, 428.0, 448.0, 419.0, 1023.0, 1063.0, 1105.0, 1030.0, 1598.0, 1834.0, 1242.0, 1346.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.068', '0.052', '0.055', '0.051', '0.250', '0.260', '0.270', '0.251', '0.780', '0.896', '0.606', '0.657']
Global density: 0.06725543737411499
06/01 02:48:45 AM | Train: [56/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:48:45 AM | layerwise density: [107.0, 174.0, 112.0, 121.0, 68.0, 567.0, 430.0, 458.0, 431.0, 1027.0, 1065.0, 1104.0, 1048.0, 1596.0, 1838.0, 1248.0, 1348.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.069', '0.052', '0.056', '0.053', '0.251', '0.260', '0.270', '0.256', '0.779', '0.897', '0.609', '0.658']
Global density: 0.067626953125
06/01 02:48:56 AM | Train: [56/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/01 02:48:56 AM | layerwise density: [104.0, 175.0, 113.0, 120.0, 71.0, 574.0, 439.0, 465.0, 449.0, 1036.0, 1067.0, 1106.0, 1042.0, 1602.0, 1843.0, 1252.0, 1349.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.070', '0.054', '0.057', '0.055', '0.253', '0.260', '0.270', '0.254', '0.782', '0.900', '0.611', '0.659']
Global density: 0.06797193735837936
06/01 02:48:56 AM | Train: [56/200] Final Prec@1 99.9340%
06/01 02:48:56 AM | Valid: [56/200] Step 000/078 Loss 1.060 Prec@(1,5) (75.8%, 91.4%)
06/01 02:48:58 AM | Valid: [56/200] Step 078/078 Loss 1.310 Prec@(1,5) (70.1%, 89.8%)
06/01 02:48:58 AM | Valid: [56/200] Final Prec@1 70.0800%
06/01 02:48:58 AM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 104.0, 0.0015869140625]
['model.relu.alpha_mask_1_0', 16384, 175.0, 0.01068115234375]
['model.relu.alpha_mask_2_0', 16384, 112.0, 0.0068359375]
['model.relu.alpha_mask_3_0', 16384, 121.0, 0.00738525390625]
['model.relu.alpha_mask_4_0', 16384, 72.0, 0.00439453125]
['model.relu.alpha_mask_5_0', 8192, 573.0, 0.0699462890625]
['model.relu.alpha_mask_6_0', 8192, 438.0, 0.053466796875]
['model.relu.alpha_mask_7_0', 8192, 466.0, 0.056884765625]
['model.relu.alpha_mask_8_0', 8192, 449.0, 0.0548095703125]
['model.relu.alpha_mask_9_0', 4096, 1037.0, 0.253173828125]
['model.relu.alpha_mask_10_0', 4096, 1066.0, 0.26025390625]
['model.relu.alpha_mask_11_0', 4096, 1106.0, 0.27001953125]
['model.relu.alpha_mask_12_0', 4096, 1043.0, 0.254638671875]
['model.relu.alpha_mask_13_0', 2048, 1602.0, 0.7822265625]
['model.relu.alpha_mask_14_0', 2048, 1844.0, 0.900390625]
['model.relu.alpha_mask_15_0', 2048, 1252.0, 0.611328125]
['model.relu.alpha_mask_16_0', 2048, 1349.0, 0.65869140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12809.0, 0.06798254925271739]
########## End ###########
06/01 02:48:59 AM | Train: [57/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:48:59 AM | layerwise density: [104.0, 175.0, 112.0, 121.0, 72.0, 573.0, 438.0, 466.0, 449.0, 1037.0, 1066.0, 1106.0, 1043.0, 1602.0, 1844.0, 1252.0, 1349.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.070', '0.053', '0.057', '0.055', '0.253', '0.260', '0.270', '0.255', '0.782', '0.900', '0.611', '0.659']
Global density: 0.06798254698514938
06/01 02:49:10 AM | Train: [57/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:49:10 AM | layerwise density: [101.0, 172.0, 109.0, 117.0, 70.0, 571.0, 429.0, 460.0, 451.0, 1023.0, 1058.0, 1103.0, 1027.0, 1603.0, 1838.0, 1238.0, 1352.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.070', '0.052', '0.056', '0.055', '0.250', '0.258', '0.269', '0.251', '0.783', '0.897', '0.604', '0.660']
Global density: 0.06752080470323563
06/01 02:49:21 AM | Train: [57/80] Step 200/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:49:21 AM | layerwise density: [102.0, 168.0, 108.0, 115.0, 65.0, 559.0, 422.0, 453.0, 444.0, 1013.0, 1054.0, 1101.0, 1023.0, 1598.0, 1820.0, 1231.0, 1352.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.068', '0.052', '0.055', '0.054', '0.247', '0.257', '0.269', '0.250', '0.780', '0.889', '0.601', '0.660']
Global density: 0.0670219138264656
06/01 02:49:32 AM | Train: [57/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:49:32 AM | layerwise density: [101.0, 163.0, 109.0, 111.0, 63.0, 541.0, 420.0, 452.0, 441.0, 1020.0, 1053.0, 1096.0, 1023.0, 1594.0, 1818.0, 1237.0, 1353.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.066', '0.051', '0.055', '0.054', '0.249', '0.257', '0.268', '0.250', '0.778', '0.888', '0.604', '0.661']
Global density: 0.06684676557779312
06/01 02:49:41 AM | Train: [57/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/01 02:49:41 AM | layerwise density: [102.0, 161.0, 111.0, 111.0, 59.0, 549.0, 411.0, 450.0, 443.0, 1018.0, 1057.0, 1093.0, 1028.0, 1595.0, 1813.0, 1238.0, 1353.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.067', '0.050', '0.055', '0.054', '0.249', '0.258', '0.267', '0.251', '0.779', '0.885', '0.604', '0.661']
Global density: 0.0668308436870575
06/01 02:49:41 AM | Train: [57/200] Final Prec@1 99.9460%
06/01 02:49:42 AM | Valid: [57/200] Step 000/078 Loss 1.045 Prec@(1,5) (75.8%, 91.4%)
06/01 02:49:44 AM | Valid: [57/200] Step 078/078 Loss 1.311 Prec@(1,5) (70.1%, 89.8%)
06/01 02:49:44 AM | Valid: [57/200] Final Prec@1 70.1000%
06/01 02:49:44 AM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 160.0, 0.009765625]
['model.relu.alpha_mask_2_0', 16384, 111.0, 0.00677490234375]
['model.relu.alpha_mask_3_0', 16384, 111.0, 0.00677490234375]
['model.relu.alpha_mask_4_0', 16384, 59.0, 0.00360107421875]
['model.relu.alpha_mask_5_0', 8192, 551.0, 0.0672607421875]
['model.relu.alpha_mask_6_0', 8192, 411.0, 0.0501708984375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 443.0, 0.0540771484375]
['model.relu.alpha_mask_9_0', 4096, 1018.0, 0.24853515625]
['model.relu.alpha_mask_10_0', 4096, 1057.0, 0.258056640625]
['model.relu.alpha_mask_11_0', 4096, 1093.0, 0.266845703125]
['model.relu.alpha_mask_12_0', 4096, 1028.0, 0.2509765625]
['model.relu.alpha_mask_13_0', 2048, 1595.0, 0.77880859375]
['model.relu.alpha_mask_14_0', 2048, 1814.0, 0.8857421875]
['model.relu.alpha_mask_15_0', 2048, 1238.0, 0.6044921875]
['model.relu.alpha_mask_16_0', 2048, 1353.0, 0.66064453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12594.0, 0.06684145720108696]
########## End ###########
06/01 02:49:45 AM | Train: [58/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 02:49:45 AM | layerwise density: [102.0, 160.0, 111.0, 111.0, 59.0, 551.0, 411.0, 450.0, 443.0, 1018.0, 1057.0, 1093.0, 1028.0, 1595.0, 1814.0, 1238.0, 1353.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.067', '0.050', '0.055', '0.054', '0.249', '0.258', '0.267', '0.251', '0.779', '0.886', '0.604', '0.661']
Global density: 0.06684146076440811
06/01 02:49:55 AM | Train: [58/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:49:55 AM | layerwise density: [102.0, 161.0, 109.0, 109.0, 60.0, 554.0, 415.0, 453.0, 449.0, 1023.0, 1062.0, 1092.0, 1036.0, 1599.0, 1821.0, 1242.0, 1354.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.068', '0.051', '0.055', '0.055', '0.250', '0.259', '0.267', '0.253', '0.781', '0.889', '0.606', '0.661']
Global density: 0.06709090620279312
06/01 02:50:06 AM | Train: [58/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/01 02:50:06 AM | layerwise density: [103.0, 165.0, 112.0, 107.0, 60.0, 561.0, 416.0, 456.0, 454.0, 1019.0, 1066.0, 1092.0, 1034.0, 1602.0, 1817.0, 1247.0, 1355.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.068', '0.051', '0.056', '0.055', '0.249', '0.260', '0.267', '0.252', '0.782', '0.887', '0.609', '0.662']
Global density: 0.06722359359264374
06/01 02:50:17 AM | Train: [58/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:50:17 AM | layerwise density: [102.0, 168.0, 115.0, 108.0, 59.0, 570.0, 421.0, 464.0, 460.0, 1031.0, 1076.0, 1094.0, 1031.0, 1602.0, 1827.0, 1249.0, 1356.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.070', '0.051', '0.057', '0.056', '0.252', '0.263', '0.267', '0.252', '0.782', '0.892', '0.610', '0.662']
Global density: 0.06757918745279312
06/01 02:50:27 AM | Train: [58/80] Step 390/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:50:27 AM | layerwise density: [101.0, 176.0, 114.0, 110.0, 57.0, 569.0, 429.0, 466.0, 463.0, 1046.0, 1087.0, 1094.0, 1039.0, 1603.0, 1828.0, 1248.0, 1358.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.069', '0.052', '0.057', '0.057', '0.255', '0.265', '0.267', '0.254', '0.783', '0.893', '0.609', '0.663']
Global density: 0.06787109375
06/01 02:50:27 AM | Train: [58/200] Final Prec@1 99.9500%
06/01 02:50:27 AM | Valid: [58/200] Step 000/078 Loss 1.125 Prec@(1,5) (76.6%, 91.4%)
06/01 02:50:29 AM | Valid: [58/200] Step 078/078 Loss 1.314 Prec@(1,5) (70.0%, 90.0%)
06/01 02:50:29 AM | Valid: [58/200] Final Prec@1 69.9500%
06/01 02:50:29 AM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 101.0, 0.0015411376953125]
['model.relu.alpha_mask_1_0', 16384, 176.0, 0.0107421875]
['model.relu.alpha_mask_2_0', 16384, 114.0, 0.0069580078125]
['model.relu.alpha_mask_3_0', 16384, 110.0, 0.0067138671875]
['model.relu.alpha_mask_4_0', 16384, 57.0, 0.00347900390625]
['model.relu.alpha_mask_5_0', 8192, 570.0, 0.069580078125]
['model.relu.alpha_mask_6_0', 8192, 430.0, 0.052490234375]
['model.relu.alpha_mask_7_0', 8192, 466.0, 0.056884765625]
['model.relu.alpha_mask_8_0', 8192, 464.0, 0.056640625]
['model.relu.alpha_mask_9_0', 4096, 1046.0, 0.25537109375]
['model.relu.alpha_mask_10_0', 4096, 1087.0, 0.265380859375]
['model.relu.alpha_mask_11_0', 4096, 1094.0, 0.26708984375]
['model.relu.alpha_mask_12_0', 4096, 1040.0, 0.25390625]
['model.relu.alpha_mask_13_0', 2048, 1603.0, 0.78271484375]
['model.relu.alpha_mask_14_0', 2048, 1828.0, 0.892578125]
['model.relu.alpha_mask_15_0', 2048, 1249.0, 0.60986328125]
['model.relu.alpha_mask_16_0', 2048, 1358.0, 0.6630859375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12793.0, 0.06789763077445653]
########## End ###########
06/01 02:50:30 AM | Train: [59/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:50:30 AM | layerwise density: [101.0, 176.0, 114.0, 110.0, 57.0, 570.0, 430.0, 466.0, 464.0, 1046.0, 1087.0, 1094.0, 1040.0, 1603.0, 1828.0, 1249.0, 1358.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.070', '0.052', '0.057', '0.057', '0.255', '0.265', '0.267', '0.254', '0.783', '0.893', '0.610', '0.663']
Global density: 0.06789763271808624
06/01 02:50:41 AM | Train: [59/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:50:41 AM | layerwise density: [103.0, 181.0, 113.0, 110.0, 58.0, 583.0, 437.0, 475.0, 473.0, 1047.0, 1096.0, 1096.0, 1045.0, 1605.0, 1837.0, 1250.0, 1359.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.071', '0.053', '0.058', '0.058', '0.256', '0.268', '0.268', '0.255', '0.784', '0.897', '0.610', '0.664']
Global density: 0.0682956874370575
06/01 02:50:52 AM | Train: [59/80] Step 200/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:50:52 AM | layerwise density: [106.0, 171.0, 108.0, 107.0, 58.0, 560.0, 429.0, 457.0, 449.0, 1036.0, 1071.0, 1096.0, 1024.0, 1591.0, 1825.0, 1247.0, 1359.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.068', '0.052', '0.056', '0.055', '0.253', '0.261', '0.268', '0.250', '0.777', '0.891', '0.609', '0.664']
Global density: 0.06737219542264938
06/01 02:51:03 AM | Train: [59/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:51:03 AM | layerwise density: [103.0, 170.0, 108.0, 105.0, 58.0, 554.0, 417.0, 454.0, 438.0, 1025.0, 1067.0, 1095.0, 1019.0, 1582.0, 1826.0, 1249.0, 1359.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.006', '0.004', '0.068', '0.051', '0.055', '0.053', '0.250', '0.260', '0.267', '0.249', '0.772', '0.892', '0.610', '0.664']
Global density: 0.06702721863985062
06/01 02:51:13 AM | Train: [59/80] Step 390/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:51:13 AM | layerwise density: [104.0, 167.0, 106.0, 105.0, 58.0, 560.0, 419.0, 454.0, 436.0, 1019.0, 1058.0, 1096.0, 1022.0, 1586.0, 1824.0, 1247.0, 1360.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.006', '0.004', '0.068', '0.051', '0.055', '0.053', '0.249', '0.258', '0.268', '0.250', '0.774', '0.891', '0.609', '0.664']
Global density: 0.06698475778102875
06/01 02:51:13 AM | Train: [59/200] Final Prec@1 99.9500%
06/01 02:51:13 AM | Valid: [59/200] Step 000/078 Loss 1.033 Prec@(1,5) (77.3%, 90.6%)
06/01 02:51:15 AM | Valid: [59/200] Step 078/078 Loss 1.312 Prec@(1,5) (70.0%, 89.9%)
06/01 02:51:15 AM | Valid: [59/200] Final Prec@1 69.9900%
06/01 02:51:15 AM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 104.0, 0.0015869140625]
['model.relu.alpha_mask_1_0', 16384, 167.0, 0.01019287109375]
['model.relu.alpha_mask_2_0', 16384, 106.0, 0.0064697265625]
['model.relu.alpha_mask_3_0', 16384, 105.0, 0.00640869140625]
['model.relu.alpha_mask_4_0', 16384, 58.0, 0.0035400390625]
['model.relu.alpha_mask_5_0', 8192, 561.0, 0.0684814453125]
['model.relu.alpha_mask_6_0', 8192, 419.0, 0.0511474609375]
['model.relu.alpha_mask_7_0', 8192, 454.0, 0.055419921875]
['model.relu.alpha_mask_8_0', 8192, 436.0, 0.05322265625]
['model.relu.alpha_mask_9_0', 4096, 1019.0, 0.248779296875]
['model.relu.alpha_mask_10_0', 4096, 1058.0, 0.25830078125]
['model.relu.alpha_mask_11_0', 4096, 1096.0, 0.267578125]
['model.relu.alpha_mask_12_0', 4096, 1023.0, 0.249755859375]
['model.relu.alpha_mask_13_0', 2048, 1587.0, 0.77490234375]
['model.relu.alpha_mask_14_0', 2048, 1824.0, 0.890625]
['model.relu.alpha_mask_15_0', 2048, 1247.0, 0.60888671875]
['model.relu.alpha_mask_16_0', 2048, 1360.0, 0.6640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12624.0, 0.06700067934782608]
########## End ###########
06/01 02:51:16 AM | Train: [60/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:51:16 AM | layerwise density: [104.0, 167.0, 106.0, 105.0, 58.0, 561.0, 419.0, 454.0, 436.0, 1019.0, 1058.0, 1096.0, 1023.0, 1587.0, 1824.0, 1247.0, 1360.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.006', '0.004', '0.068', '0.051', '0.055', '0.053', '0.249', '0.258', '0.268', '0.250', '0.775', '0.891', '0.609', '0.664']
Global density: 0.06700067967176437
06/01 02:51:27 AM | Train: [60/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:51:27 AM | layerwise density: [105.0, 171.0, 105.0, 105.0, 59.0, 564.0, 426.0, 449.0, 434.0, 1012.0, 1057.0, 1094.0, 1024.0, 1585.0, 1830.0, 1247.0, 1361.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.006', '0.004', '0.069', '0.052', '0.055', '0.053', '0.247', '0.258', '0.267', '0.250', '0.774', '0.894', '0.609', '0.665']
Global density: 0.0670219138264656
06/01 02:51:38 AM | Train: [60/80] Step 200/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:51:38 AM | layerwise density: [102.0, 172.0, 110.0, 105.0, 59.0, 563.0, 429.0, 448.0, 428.0, 1014.0, 1055.0, 1097.0, 1023.0, 1586.0, 1832.0, 1247.0, 1364.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.006', '0.004', '0.069', '0.052', '0.055', '0.052', '0.248', '0.258', '0.268', '0.250', '0.774', '0.895', '0.609', '0.666']
Global density: 0.06705375760793686
06/01 02:51:49 AM | Train: [60/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:51:49 AM | layerwise density: [103.0, 171.0, 112.0, 106.0, 59.0, 571.0, 425.0, 459.0, 427.0, 1015.0, 1050.0, 1096.0, 1024.0, 1591.0, 1829.0, 1241.0, 1366.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.006', '0.004', '0.070', '0.052', '0.056', '0.052', '0.248', '0.256', '0.268', '0.250', '0.777', '0.893', '0.606', '0.667']
Global density: 0.06711213290691376
06/01 02:51:59 AM | Train: [60/80] Step 390/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:51:59 AM | layerwise density: [104.0, 173.0, 111.0, 110.0, 61.0, 593.0, 426.0, 462.0, 429.0, 1023.0, 1051.0, 1098.0, 1027.0, 1596.0, 1835.0, 1237.0, 1366.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.072', '0.052', '0.056', '0.052', '0.250', '0.257', '0.268', '0.251', '0.779', '0.896', '0.604', '0.667']
Global density: 0.06741465628147125
06/01 02:51:59 AM | Train: [60/200] Final Prec@1 99.9580%
06/01 02:51:59 AM | Valid: [60/200] Step 000/078 Loss 1.088 Prec@(1,5) (76.6%, 91.4%)
06/01 02:52:01 AM | Valid: [60/200] Step 078/078 Loss 1.315 Prec@(1,5) (69.9%, 89.7%)
06/01 02:52:01 AM | Valid: [60/200] Final Prec@1 69.9400%
06/01 02:52:01 AM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 104.0, 0.0015869140625]
['model.relu.alpha_mask_1_0', 16384, 173.0, 0.01055908203125]
['model.relu.alpha_mask_2_0', 16384, 111.0, 0.00677490234375]
['model.relu.alpha_mask_3_0', 16384, 110.0, 0.0067138671875]
['model.relu.alpha_mask_4_0', 16384, 61.0, 0.00372314453125]
['model.relu.alpha_mask_5_0', 8192, 593.0, 0.0723876953125]
['model.relu.alpha_mask_6_0', 8192, 426.0, 0.052001953125]
['model.relu.alpha_mask_7_0', 8192, 462.0, 0.056396484375]
['model.relu.alpha_mask_8_0', 8192, 429.0, 0.0523681640625]
['model.relu.alpha_mask_9_0', 4096, 1023.0, 0.249755859375]
['model.relu.alpha_mask_10_0', 4096, 1051.0, 0.256591796875]
['model.relu.alpha_mask_11_0', 4096, 1098.0, 0.26806640625]
['model.relu.alpha_mask_12_0', 4096, 1027.0, 0.250732421875]
['model.relu.alpha_mask_13_0', 2048, 1596.0, 0.779296875]
['model.relu.alpha_mask_14_0', 2048, 1835.0, 0.89599609375]
['model.relu.alpha_mask_15_0', 2048, 1237.0, 0.60400390625]
['model.relu.alpha_mask_16_0', 2048, 1366.0, 0.6669921875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12702.0, 0.06741465692934782]
########## End ###########
06/01 02:52:02 AM | Train: [61/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:52:02 AM | layerwise density: [104.0, 173.0, 111.0, 110.0, 61.0, 593.0, 426.0, 462.0, 429.0, 1023.0, 1051.0, 1098.0, 1027.0, 1596.0, 1835.0, 1237.0, 1366.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.072', '0.052', '0.056', '0.052', '0.250', '0.257', '0.268', '0.251', '0.779', '0.896', '0.604', '0.667']
Global density: 0.06741465628147125
06/01 02:52:13 AM | Train: [61/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:52:13 AM | layerwise density: [109.0, 172.0, 112.0, 108.0, 65.0, 586.0, 424.0, 465.0, 438.0, 1026.0, 1055.0, 1103.0, 1030.0, 1596.0, 1844.0, 1239.0, 1367.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.004', '0.072', '0.052', '0.057', '0.053', '0.250', '0.258', '0.269', '0.251', '0.779', '0.900', '0.605', '0.667']
Global density: 0.06761103123426437
06/01 02:52:24 AM | Train: [61/80] Step 200/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:52:24 AM | layerwise density: [108.0, 181.0, 114.0, 114.0, 65.0, 593.0, 425.0, 465.0, 443.0, 1030.0, 1051.0, 1104.0, 1028.0, 1595.0, 1845.0, 1245.0, 1368.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.072', '0.052', '0.057', '0.054', '0.251', '0.257', '0.270', '0.251', '0.779', '0.901', '0.608', '0.668']
Global density: 0.06779678910970688
06/01 02:52:34 AM | Train: [61/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:52:34 AM | layerwise density: [110.0, 186.0, 118.0, 115.0, 64.0, 603.0, 428.0, 468.0, 450.0, 1039.0, 1059.0, 1107.0, 1037.0, 1596.0, 1849.0, 1249.0, 1369.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.074', '0.052', '0.057', '0.055', '0.254', '0.259', '0.270', '0.253', '0.779', '0.903', '0.610', '0.668']
Global density: 0.06818423420190811
06/01 02:52:44 AM | Train: [61/80] Step 390/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:52:44 AM | layerwise density: [110.0, 186.0, 120.0, 118.0, 64.0, 613.0, 435.0, 472.0, 453.0, 1049.0, 1065.0, 1110.0, 1041.0, 1596.0, 1847.0, 1250.0, 1369.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.075', '0.053', '0.058', '0.055', '0.256', '0.260', '0.271', '0.254', '0.779', '0.902', '0.610', '0.668']
Global density: 0.06845490634441376
06/01 02:52:44 AM | Train: [61/200] Final Prec@1 99.9500%
06/01 02:52:44 AM | Valid: [61/200] Step 000/078 Loss 1.072 Prec@(1,5) (76.6%, 92.2%)
06/01 02:52:47 AM | Valid: [61/200] Step 078/078 Loss 1.301 Prec@(1,5) (70.5%, 90.0%)
06/01 02:52:47 AM | Valid: [61/200] Final Prec@1 70.5400%
06/01 02:52:47 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 110.0, 0.001678466796875]
['model.relu.alpha_mask_1_0', 16384, 186.0, 0.0113525390625]
['model.relu.alpha_mask_2_0', 16384, 120.0, 0.00732421875]
['model.relu.alpha_mask_3_0', 16384, 118.0, 0.0072021484375]
['model.relu.alpha_mask_4_0', 16384, 64.0, 0.00390625]
['model.relu.alpha_mask_5_0', 8192, 612.0, 0.07470703125]
['model.relu.alpha_mask_6_0', 8192, 435.0, 0.0531005859375]
['model.relu.alpha_mask_7_0', 8192, 472.0, 0.0576171875]
['model.relu.alpha_mask_8_0', 8192, 453.0, 0.0552978515625]
['model.relu.alpha_mask_9_0', 4096, 1049.0, 0.256103515625]
['model.relu.alpha_mask_10_0', 4096, 1065.0, 0.260009765625]
['model.relu.alpha_mask_11_0', 4096, 1110.0, 0.27099609375]
['model.relu.alpha_mask_12_0', 4096, 1041.0, 0.254150390625]
['model.relu.alpha_mask_13_0', 2048, 1596.0, 0.779296875]
['model.relu.alpha_mask_14_0', 2048, 1847.0, 0.90185546875]
['model.relu.alpha_mask_15_0', 2048, 1250.0, 0.6103515625]
['model.relu.alpha_mask_16_0', 2048, 1369.0, 0.66845703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12897.0, 0.06844960088315218]
########## End ###########
06/01 02:52:48 AM | Train: [62/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:52:48 AM | layerwise density: [110.0, 186.0, 120.0, 118.0, 64.0, 612.0, 435.0, 472.0, 453.0, 1049.0, 1065.0, 1110.0, 1041.0, 1596.0, 1847.0, 1250.0, 1369.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.075', '0.053', '0.058', '0.055', '0.256', '0.260', '0.271', '0.254', '0.779', '0.902', '0.610', '0.668']
Global density: 0.06844960153102875
06/01 02:52:59 AM | Train: [62/80] Step 100/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/01 02:52:59 AM | layerwise density: [106.0, 174.0, 110.0, 114.0, 59.0, 577.0, 416.0, 452.0, 432.0, 1030.0, 1055.0, 1107.0, 1035.0, 1590.0, 1836.0, 1244.0, 1369.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.070', '0.051', '0.055', '0.053', '0.251', '0.258', '0.270', '0.253', '0.776', '0.896', '0.607', '0.668']
Global density: 0.06743589043617249
06/01 02:53:10 AM | Train: [62/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/01 02:53:10 AM | layerwise density: [105.0, 175.0, 110.0, 114.0, 60.0, 567.0, 407.0, 453.0, 429.0, 1031.0, 1055.0, 1107.0, 1034.0, 1594.0, 1834.0, 1243.0, 1369.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.004', '0.069', '0.050', '0.055', '0.052', '0.252', '0.258', '0.270', '0.252', '0.778', '0.896', '0.607', '0.668']
Global density: 0.06733504682779312
06/01 02:53:21 AM | Train: [62/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/01 02:53:21 AM | layerwise density: [100.0, 173.0, 108.0, 112.0, 57.0, 562.0, 411.0, 453.0, 427.0, 1038.0, 1056.0, 1109.0, 1046.0, 1604.0, 1839.0, 1244.0, 1369.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.069', '0.050', '0.055', '0.052', '0.253', '0.258', '0.271', '0.255', '0.783', '0.898', '0.607', '0.668']
Global density: 0.0674465000629425
06/01 02:53:31 AM | Train: [62/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/01 02:53:31 AM | layerwise density: [101.0, 174.0, 108.0, 114.0, 55.0, 559.0, 414.0, 454.0, 426.0, 1042.0, 1058.0, 1109.0, 1046.0, 1603.0, 1844.0, 1243.0, 1369.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.068', '0.051', '0.055', '0.052', '0.254', '0.258', '0.271', '0.255', '0.783', '0.900', '0.607', '0.668']
Global density: 0.0675048828125
06/01 02:53:31 AM | Train: [62/200] Final Prec@1 99.9400%
06/01 02:53:31 AM | Valid: [62/200] Step 000/078 Loss 0.991 Prec@(1,5) (76.6%, 89.8%)
06/01 02:53:33 AM | Valid: [62/200] Step 078/078 Loss 1.308 Prec@(1,5) (70.3%, 89.9%)
06/01 02:53:33 AM | Valid: [62/200] Final Prec@1 70.3200%
06/01 02:53:33 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 101.0, 0.0015411376953125]
['model.relu.alpha_mask_1_0', 16384, 174.0, 0.0106201171875]
['model.relu.alpha_mask_2_0', 16384, 108.0, 0.006591796875]
['model.relu.alpha_mask_3_0', 16384, 114.0, 0.0069580078125]
['model.relu.alpha_mask_4_0', 16384, 55.0, 0.00335693359375]
['model.relu.alpha_mask_5_0', 8192, 559.0, 0.0682373046875]
['model.relu.alpha_mask_6_0', 8192, 414.0, 0.050537109375]
['model.relu.alpha_mask_7_0', 8192, 454.0, 0.055419921875]
['model.relu.alpha_mask_8_0', 8192, 426.0, 0.052001953125]
['model.relu.alpha_mask_9_0', 4096, 1042.0, 0.25439453125]
['model.relu.alpha_mask_10_0', 4096, 1058.0, 0.25830078125]
['model.relu.alpha_mask_11_0', 4096, 1109.0, 0.270751953125]
['model.relu.alpha_mask_12_0', 4096, 1046.0, 0.25537109375]
['model.relu.alpha_mask_13_0', 2048, 1604.0, 0.783203125]
['model.relu.alpha_mask_14_0', 2048, 1844.0, 0.900390625]
['model.relu.alpha_mask_15_0', 2048, 1243.0, 0.60693359375]
['model.relu.alpha_mask_16_0', 2048, 1369.0, 0.66845703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12720.0, 0.06751019021739131]
########## End ###########
06/01 02:53:34 AM | Train: [63/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 02:53:34 AM | layerwise density: [101.0, 174.0, 108.0, 114.0, 55.0, 559.0, 414.0, 454.0, 426.0, 1042.0, 1058.0, 1109.0, 1046.0, 1604.0, 1844.0, 1243.0, 1369.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.068', '0.051', '0.055', '0.052', '0.254', '0.258', '0.271', '0.255', '0.783', '0.900', '0.607', '0.668']
Global density: 0.0675101950764656
06/01 02:53:45 AM | Train: [63/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:53:45 AM | layerwise density: [104.0, 178.0, 112.0, 118.0, 54.0, 562.0, 410.0, 453.0, 424.0, 1043.0, 1057.0, 1108.0, 1048.0, 1600.0, 1844.0, 1240.0, 1370.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.069', '0.050', '0.055', '0.052', '0.255', '0.258', '0.271', '0.256', '0.781', '0.900', '0.605', '0.669']
Global density: 0.06753672659397125
06/01 02:53:55 AM | Train: [63/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/01 02:53:55 AM | layerwise density: [105.0, 181.0, 114.0, 119.0, 54.0, 560.0, 411.0, 456.0, 436.0, 1050.0, 1056.0, 1109.0, 1053.0, 1600.0, 1847.0, 1238.0, 1370.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.068', '0.050', '0.056', '0.053', '0.256', '0.258', '0.271', '0.257', '0.781', '0.902', '0.604', '0.669']
Global density: 0.06771717965602875
06/01 02:54:06 AM | Train: [63/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:54:06 AM | layerwise density: [105.0, 183.0, 112.0, 117.0, 52.0, 565.0, 418.0, 457.0, 438.0, 1051.0, 1055.0, 1110.0, 1061.0, 1597.0, 1849.0, 1241.0, 1370.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.069', '0.051', '0.056', '0.053', '0.257', '0.258', '0.271', '0.259', '0.780', '0.903', '0.606', '0.669']
Global density: 0.06783394515514374
06/01 02:54:15 AM | Train: [63/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/01 02:54:15 AM | layerwise density: [106.0, 182.0, 113.0, 119.0, 52.0, 576.0, 422.0, 460.0, 443.0, 1042.0, 1059.0, 1110.0, 1063.0, 1599.0, 1844.0, 1244.0, 1371.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.070', '0.052', '0.056', '0.054', '0.254', '0.259', '0.271', '0.260', '0.781', '0.900', '0.607', '0.669']
Global density: 0.06796132028102875
06/01 02:54:15 AM | Train: [63/200] Final Prec@1 99.9440%
06/01 02:54:16 AM | Valid: [63/200] Step 000/078 Loss 1.041 Prec@(1,5) (76.6%, 89.8%)
06/01 02:54:18 AM | Valid: [63/200] Step 078/078 Loss 1.307 Prec@(1,5) (70.3%, 89.8%)
06/01 02:54:18 AM | Valid: [63/200] Final Prec@1 70.3100%
06/01 02:54:18 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 106.0, 0.001617431640625]
['model.relu.alpha_mask_1_0', 16384, 181.0, 0.01104736328125]
['model.relu.alpha_mask_2_0', 16384, 113.0, 0.00689697265625]
['model.relu.alpha_mask_3_0', 16384, 118.0, 0.0072021484375]
['model.relu.alpha_mask_4_0', 16384, 52.0, 0.003173828125]
['model.relu.alpha_mask_5_0', 8192, 575.0, 0.0701904296875]
['model.relu.alpha_mask_6_0', 8192, 423.0, 0.0516357421875]
['model.relu.alpha_mask_7_0', 8192, 461.0, 0.0562744140625]
['model.relu.alpha_mask_8_0', 8192, 443.0, 0.0540771484375]
['model.relu.alpha_mask_9_0', 4096, 1043.0, 0.254638671875]
['model.relu.alpha_mask_10_0', 4096, 1059.0, 0.258544921875]
['model.relu.alpha_mask_11_0', 4096, 1110.0, 0.27099609375]
['model.relu.alpha_mask_12_0', 4096, 1062.0, 0.25927734375]
['model.relu.alpha_mask_13_0', 2048, 1599.0, 0.78076171875]
['model.relu.alpha_mask_14_0', 2048, 1844.0, 0.900390625]
['model.relu.alpha_mask_15_0', 2048, 1244.0, 0.607421875]
['model.relu.alpha_mask_16_0', 2048, 1371.0, 0.66943359375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12804.0, 0.06795601222826086]
########## End ###########
06/01 02:54:19 AM | Train: [64/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:54:19 AM | layerwise density: [106.0, 181.0, 113.0, 118.0, 52.0, 575.0, 423.0, 461.0, 443.0, 1043.0, 1059.0, 1110.0, 1062.0, 1599.0, 1844.0, 1244.0, 1371.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.070', '0.052', '0.056', '0.054', '0.255', '0.259', '0.271', '0.259', '0.781', '0.900', '0.607', '0.669']
Global density: 0.06795601546764374
06/01 02:54:30 AM | Train: [64/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:54:30 AM | layerwise density: [109.0, 185.0, 114.0, 121.0, 52.0, 576.0, 420.0, 465.0, 444.0, 1056.0, 1062.0, 1112.0, 1064.0, 1602.0, 1851.0, 1249.0, 1373.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.070', '0.051', '0.057', '0.054', '0.258', '0.259', '0.271', '0.260', '0.782', '0.904', '0.610', '0.670']
Global density: 0.06822668761014938
06/01 02:54:40 AM | Train: [64/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:54:40 AM | layerwise density: [108.0, 175.0, 109.0, 119.0, 52.0, 538.0, 393.0, 445.0, 422.0, 1039.0, 1043.0, 1109.0, 1052.0, 1596.0, 1842.0, 1238.0, 1373.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.066', '0.048', '0.054', '0.052', '0.254', '0.255', '0.271', '0.257', '0.779', '0.899', '0.604', '0.670']
Global density: 0.06715459376573563
06/01 02:54:51 AM | Train: [64/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:54:51 AM | layerwise density: [106.0, 171.0, 105.0, 116.0, 50.0, 515.0, 370.0, 436.0, 409.0, 1018.0, 1038.0, 1106.0, 1038.0, 1588.0, 1827.0, 1234.0, 1373.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.063', '0.045', '0.053', '0.050', '0.249', '0.253', '0.270', '0.253', '0.775', '0.892', '0.603', '0.670']
Global density: 0.0663425624370575
06/01 02:55:01 AM | Train: [64/80] Step 390/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:55:01 AM | layerwise density: [104.0, 169.0, 104.0, 113.0, 47.0, 502.0, 365.0, 433.0, 406.0, 1015.0, 1037.0, 1105.0, 1031.0, 1587.0, 1827.0, 1229.0, 1373.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.061', '0.045', '0.053', '0.050', '0.248', '0.253', '0.270', '0.252', '0.775', '0.892', '0.600', '0.670']
Global density: 0.06606127321720123
06/01 02:55:01 AM | Train: [64/200] Final Prec@1 99.9520%
06/01 02:55:01 AM | Valid: [64/200] Step 000/078 Loss 1.104 Prec@(1,5) (75.0%, 89.8%)
06/01 02:55:04 AM | Valid: [64/200] Step 078/078 Loss 1.307 Prec@(1,5) (70.5%, 89.8%)
06/01 02:55:04 AM | Valid: [64/200] Final Prec@1 70.4800%
06/01 02:55:04 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 104.0, 0.0015869140625]
['model.relu.alpha_mask_1_0', 16384, 169.0, 0.01031494140625]
['model.relu.alpha_mask_2_0', 16384, 104.0, 0.00634765625]
['model.relu.alpha_mask_3_0', 16384, 113.0, 0.00689697265625]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 502.0, 0.061279296875]
['model.relu.alpha_mask_6_0', 8192, 365.0, 0.0445556640625]
['model.relu.alpha_mask_7_0', 8192, 433.0, 0.0528564453125]
['model.relu.alpha_mask_8_0', 8192, 406.0, 0.049560546875]
['model.relu.alpha_mask_9_0', 4096, 1015.0, 0.247802734375]
['model.relu.alpha_mask_10_0', 4096, 1036.0, 0.2529296875]
['model.relu.alpha_mask_11_0', 4096, 1105.0, 0.269775390625]
['model.relu.alpha_mask_12_0', 4096, 1031.0, 0.251708984375]
['model.relu.alpha_mask_13_0', 2048, 1587.0, 0.77490234375]
['model.relu.alpha_mask_14_0', 2048, 1827.0, 0.89208984375]
['model.relu.alpha_mask_15_0', 2048, 1229.0, 0.60009765625]
['model.relu.alpha_mask_16_0', 2048, 1373.0, 0.67041015625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12446.0, 0.06605596127717392]
########## End ###########
06/01 02:55:05 AM | Train: [65/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 02:55:05 AM | layerwise density: [104.0, 169.0, 104.0, 113.0, 47.0, 502.0, 365.0, 433.0, 406.0, 1015.0, 1036.0, 1105.0, 1031.0, 1587.0, 1827.0, 1229.0, 1373.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.061', '0.045', '0.053', '0.050', '0.248', '0.253', '0.270', '0.252', '0.775', '0.892', '0.600', '0.670']
Global density: 0.06605596095323563
06/01 02:55:15 AM | Train: [65/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:55:15 AM | layerwise density: [104.0, 167.0, 105.0, 113.0, 47.0, 498.0, 362.0, 430.0, 391.0, 1014.0, 1037.0, 1105.0, 1024.0, 1585.0, 1827.0, 1227.0, 1373.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.061', '0.044', '0.052', '0.048', '0.248', '0.253', '0.270', '0.250', '0.774', '0.892', '0.599', '0.670']
Global density: 0.0658595860004425
06/01 02:55:26 AM | Train: [65/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:55:26 AM | layerwise density: [104.0, 168.0, 104.0, 112.0, 49.0, 502.0, 359.0, 427.0, 385.0, 1012.0, 1038.0, 1104.0, 1023.0, 1584.0, 1819.0, 1227.0, 1374.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.061', '0.044', '0.052', '0.047', '0.247', '0.253', '0.270', '0.250', '0.773', '0.888', '0.599', '0.671']
Global density: 0.06576405465602875
06/01 02:55:37 AM | Train: [65/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:55:37 AM | layerwise density: [104.0, 164.0, 103.0, 112.0, 50.0, 499.0, 357.0, 426.0, 380.0, 1015.0, 1038.0, 1103.0, 1023.0, 1584.0, 1818.0, 1226.0, 1374.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.061', '0.044', '0.052', '0.046', '0.248', '0.253', '0.269', '0.250', '0.773', '0.888', '0.599', '0.671']
Global density: 0.06568444520235062
06/01 02:55:46 AM | Train: [65/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:55:46 AM | layerwise density: [102.0, 162.0, 105.0, 109.0, 48.0, 500.0, 359.0, 427.0, 382.0, 1017.0, 1040.0, 1107.0, 1026.0, 1589.0, 1821.0, 1228.0, 1374.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.061', '0.044', '0.052', '0.047', '0.248', '0.254', '0.270', '0.250', '0.776', '0.889', '0.600', '0.671']
Global density: 0.06579059362411499
06/01 02:55:46 AM | Train: [65/200] Final Prec@1 99.9580%
06/01 02:55:47 AM | Valid: [65/200] Step 000/078 Loss 1.001 Prec@(1,5) (77.3%, 91.4%)
06/01 02:55:49 AM | Valid: [65/200] Step 078/078 Loss 1.311 Prec@(1,5) (70.2%, 89.7%)
06/01 02:55:49 AM | Valid: [65/200] Final Prec@1 70.2300%
06/01 02:55:49 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 162.0, 0.0098876953125]
['model.relu.alpha_mask_2_0', 16384, 105.0, 0.00640869140625]
['model.relu.alpha_mask_3_0', 16384, 109.0, 0.00665283203125]
['model.relu.alpha_mask_4_0', 16384, 48.0, 0.0029296875]
['model.relu.alpha_mask_5_0', 8192, 500.0, 0.06103515625]
['model.relu.alpha_mask_6_0', 8192, 359.0, 0.0438232421875]
['model.relu.alpha_mask_7_0', 8192, 427.0, 0.0521240234375]
['model.relu.alpha_mask_8_0', 8192, 382.0, 0.046630859375]
['model.relu.alpha_mask_9_0', 4096, 1016.0, 0.248046875]
['model.relu.alpha_mask_10_0', 4096, 1040.0, 0.25390625]
['model.relu.alpha_mask_11_0', 4096, 1107.0, 0.270263671875]
['model.relu.alpha_mask_12_0', 4096, 1026.0, 0.25048828125]
['model.relu.alpha_mask_13_0', 2048, 1589.0, 0.77587890625]
['model.relu.alpha_mask_14_0', 2048, 1822.0, 0.8896484375]
['model.relu.alpha_mask_15_0', 2048, 1228.0, 0.599609375]
['model.relu.alpha_mask_16_0', 2048, 1374.0, 0.6708984375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12396.0, 0.06579059103260869]
########## End ###########
06/01 02:55:50 AM | Train: [66/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 02:55:50 AM | layerwise density: [102.0, 162.0, 105.0, 109.0, 48.0, 500.0, 359.0, 427.0, 382.0, 1016.0, 1040.0, 1107.0, 1026.0, 1589.0, 1822.0, 1228.0, 1374.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.061', '0.044', '0.052', '0.047', '0.248', '0.254', '0.270', '0.250', '0.776', '0.890', '0.600', '0.671']
Global density: 0.06579059362411499
06/01 02:56:01 AM | Train: [66/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:56:01 AM | layerwise density: [102.0, 161.0, 104.0, 110.0, 48.0, 510.0, 358.0, 429.0, 382.0, 1012.0, 1048.0, 1106.0, 1027.0, 1591.0, 1819.0, 1232.0, 1376.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.062', '0.044', '0.052', '0.047', '0.247', '0.256', '0.270', '0.251', '0.777', '0.888', '0.602', '0.672']
Global density: 0.06589142978191376
06/01 02:56:11 AM | Train: [66/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:56:11 AM | layerwise density: [103.0, 160.0, 106.0, 107.0, 45.0, 516.0, 364.0, 429.0, 380.0, 1023.0, 1048.0, 1107.0, 1024.0, 1596.0, 1822.0, 1234.0, 1376.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.063', '0.044', '0.052', '0.046', '0.250', '0.256', '0.270', '0.250', '0.779', '0.890', '0.603', '0.672']
Global density: 0.06602411717176437
06/01 02:56:22 AM | Train: [66/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:56:22 AM | layerwise density: [102.0, 161.0, 106.0, 107.0, 44.0, 520.0, 363.0, 430.0, 380.0, 1033.0, 1046.0, 1105.0, 1028.0, 1592.0, 1821.0, 1237.0, 1376.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.063', '0.044', '0.052', '0.046', '0.252', '0.255', '0.270', '0.251', '0.777', '0.889', '0.604', '0.672']
Global density: 0.06608249992132187
06/01 02:56:32 AM | Train: [66/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:56:32 AM | layerwise density: [101.0, 161.0, 107.0, 107.0, 47.0, 515.0, 364.0, 430.0, 386.0, 1039.0, 1044.0, 1104.0, 1025.0, 1598.0, 1822.0, 1242.0, 1377.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.063', '0.044', '0.052', '0.047', '0.254', '0.255', '0.270', '0.250', '0.780', '0.890', '0.606', '0.672']
Global density: 0.06617803126573563
06/01 02:56:32 AM | Train: [66/200] Final Prec@1 99.9600%
06/01 02:56:32 AM | Valid: [66/200] Step 000/078 Loss 1.082 Prec@(1,5) (76.6%, 90.6%)
06/01 02:56:35 AM | Valid: [66/200] Step 078/078 Loss 1.302 Prec@(1,5) (70.5%, 90.0%)
06/01 02:56:35 AM | Valid: [66/200] Final Prec@1 70.4700%
06/01 02:56:35 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 101.0, 0.0015411376953125]
['model.relu.alpha_mask_1_0', 16384, 161.0, 0.00982666015625]
['model.relu.alpha_mask_2_0', 16384, 107.0, 0.00653076171875]
['model.relu.alpha_mask_3_0', 16384, 107.0, 0.00653076171875]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 515.0, 0.0628662109375]
['model.relu.alpha_mask_6_0', 8192, 364.0, 0.04443359375]
['model.relu.alpha_mask_7_0', 8192, 430.0, 0.052490234375]
['model.relu.alpha_mask_8_0', 8192, 387.0, 0.0472412109375]
['model.relu.alpha_mask_9_0', 4096, 1039.0, 0.253662109375]
['model.relu.alpha_mask_10_0', 4096, 1044.0, 0.2548828125]
['model.relu.alpha_mask_11_0', 4096, 1104.0, 0.26953125]
['model.relu.alpha_mask_12_0', 4096, 1025.0, 0.250244140625]
['model.relu.alpha_mask_13_0', 2048, 1598.0, 0.7802734375]
['model.relu.alpha_mask_14_0', 2048, 1822.0, 0.8896484375]
['model.relu.alpha_mask_15_0', 2048, 1242.0, 0.6064453125]
['model.relu.alpha_mask_16_0', 2048, 1377.0, 0.67236328125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12470.0, 0.06618333899456522]
########## End ###########
06/01 02:56:36 AM | Train: [67/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 02:56:36 AM | layerwise density: [101.0, 161.0, 107.0, 107.0, 47.0, 515.0, 364.0, 430.0, 387.0, 1039.0, 1044.0, 1104.0, 1025.0, 1598.0, 1822.0, 1242.0, 1377.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.063', '0.044', '0.052', '0.047', '0.254', '0.255', '0.270', '0.250', '0.780', '0.890', '0.606', '0.672']
Global density: 0.06618334352970123
06/01 02:56:46 AM | Train: [67/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:56:46 AM | layerwise density: [100.0, 161.0, 108.0, 107.0, 47.0, 510.0, 361.0, 428.0, 385.0, 1042.0, 1048.0, 1104.0, 1024.0, 1597.0, 1822.0, 1245.0, 1378.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.062', '0.044', '0.052', '0.047', '0.254', '0.256', '0.270', '0.250', '0.780', '0.890', '0.608', '0.673']
Global density: 0.0661674216389656
06/01 02:56:57 AM | Train: [67/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:56:57 AM | layerwise density: [99.0, 162.0, 106.0, 109.0, 48.0, 514.0, 362.0, 429.0, 395.0, 1045.0, 1046.0, 1101.0, 1028.0, 1601.0, 1827.0, 1244.0, 1378.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.063', '0.044', '0.052', '0.048', '0.255', '0.255', '0.269', '0.251', '0.782', '0.892', '0.607', '0.673']
Global density: 0.06631071865558624
06/01 02:57:07 AM | Train: [67/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:57:07 AM | layerwise density: [101.0, 162.0, 105.0, 108.0, 48.0, 525.0, 360.0, 429.0, 397.0, 1054.0, 1046.0, 1101.0, 1035.0, 1604.0, 1832.0, 1248.0, 1380.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.064', '0.044', '0.052', '0.048', '0.257', '0.255', '0.269', '0.253', '0.783', '0.895', '0.609', '0.674']
Global density: 0.0665283203125
06/01 02:57:17 AM | Train: [67/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:57:17 AM | layerwise density: [102.0, 161.0, 107.0, 108.0, 50.0, 522.0, 362.0, 433.0, 404.0, 1056.0, 1047.0, 1101.0, 1034.0, 1603.0, 1832.0, 1249.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.064', '0.044', '0.053', '0.049', '0.258', '0.256', '0.269', '0.252', '0.783', '0.895', '0.610', '0.675']
Global density: 0.06662385165691376
06/01 02:57:17 AM | Train: [67/200] Final Prec@1 99.9600%
06/01 02:57:17 AM | Valid: [67/200] Step 000/078 Loss 0.995 Prec@(1,5) (75.8%, 91.4%)
06/01 02:57:19 AM | Valid: [67/200] Step 078/078 Loss 1.304 Prec@(1,5) (70.2%, 89.8%)
06/01 02:57:19 AM | Valid: [67/200] Final Prec@1 70.2500%
06/01 02:57:19 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 161.0, 0.00982666015625]
['model.relu.alpha_mask_2_0', 16384, 107.0, 0.00653076171875]
['model.relu.alpha_mask_3_0', 16384, 108.0, 0.006591796875]
['model.relu.alpha_mask_4_0', 16384, 50.0, 0.0030517578125]
['model.relu.alpha_mask_5_0', 8192, 522.0, 0.063720703125]
['model.relu.alpha_mask_6_0', 8192, 362.0, 0.044189453125]
['model.relu.alpha_mask_7_0', 8192, 433.0, 0.0528564453125]
['model.relu.alpha_mask_8_0', 8192, 404.0, 0.04931640625]
['model.relu.alpha_mask_9_0', 4096, 1056.0, 0.2578125]
['model.relu.alpha_mask_10_0', 4096, 1047.0, 0.255615234375]
['model.relu.alpha_mask_11_0', 4096, 1101.0, 0.268798828125]
['model.relu.alpha_mask_12_0', 4096, 1034.0, 0.25244140625]
['model.relu.alpha_mask_13_0', 2048, 1603.0, 0.78271484375]
['model.relu.alpha_mask_14_0', 2048, 1832.0, 0.89453125]
['model.relu.alpha_mask_15_0', 2048, 1249.0, 0.60986328125]
['model.relu.alpha_mask_16_0', 2048, 1382.0, 0.6748046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12553.0, 0.06662385360054347]
########## End ###########
06/01 02:57:20 AM | Train: [68/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 02:57:20 AM | layerwise density: [102.0, 161.0, 107.0, 108.0, 50.0, 522.0, 362.0, 433.0, 404.0, 1056.0, 1047.0, 1101.0, 1034.0, 1603.0, 1832.0, 1249.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.064', '0.044', '0.053', '0.049', '0.258', '0.256', '0.269', '0.252', '0.783', '0.895', '0.610', '0.675']
Global density: 0.06662385165691376
06/01 02:57:31 AM | Train: [68/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:57:31 AM | layerwise density: [102.0, 161.0, 107.0, 110.0, 50.0, 519.0, 365.0, 433.0, 402.0, 1059.0, 1047.0, 1100.0, 1035.0, 1602.0, 1832.0, 1251.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.063', '0.045', '0.053', '0.049', '0.259', '0.256', '0.269', '0.253', '0.782', '0.895', '0.611', '0.675']
Global density: 0.06664508581161499
06/01 02:57:42 AM | Train: [68/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:57:42 AM | layerwise density: [102.0, 162.0, 106.0, 109.0, 52.0, 524.0, 367.0, 434.0, 405.0, 1064.0, 1047.0, 1101.0, 1041.0, 1602.0, 1840.0, 1253.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.064', '0.045', '0.053', '0.049', '0.260', '0.256', '0.269', '0.254', '0.782', '0.898', '0.612', '0.675']
Global density: 0.06682553887367249
06/01 02:57:53 AM | Train: [68/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:57:53 AM | layerwise density: [102.0, 164.0, 105.0, 109.0, 51.0, 536.0, 369.0, 436.0, 406.0, 1066.0, 1049.0, 1103.0, 1047.0, 1604.0, 1845.0, 1254.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.065', '0.045', '0.053', '0.050', '0.260', '0.256', '0.269', '0.256', '0.783', '0.901', '0.612', '0.675']
Global density: 0.0670219138264656
06/01 02:58:02 AM | Train: [68/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:58:02 AM | layerwise density: [103.0, 165.0, 105.0, 111.0, 51.0, 536.0, 374.0, 437.0, 412.0, 1066.0, 1049.0, 1102.0, 1052.0, 1605.0, 1849.0, 1257.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.065', '0.046', '0.053', '0.050', '0.260', '0.256', '0.269', '0.257', '0.784', '0.903', '0.614', '0.675']
Global density: 0.06717051565647125
06/01 02:58:02 AM | Train: [68/200] Final Prec@1 99.9560%
06/01 02:58:02 AM | Valid: [68/200] Step 000/078 Loss 1.024 Prec@(1,5) (78.1%, 89.8%)
06/01 02:58:05 AM | Valid: [68/200] Step 078/078 Loss 1.302 Prec@(1,5) (70.4%, 89.8%)
06/01 02:58:05 AM | Valid: [68/200] Final Prec@1 70.4300%
06/01 02:58:05 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 103.0, 0.0015716552734375]
['model.relu.alpha_mask_1_0', 16384, 165.0, 0.01007080078125]
['model.relu.alpha_mask_2_0', 16384, 105.0, 0.00640869140625]
['model.relu.alpha_mask_3_0', 16384, 111.0, 0.00677490234375]
['model.relu.alpha_mask_4_0', 16384, 51.0, 0.00311279296875]
['model.relu.alpha_mask_5_0', 8192, 536.0, 0.0654296875]
['model.relu.alpha_mask_6_0', 8192, 374.0, 0.045654296875]
['model.relu.alpha_mask_7_0', 8192, 437.0, 0.0533447265625]
['model.relu.alpha_mask_8_0', 8192, 412.0, 0.05029296875]
['model.relu.alpha_mask_9_0', 4096, 1066.0, 0.26025390625]
['model.relu.alpha_mask_10_0', 4096, 1049.0, 0.256103515625]
['model.relu.alpha_mask_11_0', 4096, 1102.0, 0.26904296875]
['model.relu.alpha_mask_12_0', 4096, 1051.0, 0.256591796875]
['model.relu.alpha_mask_13_0', 2048, 1605.0, 0.78369140625]
['model.relu.alpha_mask_14_0', 2048, 1849.0, 0.90283203125]
['model.relu.alpha_mask_15_0', 2048, 1257.0, 0.61376953125]
['model.relu.alpha_mask_16_0', 2048, 1382.0, 0.6748046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12655.0, 0.06716520889945653]
########## End ###########
06/01 02:58:06 AM | Train: [69/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 02:58:06 AM | layerwise density: [103.0, 165.0, 105.0, 111.0, 51.0, 536.0, 374.0, 437.0, 412.0, 1066.0, 1049.0, 1102.0, 1051.0, 1605.0, 1849.0, 1257.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.065', '0.046', '0.053', '0.050', '0.260', '0.256', '0.269', '0.257', '0.784', '0.903', '0.614', '0.675']
Global density: 0.06716521084308624
06/01 02:58:17 AM | Train: [69/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:58:17 AM | layerwise density: [103.0, 166.0, 107.0, 111.0, 50.0, 536.0, 374.0, 438.0, 413.0, 1064.0, 1052.0, 1105.0, 1061.0, 1606.0, 1848.0, 1259.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.065', '0.046', '0.053', '0.050', '0.260', '0.257', '0.270', '0.259', '0.784', '0.902', '0.615', '0.675']
Global density: 0.06727135926485062
06/01 02:58:27 AM | Train: [69/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/01 02:58:27 AM | layerwise density: [102.0, 168.0, 110.0, 111.0, 48.0, 547.0, 378.0, 440.0, 418.0, 1070.0, 1056.0, 1105.0, 1065.0, 1603.0, 1845.0, 1258.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.067', '0.046', '0.054', '0.051', '0.261', '0.258', '0.270', '0.260', '0.783', '0.901', '0.614', '0.675']
Global density: 0.06743589043617249
06/01 02:58:38 AM | Train: [69/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:58:38 AM | layerwise density: [104.0, 168.0, 109.0, 114.0, 47.0, 552.0, 388.0, 442.0, 419.0, 1071.0, 1063.0, 1106.0, 1066.0, 1608.0, 1845.0, 1258.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.067', '0.047', '0.054', '0.051', '0.261', '0.260', '0.270', '0.260', '0.785', '0.901', '0.614', '0.675']
Global density: 0.067626953125
06/01 02:58:48 AM | Train: [69/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:58:48 AM | layerwise density: [104.0, 170.0, 107.0, 116.0, 46.0, 568.0, 389.0, 444.0, 424.0, 1080.0, 1063.0, 1110.0, 1066.0, 1608.0, 1847.0, 1262.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.069', '0.047', '0.054', '0.052', '0.264', '0.260', '0.271', '0.260', '0.785', '0.902', '0.616', '0.675']
Global density: 0.06786047667264938
06/01 02:58:48 AM | Train: [69/200] Final Prec@1 99.9560%
06/01 02:58:48 AM | Valid: [69/200] Step 000/078 Loss 1.039 Prec@(1,5) (76.6%, 91.4%)
06/01 02:58:50 AM | Valid: [69/200] Step 078/078 Loss 1.292 Prec@(1,5) (70.3%, 90.0%)
06/01 02:58:50 AM | Valid: [69/200] Final Prec@1 70.3000%
06/01 02:58:50 AM | Current mask training best Prec@1 = 70.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 104.0, 0.0015869140625]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 107.0, 0.00653076171875]
['model.relu.alpha_mask_3_0', 16384, 116.0, 0.007080078125]
['model.relu.alpha_mask_4_0', 16384, 46.0, 0.0028076171875]
['model.relu.alpha_mask_5_0', 8192, 568.0, 0.0693359375]
['model.relu.alpha_mask_6_0', 8192, 389.0, 0.0474853515625]
['model.relu.alpha_mask_7_0', 8192, 444.0, 0.05419921875]
['model.relu.alpha_mask_8_0', 8192, 424.0, 0.0517578125]
['model.relu.alpha_mask_9_0', 4096, 1080.0, 0.263671875]
['model.relu.alpha_mask_10_0', 4096, 1063.0, 0.259521484375]
['model.relu.alpha_mask_11_0', 4096, 1110.0, 0.27099609375]
['model.relu.alpha_mask_12_0', 4096, 1066.0, 0.26025390625]
['model.relu.alpha_mask_13_0', 2048, 1608.0, 0.78515625]
['model.relu.alpha_mask_14_0', 2048, 1847.0, 0.90185546875]
['model.relu.alpha_mask_15_0', 2048, 1262.0, 0.6162109375]
['model.relu.alpha_mask_16_0', 2048, 1382.0, 0.6748046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12786.0, 0.06786047894021739]
########## End ###########
06/01 02:58:51 AM | Train: [70/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 02:58:51 AM | layerwise density: [104.0, 170.0, 107.0, 116.0, 46.0, 568.0, 389.0, 444.0, 424.0, 1080.0, 1063.0, 1110.0, 1066.0, 1608.0, 1847.0, 1262.0, 1382.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.069', '0.047', '0.054', '0.052', '0.264', '0.260', '0.271', '0.260', '0.785', '0.902', '0.616', '0.675']
Global density: 0.06786047667264938
06/01 02:59:02 AM | Train: [70/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:59:02 AM | layerwise density: [103.0, 171.0, 107.0, 116.0, 44.0, 571.0, 391.0, 444.0, 427.0, 1084.0, 1064.0, 1110.0, 1066.0, 1608.0, 1849.0, 1262.0, 1383.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.070', '0.048', '0.054', '0.052', '0.265', '0.260', '0.271', '0.260', '0.785', '0.903', '0.616', '0.675']
Global density: 0.0679347813129425
06/01 02:59:13 AM | Train: [70/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:59:13 AM | layerwise density: [103.0, 173.0, 107.0, 120.0, 44.0, 577.0, 393.0, 445.0, 422.0, 1083.0, 1068.0, 1110.0, 1068.0, 1607.0, 1848.0, 1262.0, 1383.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.070', '0.048', '0.054', '0.052', '0.264', '0.261', '0.271', '0.261', '0.785', '0.902', '0.616', '0.675']
Global density: 0.06800378113985062
06/01 02:59:24 AM | Train: [70/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:59:24 AM | layerwise density: [100.0, 174.0, 107.0, 120.0, 44.0, 577.0, 395.0, 447.0, 422.0, 1081.0, 1069.0, 1111.0, 1072.0, 1611.0, 1851.0, 1261.0, 1384.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.070', '0.048', '0.055', '0.052', '0.264', '0.261', '0.271', '0.262', '0.787', '0.904', '0.616', '0.676']
Global density: 0.06807277351617813
06/01 02:59:34 AM | Train: [70/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/01 02:59:34 AM | layerwise density: [100.0, 175.0, 107.0, 119.0, 46.0, 582.0, 398.0, 450.0, 423.0, 1085.0, 1068.0, 1112.0, 1071.0, 1612.0, 1857.0, 1260.0, 1385.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.071', '0.049', '0.055', '0.052', '0.265', '0.261', '0.271', '0.261', '0.787', '0.907', '0.615', '0.676']
Global density: 0.06820015609264374
06/01 02:59:34 AM | Train: [70/200] Final Prec@1 99.9460%
06/01 02:59:34 AM | Valid: [70/200] Step 000/078 Loss 1.056 Prec@(1,5) (77.3%, 92.2%)
06/01 02:59:36 AM | Valid: [70/200] Step 078/078 Loss 1.296 Prec@(1,5) (70.7%, 90.1%)
06/01 02:59:36 AM | Valid: [70/200] Final Prec@1 70.7300%
06/01 02:59:37 AM | Current mask training best Prec@1 = 70.7300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 100.0, 0.00152587890625]
['model.relu.alpha_mask_1_0', 16384, 175.0, 0.01068115234375]
['model.relu.alpha_mask_2_0', 16384, 107.0, 0.00653076171875]
['model.relu.alpha_mask_3_0', 16384, 118.0, 0.0072021484375]
['model.relu.alpha_mask_4_0', 16384, 46.0, 0.0028076171875]
['model.relu.alpha_mask_5_0', 8192, 583.0, 0.0711669921875]
['model.relu.alpha_mask_6_0', 8192, 398.0, 0.048583984375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 423.0, 0.0516357421875]
['model.relu.alpha_mask_9_0', 4096, 1085.0, 0.264892578125]
['model.relu.alpha_mask_10_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_11_0', 4096, 1112.0, 0.271484375]
['model.relu.alpha_mask_12_0', 4096, 1072.0, 0.26171875]
['model.relu.alpha_mask_13_0', 2048, 1612.0, 0.787109375]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1260.0, 0.615234375]
['model.relu.alpha_mask_16_0', 2048, 1385.0, 0.67626953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12851.0, 0.06820546025815218]
########## End ###########
06/01 02:59:38 AM | Train: [71/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 02:59:38 AM | layerwise density: [100.0, 175.0, 107.0, 118.0, 46.0, 583.0, 398.0, 450.0, 423.0, 1085.0, 1068.0, 1112.0, 1072.0, 1612.0, 1857.0, 1260.0, 1385.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.071', '0.049', '0.055', '0.052', '0.265', '0.261', '0.271', '0.262', '0.787', '0.907', '0.615', '0.676']
Global density: 0.06820546090602875
06/01 02:59:48 AM | Train: [71/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 02:59:48 AM | layerwise density: [100.0, 176.0, 109.0, 121.0, 47.0, 583.0, 399.0, 451.0, 425.0, 1085.0, 1069.0, 1114.0, 1075.0, 1611.0, 1857.0, 1259.0, 1385.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.071', '0.049', '0.055', '0.052', '0.265', '0.261', '0.272', '0.262', '0.787', '0.907', '0.615', '0.676']
Global density: 0.06828507035970688
06/01 03:00:00 AM | Train: [71/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:00:00 AM | layerwise density: [100.0, 175.0, 111.0, 121.0, 47.0, 583.0, 400.0, 455.0, 427.0, 1085.0, 1071.0, 1116.0, 1079.0, 1611.0, 1857.0, 1257.0, 1386.0]
layerwise density percentage: ['0.002', '0.011', '0.007', '0.007', '0.003', '0.071', '0.049', '0.056', '0.052', '0.265', '0.261', '0.272', '0.263', '0.787', '0.907', '0.614', '0.677']
Global density: 0.0683646872639656
06/01 03:00:11 AM | Train: [71/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:00:11 AM | layerwise density: [101.0, 170.0, 112.0, 119.0, 46.0, 569.0, 399.0, 452.0, 420.0, 1076.0, 1071.0, 1116.0, 1075.0, 1607.0, 1853.0, 1255.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.069', '0.049', '0.055', '0.051', '0.263', '0.261', '0.272', '0.262', '0.785', '0.905', '0.613', '0.677']
Global density: 0.06808339059352875
06/01 03:00:20 AM | Train: [71/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:00:20 AM | layerwise density: [100.0, 170.0, 111.0, 119.0, 45.0, 555.0, 396.0, 447.0, 413.0, 1065.0, 1070.0, 1115.0, 1067.0, 1602.0, 1851.0, 1253.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.068', '0.048', '0.055', '0.050', '0.260', '0.261', '0.272', '0.260', '0.782', '0.904', '0.612', '0.677']
Global density: 0.0677543357014656
06/01 03:00:20 AM | Train: [71/200] Final Prec@1 99.9540%
06/01 03:00:21 AM | Valid: [71/200] Step 000/078 Loss 1.061 Prec@(1,5) (78.1%, 91.4%)
06/01 03:00:23 AM | Valid: [71/200] Step 078/078 Loss 1.303 Prec@(1,5) (70.2%, 89.8%)
06/01 03:00:23 AM | Valid: [71/200] Final Prec@1 70.2200%
06/01 03:00:23 AM | Current mask training best Prec@1 = 70.7300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 100.0, 0.00152587890625]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 111.0, 0.00677490234375]
['model.relu.alpha_mask_3_0', 16384, 119.0, 0.00726318359375]
['model.relu.alpha_mask_4_0', 16384, 45.0, 0.00274658203125]
['model.relu.alpha_mask_5_0', 8192, 555.0, 0.0677490234375]
['model.relu.alpha_mask_6_0', 8192, 396.0, 0.04833984375]
['model.relu.alpha_mask_7_0', 8192, 447.0, 0.0545654296875]
['model.relu.alpha_mask_8_0', 8192, 413.0, 0.0504150390625]
['model.relu.alpha_mask_9_0', 4096, 1065.0, 0.260009765625]
['model.relu.alpha_mask_10_0', 4096, 1070.0, 0.26123046875]
['model.relu.alpha_mask_11_0', 4096, 1115.0, 0.272216796875]
['model.relu.alpha_mask_12_0', 4096, 1067.0, 0.260498046875]
['model.relu.alpha_mask_13_0', 2048, 1602.0, 0.7822265625]
['model.relu.alpha_mask_14_0', 2048, 1851.0, 0.90380859375]
['model.relu.alpha_mask_15_0', 2048, 1253.0, 0.61181640625]
['model.relu.alpha_mask_16_0', 2048, 1387.0, 0.67724609375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12766.0, 0.06775433084239131]
########## End ###########
06/01 03:00:24 AM | Train: [72/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:00:24 AM | layerwise density: [100.0, 170.0, 111.0, 119.0, 45.0, 555.0, 396.0, 447.0, 413.0, 1065.0, 1070.0, 1115.0, 1067.0, 1602.0, 1851.0, 1253.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.068', '0.048', '0.055', '0.050', '0.260', '0.261', '0.272', '0.260', '0.782', '0.904', '0.612', '0.677']
Global density: 0.0677543357014656
06/01 03:00:35 AM | Train: [72/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:00:35 AM | layerwise density: [100.0, 169.0, 111.0, 118.0, 45.0, 549.0, 396.0, 446.0, 410.0, 1063.0, 1071.0, 1115.0, 1066.0, 1597.0, 1850.0, 1251.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.067', '0.048', '0.054', '0.050', '0.260', '0.261', '0.272', '0.260', '0.780', '0.903', '0.611', '0.677']
Global density: 0.06763757020235062
06/01 03:00:47 AM | Train: [72/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:00:47 AM | layerwise density: [100.0, 169.0, 111.0, 117.0, 45.0, 539.0, 393.0, 447.0, 409.0, 1059.0, 1073.0, 1113.0, 1063.0, 1599.0, 1845.0, 1252.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.066', '0.048', '0.055', '0.050', '0.259', '0.262', '0.272', '0.260', '0.781', '0.901', '0.611', '0.677']
Global density: 0.06751549988985062
06/01 03:00:58 AM | Train: [72/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:00:58 AM | layerwise density: [100.0, 169.0, 109.0, 117.0, 45.0, 539.0, 392.0, 445.0, 407.0, 1059.0, 1074.0, 1113.0, 1063.0, 1600.0, 1845.0, 1251.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.066', '0.048', '0.054', '0.050', '0.259', '0.262', '0.272', '0.260', '0.781', '0.901', '0.611', '0.677']
Global density: 0.06748365610837936
06/01 03:01:08 AM | Train: [72/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:01:08 AM | layerwise density: [100.0, 170.0, 109.0, 116.0, 45.0, 538.0, 392.0, 445.0, 407.0, 1061.0, 1075.0, 1113.0, 1059.0, 1599.0, 1845.0, 1250.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.066', '0.048', '0.054', '0.050', '0.259', '0.262', '0.272', '0.259', '0.781', '0.901', '0.610', '0.677']
Global density: 0.06746242195367813
06/01 03:01:08 AM | Train: [72/200] Final Prec@1 99.9620%
06/01 03:01:08 AM | Valid: [72/200] Step 000/078 Loss 1.032 Prec@(1,5) (78.9%, 90.6%)
06/01 03:01:11 AM | Valid: [72/200] Step 078/078 Loss 1.296 Prec@(1,5) (70.8%, 89.8%)
06/01 03:01:11 AM | Valid: [72/200] Final Prec@1 70.8400%
06/01 03:01:11 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 100.0, 0.00152587890625]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 109.0, 0.00665283203125]
['model.relu.alpha_mask_3_0', 16384, 116.0, 0.007080078125]
['model.relu.alpha_mask_4_0', 16384, 45.0, 0.00274658203125]
['model.relu.alpha_mask_5_0', 8192, 538.0, 0.065673828125]
['model.relu.alpha_mask_6_0', 8192, 392.0, 0.0478515625]
['model.relu.alpha_mask_7_0', 8192, 445.0, 0.0543212890625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1061.0, 0.259033203125]
['model.relu.alpha_mask_10_0', 4096, 1075.0, 0.262451171875]
['model.relu.alpha_mask_11_0', 4096, 1113.0, 0.271728515625]
['model.relu.alpha_mask_12_0', 4096, 1059.0, 0.258544921875]
['model.relu.alpha_mask_13_0', 2048, 1599.0, 0.78076171875]
['model.relu.alpha_mask_14_0', 2048, 1845.0, 0.90087890625]
['model.relu.alpha_mask_15_0', 2048, 1250.0, 0.6103515625]
['model.relu.alpha_mask_16_0', 2048, 1387.0, 0.67724609375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12711.0, 0.06746242357336957]
########## End ###########
06/01 03:01:12 AM | Train: [73/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:01:12 AM | layerwise density: [100.0, 170.0, 109.0, 116.0, 45.0, 538.0, 392.0, 445.0, 407.0, 1061.0, 1075.0, 1113.0, 1059.0, 1599.0, 1845.0, 1250.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.066', '0.048', '0.054', '0.050', '0.259', '0.262', '0.272', '0.259', '0.781', '0.901', '0.610', '0.677']
Global density: 0.06746242195367813
06/01 03:01:23 AM | Train: [73/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:01:23 AM | layerwise density: [99.0, 169.0, 107.0, 117.0, 45.0, 539.0, 391.0, 445.0, 407.0, 1061.0, 1076.0, 1113.0, 1058.0, 1600.0, 1845.0, 1250.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.007', '0.007', '0.003', '0.066', '0.048', '0.054', '0.050', '0.259', '0.263', '0.272', '0.258', '0.781', '0.901', '0.610', '0.677']
Global density: 0.06745181232690811
06/01 03:01:34 AM | Train: [73/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:01:34 AM | layerwise density: [99.0, 168.0, 106.0, 117.0, 45.0, 532.0, 391.0, 445.0, 406.0, 1065.0, 1077.0, 1113.0, 1060.0, 1601.0, 1849.0, 1247.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.065', '0.048', '0.054', '0.050', '0.260', '0.263', '0.272', '0.259', '0.782', '0.903', '0.609', '0.677']
Global density: 0.0674465000629425
06/01 03:01:45 AM | Train: [73/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:01:45 AM | layerwise density: [99.0, 167.0, 105.0, 117.0, 45.0, 535.0, 391.0, 444.0, 404.0, 1064.0, 1076.0, 1113.0, 1061.0, 1601.0, 1848.0, 1246.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.065', '0.048', '0.054', '0.049', '0.260', '0.263', '0.272', '0.259', '0.782', '0.902', '0.608', '0.677']
Global density: 0.06741996854543686
06/01 03:01:55 AM | Train: [73/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:01:55 AM | layerwise density: [99.0, 167.0, 105.0, 119.0, 45.0, 536.0, 390.0, 446.0, 404.0, 1066.0, 1076.0, 1113.0, 1065.0, 1601.0, 1848.0, 1246.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.065', '0.048', '0.054', '0.049', '0.260', '0.263', '0.272', '0.260', '0.782', '0.902', '0.608', '0.677']
Global density: 0.06747303903102875
06/01 03:01:55 AM | Train: [73/200] Final Prec@1 99.9580%
06/01 03:01:55 AM | Valid: [73/200] Step 000/078 Loss 1.031 Prec@(1,5) (78.1%, 90.6%)
06/01 03:01:57 AM | Valid: [73/200] Step 078/078 Loss 1.292 Prec@(1,5) (70.6%, 89.8%)
06/01 03:01:57 AM | Valid: [73/200] Final Prec@1 70.6300%
06/01 03:01:57 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 99.0, 0.0015106201171875]
['model.relu.alpha_mask_1_0', 16384, 167.0, 0.01019287109375]
['model.relu.alpha_mask_2_0', 16384, 105.0, 0.00640869140625]
['model.relu.alpha_mask_3_0', 16384, 119.0, 0.00726318359375]
['model.relu.alpha_mask_4_0', 16384, 45.0, 0.00274658203125]
['model.relu.alpha_mask_5_0', 8192, 536.0, 0.0654296875]
['model.relu.alpha_mask_6_0', 8192, 390.0, 0.047607421875]
['model.relu.alpha_mask_7_0', 8192, 446.0, 0.054443359375]
['model.relu.alpha_mask_8_0', 8192, 404.0, 0.04931640625]
['model.relu.alpha_mask_9_0', 4096, 1066.0, 0.26025390625]
['model.relu.alpha_mask_10_0', 4096, 1076.0, 0.2626953125]
['model.relu.alpha_mask_11_0', 4096, 1113.0, 0.271728515625]
['model.relu.alpha_mask_12_0', 4096, 1065.0, 0.260009765625]
['model.relu.alpha_mask_13_0', 2048, 1601.0, 0.78173828125]
['model.relu.alpha_mask_14_0', 2048, 1848.0, 0.90234375]
['model.relu.alpha_mask_15_0', 2048, 1246.0, 0.6083984375]
['model.relu.alpha_mask_16_0', 2048, 1387.0, 0.67724609375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12713.0, 0.06747303838315218]
########## End ###########
06/01 03:01:58 AM | Train: [74/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:01:58 AM | layerwise density: [99.0, 167.0, 105.0, 119.0, 45.0, 536.0, 390.0, 446.0, 404.0, 1066.0, 1076.0, 1113.0, 1065.0, 1601.0, 1848.0, 1246.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.065', '0.048', '0.054', '0.049', '0.260', '0.263', '0.272', '0.260', '0.782', '0.902', '0.608', '0.677']
Global density: 0.06747303903102875
06/01 03:02:09 AM | Train: [74/80] Step 100/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/01 03:02:09 AM | layerwise density: [99.0, 168.0, 105.0, 119.0, 45.0, 537.0, 391.0, 444.0, 401.0, 1064.0, 1076.0, 1113.0, 1064.0, 1599.0, 1847.0, 1243.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.066', '0.048', '0.054', '0.049', '0.260', '0.263', '0.272', '0.260', '0.781', '0.902', '0.607', '0.677']
Global density: 0.06741465628147125
06/01 03:02:20 AM | Train: [74/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:02:20 AM | layerwise density: [99.0, 168.0, 104.0, 119.0, 44.0, 541.0, 390.0, 444.0, 401.0, 1064.0, 1078.0, 1112.0, 1063.0, 1597.0, 1847.0, 1242.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.066', '0.048', '0.054', '0.049', '0.260', '0.263', '0.271', '0.260', '0.780', '0.902', '0.606', '0.677']
Global density: 0.06740404665470123
06/01 03:02:30 AM | Train: [74/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:02:30 AM | layerwise density: [100.0, 167.0, 103.0, 119.0, 44.0, 543.0, 386.0, 445.0, 402.0, 1066.0, 1079.0, 1112.0, 1061.0, 1598.0, 1846.0, 1243.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.066', '0.047', '0.054', '0.049', '0.260', '0.263', '0.271', '0.259', '0.780', '0.901', '0.607', '0.677']
Global density: 0.06740935146808624
06/01 03:02:40 AM | Train: [74/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:02:40 AM | layerwise density: [99.0, 168.0, 103.0, 119.0, 44.0, 543.0, 384.0, 444.0, 402.0, 1066.0, 1081.0, 1113.0, 1062.0, 1598.0, 1846.0, 1244.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.066', '0.047', '0.054', '0.049', '0.260', '0.264', '0.272', '0.259', '0.780', '0.901', '0.607', '0.677']
Global density: 0.06741996854543686
06/01 03:02:40 AM | Train: [74/200] Final Prec@1 99.9600%
06/01 03:02:40 AM | Valid: [74/200] Step 000/078 Loss 1.034 Prec@(1,5) (78.9%, 93.8%)
06/01 03:02:43 AM | Valid: [74/200] Step 078/078 Loss 1.291 Prec@(1,5) (70.5%, 90.1%)
06/01 03:02:43 AM | Valid: [74/200] Final Prec@1 70.4900%
06/01 03:02:43 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 99.0, 0.0015106201171875]
['model.relu.alpha_mask_1_0', 16384, 168.0, 0.01025390625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 119.0, 0.00726318359375]
['model.relu.alpha_mask_4_0', 16384, 44.0, 0.002685546875]
['model.relu.alpha_mask_5_0', 8192, 543.0, 0.0662841796875]
['model.relu.alpha_mask_6_0', 8192, 384.0, 0.046875]
['model.relu.alpha_mask_7_0', 8192, 444.0, 0.05419921875]
['model.relu.alpha_mask_8_0', 8192, 402.0, 0.049072265625]
['model.relu.alpha_mask_9_0', 4096, 1066.0, 0.26025390625]
['model.relu.alpha_mask_10_0', 4096, 1081.0, 0.263916015625]
['model.relu.alpha_mask_11_0', 4096, 1113.0, 0.271728515625]
['model.relu.alpha_mask_12_0', 4096, 1062.0, 0.25927734375]
['model.relu.alpha_mask_13_0', 2048, 1598.0, 0.7802734375]
['model.relu.alpha_mask_14_0', 2048, 1846.0, 0.9013671875]
['model.relu.alpha_mask_15_0', 2048, 1244.0, 0.607421875]
['model.relu.alpha_mask_16_0', 2048, 1387.0, 0.67724609375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12703.0, 0.06741996433423914]
########## End ###########
06/01 03:02:44 AM | Train: [75/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:02:44 AM | layerwise density: [99.0, 168.0, 103.0, 119.0, 44.0, 543.0, 384.0, 444.0, 402.0, 1066.0, 1081.0, 1113.0, 1062.0, 1598.0, 1846.0, 1244.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.066', '0.047', '0.054', '0.049', '0.260', '0.264', '0.272', '0.259', '0.780', '0.901', '0.607', '0.677']
Global density: 0.06741996854543686
06/01 03:02:55 AM | Train: [75/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:02:55 AM | layerwise density: [101.0, 168.0, 103.0, 119.0, 44.0, 545.0, 382.0, 444.0, 401.0, 1065.0, 1080.0, 1114.0, 1060.0, 1598.0, 1847.0, 1244.0, 1387.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.047', '0.054', '0.049', '0.260', '0.264', '0.272', '0.259', '0.780', '0.902', '0.607', '0.677']
Global density: 0.06741465628147125
06/01 03:03:05 AM | Train: [75/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:03:05 AM | layerwise density: [101.0, 168.0, 103.0, 118.0, 44.0, 546.0, 381.0, 444.0, 403.0, 1063.0, 1077.0, 1114.0, 1059.0, 1597.0, 1848.0, 1242.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.047', '0.054', '0.049', '0.260', '0.263', '0.272', '0.259', '0.780', '0.902', '0.606', '0.678']
Global density: 0.0673828125
06/01 03:03:16 AM | Train: [75/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:03:16 AM | layerwise density: [101.0, 168.0, 103.0, 118.0, 44.0, 549.0, 382.0, 443.0, 403.0, 1065.0, 1078.0, 1114.0, 1057.0, 1599.0, 1849.0, 1241.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.047', '0.054', '0.049', '0.260', '0.263', '0.272', '0.258', '0.781', '0.903', '0.606', '0.678']
Global density: 0.06741465628147125
06/01 03:03:26 AM | Train: [75/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:03:26 AM | layerwise density: [101.0, 168.0, 103.0, 119.0, 45.0, 550.0, 381.0, 444.0, 403.0, 1066.0, 1078.0, 1115.0, 1056.0, 1600.0, 1851.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.047', '0.054', '0.049', '0.260', '0.263', '0.272', '0.258', '0.781', '0.904', '0.605', '0.678']
Global density: 0.0674411952495575
06/01 03:03:26 AM | Train: [75/200] Final Prec@1 99.9760%
06/01 03:03:26 AM | Valid: [75/200] Step 000/078 Loss 1.030 Prec@(1,5) (78.9%, 93.0%)
06/01 03:03:28 AM | Valid: [75/200] Step 078/078 Loss 1.297 Prec@(1,5) (70.5%, 89.9%)
06/01 03:03:29 AM | Valid: [75/200] Final Prec@1 70.5300%
06/01 03:03:29 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 101.0, 0.0015411376953125]
['model.relu.alpha_mask_1_0', 16384, 168.0, 0.01025390625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 119.0, 0.00726318359375]
['model.relu.alpha_mask_4_0', 16384, 45.0, 0.00274658203125]
['model.relu.alpha_mask_5_0', 8192, 550.0, 0.067138671875]
['model.relu.alpha_mask_6_0', 8192, 381.0, 0.0465087890625]
['model.relu.alpha_mask_7_0', 8192, 444.0, 0.05419921875]
['model.relu.alpha_mask_8_0', 8192, 403.0, 0.0491943359375]
['model.relu.alpha_mask_9_0', 4096, 1066.0, 0.26025390625]
['model.relu.alpha_mask_10_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_11_0', 4096, 1115.0, 0.272216796875]
['model.relu.alpha_mask_12_0', 4096, 1056.0, 0.2578125]
['model.relu.alpha_mask_13_0', 2048, 1600.0, 0.78125]
['model.relu.alpha_mask_14_0', 2048, 1851.0, 0.90380859375]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12707.0, 0.06744119395380435]
########## End ###########
06/01 03:03:29 AM | Train: [76/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:03:29 AM | layerwise density: [101.0, 168.0, 103.0, 119.0, 45.0, 550.0, 381.0, 444.0, 403.0, 1066.0, 1078.0, 1115.0, 1056.0, 1600.0, 1851.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.047', '0.054', '0.049', '0.260', '0.263', '0.272', '0.258', '0.781', '0.904', '0.605', '0.678']
Global density: 0.0674411952495575
06/01 03:03:40 AM | Train: [76/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:03:40 AM | layerwise density: [101.0, 169.0, 102.0, 119.0, 45.0, 550.0, 381.0, 446.0, 402.0, 1064.0, 1082.0, 1115.0, 1057.0, 1603.0, 1850.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.047', '0.054', '0.049', '0.260', '0.264', '0.272', '0.258', '0.783', '0.903', '0.605', '0.678']
Global density: 0.06747303903102875
06/01 03:03:51 AM | Train: [76/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:03:51 AM | layerwise density: [102.0, 169.0, 102.0, 119.0, 46.0, 553.0, 382.0, 446.0, 402.0, 1066.0, 1082.0, 1115.0, 1057.0, 1603.0, 1851.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.068', '0.047', '0.054', '0.049', '0.260', '0.264', '0.272', '0.258', '0.783', '0.904', '0.605', '0.678']
Global density: 0.06752080470323563
06/01 03:04:02 AM | Train: [76/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:04:02 AM | layerwise density: [102.0, 170.0, 102.0, 119.0, 46.0, 552.0, 383.0, 446.0, 401.0, 1069.0, 1080.0, 1115.0, 1058.0, 1604.0, 1851.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.047', '0.054', '0.049', '0.261', '0.264', '0.272', '0.258', '0.783', '0.904', '0.605', '0.678']
Global density: 0.06753672659397125
06/01 03:04:12 AM | Train: [76/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:04:12 AM | layerwise density: [102.0, 170.0, 102.0, 119.0, 46.0, 548.0, 380.0, 448.0, 400.0, 1070.0, 1080.0, 1115.0, 1059.0, 1602.0, 1853.0, 1238.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.046', '0.055', '0.049', '0.261', '0.264', '0.272', '0.259', '0.782', '0.905', '0.604', '0.678']
Global density: 0.0675101950764656
06/01 03:04:12 AM | Train: [76/200] Final Prec@1 99.9600%
06/01 03:04:12 AM | Valid: [76/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.9%, 91.4%)
06/01 03:04:14 AM | Valid: [76/200] Step 078/078 Loss 1.298 Prec@(1,5) (70.7%, 89.9%)
06/01 03:04:15 AM | Valid: [76/200] Final Prec@1 70.6500%
06/01 03:04:15 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 102.0, 0.0062255859375]
['model.relu.alpha_mask_3_0', 16384, 119.0, 0.00726318359375]
['model.relu.alpha_mask_4_0', 16384, 46.0, 0.0028076171875]
['model.relu.alpha_mask_5_0', 8192, 548.0, 0.06689453125]
['model.relu.alpha_mask_6_0', 8192, 380.0, 0.04638671875]
['model.relu.alpha_mask_7_0', 8192, 448.0, 0.0546875]
['model.relu.alpha_mask_8_0', 8192, 400.0, 0.048828125]
['model.relu.alpha_mask_9_0', 4096, 1070.0, 0.26123046875]
['model.relu.alpha_mask_10_0', 4096, 1080.0, 0.263671875]
['model.relu.alpha_mask_11_0', 4096, 1116.0, 0.2724609375]
['model.relu.alpha_mask_12_0', 4096, 1059.0, 0.258544921875]
['model.relu.alpha_mask_13_0', 2048, 1602.0, 0.7822265625]
['model.relu.alpha_mask_14_0', 2048, 1853.0, 0.90478515625]
['model.relu.alpha_mask_15_0', 2048, 1238.0, 0.6044921875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12721.0, 0.06751549762228261]
########## End ###########
06/01 03:04:16 AM | Train: [77/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:04:16 AM | layerwise density: [102.0, 170.0, 102.0, 119.0, 46.0, 548.0, 380.0, 448.0, 400.0, 1070.0, 1080.0, 1116.0, 1059.0, 1602.0, 1853.0, 1238.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.046', '0.055', '0.049', '0.261', '0.264', '0.272', '0.259', '0.782', '0.905', '0.604', '0.678']
Global density: 0.06751549988985062
06/01 03:04:26 AM | Train: [77/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:04:26 AM | layerwise density: [102.0, 170.0, 102.0, 119.0, 46.0, 550.0, 379.0, 447.0, 400.0, 1073.0, 1080.0, 1116.0, 1061.0, 1601.0, 1852.0, 1238.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.046', '0.055', '0.049', '0.262', '0.264', '0.272', '0.259', '0.782', '0.904', '0.604', '0.678']
Global density: 0.06753142178058624
06/01 03:04:37 AM | Train: [77/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:04:37 AM | layerwise density: [102.0, 170.0, 103.0, 120.0, 46.0, 551.0, 378.0, 448.0, 403.0, 1073.0, 1079.0, 1116.0, 1062.0, 1602.0, 1853.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.067', '0.046', '0.055', '0.049', '0.262', '0.263', '0.272', '0.259', '0.782', '0.905', '0.605', '0.678']
Global density: 0.06757918745279312
06/01 03:04:48 AM | Train: [77/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:04:48 AM | layerwise density: [102.0, 170.0, 103.0, 120.0, 46.0, 553.0, 379.0, 448.0, 405.0, 1073.0, 1079.0, 1116.0, 1064.0, 1603.0, 1854.0, 1241.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.068', '0.046', '0.055', '0.049', '0.262', '0.263', '0.272', '0.260', '0.783', '0.905', '0.606', '0.678']
Global density: 0.06763757020235062
06/01 03:04:58 AM | Train: [77/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:04:58 AM | layerwise density: [102.0, 171.0, 103.0, 120.0, 47.0, 555.0, 378.0, 448.0, 405.0, 1074.0, 1079.0, 1116.0, 1064.0, 1602.0, 1854.0, 1242.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.068', '0.046', '0.055', '0.049', '0.262', '0.263', '0.272', '0.260', '0.782', '0.905', '0.606', '0.678']
Global density: 0.06765879690647125
06/01 03:04:58 AM | Train: [77/200] Final Prec@1 99.9500%
06/01 03:04:58 AM | Valid: [77/200] Step 000/078 Loss 1.025 Prec@(1,5) (77.3%, 90.6%)
06/01 03:05:00 AM | Valid: [77/200] Step 078/078 Loss 1.301 Prec@(1,5) (70.5%, 90.0%)
06/01 03:05:00 AM | Valid: [77/200] Final Prec@1 70.4800%
06/01 03:05:00 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 171.0, 0.01043701171875]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 120.0, 0.00732421875]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 555.0, 0.0677490234375]
['model.relu.alpha_mask_6_0', 8192, 378.0, 0.046142578125]
['model.relu.alpha_mask_7_0', 8192, 448.0, 0.0546875]
['model.relu.alpha_mask_8_0', 8192, 405.0, 0.0494384765625]
['model.relu.alpha_mask_9_0', 4096, 1074.0, 0.26220703125]
['model.relu.alpha_mask_10_0', 4096, 1079.0, 0.263427734375]
['model.relu.alpha_mask_11_0', 4096, 1116.0, 0.2724609375]
['model.relu.alpha_mask_12_0', 4096, 1064.0, 0.259765625]
['model.relu.alpha_mask_13_0', 2048, 1602.0, 0.7822265625]
['model.relu.alpha_mask_14_0', 2048, 1854.0, 0.9052734375]
['model.relu.alpha_mask_15_0', 2048, 1242.0, 0.6064453125]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12748.0, 0.06765879755434782]
########## End ###########
06/01 03:05:01 AM | Train: [78/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:05:01 AM | layerwise density: [102.0, 171.0, 103.0, 120.0, 47.0, 555.0, 378.0, 448.0, 405.0, 1074.0, 1079.0, 1116.0, 1064.0, 1602.0, 1854.0, 1242.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.068', '0.046', '0.055', '0.049', '0.262', '0.263', '0.272', '0.260', '0.782', '0.905', '0.606', '0.678']
Global density: 0.06765879690647125
06/01 03:05:12 AM | Train: [78/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:05:12 AM | layerwise density: [102.0, 171.0, 103.0, 122.0, 47.0, 557.0, 378.0, 448.0, 406.0, 1075.0, 1080.0, 1116.0, 1064.0, 1602.0, 1855.0, 1242.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.068', '0.046', '0.055', '0.050', '0.262', '0.264', '0.272', '0.260', '0.782', '0.906', '0.606', '0.678']
Global density: 0.06770125776529312
06/01 03:05:23 AM | Train: [78/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:05:23 AM | layerwise density: [102.0, 170.0, 103.0, 122.0, 47.0, 561.0, 378.0, 449.0, 407.0, 1075.0, 1081.0, 1116.0, 1063.0, 1602.0, 1856.0, 1242.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.068', '0.046', '0.055', '0.050', '0.262', '0.264', '0.272', '0.260', '0.782', '0.906', '0.606', '0.678']
Global density: 0.06773310154676437
06/01 03:05:33 AM | Train: [78/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:05:33 AM | layerwise density: [102.0, 170.0, 103.0, 122.0, 47.0, 562.0, 379.0, 451.0, 405.0, 1074.0, 1082.0, 1116.0, 1064.0, 1603.0, 1856.0, 1240.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.069', '0.046', '0.055', '0.049', '0.262', '0.264', '0.272', '0.260', '0.783', '0.906', '0.605', '0.678']
Global density: 0.06774371862411499
06/01 03:05:43 AM | Train: [78/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:05:43 AM | layerwise density: [102.0, 170.0, 103.0, 121.0, 47.0, 563.0, 379.0, 451.0, 405.0, 1074.0, 1081.0, 1116.0, 1066.0, 1603.0, 1857.0, 1240.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.069', '0.046', '0.055', '0.049', '0.262', '0.264', '0.272', '0.260', '0.783', '0.907', '0.605', '0.678']
Global density: 0.0677543357014656
06/01 03:05:43 AM | Train: [78/200] Final Prec@1 99.9620%
06/01 03:05:44 AM | Valid: [78/200] Step 000/078 Loss 1.008 Prec@(1,5) (77.3%, 92.2%)
06/01 03:05:46 AM | Valid: [78/200] Step 078/078 Loss 1.292 Prec@(1,5) (70.6%, 90.1%)
06/01 03:05:46 AM | Valid: [78/200] Final Prec@1 70.5900%
06/01 03:05:46 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 121.0, 0.00738525390625]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 563.0, 0.0687255859375]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 451.0, 0.0550537109375]
['model.relu.alpha_mask_8_0', 8192, 405.0, 0.0494384765625]
['model.relu.alpha_mask_9_0', 4096, 1074.0, 0.26220703125]
['model.relu.alpha_mask_10_0', 4096, 1081.0, 0.263916015625]
['model.relu.alpha_mask_11_0', 4096, 1116.0, 0.2724609375]
['model.relu.alpha_mask_12_0', 4096, 1066.0, 0.26025390625]
['model.relu.alpha_mask_13_0', 2048, 1603.0, 0.78271484375]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1240.0, 0.60546875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12766.0, 0.06775433084239131]
########## End ###########
06/01 03:05:47 AM | Train: [79/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:05:47 AM | layerwise density: [102.0, 170.0, 103.0, 121.0, 47.0, 563.0, 379.0, 451.0, 405.0, 1074.0, 1081.0, 1116.0, 1066.0, 1603.0, 1857.0, 1240.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.069', '0.046', '0.055', '0.049', '0.262', '0.264', '0.272', '0.260', '0.783', '0.907', '0.605', '0.678']
Global density: 0.0677543357014656
06/01 03:05:58 AM | Train: [79/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:05:58 AM | layerwise density: [102.0, 169.0, 103.0, 120.0, 47.0, 565.0, 379.0, 451.0, 406.0, 1074.0, 1080.0, 1116.0, 1065.0, 1603.0, 1857.0, 1240.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.069', '0.046', '0.055', '0.050', '0.262', '0.264', '0.272', '0.260', '0.783', '0.907', '0.605', '0.678']
Global density: 0.0677490234375
06/01 03:06:09 AM | Train: [79/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:06:09 AM | layerwise density: [102.0, 169.0, 103.0, 120.0, 47.0, 567.0, 380.0, 452.0, 406.0, 1076.0, 1081.0, 1116.0, 1065.0, 1604.0, 1857.0, 1240.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.069', '0.046', '0.055', '0.050', '0.263', '0.264', '0.272', '0.260', '0.783', '0.907', '0.605', '0.678']
Global density: 0.06779148429632187
06/01 03:06:19 AM | Train: [79/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:06:19 AM | layerwise density: [101.0, 171.0, 103.0, 121.0, 47.0, 568.0, 380.0, 452.0, 405.0, 1076.0, 1081.0, 1117.0, 1065.0, 1605.0, 1857.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.069', '0.046', '0.055', '0.049', '0.263', '0.264', '0.273', '0.260', '0.784', '0.907', '0.605', '0.678']
Global density: 0.0678074061870575
06/01 03:06:29 AM | Train: [79/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:06:29 AM | layerwise density: [100.0, 171.0, 103.0, 122.0, 47.0, 569.0, 380.0, 451.0, 406.0, 1075.0, 1082.0, 1117.0, 1065.0, 1605.0, 1856.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.069', '0.046', '0.055', '0.050', '0.262', '0.264', '0.273', '0.260', '0.784', '0.906', '0.605', '0.678']
Global density: 0.0678074061870575
06/01 03:06:29 AM | Train: [79/200] Final Prec@1 99.9620%
06/01 03:06:29 AM | Valid: [79/200] Step 000/078 Loss 1.027 Prec@(1,5) (78.9%, 93.0%)
06/01 03:06:31 AM | Valid: [79/200] Step 078/078 Loss 1.294 Prec@(1,5) (70.4%, 90.1%)
06/01 03:06:31 AM | Valid: [79/200] Final Prec@1 70.3900%
06/01 03:06:31 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 100.0, 0.00152587890625]
['model.relu.alpha_mask_1_0', 16384, 171.0, 0.01043701171875]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 122.0, 0.0074462890625]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 380.0, 0.04638671875]
['model.relu.alpha_mask_7_0', 8192, 451.0, 0.0550537109375]
['model.relu.alpha_mask_8_0', 8192, 406.0, 0.049560546875]
['model.relu.alpha_mask_9_0', 4096, 1075.0, 0.262451171875]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1117.0, 0.272705078125]
['model.relu.alpha_mask_12_0', 4096, 1065.0, 0.260009765625]
['model.relu.alpha_mask_13_0', 2048, 1605.0, 0.78369140625]
['model.relu.alpha_mask_14_0', 2048, 1856.0, 0.90625]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12776.0, 0.06780740489130435]
########## End ###########
06/01 03:06:32 AM | Train: [80/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:06:32 AM | layerwise density: [100.0, 171.0, 103.0, 122.0, 47.0, 569.0, 380.0, 451.0, 406.0, 1075.0, 1082.0, 1117.0, 1065.0, 1605.0, 1856.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.007', '0.003', '0.069', '0.046', '0.055', '0.050', '0.262', '0.264', '0.273', '0.260', '0.784', '0.906', '0.605', '0.678']
Global density: 0.0678074061870575
06/01 03:06:43 AM | Train: [80/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:06:43 AM | layerwise density: [101.0, 171.0, 103.0, 124.0, 47.0, 568.0, 380.0, 450.0, 406.0, 1075.0, 1082.0, 1117.0, 1065.0, 1606.0, 1856.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.008', '0.003', '0.069', '0.046', '0.055', '0.050', '0.262', '0.264', '0.273', '0.260', '0.784', '0.906', '0.605', '0.678']
Global density: 0.06781802326440811
06/01 03:06:53 AM | Train: [80/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:06:53 AM | layerwise density: [102.0, 171.0, 103.0, 124.0, 47.0, 570.0, 380.0, 450.0, 406.0, 1077.0, 1082.0, 1117.0, 1067.0, 1606.0, 1856.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.008', '0.003', '0.070', '0.046', '0.055', '0.050', '0.263', '0.264', '0.273', '0.260', '0.784', '0.906', '0.605', '0.678']
Global density: 0.06785517185926437
06/01 03:07:04 AM | Train: [80/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:07:04 AM | layerwise density: [102.0, 171.0, 103.0, 124.0, 47.0, 571.0, 380.0, 450.0, 406.0, 1078.0, 1083.0, 1117.0, 1068.0, 1608.0, 1855.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.008', '0.003', '0.070', '0.046', '0.055', '0.050', '0.263', '0.264', '0.273', '0.261', '0.785', '0.906', '0.605', '0.678']
Global density: 0.06788171082735062
06/01 03:07:14 AM | Train: [80/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:07:14 AM | layerwise density: [102.0, 170.0, 103.0, 124.0, 47.0, 569.0, 379.0, 450.0, 407.0, 1078.0, 1082.0, 1118.0, 1068.0, 1609.0, 1857.0, 1239.0, 1388.0]
layerwise density percentage: ['0.002', '0.010', '0.006', '0.008', '0.003', '0.069', '0.046', '0.055', '0.050', '0.263', '0.264', '0.273', '0.261', '0.786', '0.907', '0.605', '0.678']
Global density: 0.06788171082735062
06/01 03:07:14 AM | Train: [80/200] Final Prec@1 99.9640%
06/01 03:07:14 AM | Valid: [80/200] Step 000/078 Loss 1.044 Prec@(1,5) (78.1%, 91.4%)
06/01 03:07:17 AM | Valid: [80/200] Step 078/078 Loss 1.297 Prec@(1,5) (70.5%, 90.0%)
06/01 03:07:17 AM | Valid: [80/200] Final Prec@1 70.4500%
06/01 03:07:17 AM | Current mask training best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:07:18 AM | Train: [ 1/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:07:28 AM | Train: [ 1/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:07:38 AM | Train: [ 1/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:07:48 AM | Train: [ 1/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:07:57 AM | Train: [ 1/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:07:57 AM | Train: [ 1/200] Final Prec@1 99.9760%
06/01 03:07:57 AM | Valid: [ 1/200] Step 000/078 Loss 1.023 Prec@(1,5) (78.1%, 93.0%)
06/01 03:08:00 AM | Valid: [ 1/200] Step 078/078 Loss 1.294 Prec@(1,5) (70.3%, 90.0%)
06/01 03:08:00 AM | Valid: [ 1/200] Final Prec@1 70.3200%
06/01 03:08:00 AM | Current best Prec@1 = 70.3200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:08:01 AM | Train: [ 2/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:08:11 AM | Train: [ 2/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:08:21 AM | Train: [ 2/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:08:31 AM | Train: [ 2/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:08:41 AM | Train: [ 2/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:08:41 AM | Train: [ 2/200] Final Prec@1 99.9760%
06/01 03:08:41 AM | Valid: [ 2/200] Step 000/078 Loss 1.019 Prec@(1,5) (77.3%, 90.6%)
06/01 03:08:43 AM | Valid: [ 2/200] Step 078/078 Loss 1.289 Prec@(1,5) (70.7%, 90.1%)
06/01 03:08:43 AM | Valid: [ 2/200] Final Prec@1 70.6800%
06/01 03:08:44 AM | Current best Prec@1 = 70.6800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:08:44 AM | Train: [ 3/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:08:54 AM | Train: [ 3/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:09:04 AM | Train: [ 3/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:09:15 AM | Train: [ 3/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:09:24 AM | Train: [ 3/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:09:24 AM | Train: [ 3/200] Final Prec@1 99.9700%
06/01 03:09:24 AM | Valid: [ 3/200] Step 000/078 Loss 1.022 Prec@(1,5) (78.9%, 90.6%)
06/01 03:09:26 AM | Valid: [ 3/200] Step 078/078 Loss 1.290 Prec@(1,5) (70.5%, 90.1%)
06/01 03:09:26 AM | Valid: [ 3/200] Final Prec@1 70.5300%
06/01 03:09:27 AM | Current best Prec@1 = 70.6800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:09:27 AM | Train: [ 4/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:09:38 AM | Train: [ 4/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:09:48 AM | Train: [ 4/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:09:59 AM | Train: [ 4/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:10:08 AM | Train: [ 4/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:10:08 AM | Train: [ 4/200] Final Prec@1 99.9660%
06/01 03:10:08 AM | Valid: [ 4/200] Step 000/078 Loss 1.008 Prec@(1,5) (78.9%, 90.6%)
06/01 03:10:11 AM | Valid: [ 4/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.5%, 90.1%)
06/01 03:10:11 AM | Valid: [ 4/200] Final Prec@1 70.5100%
06/01 03:10:11 AM | Current best Prec@1 = 70.6800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:10:12 AM | Train: [ 5/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:10:22 AM | Train: [ 5/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:10:33 AM | Train: [ 5/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:10:44 AM | Train: [ 5/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:10:54 AM | Train: [ 5/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:10:54 AM | Train: [ 5/200] Final Prec@1 99.9680%
06/01 03:10:54 AM | Valid: [ 5/200] Step 000/078 Loss 1.013 Prec@(1,5) (78.9%, 89.8%)
06/01 03:10:56 AM | Valid: [ 5/200] Step 078/078 Loss 1.288 Prec@(1,5) (70.8%, 90.0%)
06/01 03:10:56 AM | Valid: [ 5/200] Final Prec@1 70.8400%
06/01 03:10:57 AM | Current best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:10:57 AM | Train: [ 6/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:11:08 AM | Train: [ 6/200] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:11:19 AM | Train: [ 6/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:11:29 AM | Train: [ 6/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:11:38 AM | Train: [ 6/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:11:38 AM | Train: [ 6/200] Final Prec@1 99.9800%
06/01 03:11:38 AM | Valid: [ 6/200] Step 000/078 Loss 1.007 Prec@(1,5) (78.1%, 91.4%)
06/01 03:11:41 AM | Valid: [ 6/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.7%, 90.0%)
06/01 03:11:41 AM | Valid: [ 6/200] Final Prec@1 70.6500%
06/01 03:11:41 AM | Current best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:11:42 AM | Train: [ 7/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:11:53 AM | Train: [ 7/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:12:03 AM | Train: [ 7/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:12:14 AM | Train: [ 7/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:12:23 AM | Train: [ 7/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:12:23 AM | Train: [ 7/200] Final Prec@1 99.9640%
06/01 03:12:23 AM | Valid: [ 7/200] Step 000/078 Loss 1.014 Prec@(1,5) (78.9%, 92.2%)
06/01 03:12:26 AM | Valid: [ 7/200] Step 078/078 Loss 1.288 Prec@(1,5) (70.6%, 90.1%)
06/01 03:12:26 AM | Valid: [ 7/200] Final Prec@1 70.6400%
06/01 03:12:26 AM | Current best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:12:27 AM | Train: [ 8/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 03:12:37 AM | Train: [ 8/200] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:12:47 AM | Train: [ 8/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:12:58 AM | Train: [ 8/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:13:07 AM | Train: [ 8/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:13:08 AM | Train: [ 8/200] Final Prec@1 99.9620%
06/01 03:13:08 AM | Valid: [ 8/200] Step 000/078 Loss 1.030 Prec@(1,5) (78.9%, 90.6%)
06/01 03:13:10 AM | Valid: [ 8/200] Step 078/078 Loss 1.293 Prec@(1,5) (70.7%, 90.0%)
06/01 03:13:10 AM | Valid: [ 8/200] Final Prec@1 70.6700%
06/01 03:13:11 AM | Current best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:13:11 AM | Train: [ 9/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:13:22 AM | Train: [ 9/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:13:32 AM | Train: [ 9/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:13:43 AM | Train: [ 9/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:13:52 AM | Train: [ 9/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:13:52 AM | Train: [ 9/200] Final Prec@1 99.9580%
06/01 03:13:52 AM | Valid: [ 9/200] Step 000/078 Loss 1.014 Prec@(1,5) (78.9%, 91.4%)
06/01 03:13:54 AM | Valid: [ 9/200] Step 078/078 Loss 1.291 Prec@(1,5) (70.7%, 90.1%)
06/01 03:13:54 AM | Valid: [ 9/200] Final Prec@1 70.6700%
06/01 03:13:55 AM | Current best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:13:55 AM | Train: [10/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:14:06 AM | Train: [10/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:14:16 AM | Train: [10/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:14:27 AM | Train: [10/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:14:36 AM | Train: [10/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:14:37 AM | Train: [10/200] Final Prec@1 99.9720%
06/01 03:14:37 AM | Valid: [10/200] Step 000/078 Loss 1.021 Prec@(1,5) (78.1%, 91.4%)
06/01 03:14:39 AM | Valid: [10/200] Step 078/078 Loss 1.286 Prec@(1,5) (70.6%, 90.0%)
06/01 03:14:39 AM | Valid: [10/200] Final Prec@1 70.5700%
06/01 03:14:40 AM | Current best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:14:40 AM | Train: [11/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:14:50 AM | Train: [11/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:15:00 AM | Train: [11/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:15:11 AM | Train: [11/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:15:20 AM | Train: [11/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:15:20 AM | Train: [11/200] Final Prec@1 99.9800%
06/01 03:15:20 AM | Valid: [11/200] Step 000/078 Loss 1.010 Prec@(1,5) (78.9%, 92.2%)
06/01 03:15:22 AM | Valid: [11/200] Step 078/078 Loss 1.288 Prec@(1,5) (70.6%, 90.1%)
06/01 03:15:23 AM | Valid: [11/200] Final Prec@1 70.5900%
06/01 03:15:23 AM | Current best Prec@1 = 70.8400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:15:24 AM | Train: [12/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:15:34 AM | Train: [12/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 03:15:44 AM | Train: [12/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:15:55 AM | Train: [12/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:16:04 AM | Train: [12/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:16:04 AM | Train: [12/200] Final Prec@1 99.9580%
06/01 03:16:04 AM | Valid: [12/200] Step 000/078 Loss 1.034 Prec@(1,5) (78.1%, 90.6%)
06/01 03:16:06 AM | Valid: [12/200] Step 078/078 Loss 1.288 Prec@(1,5) (71.0%, 90.1%)
06/01 03:16:07 AM | Valid: [12/200] Final Prec@1 70.9700%
06/01 03:16:07 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:16:08 AM | Train: [13/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:16:18 AM | Train: [13/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:16:28 AM | Train: [13/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:16:39 AM | Train: [13/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:16:48 AM | Train: [13/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:16:48 AM | Train: [13/200] Final Prec@1 99.9660%
06/01 03:16:49 AM | Valid: [13/200] Step 000/078 Loss 1.023 Prec@(1,5) (77.3%, 91.4%)
06/01 03:16:51 AM | Valid: [13/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.8%, 90.0%)
06/01 03:16:51 AM | Valid: [13/200] Final Prec@1 70.7900%
06/01 03:16:52 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:16:52 AM | Train: [14/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 03:17:02 AM | Train: [14/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:17:13 AM | Train: [14/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:17:23 AM | Train: [14/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:17:32 AM | Train: [14/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:17:32 AM | Train: [14/200] Final Prec@1 99.9700%
06/01 03:17:32 AM | Valid: [14/200] Step 000/078 Loss 1.020 Prec@(1,5) (78.1%, 91.4%)
06/01 03:17:35 AM | Valid: [14/200] Step 078/078 Loss 1.285 Prec@(1,5) (70.7%, 90.1%)
06/01 03:17:35 AM | Valid: [14/200] Final Prec@1 70.7100%
06/01 03:17:35 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:17:36 AM | Train: [15/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:17:46 AM | Train: [15/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:17:57 AM | Train: [15/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:18:07 AM | Train: [15/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:18:17 AM | Train: [15/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:18:17 AM | Train: [15/200] Final Prec@1 99.9680%
06/01 03:18:17 AM | Valid: [15/200] Step 000/078 Loss 1.002 Prec@(1,5) (78.9%, 89.8%)
06/01 03:18:19 AM | Valid: [15/200] Step 078/078 Loss 1.289 Prec@(1,5) (70.4%, 90.0%)
06/01 03:18:19 AM | Valid: [15/200] Final Prec@1 70.4400%
06/01 03:18:20 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:18:20 AM | Train: [16/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:18:31 AM | Train: [16/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:18:41 AM | Train: [16/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:18:51 AM | Train: [16/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:19:01 AM | Train: [16/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:19:01 AM | Train: [16/200] Final Prec@1 99.9780%
06/01 03:19:01 AM | Valid: [16/200] Step 000/078 Loss 1.014 Prec@(1,5) (79.7%, 91.4%)
06/01 03:19:03 AM | Valid: [16/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.8%, 90.0%)
06/01 03:19:03 AM | Valid: [16/200] Final Prec@1 70.8300%
06/01 03:19:04 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:19:04 AM | Train: [17/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:19:14 AM | Train: [17/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:19:25 AM | Train: [17/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:19:35 AM | Train: [17/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:19:45 AM | Train: [17/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:19:45 AM | Train: [17/200] Final Prec@1 99.9800%
06/01 03:19:45 AM | Valid: [17/200] Step 000/078 Loss 1.004 Prec@(1,5) (78.1%, 90.6%)
06/01 03:19:48 AM | Valid: [17/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.9%, 90.0%)
06/01 03:19:48 AM | Valid: [17/200] Final Prec@1 70.8700%
06/01 03:19:49 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:19:49 AM | Train: [18/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:19:59 AM | Train: [18/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:20:10 AM | Train: [18/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:20:20 AM | Train: [18/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:20:29 AM | Train: [18/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:20:29 AM | Train: [18/200] Final Prec@1 99.9740%
06/01 03:20:30 AM | Valid: [18/200] Step 000/078 Loss 1.007 Prec@(1,5) (78.9%, 91.4%)
06/01 03:20:32 AM | Valid: [18/200] Step 078/078 Loss 1.286 Prec@(1,5) (70.6%, 90.2%)
06/01 03:20:32 AM | Valid: [18/200] Final Prec@1 70.5700%
06/01 03:20:33 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:20:33 AM | Train: [19/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:20:43 AM | Train: [19/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:20:54 AM | Train: [19/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:21:04 AM | Train: [19/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:21:14 AM | Train: [19/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:21:14 AM | Train: [19/200] Final Prec@1 99.9700%
06/01 03:21:14 AM | Valid: [19/200] Step 000/078 Loss 1.000 Prec@(1,5) (78.9%, 91.4%)
06/01 03:21:17 AM | Valid: [19/200] Step 078/078 Loss 1.285 Prec@(1,5) (70.6%, 90.0%)
06/01 03:21:17 AM | Valid: [19/200] Final Prec@1 70.5700%
06/01 03:21:17 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:21:18 AM | Train: [20/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:21:28 AM | Train: [20/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:21:38 AM | Train: [20/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:21:49 AM | Train: [20/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:21:59 AM | Train: [20/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:21:59 AM | Train: [20/200] Final Prec@1 99.9680%
06/01 03:21:59 AM | Valid: [20/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.1%, 89.8%)
06/01 03:22:02 AM | Valid: [20/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.7%, 90.1%)
06/01 03:22:02 AM | Valid: [20/200] Final Prec@1 70.6900%
06/01 03:22:02 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:22:03 AM | Train: [21/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:22:13 AM | Train: [21/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:22:24 AM | Train: [21/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:22:34 AM | Train: [21/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:22:44 AM | Train: [21/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:22:44 AM | Train: [21/200] Final Prec@1 99.9700%
06/01 03:22:44 AM | Valid: [21/200] Step 000/078 Loss 1.005 Prec@(1,5) (78.1%, 91.4%)
06/01 03:22:46 AM | Valid: [21/200] Step 078/078 Loss 1.286 Prec@(1,5) (70.8%, 90.1%)
06/01 03:22:46 AM | Valid: [21/200] Final Prec@1 70.7900%
06/01 03:22:47 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:22:47 AM | Train: [22/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:22:58 AM | Train: [22/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:23:09 AM | Train: [22/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:23:19 AM | Train: [22/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:23:28 AM | Train: [22/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:23:28 AM | Train: [22/200] Final Prec@1 99.9680%
06/01 03:23:28 AM | Valid: [22/200] Step 000/078 Loss 1.029 Prec@(1,5) (78.9%, 92.2%)
06/01 03:23:31 AM | Valid: [22/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.6%, 90.0%)
06/01 03:23:31 AM | Valid: [22/200] Final Prec@1 70.6400%
06/01 03:23:31 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:23:31 AM | Train: [23/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:23:42 AM | Train: [23/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:23:52 AM | Train: [23/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:24:01 AM | Train: [23/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:24:10 AM | Train: [23/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:24:10 AM | Train: [23/200] Final Prec@1 99.9700%
06/01 03:24:10 AM | Valid: [23/200] Step 000/078 Loss 1.023 Prec@(1,5) (79.7%, 90.6%)
06/01 03:24:12 AM | Valid: [23/200] Step 078/078 Loss 1.285 Prec@(1,5) (70.8%, 89.9%)
06/01 03:24:13 AM | Valid: [23/200] Final Prec@1 70.7500%
06/01 03:24:13 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:24:13 AM | Train: [24/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:24:24 AM | Train: [24/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:24:35 AM | Train: [24/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:24:45 AM | Train: [24/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:24:55 AM | Train: [24/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:24:55 AM | Train: [24/200] Final Prec@1 99.9720%
06/01 03:24:55 AM | Valid: [24/200] Step 000/078 Loss 1.013 Prec@(1,5) (78.9%, 90.6%)
06/01 03:24:58 AM | Valid: [24/200] Step 078/078 Loss 1.286 Prec@(1,5) (70.7%, 90.1%)
06/01 03:24:58 AM | Valid: [24/200] Final Prec@1 70.6500%
06/01 03:24:58 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:24:59 AM | Train: [25/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:25:09 AM | Train: [25/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:25:20 AM | Train: [25/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:25:30 AM | Train: [25/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:25:39 AM | Train: [25/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:25:39 AM | Train: [25/200] Final Prec@1 99.9580%
06/01 03:25:39 AM | Valid: [25/200] Step 000/078 Loss 1.032 Prec@(1,5) (78.9%, 90.6%)
06/01 03:25:42 AM | Valid: [25/200] Step 078/078 Loss 1.286 Prec@(1,5) (70.7%, 90.1%)
06/01 03:25:42 AM | Valid: [25/200] Final Prec@1 70.6700%
06/01 03:25:42 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:25:43 AM | Train: [26/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:25:52 AM | Train: [26/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:26:03 AM | Train: [26/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:26:14 AM | Train: [26/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:26:23 AM | Train: [26/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:26:23 AM | Train: [26/200] Final Prec@1 99.9740%
06/01 03:26:24 AM | Valid: [26/200] Step 000/078 Loss 1.008 Prec@(1,5) (78.1%, 90.6%)
06/01 03:26:26 AM | Valid: [26/200] Step 078/078 Loss 1.286 Prec@(1,5) (70.7%, 90.0%)
06/01 03:26:26 AM | Valid: [26/200] Final Prec@1 70.7000%
06/01 03:26:27 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:26:27 AM | Train: [27/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:26:37 AM | Train: [27/200] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:26:48 AM | Train: [27/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:26:58 AM | Train: [27/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:27:07 AM | Train: [27/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:27:07 AM | Train: [27/200] Final Prec@1 99.9780%
06/01 03:27:07 AM | Valid: [27/200] Step 000/078 Loss 1.009 Prec@(1,5) (77.3%, 91.4%)
06/01 03:27:10 AM | Valid: [27/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.7%, 90.0%)
06/01 03:27:10 AM | Valid: [27/200] Final Prec@1 70.6900%
06/01 03:27:11 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:27:11 AM | Train: [28/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:27:21 AM | Train: [28/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:27:31 AM | Train: [28/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:27:42 AM | Train: [28/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:27:51 AM | Train: [28/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:27:51 AM | Train: [28/200] Final Prec@1 99.9660%
06/01 03:27:51 AM | Valid: [28/200] Step 000/078 Loss 1.020 Prec@(1,5) (78.9%, 90.6%)
06/01 03:27:54 AM | Valid: [28/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.9%, 90.1%)
06/01 03:27:54 AM | Valid: [28/200] Final Prec@1 70.9100%
06/01 03:27:54 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:27:55 AM | Train: [29/200] Step 000/390 Loss 0.014 Prec@(1,5) (99.2%, 100.0%)
06/01 03:28:05 AM | Train: [29/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:28:16 AM | Train: [29/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:28:26 AM | Train: [29/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:28:36 AM | Train: [29/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:28:36 AM | Train: [29/200] Final Prec@1 99.9660%
06/01 03:28:36 AM | Valid: [29/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.1%, 91.4%)
06/01 03:28:38 AM | Valid: [29/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.7%, 90.1%)
06/01 03:28:38 AM | Valid: [29/200] Final Prec@1 70.7100%
06/01 03:28:39 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:28:39 AM | Train: [30/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:28:50 AM | Train: [30/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:29:00 AM | Train: [30/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:29:09 AM | Train: [30/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:29:18 AM | Train: [30/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:29:18 AM | Train: [30/200] Final Prec@1 99.9740%
06/01 03:29:18 AM | Valid: [30/200] Step 000/078 Loss 1.010 Prec@(1,5) (78.9%, 91.4%)
06/01 03:29:21 AM | Valid: [30/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.8%, 90.2%)
06/01 03:29:21 AM | Valid: [30/200] Final Prec@1 70.8200%
06/01 03:29:21 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:29:22 AM | Train: [31/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:29:32 AM | Train: [31/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:29:42 AM | Train: [31/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:29:53 AM | Train: [31/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:30:03 AM | Train: [31/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:30:03 AM | Train: [31/200] Final Prec@1 99.9620%
06/01 03:30:03 AM | Valid: [31/200] Step 000/078 Loss 1.017 Prec@(1,5) (77.3%, 91.4%)
06/01 03:30:05 AM | Valid: [31/200] Step 078/078 Loss 1.288 Prec@(1,5) (70.8%, 90.1%)
06/01 03:30:05 AM | Valid: [31/200] Final Prec@1 70.7600%
06/01 03:30:06 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:30:06 AM | Train: [32/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:30:16 AM | Train: [32/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:30:27 AM | Train: [32/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:30:38 AM | Train: [32/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:30:47 AM | Train: [32/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:30:47 AM | Train: [32/200] Final Prec@1 99.9720%
06/01 03:30:48 AM | Valid: [32/200] Step 000/078 Loss 1.016 Prec@(1,5) (78.1%, 91.4%)
06/01 03:30:50 AM | Valid: [32/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.8%, 90.0%)
06/01 03:30:50 AM | Valid: [32/200] Final Prec@1 70.7700%
06/01 03:30:51 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:30:51 AM | Train: [33/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:31:01 AM | Train: [33/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:31:12 AM | Train: [33/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:31:23 AM | Train: [33/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:31:32 AM | Train: [33/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:31:32 AM | Train: [33/200] Final Prec@1 99.9700%
06/01 03:31:33 AM | Valid: [33/200] Step 000/078 Loss 1.019 Prec@(1,5) (78.1%, 91.4%)
06/01 03:31:35 AM | Valid: [33/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.8%, 90.1%)
06/01 03:31:35 AM | Valid: [33/200] Final Prec@1 70.8300%
06/01 03:31:36 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:31:36 AM | Train: [34/200] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 03:31:47 AM | Train: [34/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:31:57 AM | Train: [34/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:32:07 AM | Train: [34/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:32:17 AM | Train: [34/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:32:17 AM | Train: [34/200] Final Prec@1 99.9740%
06/01 03:32:17 AM | Valid: [34/200] Step 000/078 Loss 0.996 Prec@(1,5) (78.1%, 91.4%)
06/01 03:32:19 AM | Valid: [34/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.9%, 90.2%)
06/01 03:32:19 AM | Valid: [34/200] Final Prec@1 70.8900%
06/01 03:32:20 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:32:20 AM | Train: [35/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:32:31 AM | Train: [35/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:32:41 AM | Train: [35/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:32:52 AM | Train: [35/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:33:01 AM | Train: [35/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:33:01 AM | Train: [35/200] Final Prec@1 99.9700%
06/01 03:33:01 AM | Valid: [35/200] Step 000/078 Loss 1.022 Prec@(1,5) (78.9%, 91.4%)
06/01 03:33:03 AM | Valid: [35/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.8%, 90.1%)
06/01 03:33:03 AM | Valid: [35/200] Final Prec@1 70.7700%
06/01 03:33:04 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:33:04 AM | Train: [36/200] Step 000/390 Loss 0.011 Prec@(1,5) (99.2%, 100.0%)
06/01 03:33:14 AM | Train: [36/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:33:25 AM | Train: [36/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:33:35 AM | Train: [36/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:33:45 AM | Train: [36/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:33:45 AM | Train: [36/200] Final Prec@1 99.9720%
06/01 03:33:46 AM | Valid: [36/200] Step 000/078 Loss 1.023 Prec@(1,5) (77.3%, 90.6%)
06/01 03:33:48 AM | Valid: [36/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.6%, 90.0%)
06/01 03:33:48 AM | Valid: [36/200] Final Prec@1 70.6200%
06/01 03:33:48 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:33:49 AM | Train: [37/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:33:59 AM | Train: [37/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:34:10 AM | Train: [37/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:34:20 AM | Train: [37/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:34:29 AM | Train: [37/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:34:29 AM | Train: [37/200] Final Prec@1 99.9720%
06/01 03:34:30 AM | Valid: [37/200] Step 000/078 Loss 0.994 Prec@(1,5) (78.1%, 92.2%)
06/01 03:34:32 AM | Valid: [37/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.6%, 90.0%)
06/01 03:34:32 AM | Valid: [37/200] Final Prec@1 70.6200%
06/01 03:34:33 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:34:33 AM | Train: [38/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:34:43 AM | Train: [38/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:34:54 AM | Train: [38/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:35:04 AM | Train: [38/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:35:14 AM | Train: [38/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:35:14 AM | Train: [38/200] Final Prec@1 99.9760%
06/01 03:35:14 AM | Valid: [38/200] Step 000/078 Loss 1.001 Prec@(1,5) (78.9%, 91.4%)
06/01 03:35:17 AM | Valid: [38/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.6%, 90.2%)
06/01 03:35:17 AM | Valid: [38/200] Final Prec@1 70.6400%
06/01 03:35:17 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:35:18 AM | Train: [39/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:35:28 AM | Train: [39/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:35:38 AM | Train: [39/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:35:49 AM | Train: [39/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:35:58 AM | Train: [39/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:35:58 AM | Train: [39/200] Final Prec@1 99.9680%
06/01 03:35:58 AM | Valid: [39/200] Step 000/078 Loss 1.003 Prec@(1,5) (76.6%, 92.2%)
06/01 03:36:00 AM | Valid: [39/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.9%, 90.1%)
06/01 03:36:01 AM | Valid: [39/200] Final Prec@1 70.9200%
06/01 03:36:01 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:36:02 AM | Train: [40/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:36:12 AM | Train: [40/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:36:22 AM | Train: [40/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:36:32 AM | Train: [40/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:36:42 AM | Train: [40/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:36:42 AM | Train: [40/200] Final Prec@1 99.9680%
06/01 03:36:42 AM | Valid: [40/200] Step 000/078 Loss 1.017 Prec@(1,5) (78.1%, 90.6%)
06/01 03:36:44 AM | Valid: [40/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.6%, 90.2%)
06/01 03:36:44 AM | Valid: [40/200] Final Prec@1 70.6200%
06/01 03:36:45 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:36:45 AM | Train: [41/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:36:56 AM | Train: [41/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:37:06 AM | Train: [41/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:37:16 AM | Train: [41/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:37:26 AM | Train: [41/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:37:26 AM | Train: [41/200] Final Prec@1 99.9760%
06/01 03:37:26 AM | Valid: [41/200] Step 000/078 Loss 0.992 Prec@(1,5) (78.1%, 91.4%)
06/01 03:37:28 AM | Valid: [41/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.9%, 90.1%)
06/01 03:37:28 AM | Valid: [41/200] Final Prec@1 70.8700%
06/01 03:37:29 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:37:29 AM | Train: [42/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:37:40 AM | Train: [42/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 03:37:51 AM | Train: [42/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:38:01 AM | Train: [42/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:38:11 AM | Train: [42/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:38:11 AM | Train: [42/200] Final Prec@1 99.9640%
06/01 03:38:11 AM | Valid: [42/200] Step 000/078 Loss 1.011 Prec@(1,5) (78.1%, 92.2%)
06/01 03:38:13 AM | Valid: [42/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.7%, 90.1%)
06/01 03:38:13 AM | Valid: [42/200] Final Prec@1 70.7100%
06/01 03:38:14 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:38:14 AM | Train: [43/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:38:24 AM | Train: [43/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:38:35 AM | Train: [43/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:38:45 AM | Train: [43/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:38:54 AM | Train: [43/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:38:54 AM | Train: [43/200] Final Prec@1 99.9600%
06/01 03:38:55 AM | Valid: [43/200] Step 000/078 Loss 0.999 Prec@(1,5) (78.9%, 92.2%)
06/01 03:38:57 AM | Valid: [43/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.8%, 90.0%)
06/01 03:38:57 AM | Valid: [43/200] Final Prec@1 70.7600%
06/01 03:38:58 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:38:58 AM | Train: [44/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:39:08 AM | Train: [44/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 03:39:19 AM | Train: [44/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:39:30 AM | Train: [44/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:39:39 AM | Train: [44/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:39:39 AM | Train: [44/200] Final Prec@1 99.9620%
06/01 03:39:40 AM | Valid: [44/200] Step 000/078 Loss 1.022 Prec@(1,5) (78.9%, 91.4%)
06/01 03:39:42 AM | Valid: [44/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.1%)
06/01 03:39:42 AM | Valid: [44/200] Final Prec@1 70.8800%
06/01 03:39:42 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:39:43 AM | Train: [45/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:39:53 AM | Train: [45/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:40:04 AM | Train: [45/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:40:14 AM | Train: [45/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:40:24 AM | Train: [45/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:40:24 AM | Train: [45/200] Final Prec@1 99.9720%
06/01 03:40:24 AM | Valid: [45/200] Step 000/078 Loss 0.991 Prec@(1,5) (78.1%, 90.6%)
06/01 03:40:26 AM | Valid: [45/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.8%, 90.1%)
06/01 03:40:26 AM | Valid: [45/200] Final Prec@1 70.7600%
06/01 03:40:27 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:40:27 AM | Train: [46/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:40:38 AM | Train: [46/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:40:48 AM | Train: [46/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:40:59 AM | Train: [46/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:41:08 AM | Train: [46/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:41:08 AM | Train: [46/200] Final Prec@1 99.9700%
06/01 03:41:08 AM | Valid: [46/200] Step 000/078 Loss 0.991 Prec@(1,5) (78.9%, 91.4%)
06/01 03:41:11 AM | Valid: [46/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.8%, 90.1%)
06/01 03:41:11 AM | Valid: [46/200] Final Prec@1 70.8200%
06/01 03:41:12 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:41:12 AM | Train: [47/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:41:22 AM | Train: [47/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:41:33 AM | Train: [47/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:41:44 AM | Train: [47/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:41:53 AM | Train: [47/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:41:54 AM | Train: [47/200] Final Prec@1 99.9660%
06/01 03:41:54 AM | Valid: [47/200] Step 000/078 Loss 1.000 Prec@(1,5) (77.3%, 92.2%)
06/01 03:41:56 AM | Valid: [47/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.7%, 90.2%)
06/01 03:41:56 AM | Valid: [47/200] Final Prec@1 70.6600%
06/01 03:41:57 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:41:57 AM | Train: [48/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:42:07 AM | Train: [48/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:42:17 AM | Train: [48/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:42:27 AM | Train: [48/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:42:37 AM | Train: [48/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:42:37 AM | Train: [48/200] Final Prec@1 99.9740%
06/01 03:42:37 AM | Valid: [48/200] Step 000/078 Loss 1.006 Prec@(1,5) (79.7%, 92.2%)
06/01 03:42:39 AM | Valid: [48/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.9%, 90.0%)
06/01 03:42:39 AM | Valid: [48/200] Final Prec@1 70.9100%
06/01 03:42:40 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:42:40 AM | Train: [49/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:42:51 AM | Train: [49/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:43:01 AM | Train: [49/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:43:11 AM | Train: [49/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:43:20 AM | Train: [49/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:43:20 AM | Train: [49/200] Final Prec@1 99.9680%
06/01 03:43:21 AM | Valid: [49/200] Step 000/078 Loss 1.007 Prec@(1,5) (78.9%, 91.4%)
06/01 03:43:23 AM | Valid: [49/200] Step 078/078 Loss 1.285 Prec@(1,5) (70.8%, 90.0%)
06/01 03:43:23 AM | Valid: [49/200] Final Prec@1 70.7700%
06/01 03:43:24 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:43:24 AM | Train: [50/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:43:35 AM | Train: [50/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:43:45 AM | Train: [50/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:43:56 AM | Train: [50/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:44:05 AM | Train: [50/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:44:05 AM | Train: [50/200] Final Prec@1 99.9740%
06/01 03:44:06 AM | Valid: [50/200] Step 000/078 Loss 1.014 Prec@(1,5) (79.7%, 91.4%)
06/01 03:44:08 AM | Valid: [50/200] Step 078/078 Loss 1.280 Prec@(1,5) (71.0%, 90.1%)
06/01 03:44:08 AM | Valid: [50/200] Final Prec@1 70.9500%
06/01 03:44:09 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:44:09 AM | Train: [51/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:44:20 AM | Train: [51/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:44:30 AM | Train: [51/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:44:40 AM | Train: [51/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:44:48 AM | Train: [51/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:44:48 AM | Train: [51/200] Final Prec@1 99.9580%
06/01 03:44:49 AM | Valid: [51/200] Step 000/078 Loss 0.993 Prec@(1,5) (78.9%, 93.0%)
06/01 03:44:51 AM | Valid: [51/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.9%, 90.1%)
06/01 03:44:51 AM | Valid: [51/200] Final Prec@1 70.8800%
06/01 03:44:52 AM | Current best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:44:52 AM | Train: [52/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:45:02 AM | Train: [52/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:45:13 AM | Train: [52/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:45:23 AM | Train: [52/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:45:33 AM | Train: [52/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:45:33 AM | Train: [52/200] Final Prec@1 99.9700%
06/01 03:45:33 AM | Valid: [52/200] Step 000/078 Loss 1.015 Prec@(1,5) (78.9%, 92.2%)
06/01 03:45:35 AM | Valid: [52/200] Step 078/078 Loss 1.281 Prec@(1,5) (71.0%, 90.1%)
06/01 03:45:35 AM | Valid: [52/200] Final Prec@1 71.0100%
06/01 03:45:36 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:45:36 AM | Train: [53/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:45:46 AM | Train: [53/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:45:56 AM | Train: [53/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:46:06 AM | Train: [53/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:46:15 AM | Train: [53/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:46:15 AM | Train: [53/200] Final Prec@1 99.9740%
06/01 03:46:16 AM | Valid: [53/200] Step 000/078 Loss 1.020 Prec@(1,5) (77.3%, 91.4%)
06/01 03:46:18 AM | Valid: [53/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.1%)
06/01 03:46:18 AM | Valid: [53/200] Final Prec@1 70.8300%
06/01 03:46:19 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:46:19 AM | Train: [54/200] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/01 03:46:29 AM | Train: [54/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:46:40 AM | Train: [54/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:46:50 AM | Train: [54/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:46:59 AM | Train: [54/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:47:00 AM | Train: [54/200] Final Prec@1 99.9720%
06/01 03:47:00 AM | Valid: [54/200] Step 000/078 Loss 1.028 Prec@(1,5) (77.3%, 93.0%)
06/01 03:47:02 AM | Valid: [54/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.9%, 90.2%)
06/01 03:47:02 AM | Valid: [54/200] Final Prec@1 70.8600%
06/01 03:47:03 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:47:03 AM | Train: [55/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:47:14 AM | Train: [55/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:47:24 AM | Train: [55/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:47:33 AM | Train: [55/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:47:43 AM | Train: [55/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:47:43 AM | Train: [55/200] Final Prec@1 99.9820%
06/01 03:47:43 AM | Valid: [55/200] Step 000/078 Loss 1.026 Prec@(1,5) (78.9%, 91.4%)
06/01 03:47:45 AM | Valid: [55/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.1%)
06/01 03:47:46 AM | Valid: [55/200] Final Prec@1 70.8700%
06/01 03:47:46 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:47:46 AM | Train: [56/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:47:57 AM | Train: [56/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:48:07 AM | Train: [56/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:48:18 AM | Train: [56/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:48:27 AM | Train: [56/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:48:27 AM | Train: [56/200] Final Prec@1 99.9760%
06/01 03:48:28 AM | Valid: [56/200] Step 000/078 Loss 1.032 Prec@(1,5) (78.1%, 91.4%)
06/01 03:48:30 AM | Valid: [56/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.7%, 90.0%)
06/01 03:48:30 AM | Valid: [56/200] Final Prec@1 70.7200%
06/01 03:48:31 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:48:31 AM | Train: [57/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:48:41 AM | Train: [57/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:48:51 AM | Train: [57/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:49:02 AM | Train: [57/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:49:11 AM | Train: [57/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:49:11 AM | Train: [57/200] Final Prec@1 99.9760%
06/01 03:49:11 AM | Valid: [57/200] Step 000/078 Loss 1.014 Prec@(1,5) (77.3%, 91.4%)
06/01 03:49:13 AM | Valid: [57/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.2%)
06/01 03:49:13 AM | Valid: [57/200] Final Prec@1 70.8100%
06/01 03:49:14 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:49:14 AM | Train: [58/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:49:25 AM | Train: [58/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:49:35 AM | Train: [58/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:49:46 AM | Train: [58/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:49:55 AM | Train: [58/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:49:55 AM | Train: [58/200] Final Prec@1 99.9760%
06/01 03:49:55 AM | Valid: [58/200] Step 000/078 Loss 1.028 Prec@(1,5) (78.9%, 91.4%)
06/01 03:49:57 AM | Valid: [58/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.9%, 90.2%)
06/01 03:49:57 AM | Valid: [58/200] Final Prec@1 70.8900%
06/01 03:49:58 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:49:58 AM | Train: [59/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:50:09 AM | Train: [59/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:50:20 AM | Train: [59/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:50:30 AM | Train: [59/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:50:40 AM | Train: [59/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:50:40 AM | Train: [59/200] Final Prec@1 99.9700%
06/01 03:50:40 AM | Valid: [59/200] Step 000/078 Loss 1.017 Prec@(1,5) (78.9%, 90.6%)
06/01 03:50:42 AM | Valid: [59/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.7%, 90.1%)
06/01 03:50:43 AM | Valid: [59/200] Final Prec@1 70.7100%
06/01 03:50:43 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:50:44 AM | Train: [60/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 03:50:54 AM | Train: [60/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:51:04 AM | Train: [60/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:51:15 AM | Train: [60/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:51:24 AM | Train: [60/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:51:24 AM | Train: [60/200] Final Prec@1 99.9720%
06/01 03:51:25 AM | Valid: [60/200] Step 000/078 Loss 1.024 Prec@(1,5) (78.1%, 91.4%)
06/01 03:51:27 AM | Valid: [60/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.3%)
06/01 03:51:27 AM | Valid: [60/200] Final Prec@1 70.8000%
06/01 03:51:28 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:51:28 AM | Train: [61/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:51:38 AM | Train: [61/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:51:49 AM | Train: [61/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:51:59 AM | Train: [61/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:52:08 AM | Train: [61/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:52:08 AM | Train: [61/200] Final Prec@1 99.9680%
06/01 03:52:09 AM | Valid: [61/200] Step 000/078 Loss 1.010 Prec@(1,5) (78.9%, 92.2%)
06/01 03:52:11 AM | Valid: [61/200] Step 078/078 Loss 1.278 Prec@(1,5) (71.0%, 90.1%)
06/01 03:52:11 AM | Valid: [61/200] Final Prec@1 70.9700%
06/01 03:52:12 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:52:12 AM | Train: [62/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:52:23 AM | Train: [62/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:52:33 AM | Train: [62/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:52:43 AM | Train: [62/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:52:53 AM | Train: [62/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:52:53 AM | Train: [62/200] Final Prec@1 99.9640%
06/01 03:52:53 AM | Valid: [62/200] Step 000/078 Loss 1.016 Prec@(1,5) (78.1%, 92.2%)
06/01 03:52:56 AM | Valid: [62/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.9%, 90.2%)
06/01 03:52:56 AM | Valid: [62/200] Final Prec@1 70.9100%
06/01 03:52:56 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:52:57 AM | Train: [63/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:53:07 AM | Train: [63/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:53:18 AM | Train: [63/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:53:28 AM | Train: [63/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:53:37 AM | Train: [63/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:53:37 AM | Train: [63/200] Final Prec@1 99.9700%
06/01 03:53:37 AM | Valid: [63/200] Step 000/078 Loss 1.005 Prec@(1,5) (78.9%, 92.2%)
06/01 03:53:40 AM | Valid: [63/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.7%, 90.2%)
06/01 03:53:40 AM | Valid: [63/200] Final Prec@1 70.7200%
06/01 03:53:40 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:53:41 AM | Train: [64/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:53:51 AM | Train: [64/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:54:01 AM | Train: [64/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:54:11 AM | Train: [64/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:54:21 AM | Train: [64/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:54:21 AM | Train: [64/200] Final Prec@1 99.9720%
06/01 03:54:21 AM | Valid: [64/200] Step 000/078 Loss 1.015 Prec@(1,5) (78.9%, 91.4%)
06/01 03:54:24 AM | Valid: [64/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.1%)
06/01 03:54:24 AM | Valid: [64/200] Final Prec@1 70.8400%
06/01 03:54:24 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:54:25 AM | Train: [65/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 03:54:35 AM | Train: [65/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:54:45 AM | Train: [65/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:54:55 AM | Train: [65/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:55:05 AM | Train: [65/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:55:05 AM | Train: [65/200] Final Prec@1 99.9660%
06/01 03:55:05 AM | Valid: [65/200] Step 000/078 Loss 1.022 Prec@(1,5) (78.9%, 90.6%)
06/01 03:55:07 AM | Valid: [65/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.2%)
06/01 03:55:07 AM | Valid: [65/200] Final Prec@1 70.8200%
06/01 03:55:08 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:55:08 AM | Train: [66/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:55:19 AM | Train: [66/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:55:29 AM | Train: [66/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:55:40 AM | Train: [66/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:55:49 AM | Train: [66/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:55:50 AM | Train: [66/200] Final Prec@1 99.9620%
06/01 03:55:50 AM | Valid: [66/200] Step 000/078 Loss 1.004 Prec@(1,5) (78.1%, 93.0%)
06/01 03:55:52 AM | Valid: [66/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.7%, 90.2%)
06/01 03:55:52 AM | Valid: [66/200] Final Prec@1 70.6900%
06/01 03:55:53 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:55:53 AM | Train: [67/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:56:03 AM | Train: [67/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:56:14 AM | Train: [67/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:56:24 AM | Train: [67/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:56:34 AM | Train: [67/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:56:34 AM | Train: [67/200] Final Prec@1 99.9780%
06/01 03:56:34 AM | Valid: [67/200] Step 000/078 Loss 1.004 Prec@(1,5) (78.9%, 90.6%)
06/01 03:56:36 AM | Valid: [67/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.8%, 90.1%)
06/01 03:56:36 AM | Valid: [67/200] Final Prec@1 70.7600%
06/01 03:56:37 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:56:37 AM | Train: [68/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:56:48 AM | Train: [68/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:56:58 AM | Train: [68/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:57:09 AM | Train: [68/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:57:18 AM | Train: [68/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:57:18 AM | Train: [68/200] Final Prec@1 99.9700%
06/01 03:57:18 AM | Valid: [68/200] Step 000/078 Loss 1.012 Prec@(1,5) (78.9%, 92.2%)
06/01 03:57:20 AM | Valid: [68/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.7%, 90.2%)
06/01 03:57:21 AM | Valid: [68/200] Final Prec@1 70.7200%
06/01 03:57:21 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:57:22 AM | Train: [69/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 03:57:32 AM | Train: [69/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:57:42 AM | Train: [69/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:57:53 AM | Train: [69/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:58:01 AM | Train: [69/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:58:02 AM | Train: [69/200] Final Prec@1 99.9660%
06/01 03:58:02 AM | Valid: [69/200] Step 000/078 Loss 0.995 Prec@(1,5) (78.9%, 92.2%)
06/01 03:58:04 AM | Valid: [69/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.6%, 90.2%)
06/01 03:58:04 AM | Valid: [69/200] Final Prec@1 70.6400%
06/01 03:58:05 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:58:05 AM | Train: [70/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:58:16 AM | Train: [70/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:58:27 AM | Train: [70/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:58:37 AM | Train: [70/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:58:47 AM | Train: [70/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:58:47 AM | Train: [70/200] Final Prec@1 99.9740%
06/01 03:58:47 AM | Valid: [70/200] Step 000/078 Loss 1.025 Prec@(1,5) (78.1%, 91.4%)
06/01 03:58:50 AM | Valid: [70/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.7%, 90.1%)
06/01 03:58:50 AM | Valid: [70/200] Final Prec@1 70.7100%
06/01 03:58:50 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:58:51 AM | Train: [71/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 03:59:01 AM | Train: [71/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 03:59:11 AM | Train: [71/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:59:22 AM | Train: [71/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:59:32 AM | Train: [71/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:59:32 AM | Train: [71/200] Final Prec@1 99.9660%
06/01 03:59:32 AM | Valid: [71/200] Step 000/078 Loss 1.011 Prec@(1,5) (78.9%, 89.8%)
06/01 03:59:34 AM | Valid: [71/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.6%, 90.2%)
06/01 03:59:34 AM | Valid: [71/200] Final Prec@1 70.6200%
06/01 03:59:35 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 03:59:35 AM | Train: [72/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:59:46 AM | Train: [72/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 03:59:56 AM | Train: [72/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:00:07 AM | Train: [72/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:00:16 AM | Train: [72/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:00:16 AM | Train: [72/200] Final Prec@1 99.9680%
06/01 04:00:16 AM | Valid: [72/200] Step 000/078 Loss 0.993 Prec@(1,5) (78.1%, 91.4%)
06/01 04:00:19 AM | Valid: [72/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.8%, 90.2%)
06/01 04:00:19 AM | Valid: [72/200] Final Prec@1 70.8300%
06/01 04:00:20 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:00:20 AM | Train: [73/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 04:00:31 AM | Train: [73/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:00:41 AM | Train: [73/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:00:52 AM | Train: [73/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:01:01 AM | Train: [73/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:01:01 AM | Train: [73/200] Final Prec@1 99.9780%
06/01 04:01:02 AM | Valid: [73/200] Step 000/078 Loss 0.999 Prec@(1,5) (78.9%, 92.2%)
06/01 04:01:04 AM | Valid: [73/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.6%, 89.9%)
06/01 04:01:04 AM | Valid: [73/200] Final Prec@1 70.6400%
06/01 04:01:05 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:01:05 AM | Train: [74/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:01:15 AM | Train: [74/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:01:26 AM | Train: [74/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:01:36 AM | Train: [74/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:01:46 AM | Train: [74/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:01:46 AM | Train: [74/200] Final Prec@1 99.9700%
06/01 04:01:46 AM | Valid: [74/200] Step 000/078 Loss 1.010 Prec@(1,5) (79.7%, 89.8%)
06/01 04:01:48 AM | Valid: [74/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.9%, 90.0%)
06/01 04:01:48 AM | Valid: [74/200] Final Prec@1 70.8600%
06/01 04:01:49 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:01:49 AM | Train: [75/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:02:00 AM | Train: [75/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:02:10 AM | Train: [75/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:02:21 AM | Train: [75/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:02:30 AM | Train: [75/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:02:30 AM | Train: [75/200] Final Prec@1 99.9620%
06/01 04:02:30 AM | Valid: [75/200] Step 000/078 Loss 1.021 Prec@(1,5) (78.9%, 91.4%)
06/01 04:02:32 AM | Valid: [75/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.8%, 90.1%)
06/01 04:02:32 AM | Valid: [75/200] Final Prec@1 70.8400%
06/01 04:02:33 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:02:34 AM | Train: [76/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:02:44 AM | Train: [76/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:02:54 AM | Train: [76/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:03:04 AM | Train: [76/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:03:13 AM | Train: [76/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:03:13 AM | Train: [76/200] Final Prec@1 99.9700%
06/01 04:03:13 AM | Valid: [76/200] Step 000/078 Loss 1.019 Prec@(1,5) (78.1%, 90.6%)
06/01 04:03:16 AM | Valid: [76/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.8%, 90.1%)
06/01 04:03:16 AM | Valid: [76/200] Final Prec@1 70.7700%
06/01 04:03:17 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:03:17 AM | Train: [77/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:03:27 AM | Train: [77/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:03:37 AM | Train: [77/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:03:48 AM | Train: [77/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:03:57 AM | Train: [77/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:03:57 AM | Train: [77/200] Final Prec@1 99.9780%
06/01 04:03:58 AM | Valid: [77/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.1%, 90.6%)
06/01 04:04:00 AM | Valid: [77/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.8%, 90.1%)
06/01 04:04:00 AM | Valid: [77/200] Final Prec@1 70.7600%
06/01 04:04:01 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:04:01 AM | Train: [78/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:04:11 AM | Train: [78/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:04:21 AM | Train: [78/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:04:31 AM | Train: [78/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:04:41 AM | Train: [78/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:04:41 AM | Train: [78/200] Final Prec@1 99.9760%
06/01 04:04:41 AM | Valid: [78/200] Step 000/078 Loss 1.007 Prec@(1,5) (78.9%, 92.2%)
06/01 04:04:43 AM | Valid: [78/200] Step 078/078 Loss 1.279 Prec@(1,5) (71.0%, 90.1%)
06/01 04:04:43 AM | Valid: [78/200] Final Prec@1 71.0100%
06/01 04:04:44 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:04:44 AM | Train: [79/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 04:04:55 AM | Train: [79/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:05:05 AM | Train: [79/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:05:15 AM | Train: [79/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:05:25 AM | Train: [79/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:05:25 AM | Train: [79/200] Final Prec@1 99.9660%
06/01 04:05:25 AM | Valid: [79/200] Step 000/078 Loss 0.997 Prec@(1,5) (78.9%, 92.2%)
06/01 04:05:28 AM | Valid: [79/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.1%)
06/01 04:05:28 AM | Valid: [79/200] Final Prec@1 70.7900%
06/01 04:05:28 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:05:29 AM | Train: [80/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:05:39 AM | Train: [80/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:05:50 AM | Train: [80/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:06:01 AM | Train: [80/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:06:10 AM | Train: [80/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:06:10 AM | Train: [80/200] Final Prec@1 99.9660%
06/01 04:06:10 AM | Valid: [80/200] Step 000/078 Loss 1.018 Prec@(1,5) (78.9%, 91.4%)
06/01 04:06:12 AM | Valid: [80/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.1%)
06/01 04:06:13 AM | Valid: [80/200] Final Prec@1 70.7700%
06/01 04:06:13 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:06:13 AM | Train: [81/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:06:24 AM | Train: [81/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:06:34 AM | Train: [81/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:06:45 AM | Train: [81/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:06:54 AM | Train: [81/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:06:54 AM | Train: [81/200] Final Prec@1 99.9600%
06/01 04:06:55 AM | Valid: [81/200] Step 000/078 Loss 1.023 Prec@(1,5) (78.9%, 89.8%)
06/01 04:06:57 AM | Valid: [81/200] Step 078/078 Loss 1.282 Prec@(1,5) (71.0%, 89.9%)
06/01 04:06:57 AM | Valid: [81/200] Final Prec@1 70.9500%
06/01 04:06:58 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:06:58 AM | Train: [82/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:07:08 AM | Train: [82/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:07:19 AM | Train: [82/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:07:29 AM | Train: [82/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:07:39 AM | Train: [82/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:07:39 AM | Train: [82/200] Final Prec@1 99.9720%
06/01 04:07:39 AM | Valid: [82/200] Step 000/078 Loss 1.008 Prec@(1,5) (78.9%, 90.6%)
06/01 04:07:41 AM | Valid: [82/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.7%, 90.2%)
06/01 04:07:41 AM | Valid: [82/200] Final Prec@1 70.7000%
06/01 04:07:42 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:07:42 AM | Train: [83/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:07:52 AM | Train: [83/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:08:03 AM | Train: [83/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:08:13 AM | Train: [83/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:08:23 AM | Train: [83/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:08:23 AM | Train: [83/200] Final Prec@1 99.9700%
06/01 04:08:23 AM | Valid: [83/200] Step 000/078 Loss 1.008 Prec@(1,5) (78.9%, 93.0%)
06/01 04:08:26 AM | Valid: [83/200] Step 078/078 Loss 1.278 Prec@(1,5) (71.0%, 90.2%)
06/01 04:08:26 AM | Valid: [83/200] Final Prec@1 70.9900%
06/01 04:08:26 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:08:27 AM | Train: [84/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:08:37 AM | Train: [84/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:08:47 AM | Train: [84/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:08:58 AM | Train: [84/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:09:07 AM | Train: [84/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:09:07 AM | Train: [84/200] Final Prec@1 99.9760%
06/01 04:09:08 AM | Valid: [84/200] Step 000/078 Loss 1.013 Prec@(1,5) (78.9%, 89.8%)
06/01 04:09:10 AM | Valid: [84/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.9%, 90.0%)
06/01 04:09:10 AM | Valid: [84/200] Final Prec@1 70.9000%
06/01 04:09:11 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:09:11 AM | Train: [85/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:09:21 AM | Train: [85/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:09:32 AM | Train: [85/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:09:43 AM | Train: [85/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:09:52 AM | Train: [85/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:09:52 AM | Train: [85/200] Final Prec@1 99.9760%
06/01 04:09:52 AM | Valid: [85/200] Step 000/078 Loss 1.011 Prec@(1,5) (78.1%, 92.2%)
06/01 04:09:55 AM | Valid: [85/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.7%, 90.1%)
06/01 04:09:55 AM | Valid: [85/200] Final Prec@1 70.7200%
06/01 04:09:55 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:09:56 AM | Train: [86/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:10:05 AM | Train: [86/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:10:14 AM | Train: [86/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:10:24 AM | Train: [86/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:10:32 AM | Train: [86/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:10:32 AM | Train: [86/200] Final Prec@1 99.9700%
06/01 04:10:32 AM | Valid: [86/200] Step 000/078 Loss 0.999 Prec@(1,5) (78.9%, 93.0%)
06/01 04:10:35 AM | Valid: [86/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.7%, 90.1%)
06/01 04:10:35 AM | Valid: [86/200] Final Prec@1 70.7400%
06/01 04:10:35 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:10:36 AM | Train: [87/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:10:46 AM | Train: [87/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:10:56 AM | Train: [87/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:11:05 AM | Train: [87/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:11:15 AM | Train: [87/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:11:15 AM | Train: [87/200] Final Prec@1 99.9660%
06/01 04:11:15 AM | Valid: [87/200] Step 000/078 Loss 1.011 Prec@(1,5) (78.1%, 91.4%)
06/01 04:11:17 AM | Valid: [87/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.9%, 90.2%)
06/01 04:11:17 AM | Valid: [87/200] Final Prec@1 70.9400%
06/01 04:11:18 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:11:18 AM | Train: [88/200] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/01 04:11:27 AM | Train: [88/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:11:37 AM | Train: [88/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:11:47 AM | Train: [88/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:11:56 AM | Train: [88/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:11:56 AM | Train: [88/200] Final Prec@1 99.9680%
06/01 04:11:57 AM | Valid: [88/200] Step 000/078 Loss 1.007 Prec@(1,5) (78.9%, 92.2%)
06/01 04:11:59 AM | Valid: [88/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.9%, 90.1%)
06/01 04:11:59 AM | Valid: [88/200] Final Prec@1 70.9000%
06/01 04:12:00 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:12:00 AM | Train: [89/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:12:10 AM | Train: [89/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:12:20 AM | Train: [89/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:12:31 AM | Train: [89/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:12:41 AM | Train: [89/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:12:41 AM | Train: [89/200] Final Prec@1 99.9680%
06/01 04:12:41 AM | Valid: [89/200] Step 000/078 Loss 0.999 Prec@(1,5) (79.7%, 92.2%)
06/01 04:12:44 AM | Valid: [89/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.8%, 90.2%)
06/01 04:12:44 AM | Valid: [89/200] Final Prec@1 70.8100%
06/01 04:12:44 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:12:45 AM | Train: [90/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:12:55 AM | Train: [90/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:13:06 AM | Train: [90/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:13:16 AM | Train: [90/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:13:26 AM | Train: [90/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:13:26 AM | Train: [90/200] Final Prec@1 99.9820%
06/01 04:13:26 AM | Valid: [90/200] Step 000/078 Loss 1.015 Prec@(1,5) (78.9%, 91.4%)
06/01 04:13:28 AM | Valid: [90/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.6%, 90.2%)
06/01 04:13:28 AM | Valid: [90/200] Final Prec@1 70.6400%
06/01 04:13:29 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:13:29 AM | Train: [91/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:13:40 AM | Train: [91/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:13:50 AM | Train: [91/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:14:00 AM | Train: [91/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:14:10 AM | Train: [91/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:14:10 AM | Train: [91/200] Final Prec@1 99.9760%
06/01 04:14:10 AM | Valid: [91/200] Step 000/078 Loss 1.008 Prec@(1,5) (78.1%, 92.2%)
06/01 04:14:12 AM | Valid: [91/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.9%, 90.2%)
06/01 04:14:12 AM | Valid: [91/200] Final Prec@1 70.9100%
06/01 04:14:13 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:14:13 AM | Train: [92/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:14:23 AM | Train: [92/200] Step 100/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/01 04:14:34 AM | Train: [92/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:14:44 AM | Train: [92/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:14:53 AM | Train: [92/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:14:54 AM | Train: [92/200] Final Prec@1 99.9680%
06/01 04:14:54 AM | Valid: [92/200] Step 000/078 Loss 1.000 Prec@(1,5) (78.9%, 90.6%)
06/01 04:14:56 AM | Valid: [92/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.7%, 90.2%)
06/01 04:14:56 AM | Valid: [92/200] Final Prec@1 70.7400%
06/01 04:14:57 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:14:57 AM | Train: [93/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:15:07 AM | Train: [93/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:15:18 AM | Train: [93/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:15:28 AM | Train: [93/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:15:37 AM | Train: [93/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:15:38 AM | Train: [93/200] Final Prec@1 99.9740%
06/01 04:15:38 AM | Valid: [93/200] Step 000/078 Loss 1.017 Prec@(1,5) (78.1%, 90.6%)
06/01 04:15:40 AM | Valid: [93/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.1%)
06/01 04:15:40 AM | Valid: [93/200] Final Prec@1 70.7800%
06/01 04:15:41 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:15:41 AM | Train: [94/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:15:51 AM | Train: [94/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:16:02 AM | Train: [94/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:16:12 AM | Train: [94/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:16:21 AM | Train: [94/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:16:21 AM | Train: [94/200] Final Prec@1 99.9680%
06/01 04:16:22 AM | Valid: [94/200] Step 000/078 Loss 1.012 Prec@(1,5) (78.1%, 91.4%)
06/01 04:16:24 AM | Valid: [94/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.9%, 90.2%)
06/01 04:16:24 AM | Valid: [94/200] Final Prec@1 70.8900%
06/01 04:16:25 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:16:25 AM | Train: [95/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:16:35 AM | Train: [95/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:16:46 AM | Train: [95/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:16:56 AM | Train: [95/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:17:06 AM | Train: [95/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:17:06 AM | Train: [95/200] Final Prec@1 99.9680%
06/01 04:17:06 AM | Valid: [95/200] Step 000/078 Loss 0.991 Prec@(1,5) (78.9%, 92.2%)
06/01 04:17:08 AM | Valid: [95/200] Step 078/078 Loss 1.280 Prec@(1,5) (71.0%, 90.1%)
06/01 04:17:08 AM | Valid: [95/200] Final Prec@1 70.9600%
06/01 04:17:09 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:17:09 AM | Train: [96/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:17:20 AM | Train: [96/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:17:30 AM | Train: [96/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:17:41 AM | Train: [96/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:17:51 AM | Train: [96/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:17:51 AM | Train: [96/200] Final Prec@1 99.9740%
06/01 04:17:51 AM | Valid: [96/200] Step 000/078 Loss 0.996 Prec@(1,5) (78.9%, 92.2%)
06/01 04:17:53 AM | Valid: [96/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.9%, 90.0%)
06/01 04:17:53 AM | Valid: [96/200] Final Prec@1 70.8700%
06/01 04:17:54 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:17:54 AM | Train: [97/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:18:05 AM | Train: [97/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:18:14 AM | Train: [97/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:18:25 AM | Train: [97/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:18:33 AM | Train: [97/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:18:33 AM | Train: [97/200] Final Prec@1 99.9800%
06/01 04:18:34 AM | Valid: [97/200] Step 000/078 Loss 1.001 Prec@(1,5) (79.7%, 93.0%)
06/01 04:18:36 AM | Valid: [97/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.0%)
06/01 04:18:36 AM | Valid: [97/200] Final Prec@1 70.9400%
06/01 04:18:37 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:18:37 AM | Train: [98/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:18:47 AM | Train: [98/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:18:57 AM | Train: [98/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:19:07 AM | Train: [98/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:19:17 AM | Train: [98/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:19:17 AM | Train: [98/200] Final Prec@1 99.9680%
06/01 04:19:17 AM | Valid: [98/200] Step 000/078 Loss 0.997 Prec@(1,5) (79.7%, 91.4%)
06/01 04:19:19 AM | Valid: [98/200] Step 078/078 Loss 1.279 Prec@(1,5) (71.0%, 90.1%)
06/01 04:19:19 AM | Valid: [98/200] Final Prec@1 70.9700%
06/01 04:19:20 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:19:20 AM | Train: [99/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:19:31 AM | Train: [99/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:19:41 AM | Train: [99/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:19:52 AM | Train: [99/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:20:01 AM | Train: [99/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:20:01 AM | Train: [99/200] Final Prec@1 99.9780%
06/01 04:20:02 AM | Valid: [99/200] Step 000/078 Loss 1.006 Prec@(1,5) (78.9%, 92.2%)
06/01 04:20:04 AM | Valid: [99/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.1%)
06/01 04:20:04 AM | Valid: [99/200] Final Prec@1 70.8400%
06/01 04:20:05 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:20:05 AM | Train: [100/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:20:16 AM | Train: [100/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:20:26 AM | Train: [100/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:20:37 AM | Train: [100/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:20:47 AM | Train: [100/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:20:47 AM | Train: [100/200] Final Prec@1 99.9720%
06/01 04:20:47 AM | Valid: [100/200] Step 000/078 Loss 1.020 Prec@(1,5) (78.9%, 91.4%)
06/01 04:20:49 AM | Valid: [100/200] Step 078/078 Loss 1.279 Prec@(1,5) (71.0%, 90.2%)
06/01 04:20:49 AM | Valid: [100/200] Final Prec@1 71.0000%
06/01 04:20:50 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:20:50 AM | Train: [101/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:21:00 AM | Train: [101/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:21:11 AM | Train: [101/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:21:22 AM | Train: [101/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:21:31 AM | Train: [101/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:21:31 AM | Train: [101/200] Final Prec@1 99.9760%
06/01 04:21:31 AM | Valid: [101/200] Step 000/078 Loss 0.998 Prec@(1,5) (78.9%, 91.4%)
06/01 04:21:34 AM | Valid: [101/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.2%)
06/01 04:21:34 AM | Valid: [101/200] Final Prec@1 70.8200%
06/01 04:21:34 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:21:35 AM | Train: [102/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:21:45 AM | Train: [102/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:21:55 AM | Train: [102/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:22:06 AM | Train: [102/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:22:15 AM | Train: [102/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:22:15 AM | Train: [102/200] Final Prec@1 99.9700%
06/01 04:22:15 AM | Valid: [102/200] Step 000/078 Loss 1.017 Prec@(1,5) (78.1%, 93.0%)
06/01 04:22:17 AM | Valid: [102/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.9%, 90.2%)
06/01 04:22:17 AM | Valid: [102/200] Final Prec@1 70.9200%
06/01 04:22:18 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:22:18 AM | Train: [103/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:22:29 AM | Train: [103/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:22:39 AM | Train: [103/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:22:50 AM | Train: [103/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:23:00 AM | Train: [103/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:23:00 AM | Train: [103/200] Final Prec@1 99.9720%
06/01 04:23:00 AM | Valid: [103/200] Step 000/078 Loss 1.011 Prec@(1,5) (78.9%, 92.2%)
06/01 04:23:03 AM | Valid: [103/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.9%, 90.0%)
06/01 04:23:03 AM | Valid: [103/200] Final Prec@1 70.9100%
06/01 04:23:04 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:23:04 AM | Train: [104/200] Step 000/390 Loss 0.009 Prec@(1,5) (99.2%, 100.0%)
06/01 04:23:14 AM | Train: [104/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:23:25 AM | Train: [104/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:23:35 AM | Train: [104/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:23:45 AM | Train: [104/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:23:45 AM | Train: [104/200] Final Prec@1 99.9600%
06/01 04:23:45 AM | Valid: [104/200] Step 000/078 Loss 0.995 Prec@(1,5) (78.9%, 92.2%)
06/01 04:23:47 AM | Valid: [104/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.0%)
06/01 04:23:47 AM | Valid: [104/200] Final Prec@1 70.9000%
06/01 04:23:48 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:23:48 AM | Train: [105/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:23:59 AM | Train: [105/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:24:09 AM | Train: [105/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:24:20 AM | Train: [105/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:24:29 AM | Train: [105/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:24:29 AM | Train: [105/200] Final Prec@1 99.9740%
06/01 04:24:30 AM | Valid: [105/200] Step 000/078 Loss 1.015 Prec@(1,5) (77.3%, 92.2%)
06/01 04:24:32 AM | Valid: [105/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.8%, 90.1%)
06/01 04:24:32 AM | Valid: [105/200] Final Prec@1 70.7700%
06/01 04:24:33 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:24:33 AM | Train: [106/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:24:43 AM | Train: [106/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:24:54 AM | Train: [106/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:25:04 AM | Train: [106/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:25:13 AM | Train: [106/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:25:13 AM | Train: [106/200] Final Prec@1 99.9780%
06/01 04:25:13 AM | Valid: [106/200] Step 000/078 Loss 1.000 Prec@(1,5) (78.9%, 93.0%)
06/01 04:25:16 AM | Valid: [106/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.1%)
06/01 04:25:16 AM | Valid: [106/200] Final Prec@1 70.7800%
06/01 04:25:17 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:25:17 AM | Train: [107/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:25:27 AM | Train: [107/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:25:37 AM | Train: [107/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:25:48 AM | Train: [107/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:25:57 AM | Train: [107/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:25:57 AM | Train: [107/200] Final Prec@1 99.9600%
06/01 04:25:58 AM | Valid: [107/200] Step 000/078 Loss 1.018 Prec@(1,5) (78.9%, 92.2%)
06/01 04:26:00 AM | Valid: [107/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.9%, 90.1%)
06/01 04:26:00 AM | Valid: [107/200] Final Prec@1 70.9100%
06/01 04:26:01 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:26:01 AM | Train: [108/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:26:11 AM | Train: [108/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:26:22 AM | Train: [108/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:26:33 AM | Train: [108/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:26:42 AM | Train: [108/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:26:42 AM | Train: [108/200] Final Prec@1 99.9700%
06/01 04:26:42 AM | Valid: [108/200] Step 000/078 Loss 1.022 Prec@(1,5) (78.1%, 91.4%)
06/01 04:26:45 AM | Valid: [108/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.0%)
06/01 04:26:45 AM | Valid: [108/200] Final Prec@1 70.9000%
06/01 04:26:45 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:26:46 AM | Train: [109/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:26:56 AM | Train: [109/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:27:06 AM | Train: [109/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:27:17 AM | Train: [109/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:27:27 AM | Train: [109/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:27:27 AM | Train: [109/200] Final Prec@1 99.9640%
06/01 04:27:27 AM | Valid: [109/200] Step 000/078 Loss 0.994 Prec@(1,5) (78.9%, 91.4%)
06/01 04:27:29 AM | Valid: [109/200] Step 078/078 Loss 1.278 Prec@(1,5) (71.0%, 90.1%)
06/01 04:27:29 AM | Valid: [109/200] Final Prec@1 70.9800%
06/01 04:27:30 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:27:30 AM | Train: [110/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:27:41 AM | Train: [110/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:27:51 AM | Train: [110/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:28:02 AM | Train: [110/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:28:11 AM | Train: [110/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:28:11 AM | Train: [110/200] Final Prec@1 99.9720%
06/01 04:28:12 AM | Valid: [110/200] Step 000/078 Loss 1.013 Prec@(1,5) (78.9%, 93.0%)
06/01 04:28:14 AM | Valid: [110/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.7%, 90.1%)
06/01 04:28:14 AM | Valid: [110/200] Final Prec@1 70.7100%
06/01 04:28:15 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:28:15 AM | Train: [111/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:28:26 AM | Train: [111/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:28:36 AM | Train: [111/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:28:47 AM | Train: [111/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:28:57 AM | Train: [111/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:28:57 AM | Train: [111/200] Final Prec@1 99.9640%
06/01 04:28:57 AM | Valid: [111/200] Step 000/078 Loss 1.033 Prec@(1,5) (78.1%, 91.4%)
06/01 04:28:59 AM | Valid: [111/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.9%, 90.0%)
06/01 04:28:59 AM | Valid: [111/200] Final Prec@1 70.8700%
06/01 04:29:00 AM | Current best Prec@1 = 71.0100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:29:00 AM | Train: [112/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:29:11 AM | Train: [112/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:29:21 AM | Train: [112/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:29:32 AM | Train: [112/200] Step 300/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:29:42 AM | Train: [112/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:29:42 AM | Train: [112/200] Final Prec@1 99.9580%
06/01 04:29:42 AM | Valid: [112/200] Step 000/078 Loss 1.010 Prec@(1,5) (79.7%, 90.6%)
06/01 04:29:44 AM | Valid: [112/200] Step 078/078 Loss 1.278 Prec@(1,5) (71.0%, 90.1%)
06/01 04:29:45 AM | Valid: [112/200] Final Prec@1 71.0200%
06/01 04:29:45 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:29:46 AM | Train: [113/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:29:56 AM | Train: [113/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:30:06 AM | Train: [113/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:30:17 AM | Train: [113/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:30:26 AM | Train: [113/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:30:26 AM | Train: [113/200] Final Prec@1 99.9580%
06/01 04:30:27 AM | Valid: [113/200] Step 000/078 Loss 1.016 Prec@(1,5) (79.7%, 91.4%)
06/01 04:30:29 AM | Valid: [113/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.8%, 90.1%)
06/01 04:30:29 AM | Valid: [113/200] Final Prec@1 70.8300%
06/01 04:30:30 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:30:30 AM | Train: [114/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:30:41 AM | Train: [114/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:30:51 AM | Train: [114/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:31:02 AM | Train: [114/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:31:11 AM | Train: [114/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:31:11 AM | Train: [114/200] Final Prec@1 99.9660%
06/01 04:31:12 AM | Valid: [114/200] Step 000/078 Loss 1.019 Prec@(1,5) (78.1%, 91.4%)
06/01 04:31:14 AM | Valid: [114/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.7%, 90.1%)
06/01 04:31:14 AM | Valid: [114/200] Final Prec@1 70.6900%
06/01 04:31:15 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:31:15 AM | Train: [115/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:31:26 AM | Train: [115/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:31:36 AM | Train: [115/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:31:46 AM | Train: [115/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:31:56 AM | Train: [115/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:31:56 AM | Train: [115/200] Final Prec@1 99.9720%
06/01 04:31:57 AM | Valid: [115/200] Step 000/078 Loss 1.015 Prec@(1,5) (78.9%, 92.2%)
06/01 04:31:59 AM | Valid: [115/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.8%, 90.0%)
06/01 04:31:59 AM | Valid: [115/200] Final Prec@1 70.8300%
06/01 04:32:00 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:32:00 AM | Train: [116/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:32:10 AM | Train: [116/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:32:20 AM | Train: [116/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:32:31 AM | Train: [116/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:32:40 AM | Train: [116/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:32:40 AM | Train: [116/200] Final Prec@1 99.9660%
06/01 04:32:41 AM | Valid: [116/200] Step 000/078 Loss 1.008 Prec@(1,5) (78.1%, 91.4%)
06/01 04:32:43 AM | Valid: [116/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.0%)
06/01 04:32:43 AM | Valid: [116/200] Final Prec@1 70.9400%
06/01 04:32:44 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:32:44 AM | Train: [117/200] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 04:32:55 AM | Train: [117/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:33:05 AM | Train: [117/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:33:16 AM | Train: [117/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:33:25 AM | Train: [117/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:33:25 AM | Train: [117/200] Final Prec@1 99.9720%
06/01 04:33:25 AM | Valid: [117/200] Step 000/078 Loss 0.999 Prec@(1,5) (78.9%, 92.2%)
06/01 04:33:27 AM | Valid: [117/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.9%, 90.2%)
06/01 04:33:27 AM | Valid: [117/200] Final Prec@1 70.8900%
06/01 04:33:28 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:33:28 AM | Train: [118/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:33:39 AM | Train: [118/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:33:49 AM | Train: [118/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:33:59 AM | Train: [118/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:34:09 AM | Train: [118/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:34:09 AM | Train: [118/200] Final Prec@1 99.9540%
06/01 04:34:09 AM | Valid: [118/200] Step 000/078 Loss 1.010 Prec@(1,5) (78.9%, 91.4%)
06/01 04:34:11 AM | Valid: [118/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.2%)
06/01 04:34:11 AM | Valid: [118/200] Final Prec@1 70.8300%
06/01 04:34:12 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:34:12 AM | Train: [119/200] Step 000/390 Loss 0.009 Prec@(1,5) (99.2%, 100.0%)
06/01 04:34:23 AM | Train: [119/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:34:33 AM | Train: [119/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:34:44 AM | Train: [119/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:34:53 AM | Train: [119/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:34:53 AM | Train: [119/200] Final Prec@1 99.9800%
06/01 04:34:54 AM | Valid: [119/200] Step 000/078 Loss 1.018 Prec@(1,5) (78.9%, 91.4%)
06/01 04:34:56 AM | Valid: [119/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.9%, 90.2%)
06/01 04:34:56 AM | Valid: [119/200] Final Prec@1 70.8800%
06/01 04:34:56 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:34:57 AM | Train: [120/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:35:07 AM | Train: [120/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:35:18 AM | Train: [120/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:35:29 AM | Train: [120/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:35:38 AM | Train: [120/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:35:38 AM | Train: [120/200] Final Prec@1 99.9720%
06/01 04:35:39 AM | Valid: [120/200] Step 000/078 Loss 1.021 Prec@(1,5) (78.9%, 91.4%)
06/01 04:35:41 AM | Valid: [120/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.7%, 90.1%)
06/01 04:35:41 AM | Valid: [120/200] Final Prec@1 70.7400%
06/01 04:35:42 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:35:42 AM | Train: [121/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 04:35:53 AM | Train: [121/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:36:03 AM | Train: [121/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:36:13 AM | Train: [121/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:36:23 AM | Train: [121/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:36:23 AM | Train: [121/200] Final Prec@1 99.9620%
06/01 04:36:23 AM | Valid: [121/200] Step 000/078 Loss 1.002 Prec@(1,5) (78.1%, 91.4%)
06/01 04:36:25 AM | Valid: [121/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.7%, 90.1%)
06/01 04:36:25 AM | Valid: [121/200] Final Prec@1 70.7300%
06/01 04:36:26 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:36:26 AM | Train: [122/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:36:37 AM | Train: [122/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:36:47 AM | Train: [122/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:36:57 AM | Train: [122/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:37:07 AM | Train: [122/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:37:07 AM | Train: [122/200] Final Prec@1 99.9800%
06/01 04:37:07 AM | Valid: [122/200] Step 000/078 Loss 1.013 Prec@(1,5) (78.9%, 91.4%)
06/01 04:37:10 AM | Valid: [122/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.2%)
06/01 04:37:10 AM | Valid: [122/200] Final Prec@1 70.8900%
06/01 04:37:10 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:37:11 AM | Train: [123/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:37:21 AM | Train: [123/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:37:31 AM | Train: [123/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:37:42 AM | Train: [123/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:37:52 AM | Train: [123/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:37:52 AM | Train: [123/200] Final Prec@1 99.9700%
06/01 04:37:52 AM | Valid: [123/200] Step 000/078 Loss 1.002 Prec@(1,5) (78.9%, 91.4%)
06/01 04:37:55 AM | Valid: [123/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.0%)
06/01 04:37:55 AM | Valid: [123/200] Final Prec@1 70.8900%
06/01 04:37:55 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:37:56 AM | Train: [124/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:38:06 AM | Train: [124/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:38:17 AM | Train: [124/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:38:27 AM | Train: [124/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:38:37 AM | Train: [124/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:38:37 AM | Train: [124/200] Final Prec@1 99.9720%
06/01 04:38:37 AM | Valid: [124/200] Step 000/078 Loss 0.993 Prec@(1,5) (78.9%, 91.4%)
06/01 04:38:40 AM | Valid: [124/200] Step 078/078 Loss 1.277 Prec@(1,5) (71.0%, 90.1%)
06/01 04:38:40 AM | Valid: [124/200] Final Prec@1 70.9800%
06/01 04:38:40 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:38:41 AM | Train: [125/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:38:51 AM | Train: [125/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:39:01 AM | Train: [125/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:39:12 AM | Train: [125/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:39:22 AM | Train: [125/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:39:22 AM | Train: [125/200] Final Prec@1 99.9680%
06/01 04:39:22 AM | Valid: [125/200] Step 000/078 Loss 1.001 Prec@(1,5) (78.9%, 92.2%)
06/01 04:39:24 AM | Valid: [125/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.0%)
06/01 04:39:24 AM | Valid: [125/200] Final Prec@1 70.8100%
06/01 04:39:25 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:39:25 AM | Train: [126/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:39:36 AM | Train: [126/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:39:46 AM | Train: [126/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:39:57 AM | Train: [126/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:40:06 AM | Train: [126/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:40:06 AM | Train: [126/200] Final Prec@1 99.9700%
06/01 04:40:06 AM | Valid: [126/200] Step 000/078 Loss 1.000 Prec@(1,5) (78.9%, 92.2%)
06/01 04:40:09 AM | Valid: [126/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.9%, 90.1%)
06/01 04:40:09 AM | Valid: [126/200] Final Prec@1 70.8900%
06/01 04:40:10 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:40:10 AM | Train: [127/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:40:20 AM | Train: [127/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:40:31 AM | Train: [127/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:40:41 AM | Train: [127/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:40:51 AM | Train: [127/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:40:51 AM | Train: [127/200] Final Prec@1 99.9780%
06/01 04:40:51 AM | Valid: [127/200] Step 000/078 Loss 0.999 Prec@(1,5) (78.9%, 92.2%)
06/01 04:40:53 AM | Valid: [127/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.9%, 90.2%)
06/01 04:40:53 AM | Valid: [127/200] Final Prec@1 70.9400%
06/01 04:40:54 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:40:54 AM | Train: [128/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:41:05 AM | Train: [128/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:41:15 AM | Train: [128/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:41:26 AM | Train: [128/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:41:35 AM | Train: [128/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:41:35 AM | Train: [128/200] Final Prec@1 99.9660%
06/01 04:41:35 AM | Valid: [128/200] Step 000/078 Loss 1.005 Prec@(1,5) (79.7%, 93.0%)
06/01 04:41:38 AM | Valid: [128/200] Step 078/078 Loss 1.275 Prec@(1,5) (71.0%, 90.2%)
06/01 04:41:38 AM | Valid: [128/200] Final Prec@1 71.0200%
06/01 04:41:39 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:41:39 AM | Train: [129/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:41:49 AM | Train: [129/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:42:00 AM | Train: [129/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:42:10 AM | Train: [129/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:42:19 AM | Train: [129/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:42:19 AM | Train: [129/200] Final Prec@1 99.9800%
06/01 04:42:20 AM | Valid: [129/200] Step 000/078 Loss 1.007 Prec@(1,5) (78.9%, 91.4%)
06/01 04:42:22 AM | Valid: [129/200] Step 078/078 Loss 1.278 Prec@(1,5) (71.0%, 90.2%)
06/01 04:42:22 AM | Valid: [129/200] Final Prec@1 70.9600%
06/01 04:42:23 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:42:23 AM | Train: [130/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:42:34 AM | Train: [130/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:42:44 AM | Train: [130/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:42:55 AM | Train: [130/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:43:04 AM | Train: [130/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:43:04 AM | Train: [130/200] Final Prec@1 99.9740%
06/01 04:43:04 AM | Valid: [130/200] Step 000/078 Loss 0.995 Prec@(1,5) (78.9%, 93.0%)
06/01 04:43:07 AM | Valid: [130/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.1%)
06/01 04:43:07 AM | Valid: [130/200] Final Prec@1 70.8700%
06/01 04:43:07 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:43:08 AM | Train: [131/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:43:18 AM | Train: [131/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:43:29 AM | Train: [131/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:43:40 AM | Train: [131/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:43:49 AM | Train: [131/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:43:49 AM | Train: [131/200] Final Prec@1 99.9700%
06/01 04:43:49 AM | Valid: [131/200] Step 000/078 Loss 0.999 Prec@(1,5) (78.9%, 90.6%)
06/01 04:43:52 AM | Valid: [131/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.9%, 90.1%)
06/01 04:43:52 AM | Valid: [131/200] Final Prec@1 70.9200%
06/01 04:43:52 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:43:53 AM | Train: [132/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:44:03 AM | Train: [132/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:44:13 AM | Train: [132/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:44:24 AM | Train: [132/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:44:33 AM | Train: [132/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:44:33 AM | Train: [132/200] Final Prec@1 99.9700%
06/01 04:44:33 AM | Valid: [132/200] Step 000/078 Loss 1.011 Prec@(1,5) (78.9%, 92.2%)
06/01 04:44:36 AM | Valid: [132/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.0%)
06/01 04:44:36 AM | Valid: [132/200] Final Prec@1 70.8400%
06/01 04:44:36 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:44:37 AM | Train: [133/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/01 04:44:47 AM | Train: [133/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:44:57 AM | Train: [133/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:45:08 AM | Train: [133/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:45:18 AM | Train: [133/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:45:18 AM | Train: [133/200] Final Prec@1 99.9740%
06/01 04:45:18 AM | Valid: [133/200] Step 000/078 Loss 1.008 Prec@(1,5) (78.9%, 90.6%)
06/01 04:45:20 AM | Valid: [133/200] Step 078/078 Loss 1.276 Prec@(1,5) (71.0%, 90.0%)
06/01 04:45:20 AM | Valid: [133/200] Final Prec@1 70.9500%
06/01 04:45:21 AM | Current best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:45:21 AM | Train: [134/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:45:32 AM | Train: [134/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:45:42 AM | Train: [134/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:45:53 AM | Train: [134/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:46:02 AM | Train: [134/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:46:02 AM | Train: [134/200] Final Prec@1 99.9720%
06/01 04:46:02 AM | Valid: [134/200] Step 000/078 Loss 1.019 Prec@(1,5) (78.1%, 92.2%)
06/01 04:46:05 AM | Valid: [134/200] Step 078/078 Loss 1.279 Prec@(1,5) (71.0%, 90.0%)
06/01 04:46:05 AM | Valid: [134/200] Final Prec@1 71.0300%
06/01 04:46:05 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:46:06 AM | Train: [135/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:46:16 AM | Train: [135/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:46:27 AM | Train: [135/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:46:38 AM | Train: [135/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:46:47 AM | Train: [135/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:46:47 AM | Train: [135/200] Final Prec@1 99.9700%
06/01 04:46:47 AM | Valid: [135/200] Step 000/078 Loss 0.996 Prec@(1,5) (78.9%, 92.2%)
06/01 04:46:50 AM | Valid: [135/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.0%)
06/01 04:46:50 AM | Valid: [135/200] Final Prec@1 70.8100%
06/01 04:46:50 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:46:51 AM | Train: [136/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:47:01 AM | Train: [136/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:47:12 AM | Train: [136/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:47:22 AM | Train: [136/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:47:32 AM | Train: [136/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:47:32 AM | Train: [136/200] Final Prec@1 99.9840%
06/01 04:47:32 AM | Valid: [136/200] Step 000/078 Loss 1.000 Prec@(1,5) (78.9%, 91.4%)
06/01 04:47:34 AM | Valid: [136/200] Step 078/078 Loss 1.280 Prec@(1,5) (71.0%, 90.1%)
06/01 04:47:34 AM | Valid: [136/200] Final Prec@1 70.9800%
06/01 04:47:35 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:47:35 AM | Train: [137/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 04:47:46 AM | Train: [137/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:47:56 AM | Train: [137/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:48:07 AM | Train: [137/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:48:16 AM | Train: [137/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:48:16 AM | Train: [137/200] Final Prec@1 99.9760%
06/01 04:48:16 AM | Valid: [137/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.9%, 91.4%)
06/01 04:48:18 AM | Valid: [137/200] Step 078/078 Loss 1.280 Prec@(1,5) (71.0%, 89.9%)
06/01 04:48:18 AM | Valid: [137/200] Final Prec@1 70.9500%
06/01 04:48:19 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:48:19 AM | Train: [138/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:48:30 AM | Train: [138/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:48:40 AM | Train: [138/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:48:51 AM | Train: [138/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:49:00 AM | Train: [138/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:49:00 AM | Train: [138/200] Final Prec@1 99.9780%
06/01 04:49:01 AM | Valid: [138/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.9%, 92.2%)
06/01 04:49:03 AM | Valid: [138/200] Step 078/078 Loss 1.275 Prec@(1,5) (71.0%, 90.2%)
06/01 04:49:03 AM | Valid: [138/200] Final Prec@1 70.9700%
06/01 04:49:04 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:49:04 AM | Train: [139/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 04:49:15 AM | Train: [139/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:49:25 AM | Train: [139/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:49:36 AM | Train: [139/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:49:45 AM | Train: [139/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:49:45 AM | Train: [139/200] Final Prec@1 99.9740%
06/01 04:49:45 AM | Valid: [139/200] Step 000/078 Loss 0.990 Prec@(1,5) (78.9%, 93.0%)
06/01 04:49:48 AM | Valid: [139/200] Step 078/078 Loss 1.277 Prec@(1,5) (71.0%, 90.0%)
06/01 04:49:48 AM | Valid: [139/200] Final Prec@1 70.9600%
06/01 04:49:48 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:49:49 AM | Train: [140/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:49:59 AM | Train: [140/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:50:10 AM | Train: [140/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:50:20 AM | Train: [140/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:50:30 AM | Train: [140/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:50:30 AM | Train: [140/200] Final Prec@1 99.9700%
06/01 04:50:30 AM | Valid: [140/200] Step 000/078 Loss 1.002 Prec@(1,5) (78.9%, 91.4%)
06/01 04:50:32 AM | Valid: [140/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.9%, 90.0%)
06/01 04:50:32 AM | Valid: [140/200] Final Prec@1 70.9300%
06/01 04:50:33 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:50:33 AM | Train: [141/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:50:44 AM | Train: [141/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:50:54 AM | Train: [141/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:51:04 AM | Train: [141/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:51:14 AM | Train: [141/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:51:14 AM | Train: [141/200] Final Prec@1 99.9720%
06/01 04:51:14 AM | Valid: [141/200] Step 000/078 Loss 0.996 Prec@(1,5) (78.9%, 93.0%)
06/01 04:51:16 AM | Valid: [141/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.7%, 90.2%)
06/01 04:51:16 AM | Valid: [141/200] Final Prec@1 70.6600%
06/01 04:51:17 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:51:17 AM | Train: [142/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:51:28 AM | Train: [142/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:51:38 AM | Train: [142/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:51:49 AM | Train: [142/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:51:58 AM | Train: [142/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:51:58 AM | Train: [142/200] Final Prec@1 99.9700%
06/01 04:51:58 AM | Valid: [142/200] Step 000/078 Loss 1.005 Prec@(1,5) (78.9%, 92.2%)
06/01 04:52:01 AM | Valid: [142/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.9%, 90.1%)
06/01 04:52:01 AM | Valid: [142/200] Final Prec@1 70.9300%
06/01 04:52:01 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:52:02 AM | Train: [143/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:52:12 AM | Train: [143/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:52:23 AM | Train: [143/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:52:33 AM | Train: [143/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:52:43 AM | Train: [143/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:52:43 AM | Train: [143/200] Final Prec@1 99.9700%
06/01 04:52:43 AM | Valid: [143/200] Step 000/078 Loss 1.003 Prec@(1,5) (78.9%, 91.4%)
06/01 04:52:45 AM | Valid: [143/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.8%, 90.3%)
06/01 04:52:45 AM | Valid: [143/200] Final Prec@1 70.7500%
06/01 04:52:46 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:52:46 AM | Train: [144/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:52:56 AM | Train: [144/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:53:07 AM | Train: [144/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:53:17 AM | Train: [144/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:53:27 AM | Train: [144/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:53:27 AM | Train: [144/200] Final Prec@1 99.9640%
06/01 04:53:27 AM | Valid: [144/200] Step 000/078 Loss 1.014 Prec@(1,5) (78.9%, 92.2%)
06/01 04:53:29 AM | Valid: [144/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.8%, 90.2%)
06/01 04:53:29 AM | Valid: [144/200] Final Prec@1 70.8100%
06/01 04:53:30 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:53:30 AM | Train: [145/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:53:40 AM | Train: [145/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:53:51 AM | Train: [145/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:54:01 AM | Train: [145/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:54:11 AM | Train: [145/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:54:11 AM | Train: [145/200] Final Prec@1 99.9780%
06/01 04:54:11 AM | Valid: [145/200] Step 000/078 Loss 1.014 Prec@(1,5) (78.9%, 90.6%)
06/01 04:54:13 AM | Valid: [145/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.9%, 90.1%)
06/01 04:54:13 AM | Valid: [145/200] Final Prec@1 70.9400%
06/01 04:54:14 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:54:14 AM | Train: [146/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:54:25 AM | Train: [146/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:54:35 AM | Train: [146/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:54:46 AM | Train: [146/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:54:55 AM | Train: [146/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:54:55 AM | Train: [146/200] Final Prec@1 99.9680%
06/01 04:54:55 AM | Valid: [146/200] Step 000/078 Loss 1.018 Prec@(1,5) (78.1%, 90.6%)
06/01 04:54:57 AM | Valid: [146/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.8%, 90.1%)
06/01 04:54:57 AM | Valid: [146/200] Final Prec@1 70.8000%
06/01 04:54:58 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:54:58 AM | Train: [147/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:55:09 AM | Train: [147/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 04:55:19 AM | Train: [147/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:55:30 AM | Train: [147/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:55:39 AM | Train: [147/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:55:39 AM | Train: [147/200] Final Prec@1 99.9600%
06/01 04:55:39 AM | Valid: [147/200] Step 000/078 Loss 1.006 Prec@(1,5) (79.7%, 90.6%)
06/01 04:55:42 AM | Valid: [147/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.9%, 90.1%)
06/01 04:55:42 AM | Valid: [147/200] Final Prec@1 70.9200%
06/01 04:55:42 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:55:43 AM | Train: [148/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 04:55:53 AM | Train: [148/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:56:04 AM | Train: [148/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:56:14 AM | Train: [148/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:56:24 AM | Train: [148/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:56:24 AM | Train: [148/200] Final Prec@1 99.9920%
06/01 04:56:24 AM | Valid: [148/200] Step 000/078 Loss 1.021 Prec@(1,5) (78.9%, 91.4%)
06/01 04:56:26 AM | Valid: [148/200] Step 078/078 Loss 1.274 Prec@(1,5) (71.0%, 90.1%)
06/01 04:56:27 AM | Valid: [148/200] Final Prec@1 71.0000%
06/01 04:56:27 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:56:28 AM | Train: [149/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:56:38 AM | Train: [149/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:56:49 AM | Train: [149/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:57:00 AM | Train: [149/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:57:09 AM | Train: [149/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:57:09 AM | Train: [149/200] Final Prec@1 99.9820%
06/01 04:57:10 AM | Valid: [149/200] Step 000/078 Loss 1.005 Prec@(1,5) (78.9%, 91.4%)
06/01 04:57:12 AM | Valid: [149/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.8%, 90.2%)
06/01 04:57:12 AM | Valid: [149/200] Final Prec@1 70.7600%
06/01 04:57:13 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:57:13 AM | Train: [150/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 04:57:23 AM | Train: [150/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:57:34 AM | Train: [150/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:57:44 AM | Train: [150/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:57:54 AM | Train: [150/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:57:54 AM | Train: [150/200] Final Prec@1 99.9700%
06/01 04:57:54 AM | Valid: [150/200] Step 000/078 Loss 1.005 Prec@(1,5) (78.9%, 92.2%)
06/01 04:57:56 AM | Valid: [150/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.7%, 90.2%)
06/01 04:57:56 AM | Valid: [150/200] Final Prec@1 70.6800%
06/01 04:57:57 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:57:57 AM | Train: [151/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 04:58:07 AM | Train: [151/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:58:18 AM | Train: [151/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:58:28 AM | Train: [151/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:58:38 AM | Train: [151/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:58:38 AM | Train: [151/200] Final Prec@1 99.9700%
06/01 04:58:38 AM | Valid: [151/200] Step 000/078 Loss 1.002 Prec@(1,5) (80.5%, 90.6%)
06/01 04:58:40 AM | Valid: [151/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.8%, 89.9%)
06/01 04:58:40 AM | Valid: [151/200] Final Prec@1 70.7600%
06/01 04:58:41 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:58:41 AM | Train: [152/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:58:52 AM | Train: [152/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:59:02 AM | Train: [152/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:59:12 AM | Train: [152/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:59:21 AM | Train: [152/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:59:21 AM | Train: [152/200] Final Prec@1 99.9660%
06/01 04:59:22 AM | Valid: [152/200] Step 000/078 Loss 1.002 Prec@(1,5) (78.9%, 92.2%)
06/01 04:59:24 AM | Valid: [152/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.2%)
06/01 04:59:24 AM | Valid: [152/200] Final Prec@1 70.8400%
06/01 04:59:25 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 04:59:25 AM | Train: [153/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/01 04:59:35 AM | Train: [153/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:59:46 AM | Train: [153/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 04:59:56 AM | Train: [153/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:00:05 AM | Train: [153/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:00:05 AM | Train: [153/200] Final Prec@1 99.9760%
06/01 05:00:06 AM | Valid: [153/200] Step 000/078 Loss 0.995 Prec@(1,5) (78.9%, 91.4%)
06/01 05:00:08 AM | Valid: [153/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.9%, 90.1%)
06/01 05:00:08 AM | Valid: [153/200] Final Prec@1 70.8900%
06/01 05:00:09 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:00:09 AM | Train: [154/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:00:19 AM | Train: [154/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:00:30 AM | Train: [154/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:00:40 AM | Train: [154/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:00:50 AM | Train: [154/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:00:50 AM | Train: [154/200] Final Prec@1 99.9640%
06/01 05:00:50 AM | Valid: [154/200] Step 000/078 Loss 1.000 Prec@(1,5) (78.1%, 92.2%)
06/01 05:00:52 AM | Valid: [154/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.2%)
06/01 05:00:52 AM | Valid: [154/200] Final Prec@1 70.7600%
06/01 05:00:53 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:00:53 AM | Train: [155/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:01:04 AM | Train: [155/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:01:14 AM | Train: [155/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:01:24 AM | Train: [155/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:01:33 AM | Train: [155/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:01:33 AM | Train: [155/200] Final Prec@1 99.9780%
06/01 05:01:33 AM | Valid: [155/200] Step 000/078 Loss 1.005 Prec@(1,5) (78.9%, 92.2%)
06/01 05:01:35 AM | Valid: [155/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.8%, 90.2%)
06/01 05:01:35 AM | Valid: [155/200] Final Prec@1 70.7800%
06/01 05:01:36 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:01:36 AM | Train: [156/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:01:47 AM | Train: [156/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:01:57 AM | Train: [156/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:02:07 AM | Train: [156/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:02:16 AM | Train: [156/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:02:16 AM | Train: [156/200] Final Prec@1 99.9740%
06/01 05:02:16 AM | Valid: [156/200] Step 000/078 Loss 1.004 Prec@(1,5) (78.9%, 92.2%)
06/01 05:02:19 AM | Valid: [156/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.7%, 90.1%)
06/01 05:02:19 AM | Valid: [156/200] Final Prec@1 70.7000%
06/01 05:02:19 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:02:20 AM | Train: [157/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:02:30 AM | Train: [157/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:02:41 AM | Train: [157/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:02:51 AM | Train: [157/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:03:01 AM | Train: [157/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:03:01 AM | Train: [157/200] Final Prec@1 99.9700%
06/01 05:03:01 AM | Valid: [157/200] Step 000/078 Loss 1.015 Prec@(1,5) (78.1%, 92.2%)
06/01 05:03:03 AM | Valid: [157/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.9%, 90.1%)
06/01 05:03:04 AM | Valid: [157/200] Final Prec@1 70.8500%
06/01 05:03:04 AM | Current best Prec@1 = 71.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:03:05 AM | Train: [158/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:03:15 AM | Train: [158/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:03:25 AM | Train: [158/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:03:36 AM | Train: [158/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:03:45 AM | Train: [158/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:03:45 AM | Train: [158/200] Final Prec@1 99.9720%
06/01 05:03:46 AM | Valid: [158/200] Step 000/078 Loss 1.013 Prec@(1,5) (78.9%, 91.4%)
06/01 05:03:48 AM | Valid: [158/200] Step 078/078 Loss 1.275 Prec@(1,5) (71.0%, 90.2%)
06/01 05:03:48 AM | Valid: [158/200] Final Prec@1 71.0500%
06/01 05:03:49 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:03:49 AM | Train: [159/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:03:59 AM | Train: [159/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:04:10 AM | Train: [159/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:04:19 AM | Train: [159/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:04:28 AM | Train: [159/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:04:28 AM | Train: [159/200] Final Prec@1 99.9620%
06/01 05:04:29 AM | Valid: [159/200] Step 000/078 Loss 1.029 Prec@(1,5) (78.9%, 90.6%)
06/01 05:04:31 AM | Valid: [159/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.9%, 90.1%)
06/01 05:04:31 AM | Valid: [159/200] Final Prec@1 70.9400%
06/01 05:04:32 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:04:32 AM | Train: [160/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 05:04:43 AM | Train: [160/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:04:53 AM | Train: [160/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:05:03 AM | Train: [160/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:05:12 AM | Train: [160/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:05:12 AM | Train: [160/200] Final Prec@1 99.9720%
06/01 05:05:12 AM | Valid: [160/200] Step 000/078 Loss 0.994 Prec@(1,5) (78.9%, 92.2%)
06/01 05:05:15 AM | Valid: [160/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.5%, 90.1%)
06/01 05:05:15 AM | Valid: [160/200] Final Prec@1 70.5000%
06/01 05:05:15 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:05:16 AM | Train: [161/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:05:26 AM | Train: [161/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:05:37 AM | Train: [161/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:05:47 AM | Train: [161/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:05:56 AM | Train: [161/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:05:57 AM | Train: [161/200] Final Prec@1 99.9800%
06/01 05:05:57 AM | Valid: [161/200] Step 000/078 Loss 1.004 Prec@(1,5) (78.9%, 92.2%)
06/01 05:05:59 AM | Valid: [161/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.0%)
06/01 05:05:59 AM | Valid: [161/200] Final Prec@1 70.7900%
06/01 05:06:00 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:06:00 AM | Train: [162/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:06:11 AM | Train: [162/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:06:20 AM | Train: [162/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:06:30 AM | Train: [162/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:06:40 AM | Train: [162/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:06:40 AM | Train: [162/200] Final Prec@1 99.9740%
06/01 05:06:40 AM | Valid: [162/200] Step 000/078 Loss 1.004 Prec@(1,5) (78.9%, 91.4%)
06/01 05:06:42 AM | Valid: [162/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.8%, 90.1%)
06/01 05:06:43 AM | Valid: [162/200] Final Prec@1 70.8200%
06/01 05:06:43 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:06:43 AM | Train: [163/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 05:06:54 AM | Train: [163/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:07:04 AM | Train: [163/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:07:15 AM | Train: [163/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:07:23 AM | Train: [163/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:07:24 AM | Train: [163/200] Final Prec@1 99.9640%
06/01 05:07:24 AM | Valid: [163/200] Step 000/078 Loss 0.998 Prec@(1,5) (79.7%, 93.0%)
06/01 05:07:26 AM | Valid: [163/200] Step 078/078 Loss 1.275 Prec@(1,5) (71.0%, 90.2%)
06/01 05:07:26 AM | Valid: [163/200] Final Prec@1 70.9700%
06/01 05:07:27 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:07:27 AM | Train: [164/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 05:07:38 AM | Train: [164/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:07:48 AM | Train: [164/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:07:58 AM | Train: [164/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:08:08 AM | Train: [164/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:08:08 AM | Train: [164/200] Final Prec@1 99.9740%
06/01 05:08:08 AM | Valid: [164/200] Step 000/078 Loss 1.016 Prec@(1,5) (78.9%, 91.4%)
06/01 05:08:11 AM | Valid: [164/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.2%)
06/01 05:08:11 AM | Valid: [164/200] Final Prec@1 70.8000%
06/01 05:08:11 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:08:12 AM | Train: [165/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:08:22 AM | Train: [165/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:08:32 AM | Train: [165/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:08:43 AM | Train: [165/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:08:52 AM | Train: [165/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:08:52 AM | Train: [165/200] Final Prec@1 99.9740%
06/01 05:08:52 AM | Valid: [165/200] Step 000/078 Loss 1.018 Prec@(1,5) (78.9%, 92.2%)
06/01 05:08:55 AM | Valid: [165/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.2%)
06/01 05:08:55 AM | Valid: [165/200] Final Prec@1 70.8100%
06/01 05:08:55 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:08:56 AM | Train: [166/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:09:06 AM | Train: [166/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:09:17 AM | Train: [166/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:09:27 AM | Train: [166/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:09:37 AM | Train: [166/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:09:37 AM | Train: [166/200] Final Prec@1 99.9760%
06/01 05:09:37 AM | Valid: [166/200] Step 000/078 Loss 1.001 Prec@(1,5) (78.1%, 90.6%)
06/01 05:09:39 AM | Valid: [166/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.9%, 90.0%)
06/01 05:09:39 AM | Valid: [166/200] Final Prec@1 70.8500%
06/01 05:09:40 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:09:40 AM | Train: [167/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:09:51 AM | Train: [167/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:10:01 AM | Train: [167/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:10:12 AM | Train: [167/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:10:21 AM | Train: [167/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:10:21 AM | Train: [167/200] Final Prec@1 99.9600%
06/01 05:10:21 AM | Valid: [167/200] Step 000/078 Loss 0.993 Prec@(1,5) (78.1%, 93.0%)
06/01 05:10:24 AM | Valid: [167/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.7%, 90.2%)
06/01 05:10:24 AM | Valid: [167/200] Final Prec@1 70.7400%
06/01 05:10:24 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:10:25 AM | Train: [168/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 05:10:35 AM | Train: [168/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:10:46 AM | Train: [168/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:10:56 AM | Train: [168/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:11:06 AM | Train: [168/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:11:06 AM | Train: [168/200] Final Prec@1 99.9680%
06/01 05:11:06 AM | Valid: [168/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.1%, 90.6%)
06/01 05:11:09 AM | Valid: [168/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.0%)
06/01 05:11:09 AM | Valid: [168/200] Final Prec@1 70.9000%
06/01 05:11:09 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:11:10 AM | Train: [169/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:11:20 AM | Train: [169/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:11:31 AM | Train: [169/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:11:41 AM | Train: [169/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:11:51 AM | Train: [169/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:11:51 AM | Train: [169/200] Final Prec@1 99.9840%
06/01 05:11:51 AM | Valid: [169/200] Step 000/078 Loss 1.008 Prec@(1,5) (78.1%, 91.4%)
06/01 05:11:53 AM | Valid: [169/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.1%)
06/01 05:11:53 AM | Valid: [169/200] Final Prec@1 70.7700%
06/01 05:11:54 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:11:54 AM | Train: [170/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:12:05 AM | Train: [170/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/01 05:12:15 AM | Train: [170/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:12:26 AM | Train: [170/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:12:35 AM | Train: [170/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:12:35 AM | Train: [170/200] Final Prec@1 99.9700%
06/01 05:12:35 AM | Valid: [170/200] Step 000/078 Loss 1.020 Prec@(1,5) (77.3%, 90.6%)
06/01 05:12:38 AM | Valid: [170/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.7%, 90.0%)
06/01 05:12:38 AM | Valid: [170/200] Final Prec@1 70.7400%
06/01 05:12:38 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:12:39 AM | Train: [171/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:12:49 AM | Train: [171/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:12:59 AM | Train: [171/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:13:10 AM | Train: [171/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:13:19 AM | Train: [171/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:13:19 AM | Train: [171/200] Final Prec@1 99.9840%
06/01 05:13:19 AM | Valid: [171/200] Step 000/078 Loss 1.016 Prec@(1,5) (78.9%, 93.0%)
06/01 05:13:21 AM | Valid: [171/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.8%, 90.2%)
06/01 05:13:21 AM | Valid: [171/200] Final Prec@1 70.8100%
06/01 05:13:22 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:13:22 AM | Train: [172/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:13:33 AM | Train: [172/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:13:44 AM | Train: [172/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:13:54 AM | Train: [172/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:14:04 AM | Train: [172/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:14:04 AM | Train: [172/200] Final Prec@1 99.9740%
06/01 05:14:04 AM | Valid: [172/200] Step 000/078 Loss 1.011 Prec@(1,5) (78.9%, 91.4%)
06/01 05:14:06 AM | Valid: [172/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.9%, 90.1%)
06/01 05:14:06 AM | Valid: [172/200] Final Prec@1 70.9300%
06/01 05:14:07 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:14:07 AM | Train: [173/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:14:18 AM | Train: [173/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:14:29 AM | Train: [173/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:14:39 AM | Train: [173/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:14:48 AM | Train: [173/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:14:48 AM | Train: [173/200] Final Prec@1 99.9680%
06/01 05:14:49 AM | Valid: [173/200] Step 000/078 Loss 1.002 Prec@(1,5) (78.9%, 92.2%)
06/01 05:14:51 AM | Valid: [173/200] Step 078/078 Loss 1.276 Prec@(1,5) (71.0%, 90.1%)
06/01 05:14:51 AM | Valid: [173/200] Final Prec@1 70.9800%
06/01 05:14:52 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:14:52 AM | Train: [174/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:15:02 AM | Train: [174/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:15:13 AM | Train: [174/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:15:24 AM | Train: [174/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:15:33 AM | Train: [174/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:15:33 AM | Train: [174/200] Final Prec@1 99.9700%
06/01 05:15:34 AM | Valid: [174/200] Step 000/078 Loss 1.017 Prec@(1,5) (78.9%, 90.6%)
06/01 05:15:36 AM | Valid: [174/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.1%)
06/01 05:15:36 AM | Valid: [174/200] Final Prec@1 70.7900%
06/01 05:15:37 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:15:37 AM | Train: [175/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:15:48 AM | Train: [175/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:15:58 AM | Train: [175/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:16:08 AM | Train: [175/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:16:17 AM | Train: [175/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:16:17 AM | Train: [175/200] Final Prec@1 99.9620%
06/01 05:16:17 AM | Valid: [175/200] Step 000/078 Loss 1.023 Prec@(1,5) (78.9%, 91.4%)
06/01 05:16:19 AM | Valid: [175/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.7%, 90.2%)
06/01 05:16:19 AM | Valid: [175/200] Final Prec@1 70.7100%
06/01 05:16:20 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:16:20 AM | Train: [176/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:16:31 AM | Train: [176/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:16:42 AM | Train: [176/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:16:52 AM | Train: [176/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:17:02 AM | Train: [176/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:17:02 AM | Train: [176/200] Final Prec@1 99.9840%
06/01 05:17:02 AM | Valid: [176/200] Step 000/078 Loss 1.020 Prec@(1,5) (78.1%, 92.2%)
06/01 05:17:04 AM | Valid: [176/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.7%, 90.2%)
06/01 05:17:04 AM | Valid: [176/200] Final Prec@1 70.7100%
06/01 05:17:05 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:17:05 AM | Train: [177/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:17:16 AM | Train: [177/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:17:26 AM | Train: [177/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:17:37 AM | Train: [177/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:17:47 AM | Train: [177/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:17:47 AM | Train: [177/200] Final Prec@1 99.9760%
06/01 05:17:47 AM | Valid: [177/200] Step 000/078 Loss 1.033 Prec@(1,5) (78.1%, 91.4%)
06/01 05:17:49 AM | Valid: [177/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.7%, 90.2%)
06/01 05:17:49 AM | Valid: [177/200] Final Prec@1 70.7300%
06/01 05:17:50 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:17:50 AM | Train: [178/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:18:01 AM | Train: [178/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:18:11 AM | Train: [178/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:18:22 AM | Train: [178/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:18:31 AM | Train: [178/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:18:31 AM | Train: [178/200] Final Prec@1 99.9720%
06/01 05:18:31 AM | Valid: [178/200] Step 000/078 Loss 1.002 Prec@(1,5) (78.9%, 92.2%)
06/01 05:18:33 AM | Valid: [178/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.9%, 90.2%)
06/01 05:18:33 AM | Valid: [178/200] Final Prec@1 70.9300%
06/01 05:18:34 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:18:34 AM | Train: [179/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:18:45 AM | Train: [179/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:18:56 AM | Train: [179/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:19:06 AM | Train: [179/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:19:15 AM | Train: [179/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:19:15 AM | Train: [179/200] Final Prec@1 99.9760%
06/01 05:19:16 AM | Valid: [179/200] Step 000/078 Loss 1.006 Prec@(1,5) (78.9%, 92.2%)
06/01 05:19:18 AM | Valid: [179/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.8%, 90.0%)
06/01 05:19:18 AM | Valid: [179/200] Final Prec@1 70.7800%
06/01 05:19:19 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:19:19 AM | Train: [180/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:19:29 AM | Train: [180/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:19:40 AM | Train: [180/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:19:51 AM | Train: [180/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:20:00 AM | Train: [180/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:20:00 AM | Train: [180/200] Final Prec@1 99.9760%
06/01 05:20:00 AM | Valid: [180/200] Step 000/078 Loss 1.021 Prec@(1,5) (78.1%, 91.4%)
06/01 05:20:03 AM | Valid: [180/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.8%, 90.1%)
06/01 05:20:03 AM | Valid: [180/200] Final Prec@1 70.8300%
06/01 05:20:03 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:20:04 AM | Train: [181/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:20:14 AM | Train: [181/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:20:25 AM | Train: [181/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:20:35 AM | Train: [181/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:20:45 AM | Train: [181/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:20:45 AM | Train: [181/200] Final Prec@1 99.9700%
06/01 05:20:45 AM | Valid: [181/200] Step 000/078 Loss 1.018 Prec@(1,5) (78.1%, 93.0%)
06/01 05:20:47 AM | Valid: [181/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.1%)
06/01 05:20:47 AM | Valid: [181/200] Final Prec@1 70.7700%
06/01 05:20:48 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:20:48 AM | Train: [182/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:20:59 AM | Train: [182/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:21:09 AM | Train: [182/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:21:19 AM | Train: [182/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:21:29 AM | Train: [182/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:21:29 AM | Train: [182/200] Final Prec@1 99.9660%
06/01 05:21:29 AM | Valid: [182/200] Step 000/078 Loss 1.021 Prec@(1,5) (78.9%, 92.2%)
06/01 05:21:31 AM | Valid: [182/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.9%, 90.2%)
06/01 05:21:31 AM | Valid: [182/200] Final Prec@1 70.9100%
06/01 05:21:32 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:21:32 AM | Train: [183/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/01 05:21:43 AM | Train: [183/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:21:53 AM | Train: [183/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:22:04 AM | Train: [183/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:22:13 AM | Train: [183/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:22:13 AM | Train: [183/200] Final Prec@1 99.9700%
06/01 05:22:13 AM | Valid: [183/200] Step 000/078 Loss 1.006 Prec@(1,5) (78.1%, 91.4%)
06/01 05:22:15 AM | Valid: [183/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.1%)
06/01 05:22:15 AM | Valid: [183/200] Final Prec@1 70.7500%
06/01 05:22:16 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:22:16 AM | Train: [184/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:22:27 AM | Train: [184/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:22:37 AM | Train: [184/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:22:48 AM | Train: [184/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:22:58 AM | Train: [184/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:22:58 AM | Train: [184/200] Final Prec@1 99.9720%
06/01 05:22:58 AM | Valid: [184/200] Step 000/078 Loss 1.011 Prec@(1,5) (78.9%, 91.4%)
06/01 05:23:00 AM | Valid: [184/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.9%, 90.1%)
06/01 05:23:00 AM | Valid: [184/200] Final Prec@1 70.9100%
06/01 05:23:01 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:23:01 AM | Train: [185/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:23:11 AM | Train: [185/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:23:22 AM | Train: [185/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:23:32 AM | Train: [185/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:23:42 AM | Train: [185/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:23:42 AM | Train: [185/200] Final Prec@1 99.9740%
06/01 05:23:42 AM | Valid: [185/200] Step 000/078 Loss 1.002 Prec@(1,5) (78.9%, 92.2%)
06/01 05:23:44 AM | Valid: [185/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.9%, 90.1%)
06/01 05:23:44 AM | Valid: [185/200] Final Prec@1 70.9000%
06/01 05:23:45 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:23:45 AM | Train: [186/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:23:56 AM | Train: [186/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:24:06 AM | Train: [186/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:24:16 AM | Train: [186/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:24:26 AM | Train: [186/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:24:26 AM | Train: [186/200] Final Prec@1 99.9740%
06/01 05:24:26 AM | Valid: [186/200] Step 000/078 Loss 1.012 Prec@(1,5) (78.9%, 90.6%)
06/01 05:24:28 AM | Valid: [186/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.9%, 90.0%)
06/01 05:24:29 AM | Valid: [186/200] Final Prec@1 70.9400%
06/01 05:24:29 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:24:29 AM | Train: [187/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:24:40 AM | Train: [187/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:24:50 AM | Train: [187/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:25:00 AM | Train: [187/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:25:10 AM | Train: [187/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:25:10 AM | Train: [187/200] Final Prec@1 99.9800%
06/01 05:25:10 AM | Valid: [187/200] Step 000/078 Loss 1.018 Prec@(1,5) (78.9%, 89.8%)
06/01 05:25:13 AM | Valid: [187/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.2%)
06/01 05:25:13 AM | Valid: [187/200] Final Prec@1 70.8400%
06/01 05:25:13 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:25:14 AM | Train: [188/200] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 05:25:24 AM | Train: [188/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:25:35 AM | Train: [188/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:25:45 AM | Train: [188/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:25:55 AM | Train: [188/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:25:55 AM | Train: [188/200] Final Prec@1 99.9660%
06/01 05:25:55 AM | Valid: [188/200] Step 000/078 Loss 1.017 Prec@(1,5) (78.9%, 91.4%)
06/01 05:25:57 AM | Valid: [188/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.8%, 90.1%)
06/01 05:25:57 AM | Valid: [188/200] Final Prec@1 70.8300%
06/01 05:25:58 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:25:58 AM | Train: [189/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 05:26:09 AM | Train: [189/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:26:19 AM | Train: [189/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:26:29 AM | Train: [189/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:26:39 AM | Train: [189/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:26:39 AM | Train: [189/200] Final Prec@1 99.9680%
06/01 05:26:39 AM | Valid: [189/200] Step 000/078 Loss 1.017 Prec@(1,5) (78.9%, 91.4%)
06/01 05:26:41 AM | Valid: [189/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.9%, 90.1%)
06/01 05:26:41 AM | Valid: [189/200] Final Prec@1 70.8900%
06/01 05:26:42 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:26:42 AM | Train: [190/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:26:53 AM | Train: [190/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:27:04 AM | Train: [190/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:27:14 AM | Train: [190/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:27:24 AM | Train: [190/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:27:24 AM | Train: [190/200] Final Prec@1 99.9800%
06/01 05:27:24 AM | Valid: [190/200] Step 000/078 Loss 1.013 Prec@(1,5) (78.9%, 91.4%)
06/01 05:27:26 AM | Valid: [190/200] Step 078/078 Loss 1.275 Prec@(1,5) (71.0%, 90.1%)
06/01 05:27:26 AM | Valid: [190/200] Final Prec@1 70.9500%
06/01 05:27:27 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:27:27 AM | Train: [191/200] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 05:27:38 AM | Train: [191/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:27:48 AM | Train: [191/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:27:59 AM | Train: [191/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:28:08 AM | Train: [191/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:28:08 AM | Train: [191/200] Final Prec@1 99.9720%
06/01 05:28:09 AM | Valid: [191/200] Step 000/078 Loss 1.021 Prec@(1,5) (78.1%, 91.4%)
06/01 05:28:11 AM | Valid: [191/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.8%, 90.2%)
06/01 05:28:11 AM | Valid: [191/200] Final Prec@1 70.7600%
06/01 05:28:11 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:28:12 AM | Train: [192/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:28:22 AM | Train: [192/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:28:32 AM | Train: [192/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:28:42 AM | Train: [192/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:28:51 AM | Train: [192/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:28:51 AM | Train: [192/200] Final Prec@1 99.9700%
06/01 05:28:51 AM | Valid: [192/200] Step 000/078 Loss 0.999 Prec@(1,5) (78.9%, 92.2%)
06/01 05:28:53 AM | Valid: [192/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.9%, 90.2%)
06/01 05:28:53 AM | Valid: [192/200] Final Prec@1 70.8500%
06/01 05:28:54 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:28:54 AM | Train: [193/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:29:04 AM | Train: [193/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:29:15 AM | Train: [193/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:29:26 AM | Train: [193/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:29:35 AM | Train: [193/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:29:35 AM | Train: [193/200] Final Prec@1 99.9720%
06/01 05:29:36 AM | Valid: [193/200] Step 000/078 Loss 1.014 Prec@(1,5) (78.9%, 90.6%)
06/01 05:29:38 AM | Valid: [193/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.9%, 90.1%)
06/01 05:29:38 AM | Valid: [193/200] Final Prec@1 70.8900%
06/01 05:29:39 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:29:39 AM | Train: [194/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 05:29:50 AM | Train: [194/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:30:00 AM | Train: [194/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:30:11 AM | Train: [194/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:30:20 AM | Train: [194/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:30:20 AM | Train: [194/200] Final Prec@1 99.9760%
06/01 05:30:20 AM | Valid: [194/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.1%, 91.4%)
06/01 05:30:23 AM | Valid: [194/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.8%, 90.2%)
06/01 05:30:23 AM | Valid: [194/200] Final Prec@1 70.7700%
06/01 05:30:23 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:30:24 AM | Train: [195/200] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/01 05:30:34 AM | Train: [195/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:30:45 AM | Train: [195/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:30:55 AM | Train: [195/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:31:04 AM | Train: [195/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:31:04 AM | Train: [195/200] Final Prec@1 99.9760%
06/01 05:31:05 AM | Valid: [195/200] Step 000/078 Loss 0.991 Prec@(1,5) (77.3%, 92.2%)
06/01 05:31:07 AM | Valid: [195/200] Step 078/078 Loss 1.276 Prec@(1,5) (71.0%, 90.2%)
06/01 05:31:07 AM | Valid: [195/200] Final Prec@1 70.9700%
06/01 05:31:08 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:31:08 AM | Train: [196/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/01 05:31:18 AM | Train: [196/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:31:29 AM | Train: [196/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:31:39 AM | Train: [196/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:31:48 AM | Train: [196/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:31:48 AM | Train: [196/200] Final Prec@1 99.9740%
06/01 05:31:49 AM | Valid: [196/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.9%, 91.4%)
06/01 05:31:51 AM | Valid: [196/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.9%, 90.1%)
06/01 05:31:51 AM | Valid: [196/200] Final Prec@1 70.9300%
06/01 05:31:52 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:31:52 AM | Train: [197/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/01 05:32:02 AM | Train: [197/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:32:13 AM | Train: [197/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:32:23 AM | Train: [197/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:32:33 AM | Train: [197/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:32:33 AM | Train: [197/200] Final Prec@1 99.9780%
06/01 05:32:33 AM | Valid: [197/200] Step 000/078 Loss 1.009 Prec@(1,5) (78.9%, 92.2%)
06/01 05:32:35 AM | Valid: [197/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.9%, 90.1%)
06/01 05:32:35 AM | Valid: [197/200] Final Prec@1 70.9400%
06/01 05:32:36 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:32:36 AM | Train: [198/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:32:47 AM | Train: [198/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:32:57 AM | Train: [198/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:33:08 AM | Train: [198/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:33:17 AM | Train: [198/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:33:17 AM | Train: [198/200] Final Prec@1 99.9680%
06/01 05:33:17 AM | Valid: [198/200] Step 000/078 Loss 1.013 Prec@(1,5) (78.9%, 90.6%)
06/01 05:33:20 AM | Valid: [198/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.9%, 90.1%)
06/01 05:33:20 AM | Valid: [198/200] Final Prec@1 70.8600%
06/01 05:33:20 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:33:21 AM | Train: [199/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:33:31 AM | Train: [199/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:33:41 AM | Train: [199/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:33:52 AM | Train: [199/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:34:01 AM | Train: [199/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:34:01 AM | Train: [199/200] Final Prec@1 99.9740%
06/01 05:34:02 AM | Valid: [199/200] Step 000/078 Loss 1.020 Prec@(1,5) (78.9%, 91.4%)
06/01 05:34:04 AM | Valid: [199/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.1%)
06/01 05:34:04 AM | Valid: [199/200] Final Prec@1 70.8100%
06/01 05:34:05 AM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 102.0, 0.001556396484375]
['model.relu.alpha_mask_1_0', 16384, 170.0, 0.0103759765625]
['model.relu.alpha_mask_2_0', 16384, 103.0, 0.00628662109375]
['model.relu.alpha_mask_3_0', 16384, 124.0, 0.007568359375]
['model.relu.alpha_mask_4_0', 16384, 47.0, 0.00286865234375]
['model.relu.alpha_mask_5_0', 8192, 569.0, 0.0694580078125]
['model.relu.alpha_mask_6_0', 8192, 379.0, 0.0462646484375]
['model.relu.alpha_mask_7_0', 8192, 450.0, 0.054931640625]
['model.relu.alpha_mask_8_0', 8192, 407.0, 0.0496826171875]
['model.relu.alpha_mask_9_0', 4096, 1078.0, 0.26318359375]
['model.relu.alpha_mask_10_0', 4096, 1082.0, 0.26416015625]
['model.relu.alpha_mask_11_0', 4096, 1118.0, 0.27294921875]
['model.relu.alpha_mask_12_0', 4096, 1068.0, 0.2607421875]
['model.relu.alpha_mask_13_0', 2048, 1609.0, 0.78564453125]
['model.relu.alpha_mask_14_0', 2048, 1857.0, 0.90673828125]
['model.relu.alpha_mask_15_0', 2048, 1239.0, 0.60498046875]
['model.relu.alpha_mask_16_0', 2048, 1388.0, 0.677734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12790.0, 0.06788170855978261]
########## End ###########
06/01 05:34:05 AM | Train: [200/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/01 05:34:15 AM | Train: [200/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:34:26 AM | Train: [200/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:34:37 AM | Train: [200/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:34:46 AM | Train: [200/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/01 05:34:46 AM | Train: [200/200] Final Prec@1 99.9740%
06/01 05:34:47 AM | Valid: [200/200] Step 000/078 Loss 1.005 Prec@(1,5) (78.1%, 92.2%)
06/01 05:34:49 AM | Valid: [200/200] Step 078/078 Loss 1.273 Prec@(1,5) (70.9%, 90.2%)
06/01 05:34:49 AM | Valid: [200/200] Final Prec@1 70.9200%
06/01 05:34:49 AM | Current best Prec@1 = 71.0500%
06/01 05:34:49 AM | Final best validation Prec@1 = 71.0500%
[HAMI-core Msg(465233:124733484747648:multiprocess_memory_limit.c:498)]: Calling exit handler 465233
