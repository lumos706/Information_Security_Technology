[HAMI-core Msg(543964:127731242920832:libvgpu.c:837)]: Initializing.....
06/02 01:11:09 PM | 
06/02 01:11:09 PM | Parameters:
06/02 01:11:09 PM | NUM_MASK=1
06/02 01:11:09 PM | RELU_COUNT=50.0
06/02 01:11:09 PM | ACT_TYPE=ReLU_masked_dapa_relay
06/02 01:11:09 PM | ALPHA_LR=0.0002
06/02 01:11:09 PM | ALPHA_WEIGHT_DECAY=0.001
06/02 01:11:09 PM | ARCH=resnet18
06/02 01:11:09 PM | BATCH_SIZE=128
06/02 01:11:09 PM | CHECKPOINT_PATH=None
06/02 01:11:09 PM | CLIP_X2=1.0
06/02 01:11:09 PM | CLIP_X2_BOOL=True
06/02 01:11:09 PM | DATA_PATH=./data/
06/02 01:11:09 PM | DATASET=cifar100
06/02 01:11:09 PM | DEGREE=2
06/02 01:11:09 PM | DISTIL=True
06/02 01:11:09 PM | DROPOUT=0
06/02 01:11:09 PM | ENABLE_GRAD_NORM=False
06/02 01:11:09 PM | ENABLE_LOOKAHEAD=True
06/02 01:11:09 PM | EPOCHS=200
06/02 01:11:09 PM | EVALUATE=None
06/02 01:11:09 PM | EXT=baseline
06/02 01:11:09 PM | FREEZEACT=False
06/02 01:11:09 PM | GPUS=[0]
06/02 01:11:09 PM | LAMDA=240.0
06/02 01:11:09 PM | MASK_DROPOUT=0
06/02 01:11:09 PM | MASK_EPOCHS=80
06/02 01:11:09 PM | NUM_CLASSES=100
06/02 01:11:09 PM | OPTIM=cosine
06/02 01:11:09 PM | PATH=train_cifar_dapa2_distil_relay/resnet18_resnet18_cifar100_relay_0.003/cosine_ReLUs50.0wm_lr0.001mep80_baseline
06/02 01:11:09 PM | PLOT_PATH=train_cifar_dapa2_distil_relay/resnet18_resnet18_cifar100_relay_0.003/cosine_ReLUs50.0wm_lr0.001mep80_baseline/plots
06/02 01:11:09 PM | PRECISION=full
06/02 01:11:09 PM | PRETRAINED=False
06/02 01:11:09 PM | PRETRAINED_PATH=./train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar
06/02 01:11:09 PM | PRINT_FREQ=100
06/02 01:11:09 PM | SCALE_X1=1.0
06/02 01:11:09 PM | SCALE_X2=2.0
06/02 01:11:09 PM | SEED=2
06/02 01:11:09 PM | START_EPOCH=0
06/02 01:11:09 PM | TEACHER_ARCH=resnet18
06/02 01:11:09 PM | TEACHER_PATH=./train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar
06/02 01:11:09 PM | THRESHOLD=0.003
06/02 01:11:09 PM | VAR_MIN=0.5
06/02 01:11:09 PM | W_DECAY_EPOCH=20
06/02 01:11:09 PM | W_GRAD_CLIP=5.0
06/02 01:11:09 PM | W_LR=0.0001
06/02 01:11:09 PM | W_LR_MIN=1e-05
06/02 01:11:09 PM | W_MASK_LR=0.001
06/02 01:11:09 PM | W_MOMENTUM=0.9
06/02 01:11:09 PM | W_WEIGHT_DECAY=0.0005
06/02 01:11:09 PM | WORKERS=4
06/02 01:11:09 PM | X_SIZE=[1, 3, 32, 32]
06/02 01:11:09 PM | 
06/02 01:11:09 PM | Logger is set - training start
[HAMI-core Msg(543964:127731242920832:libvgpu.c:856)]: Initialized
==> Load pretrained
=> loading checkpoint './train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar'
/mnt/ann25-22336216/AutoReP/models_util/model_util.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_path, map_location = "cpu")
=> loading checkpoint './train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar'
Files already downloaded and verified
Files already downloaded and verified
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 65536.0, 1.0]
['model.relu.alpha_mask_1_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_2_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_3_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_4_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_5_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_6_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_7_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_8_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_9_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_10_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_11_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_12_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_13_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_14_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_15_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_16_0', 2048, 2048.0, 1.0]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 188416.0, 1.0]
########## End ###########
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16601579520 total=17059545088 limit=4194304000 usage=415246848
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16366698496 total=17059545088 limit=4194304000 usage=650127872
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16106651648 total=17059545088 limit=4194304000 usage=910174720
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16085680128 total=17059545088 limit=4194304000 usage=931146240
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16066805760 total=17059545088 limit=4194304000 usage=950020608
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16045834240 total=17059545088 limit=4194304000 usage=970992128
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16039542784 total=17059545088 limit=4194304000 usage=977283584
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16037445632 total=17059545088 limit=4194304000 usage=979380736
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15760621568 total=17059545088 limit=4194304000 usage=1252285440
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15703998464 total=17059545088 limit=4194304000 usage=1308908544
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15703998464 total=17059545088 limit=4194304000 usage=1308908544
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15708192768 total=17059545088 limit=4194304000 usage=1304714240
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15651569664 total=17059545088 limit=4194304000 usage=1361337344
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15636889600 total=17059545088 limit=4194304000 usage=1376017408
06/02 01:11:14 PM | Train: [ 1/80] Step 000/390 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)
06/02 01:11:14 PM | layerwise density: [65536.0, 16384.0, 16384.0, 16384.0, 16384.0, 8192.0, 8192.0, 8192.0, 8192.0, 4096.0, 4096.0, 4096.0, 4096.0, 2048.0, 2048.0, 2048.0, 2048.0]
layerwise density percentage: ['1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000']
Global density: 1.0
06/02 01:11:25 PM | Train: [ 1/80] Step 100/390 Loss 0.024 Prec@(1,5) (99.9%, 100.0%)
06/02 01:11:25 PM | layerwise density: [62127.0, 15548.0, 15555.0, 15539.0, 15548.0, 7760.0, 7768.0, 7748.0, 7760.0, 3882.0, 3893.0, 3905.0, 3901.0, 1975.0, 1970.0, 1954.0, 1934.0]
layerwise density percentage: ['0.948', '0.949', '0.949', '0.948', '0.949', '0.947', '0.948', '0.946', '0.947', '0.948', '0.950', '0.953', '0.952', '0.964', '0.962', '0.954', '0.944']
Global density: 0.9487888813018799
06/02 01:11:36 PM | Train: [ 1/80] Step 200/390 Loss 0.032 Prec@(1,5) (99.8%, 100.0%)
06/02 01:11:36 PM | layerwise density: [58887.0, 14811.0, 14757.0, 14797.0, 14780.0, 7359.0, 7305.0, 7347.0, 7409.0, 3708.0, 3700.0, 3726.0, 3705.0, 1905.0, 1893.0, 1876.0, 1854.0]
layerwise density percentage: ['0.899', '0.904', '0.901', '0.903', '0.902', '0.898', '0.892', '0.897', '0.904', '0.905', '0.903', '0.910', '0.905', '0.930', '0.924', '0.916', '0.905']
Global density: 0.9012982249259949
06/02 01:11:48 PM | Train: [ 1/80] Step 300/390 Loss 0.044 Prec@(1,5) (99.7%, 100.0%)
06/02 01:11:48 PM | layerwise density: [55628.0, 13997.0, 13959.0, 13989.0, 14020.0, 6921.0, 6925.0, 6960.0, 7003.0, 3523.0, 3518.0, 3532.0, 3528.0, 1819.0, 1817.0, 1772.0, 1778.0]
layerwise density percentage: ['0.849', '0.854', '0.852', '0.854', '0.856', '0.845', '0.845', '0.850', '0.855', '0.860', '0.859', '0.862', '0.861', '0.888', '0.887', '0.865', '0.868']
Global density: 0.8528416156768799
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=15464923136 total=17059545088 limit=4194304000 usage=1547983872
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16190537728 total=17059545088 limit=4194304000 usage=822369280
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16085680128 total=17059545088 limit=4194304000 usage=927226880
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16064708608 total=17059545088 limit=4194304000 usage=948198400
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16064708608 total=17059545088 limit=4194304000 usage=948198400
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16022765568 total=17059545088 limit=4194304000 usage=990141440
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16022765568 total=17059545088 limit=4194304000 usage=990141440
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16005988352 total=17059545088 limit=4194304000 usage=1006918656
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=15989211136 total=17059545088 limit=4194304000 usage=1023695872
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=15999696896 total=17059545088 limit=4194304000 usage=1013210112
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=15999696896 total=17059545088 limit=4194304000 usage=1013210112
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15821438976 total=17059545088 limit=4194304000 usage=1191468032
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15792078848 total=17059545088 limit=4194304000 usage=1220828160
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15792078848 total=17059545088 limit=4194304000 usage=1220828160
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15794176000 total=17059545088 limit=4194304000 usage=1218731008
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15827730432 total=17059545088 limit=4194304000 usage=1185176576
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15827730432 total=17059545088 limit=4194304000 usage=1185176576
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(543964:127726262421056:memory.c:512)]: orig free=15857090560 total=17059545088 limit=4194304000 usage=1155816448
06/02 01:11:58 PM | Train: [ 1/80] Step 390/390 Loss 0.058 Prec@(1,5) (99.4%, 100.0%)
06/02 01:11:58 PM | layerwise density: [52856.0, 13287.0, 13277.0, 13327.0, 13249.0, 6583.0, 6594.0, 6586.0, 6650.0, 3356.0, 3354.0, 3357.0, 3346.0, 1748.0, 1751.0, 1701.0, 1672.0]
layerwise density percentage: ['0.807', '0.811', '0.810', '0.813', '0.809', '0.804', '0.805', '0.804', '0.812', '0.819', '0.819', '0.820', '0.817', '0.854', '0.855', '0.831', '0.816']
Global density: 0.810408890247345
06/02 01:11:59 PM | Train: [ 1/200] Final Prec@1 99.4280%
06/02 01:11:59 PM | Valid: [ 1/200] Step 000/078 Loss 1.103 Prec@(1,5) (70.3%, 93.0%)
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=15781593088 total=17059545088 limit=4194304000 usage=1231313920
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16305881088 total=17059545088 limit=4194304000 usage=707025920
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(543964:127731242920832:memory.c:512)]: orig free=16282812416 total=17059545088 limit=4194304000 usage=730094592
06/02 01:12:02 PM | Valid: [ 1/200] Step 078/078 Loss 1.326 Prec@(1,5) (68.8%, 89.1%)
06/02 01:12:02 PM | Valid: [ 1/200] Final Prec@1 68.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 52856.0, 0.8065185546875]
['model.relu.alpha_mask_1_0', 16384, 13287.0, 0.81097412109375]
['model.relu.alpha_mask_2_0', 16384, 13277.0, 0.81036376953125]
['model.relu.alpha_mask_3_0', 16384, 13327.0, 0.81341552734375]
['model.relu.alpha_mask_4_0', 16384, 13249.0, 0.80865478515625]
['model.relu.alpha_mask_5_0', 8192, 6583.0, 0.8035888671875]
['model.relu.alpha_mask_6_0', 8192, 6594.0, 0.804931640625]
['model.relu.alpha_mask_7_0', 8192, 6586.0, 0.803955078125]
['model.relu.alpha_mask_8_0', 8192, 6650.0, 0.811767578125]
['model.relu.alpha_mask_9_0', 4096, 3356.0, 0.8193359375]
['model.relu.alpha_mask_10_0', 4096, 3354.0, 0.81884765625]
['model.relu.alpha_mask_11_0', 4096, 3357.0, 0.819580078125]
['model.relu.alpha_mask_12_0', 4096, 3346.0, 0.81689453125]
['model.relu.alpha_mask_13_0', 2048, 1748.0, 0.853515625]
['model.relu.alpha_mask_14_0', 2048, 1751.0, 0.85498046875]
['model.relu.alpha_mask_15_0', 2048, 1701.0, 0.83056640625]
['model.relu.alpha_mask_16_0', 2048, 1672.0, 0.81640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 152694.0, 0.810408882472826]
########## End ###########
06/02 01:12:03 PM | Train: [ 2/80] Step 000/390 Loss 0.076 Prec@(1,5) (99.2%, 100.0%)
06/02 01:12:03 PM | layerwise density: [52856.0, 13287.0, 13277.0, 13327.0, 13249.0, 6583.0, 6594.0, 6586.0, 6650.0, 3356.0, 3354.0, 3357.0, 3346.0, 1748.0, 1751.0, 1701.0, 1672.0]
layerwise density percentage: ['0.807', '0.811', '0.810', '0.813', '0.809', '0.804', '0.805', '0.804', '0.812', '0.819', '0.819', '0.820', '0.817', '0.854', '0.855', '0.831', '0.816']
Global density: 0.810408890247345
06/02 01:12:14 PM | Train: [ 2/80] Step 100/390 Loss 0.111 Prec@(1,5) (98.2%, 100.0%)
06/02 01:12:14 PM | layerwise density: [49712.0, 12542.0, 12531.0, 12544.0, 12444.0, 6220.0, 6196.0, 6193.0, 6269.0, 3158.0, 3195.0, 3148.0, 3147.0, 1681.0, 1679.0, 1607.0, 1574.0]
layerwise density percentage: ['0.759', '0.766', '0.765', '0.766', '0.760', '0.759', '0.756', '0.756', '0.765', '0.771', '0.780', '0.769', '0.768', '0.821', '0.820', '0.785', '0.769']
Global density: 0.76341712474823
06/02 01:12:25 PM | Train: [ 2/80] Step 200/390 Loss 0.133 Prec@(1,5) (97.7%, 100.0%)
06/02 01:12:25 PM | layerwise density: [46769.0, 11769.0, 11805.0, 11829.0, 11708.0, 5859.0, 5822.0, 5858.0, 5909.0, 2983.0, 3011.0, 2984.0, 2957.0, 1607.0, 1604.0, 1528.0, 1491.0]
layerwise density percentage: ['0.714', '0.718', '0.721', '0.722', '0.715', '0.715', '0.711', '0.715', '0.721', '0.728', '0.735', '0.729', '0.722', '0.785', '0.783', '0.746', '0.728']
Global density: 0.7191162109375
06/02 01:12:36 PM | Train: [ 2/80] Step 300/390 Loss 0.158 Prec@(1,5) (97.1%, 100.0%)
06/02 01:12:36 PM | layerwise density: [43763.0, 11044.0, 11055.0, 11050.0, 10976.0, 5520.0, 5484.0, 5462.0, 5527.0, 2806.0, 2826.0, 2820.0, 2766.0, 1541.0, 1523.0, 1446.0, 1400.0]
layerwise density percentage: ['0.668', '0.674', '0.675', '0.674', '0.670', '0.674', '0.669', '0.667', '0.675', '0.685', '0.690', '0.688', '0.675', '0.752', '0.744', '0.706', '0.684']
Global density: 0.674088180065155
06/02 01:12:45 PM | Train: [ 2/80] Step 390/390 Loss 0.182 Prec@(1,5) (96.4%, 100.0%)
06/02 01:12:45 PM | layerwise density: [41120.0, 10379.0, 10381.0, 10350.0, 10262.0, 5191.0, 5150.0, 5101.0, 5229.0, 2673.0, 2656.0, 2664.0, 2622.0, 1495.0, 1482.0, 1377.0, 1313.0]
layerwise density percentage: ['0.627', '0.633', '0.634', '0.632', '0.626', '0.634', '0.629', '0.623', '0.638', '0.653', '0.648', '0.650', '0.640', '0.730', '0.724', '0.672', '0.641']
Global density: 0.6339429616928101
06/02 01:12:46 PM | Train: [ 2/200] Final Prec@1 96.4200%
06/02 01:12:46 PM | Valid: [ 2/200] Step 000/078 Loss 1.070 Prec@(1,5) (76.6%, 92.2%)
06/02 01:12:48 PM | Valid: [ 2/200] Step 078/078 Loss 1.384 Prec@(1,5) (67.4%, 88.5%)
06/02 01:12:48 PM | Valid: [ 2/200] Final Prec@1 67.3900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 41119.0, 0.6274261474609375]
['model.relu.alpha_mask_1_0', 16384, 10379.0, 0.63348388671875]
['model.relu.alpha_mask_2_0', 16384, 10380.0, 0.633544921875]
['model.relu.alpha_mask_3_0', 16384, 10348.0, 0.631591796875]
['model.relu.alpha_mask_4_0', 16384, 10261.0, 0.62628173828125]
['model.relu.alpha_mask_5_0', 8192, 5191.0, 0.6336669921875]
['model.relu.alpha_mask_6_0', 8192, 5150.0, 0.628662109375]
['model.relu.alpha_mask_7_0', 8192, 5101.0, 0.6226806640625]
['model.relu.alpha_mask_8_0', 8192, 5226.0, 0.637939453125]
['model.relu.alpha_mask_9_0', 4096, 2673.0, 0.652587890625]
['model.relu.alpha_mask_10_0', 4096, 2656.0, 0.6484375]
['model.relu.alpha_mask_11_0', 4096, 2664.0, 0.650390625]
['model.relu.alpha_mask_12_0', 4096, 2621.0, 0.639892578125]
['model.relu.alpha_mask_13_0', 2048, 1495.0, 0.72998046875]
['model.relu.alpha_mask_14_0', 2048, 1482.0, 0.7236328125]
['model.relu.alpha_mask_15_0', 2048, 1377.0, 0.67236328125]
['model.relu.alpha_mask_16_0', 2048, 1313.0, 0.64111328125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119436.0, 0.633895210597826]
########## End ###########
06/02 01:12:49 PM | Train: [ 3/80] Step 000/390 Loss 0.248 Prec@(1,5) (95.3%, 100.0%)
06/02 01:12:49 PM | layerwise density: [41119.0, 10379.0, 10380.0, 10348.0, 10261.0, 5191.0, 5150.0, 5101.0, 5226.0, 2673.0, 2656.0, 2664.0, 2621.0, 1495.0, 1482.0, 1377.0, 1313.0]
layerwise density percentage: ['0.627', '0.633', '0.634', '0.632', '0.626', '0.634', '0.629', '0.623', '0.638', '0.653', '0.648', '0.650', '0.640', '0.730', '0.724', '0.672', '0.641']
Global density: 0.633895218372345
06/02 01:13:00 PM | Train: [ 3/80] Step 100/390 Loss 0.220 Prec@(1,5) (95.5%, 99.9%)
06/02 01:13:00 PM | layerwise density: [38192.0, 9629.0, 9640.0, 9644.0, 9556.0, 4832.0, 4780.0, 4733.0, 4865.0, 2489.0, 2477.0, 2492.0, 2448.0, 1433.0, 1421.0, 1313.0, 1243.0]
layerwise density percentage: ['0.583', '0.588', '0.588', '0.589', '0.583', '0.590', '0.583', '0.578', '0.594', '0.608', '0.605', '0.608', '0.598', '0.700', '0.694', '0.641', '0.607']
Global density: 0.590114414691925
06/02 01:13:11 PM | Train: [ 3/80] Step 200/390 Loss 0.253 Prec@(1,5) (94.4%, 99.9%)
06/02 01:13:11 PM | layerwise density: [35420.0, 8927.0, 8962.0, 8932.0, 8798.0, 4484.0, 4450.0, 4368.0, 4503.0, 2321.0, 2304.0, 2327.0, 2316.0, 1373.0, 1358.0, 1234.0, 1162.0]
layerwise density percentage: ['0.540', '0.545', '0.547', '0.545', '0.537', '0.547', '0.543', '0.533', '0.550', '0.567', '0.562', '0.568', '0.565', '0.670', '0.663', '0.603', '0.567']
Global density: 0.5479311943054199
06/02 01:13:22 PM | Train: [ 3/80] Step 300/390 Loss 0.285 Prec@(1,5) (93.4%, 99.8%)
06/02 01:13:22 PM | layerwise density: [32658.0, 8188.0, 8256.0, 8230.0, 8182.0, 4189.0, 4110.0, 4054.0, 4169.0, 2158.0, 2145.0, 2147.0, 2154.0, 1320.0, 1303.0, 1163.0, 1080.0]
layerwise density percentage: ['0.498', '0.500', '0.504', '0.502', '0.499', '0.511', '0.502', '0.495', '0.509', '0.527', '0.524', '0.524', '0.526', '0.645', '0.636', '0.568', '0.527']
Global density: 0.5068890452384949
06/02 01:13:32 PM | Train: [ 3/80] Step 390/390 Loss 0.317 Prec@(1,5) (92.4%, 99.7%)
06/02 01:13:32 PM | layerwise density: [30118.0, 7565.0, 7654.0, 7635.0, 7548.0, 3873.0, 3839.0, 3754.0, 3823.0, 2012.0, 2001.0, 2005.0, 2008.0, 1273.0, 1258.0, 1093.0, 992.0]
layerwise density percentage: ['0.460', '0.462', '0.467', '0.466', '0.461', '0.473', '0.469', '0.458', '0.467', '0.491', '0.489', '0.490', '0.490', '0.622', '0.614', '0.534', '0.484']
Global density: 0.46944528818130493
06/02 01:13:32 PM | Train: [ 3/200] Final Prec@1 92.3580%
06/02 01:13:33 PM | Valid: [ 3/200] Step 000/078 Loss 1.281 Prec@(1,5) (67.2%, 93.0%)
06/02 01:13:35 PM | Valid: [ 3/200] Step 078/078 Loss 1.490 Prec@(1,5) (64.5%, 86.9%)
06/02 01:13:35 PM | Valid: [ 3/200] Final Prec@1 64.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 30073.0, 0.4588775634765625]
['model.relu.alpha_mask_1_0', 16384, 7554.0, 0.4610595703125]
['model.relu.alpha_mask_2_0', 16384, 7647.0, 0.46673583984375]
['model.relu.alpha_mask_3_0', 16384, 7626.0, 0.4654541015625]
['model.relu.alpha_mask_4_0', 16384, 7533.0, 0.45977783203125]
['model.relu.alpha_mask_5_0', 8192, 3867.0, 0.4720458984375]
['model.relu.alpha_mask_6_0', 8192, 3830.0, 0.467529296875]
['model.relu.alpha_mask_7_0', 8192, 3747.0, 0.4573974609375]
['model.relu.alpha_mask_8_0', 8192, 3818.0, 0.466064453125]
['model.relu.alpha_mask_9_0', 4096, 2009.0, 0.490478515625]
['model.relu.alpha_mask_10_0', 4096, 1997.0, 0.487548828125]
['model.relu.alpha_mask_11_0', 4096, 2004.0, 0.4892578125]
['model.relu.alpha_mask_12_0', 4096, 2007.0, 0.489990234375]
['model.relu.alpha_mask_13_0', 2048, 1274.0, 0.6220703125]
['model.relu.alpha_mask_14_0', 2048, 1257.0, 0.61376953125]
['model.relu.alpha_mask_15_0', 2048, 1091.0, 0.53271484375]
['model.relu.alpha_mask_16_0', 2048, 991.0, 0.48388671875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 88325.0, 0.46877653702445654]
########## End ###########
06/02 01:13:36 PM | Train: [ 4/80] Step 000/390 Loss 0.379 Prec@(1,5) (92.2%, 99.2%)
06/02 01:13:36 PM | layerwise density: [30073.0, 7554.0, 7647.0, 7626.0, 7533.0, 3867.0, 3830.0, 3747.0, 3818.0, 2009.0, 1997.0, 2004.0, 2007.0, 1274.0, 1257.0, 1091.0, 991.0]
layerwise density percentage: ['0.459', '0.461', '0.467', '0.465', '0.460', '0.472', '0.468', '0.457', '0.466', '0.490', '0.488', '0.489', '0.490', '0.622', '0.614', '0.533', '0.484']
Global density: 0.46877655386924744
06/02 01:13:47 PM | Train: [ 4/80] Step 100/390 Loss 0.362 Prec@(1,5) (90.9%, 99.7%)
06/02 01:13:47 PM | layerwise density: [27429.0, 6852.0, 6959.0, 6949.0, 6851.0, 3545.0, 3504.0, 3448.0, 3477.0, 1834.0, 1846.0, 1830.0, 1865.0, 1211.0, 1210.0, 1029.0, 915.0]
layerwise density percentage: ['0.419', '0.418', '0.425', '0.424', '0.418', '0.433', '0.428', '0.421', '0.424', '0.448', '0.451', '0.447', '0.455', '0.591', '0.591', '0.502', '0.447']
Global density: 0.428594172000885
06/02 01:13:58 PM | Train: [ 4/80] Step 200/390 Loss 0.400 Prec@(1,5) (89.7%, 99.5%)
06/02 01:13:58 PM | layerwise density: [24612.0, 6125.0, 6217.0, 6246.0, 6199.0, 3200.0, 3201.0, 3140.0, 3172.0, 1678.0, 1695.0, 1677.0, 1699.0, 1165.0, 1171.0, 965.0, 835.0]
layerwise density percentage: ['0.376', '0.374', '0.379', '0.381', '0.378', '0.391', '0.391', '0.383', '0.387', '0.410', '0.414', '0.409', '0.415', '0.569', '0.572', '0.471', '0.408']
Global density: 0.38742464780807495
06/02 01:14:09 PM | Train: [ 4/80] Step 300/390 Loss 0.445 Prec@(1,5) (88.3%, 99.3%)
06/02 01:14:09 PM | layerwise density: [22003.0, 5492.0, 5568.0, 5568.0, 5570.0, 2892.0, 2898.0, 2849.0, 2839.0, 1514.0, 1545.0, 1520.0, 1529.0, 1126.0, 1130.0, 909.0, 757.0]
layerwise density percentage: ['0.336', '0.335', '0.340', '0.340', '0.340', '0.353', '0.354', '0.348', '0.347', '0.370', '0.377', '0.371', '0.373', '0.550', '0.552', '0.444', '0.370']
Global density: 0.34874427318573
06/02 01:14:19 PM | Train: [ 4/80] Step 390/390 Loss 0.478 Prec@(1,5) (87.3%, 99.1%)
06/02 01:14:19 PM | layerwise density: [19651.0, 4870.0, 4979.0, 4922.0, 4969.0, 2579.0, 2622.0, 2548.0, 2565.0, 1356.0, 1432.0, 1398.0, 1385.0, 1084.0, 1099.0, 859.0, 666.0]
layerwise density percentage: ['0.300', '0.297', '0.304', '0.300', '0.303', '0.315', '0.320', '0.311', '0.313', '0.331', '0.350', '0.341', '0.338', '0.529', '0.537', '0.419', '0.325']
Global density: 0.3130519688129425
06/02 01:14:19 PM | Train: [ 4/200] Final Prec@1 87.3300%
06/02 01:14:19 PM | Valid: [ 4/200] Step 000/078 Loss 1.305 Prec@(1,5) (67.2%, 89.8%)
06/02 01:14:22 PM | Valid: [ 4/200] Step 078/078 Loss 1.567 Prec@(1,5) (62.5%, 86.2%)
06/02 01:14:22 PM | Valid: [ 4/200] Final Prec@1 62.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 19592.0, 0.2989501953125]
['model.relu.alpha_mask_1_0', 16384, 4859.0, 0.29656982421875]
['model.relu.alpha_mask_2_0', 16384, 4965.0, 0.30303955078125]
['model.relu.alpha_mask_3_0', 16384, 4905.0, 0.29937744140625]
['model.relu.alpha_mask_4_0', 16384, 4963.0, 0.30291748046875]
['model.relu.alpha_mask_5_0', 8192, 2572.0, 0.31396484375]
['model.relu.alpha_mask_6_0', 8192, 2617.0, 0.3194580078125]
['model.relu.alpha_mask_7_0', 8192, 2538.0, 0.309814453125]
['model.relu.alpha_mask_8_0', 8192, 2562.0, 0.312744140625]
['model.relu.alpha_mask_9_0', 4096, 1351.0, 0.329833984375]
['model.relu.alpha_mask_10_0', 4096, 1429.0, 0.348876953125]
['model.relu.alpha_mask_11_0', 4096, 1396.0, 0.3408203125]
['model.relu.alpha_mask_12_0', 4096, 1384.0, 0.337890625]
['model.relu.alpha_mask_13_0', 2048, 1085.0, 0.52978515625]
['model.relu.alpha_mask_14_0', 2048, 1099.0, 0.53662109375]
['model.relu.alpha_mask_15_0', 2048, 858.0, 0.4189453125]
['model.relu.alpha_mask_16_0', 2048, 662.0, 0.3232421875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 58837.0, 0.3122717815896739]
########## End ###########
06/02 01:14:23 PM | Train: [ 5/80] Step 000/390 Loss 0.390 Prec@(1,5) (89.8%, 99.2%)
06/02 01:14:23 PM | layerwise density: [19592.0, 4859.0, 4965.0, 4905.0, 4963.0, 2572.0, 2617.0, 2538.0, 2562.0, 1351.0, 1429.0, 1396.0, 1384.0, 1085.0, 1099.0, 858.0, 662.0]
layerwise density percentage: ['0.299', '0.297', '0.303', '0.299', '0.303', '0.314', '0.319', '0.310', '0.313', '0.330', '0.349', '0.341', '0.338', '0.530', '0.537', '0.419', '0.323']
Global density: 0.31227177381515503
06/02 01:14:33 PM | Train: [ 5/80] Step 100/390 Loss 0.537 Prec@(1,5) (85.7%, 98.8%)
06/02 01:14:33 PM | layerwise density: [17014.0, 4203.0, 4303.0, 4217.0, 4303.0, 2268.0, 2289.0, 2232.0, 2219.0, 1216.0, 1296.0, 1241.0, 1253.0, 1040.0, 1063.0, 804.0, 569.0]
layerwise density percentage: ['0.260', '0.257', '0.263', '0.257', '0.263', '0.277', '0.279', '0.272', '0.271', '0.297', '0.316', '0.303', '0.306', '0.508', '0.519', '0.393', '0.278']
Global density: 0.2734905779361725
06/02 01:14:44 PM | Train: [ 5/80] Step 200/390 Loss 0.561 Prec@(1,5) (84.7%, 98.6%)
06/02 01:14:44 PM | layerwise density: [16253.0, 4002.0, 4122.0, 4043.0, 4100.0, 2177.0, 2192.0, 2139.0, 2124.0, 1176.0, 1252.0, 1205.0, 1213.0, 1093.0, 1112.0, 830.0, 546.0]
layerwise density percentage: ['0.248', '0.244', '0.252', '0.247', '0.250', '0.266', '0.268', '0.261', '0.259', '0.287', '0.306', '0.294', '0.296', '0.534', '0.543', '0.405', '0.267']
Global density: 0.2631358206272125
06/02 01:14:55 PM | Train: [ 5/80] Step 300/390 Loss 0.547 Prec@(1,5) (85.1%, 98.7%)
06/02 01:14:55 PM | layerwise density: [16216.0, 4000.0, 4119.0, 4040.0, 4099.0, 2177.0, 2191.0, 2139.0, 2126.0, 1185.0, 1269.0, 1223.0, 1231.0, 1136.0, 1169.0, 866.0, 548.0]
layerwise density percentage: ['0.247', '0.244', '0.251', '0.247', '0.250', '0.266', '0.267', '0.261', '0.260', '0.289', '0.310', '0.299', '0.301', '0.555', '0.571', '0.423', '0.268']
Global density: 0.26395848393440247
06/02 01:15:04 PM | Train: [ 5/80] Step 390/390 Loss 0.530 Prec@(1,5) (85.6%, 98.8%)
06/02 01:15:04 PM | layerwise density: [16191.0, 4006.0, 4113.0, 4043.0, 4101.0, 2191.0, 2203.0, 2152.0, 2143.0, 1197.0, 1286.0, 1234.0, 1250.0, 1171.0, 1208.0, 898.0, 557.0]
layerwise density percentage: ['0.247', '0.245', '0.251', '0.247', '0.250', '0.267', '0.269', '0.263', '0.262', '0.292', '0.314', '0.301', '0.305', '0.572', '0.590', '0.438', '0.272']
Global density: 0.2650730311870575
06/02 01:15:04 PM | Train: [ 5/200] Final Prec@1 85.5600%
06/02 01:15:05 PM | Valid: [ 5/200] Step 000/078 Loss 1.401 Prec@(1,5) (70.3%, 86.7%)
06/02 01:15:07 PM | Valid: [ 5/200] Step 078/078 Loss 1.616 Prec@(1,5) (62.9%, 85.7%)
06/02 01:15:07 PM | Valid: [ 5/200] Final Prec@1 62.9100%
06/02 01:15:08 PM | Current mask training best Prec@1 = 62.9100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 16190.0, 0.247039794921875]
['model.relu.alpha_mask_1_0', 16384, 4006.0, 0.2445068359375]
['model.relu.alpha_mask_2_0', 16384, 4113.0, 0.25103759765625]
['model.relu.alpha_mask_3_0', 16384, 4043.0, 0.24676513671875]
['model.relu.alpha_mask_4_0', 16384, 4101.0, 0.25030517578125]
['model.relu.alpha_mask_5_0', 8192, 2191.0, 0.2674560546875]
['model.relu.alpha_mask_6_0', 8192, 2203.0, 0.2689208984375]
['model.relu.alpha_mask_7_0', 8192, 2152.0, 0.2626953125]
['model.relu.alpha_mask_8_0', 8192, 2143.0, 0.2615966796875]
['model.relu.alpha_mask_9_0', 4096, 1197.0, 0.292236328125]
['model.relu.alpha_mask_10_0', 4096, 1287.0, 0.314208984375]
['model.relu.alpha_mask_11_0', 4096, 1234.0, 0.30126953125]
['model.relu.alpha_mask_12_0', 4096, 1250.0, 0.30517578125]
['model.relu.alpha_mask_13_0', 2048, 1171.0, 0.57177734375]
['model.relu.alpha_mask_14_0', 2048, 1209.0, 0.59033203125]
['model.relu.alpha_mask_15_0', 2048, 899.0, 0.43896484375]
['model.relu.alpha_mask_16_0', 2048, 557.0, 0.27197265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49946.0, 0.265083644701087]
########## End ###########
06/02 01:15:09 PM | Train: [ 6/80] Step 000/390 Loss 0.270 Prec@(1,5) (93.8%, 100.0%)
06/02 01:15:09 PM | layerwise density: [16190.0, 4006.0, 4113.0, 4043.0, 4101.0, 2191.0, 2203.0, 2152.0, 2143.0, 1197.0, 1287.0, 1234.0, 1250.0, 1171.0, 1209.0, 899.0, 557.0]
layerwise density percentage: ['0.247', '0.245', '0.251', '0.247', '0.250', '0.267', '0.269', '0.263', '0.262', '0.292', '0.314', '0.301', '0.305', '0.572', '0.590', '0.439', '0.272']
Global density: 0.2650836408138275
06/02 01:15:20 PM | Train: [ 6/80] Step 100/390 Loss 0.311 Prec@(1,5) (91.9%, 99.6%)
06/02 01:15:20 PM | layerwise density: [16137.0, 4003.0, 4099.0, 4040.0, 4085.0, 2197.0, 2206.0, 2155.0, 2146.0, 1204.0, 1293.0, 1246.0, 1260.0, 1190.0, 1234.0, 927.0, 561.0]
layerwise density percentage: ['0.246', '0.244', '0.250', '0.247', '0.249', '0.268', '0.269', '0.263', '0.262', '0.294', '0.316', '0.304', '0.308', '0.581', '0.603', '0.453', '0.274']
Global density: 0.2652800381183624
06/02 01:15:31 PM | Train: [ 6/80] Step 200/390 Loss 0.308 Prec@(1,5) (92.0%, 99.7%)
06/02 01:15:31 PM | layerwise density: [16062.0, 3989.0, 4080.0, 4029.0, 4064.0, 2203.0, 2209.0, 2155.0, 2154.0, 1215.0, 1306.0, 1258.0, 1275.0, 1209.0, 1254.0, 956.0, 565.0]
layerwise density percentage: ['0.245', '0.243', '0.249', '0.246', '0.248', '0.269', '0.270', '0.263', '0.263', '0.297', '0.319', '0.307', '0.311', '0.590', '0.612', '0.467', '0.276']
Global density: 0.2652800381183624
06/02 01:15:42 PM | Train: [ 6/80] Step 300/390 Loss 0.300 Prec@(1,5) (92.2%, 99.7%)
06/02 01:15:42 PM | layerwise density: [15973.0, 3970.0, 4068.0, 4016.0, 4048.0, 2208.0, 2215.0, 2153.0, 2151.0, 1213.0, 1313.0, 1273.0, 1291.0, 1234.0, 1285.0, 976.0, 571.0]
layerwise density percentage: ['0.244', '0.242', '0.248', '0.245', '0.247', '0.270', '0.270', '0.263', '0.263', '0.296', '0.321', '0.311', '0.315', '0.603', '0.627', '0.477', '0.279']
Global density: 0.26514732837677
06/02 01:15:52 PM | Train: [ 6/80] Step 390/390 Loss 0.293 Prec@(1,5) (92.4%, 99.7%)
06/02 01:15:52 PM | layerwise density: [15920.0, 3968.0, 4057.0, 4010.0, 4033.0, 2207.0, 2224.0, 2151.0, 2151.0, 1228.0, 1327.0, 1280.0, 1304.0, 1245.0, 1309.0, 984.0, 579.0]
layerwise density percentage: ['0.243', '0.242', '0.248', '0.245', '0.246', '0.269', '0.271', '0.263', '0.263', '0.300', '0.324', '0.312', '0.318', '0.608', '0.639', '0.480', '0.283']
Global density: 0.26524817943573
06/02 01:15:52 PM | Train: [ 6/200] Final Prec@1 92.3900%
06/02 01:15:52 PM | Valid: [ 6/200] Step 000/078 Loss 1.115 Prec@(1,5) (75.0%, 91.4%)
06/02 01:15:55 PM | Valid: [ 6/200] Step 078/078 Loss 1.465 Prec@(1,5) (66.0%, 87.0%)
06/02 01:15:55 PM | Valid: [ 6/200] Final Prec@1 65.9500%
06/02 01:15:56 PM | Current mask training best Prec@1 = 65.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 15920.0, 0.242919921875]
['model.relu.alpha_mask_1_0', 16384, 3968.0, 0.2421875]
['model.relu.alpha_mask_2_0', 16384, 4057.0, 0.24761962890625]
['model.relu.alpha_mask_3_0', 16384, 4010.0, 0.2447509765625]
['model.relu.alpha_mask_4_0', 16384, 4033.0, 0.24615478515625]
['model.relu.alpha_mask_5_0', 8192, 2207.0, 0.2694091796875]
['model.relu.alpha_mask_6_0', 8192, 2224.0, 0.271484375]
['model.relu.alpha_mask_7_0', 8192, 2151.0, 0.2625732421875]
['model.relu.alpha_mask_8_0', 8192, 2151.0, 0.2625732421875]
['model.relu.alpha_mask_9_0', 4096, 1228.0, 0.2998046875]
['model.relu.alpha_mask_10_0', 4096, 1327.0, 0.323974609375]
['model.relu.alpha_mask_11_0', 4096, 1280.0, 0.3125]
['model.relu.alpha_mask_12_0', 4096, 1304.0, 0.318359375]
['model.relu.alpha_mask_13_0', 2048, 1245.0, 0.60791015625]
['model.relu.alpha_mask_14_0', 2048, 1309.0, 0.63916015625]
['model.relu.alpha_mask_15_0', 2048, 984.0, 0.48046875]
['model.relu.alpha_mask_16_0', 2048, 579.0, 0.28271484375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49977.0, 0.2652481742527174]
########## End ###########
06/02 01:15:57 PM | Train: [ 7/80] Step 000/390 Loss 0.151 Prec@(1,5) (96.9%, 100.0%)
06/02 01:15:57 PM | layerwise density: [15920.0, 3968.0, 4057.0, 4010.0, 4033.0, 2207.0, 2224.0, 2151.0, 2151.0, 1228.0, 1327.0, 1280.0, 1304.0, 1245.0, 1309.0, 984.0, 579.0]
layerwise density percentage: ['0.243', '0.242', '0.248', '0.245', '0.246', '0.269', '0.271', '0.263', '0.263', '0.300', '0.324', '0.312', '0.318', '0.608', '0.639', '0.480', '0.283']
Global density: 0.26524817943573
06/02 01:16:07 PM | Train: [ 7/80] Step 100/390 Loss 0.189 Prec@(1,5) (95.6%, 99.9%)
06/02 01:16:07 PM | layerwise density: [15866.0, 3966.0, 4042.0, 4010.0, 4029.0, 2208.0, 2224.0, 2156.0, 2155.0, 1237.0, 1340.0, 1292.0, 1314.0, 1261.0, 1320.0, 995.0, 584.0]
layerwise density percentage: ['0.242', '0.242', '0.247', '0.245', '0.246', '0.270', '0.271', '0.263', '0.263', '0.302', '0.327', '0.315', '0.321', '0.616', '0.645', '0.486', '0.285']
Global density: 0.26536494493484497
06/02 01:16:18 PM | Train: [ 7/80] Step 200/390 Loss 0.189 Prec@(1,5) (95.5%, 99.9%)
06/02 01:16:18 PM | layerwise density: [15796.0, 3946.0, 4025.0, 3996.0, 4016.0, 2211.0, 2214.0, 2156.0, 2157.0, 1241.0, 1342.0, 1299.0, 1324.0, 1265.0, 1350.0, 1003.0, 587.0]
layerwise density percentage: ['0.241', '0.241', '0.246', '0.244', '0.245', '0.270', '0.270', '0.263', '0.263', '0.303', '0.328', '0.317', '0.323', '0.618', '0.659', '0.490', '0.287']
Global density: 0.26498812437057495
06/02 01:16:29 PM | Train: [ 7/80] Step 300/390 Loss 0.192 Prec@(1,5) (95.4%, 99.9%)
06/02 01:16:29 PM | layerwise density: [15686.0, 3929.0, 4001.0, 3976.0, 3988.0, 2217.0, 2229.0, 2156.0, 2152.0, 1252.0, 1346.0, 1301.0, 1324.0, 1283.0, 1371.0, 1010.0, 597.0]
layerwise density percentage: ['0.239', '0.240', '0.244', '0.243', '0.243', '0.271', '0.272', '0.263', '0.263', '0.306', '0.329', '0.318', '0.323', '0.626', '0.669', '0.493', '0.292']
Global density: 0.264404296875
06/02 01:16:39 PM | Train: [ 7/80] Step 390/390 Loss 0.191 Prec@(1,5) (95.5%, 99.9%)
06/02 01:16:39 PM | layerwise density: [15639.0, 3926.0, 3991.0, 3973.0, 3974.0, 2223.0, 2214.0, 2160.0, 2154.0, 1253.0, 1355.0, 1306.0, 1335.0, 1297.0, 1388.0, 1020.0, 603.0]
layerwise density percentage: ['0.239', '0.240', '0.244', '0.242', '0.243', '0.271', '0.270', '0.264', '0.263', '0.306', '0.331', '0.319', '0.326', '0.633', '0.678', '0.498', '0.294']
Global density: 0.26436716318130493
06/02 01:16:39 PM | Train: [ 7/200] Final Prec@1 95.4780%
06/02 01:16:39 PM | Valid: [ 7/200] Step 000/078 Loss 1.197 Prec@(1,5) (74.2%, 93.0%)
06/02 01:16:41 PM | Valid: [ 7/200] Step 078/078 Loss 1.466 Prec@(1,5) (66.2%, 87.4%)
06/02 01:16:41 PM | Valid: [ 7/200] Final Prec@1 66.1600%
06/02 01:16:42 PM | Current mask training best Prec@1 = 66.1600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 15639.0, 0.2386322021484375]
['model.relu.alpha_mask_1_0', 16384, 3926.0, 0.2396240234375]
['model.relu.alpha_mask_2_0', 16384, 3991.0, 0.24359130859375]
['model.relu.alpha_mask_3_0', 16384, 3973.0, 0.24249267578125]
['model.relu.alpha_mask_4_0', 16384, 3974.0, 0.2425537109375]
['model.relu.alpha_mask_5_0', 8192, 2223.0, 0.2713623046875]
['model.relu.alpha_mask_6_0', 8192, 2214.0, 0.270263671875]
['model.relu.alpha_mask_7_0', 8192, 2160.0, 0.263671875]
['model.relu.alpha_mask_8_0', 8192, 2154.0, 0.262939453125]
['model.relu.alpha_mask_9_0', 4096, 1253.0, 0.305908203125]
['model.relu.alpha_mask_10_0', 4096, 1355.0, 0.330810546875]
['model.relu.alpha_mask_11_0', 4096, 1306.0, 0.31884765625]
['model.relu.alpha_mask_12_0', 4096, 1335.0, 0.325927734375]
['model.relu.alpha_mask_13_0', 2048, 1297.0, 0.63330078125]
['model.relu.alpha_mask_14_0', 2048, 1389.0, 0.67822265625]
['model.relu.alpha_mask_15_0', 2048, 1020.0, 0.498046875]
['model.relu.alpha_mask_16_0', 2048, 603.0, 0.29443359375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49812.0, 0.26437245244565216]
########## End ###########
06/02 01:16:43 PM | Train: [ 8/80] Step 000/390 Loss 0.109 Prec@(1,5) (97.7%, 100.0%)
06/02 01:16:43 PM | layerwise density: [15639.0, 3926.0, 3991.0, 3973.0, 3974.0, 2223.0, 2214.0, 2160.0, 2154.0, 1253.0, 1355.0, 1306.0, 1335.0, 1297.0, 1389.0, 1020.0, 603.0]
layerwise density percentage: ['0.239', '0.240', '0.244', '0.242', '0.243', '0.271', '0.270', '0.264', '0.263', '0.306', '0.331', '0.319', '0.326', '0.633', '0.678', '0.498', '0.294']
Global density: 0.26437246799468994
06/02 01:16:54 PM | Train: [ 8/80] Step 100/390 Loss 0.137 Prec@(1,5) (97.1%, 99.9%)
06/02 01:16:54 PM | layerwise density: [15620.0, 3924.0, 3979.0, 3979.0, 3970.0, 2231.0, 2219.0, 2167.0, 2169.0, 1273.0, 1369.0, 1310.0, 1348.0, 1310.0, 1398.0, 1027.0, 612.0]
layerwise density percentage: ['0.238', '0.240', '0.243', '0.243', '0.242', '0.272', '0.271', '0.265', '0.265', '0.311', '0.334', '0.320', '0.329', '0.640', '0.683', '0.501', '0.299']
Global density: 0.26486605405807495
06/02 01:17:05 PM | Train: [ 8/80] Step 200/390 Loss 0.140 Prec@(1,5) (97.0%, 99.9%)
06/02 01:17:05 PM | layerwise density: [15578.0, 3928.0, 3968.0, 3977.0, 3974.0, 2240.0, 2233.0, 2176.0, 2163.0, 1283.0, 1381.0, 1324.0, 1362.0, 1320.0, 1418.0, 1037.0, 620.0]
layerwise density percentage: ['0.238', '0.240', '0.242', '0.243', '0.243', '0.273', '0.273', '0.266', '0.264', '0.313', '0.337', '0.323', '0.333', '0.645', '0.692', '0.506', '0.303']
Global density: 0.26527470350265503
06/02 01:17:16 PM | Train: [ 8/80] Step 300/390 Loss 0.139 Prec@(1,5) (97.1%, 99.9%)
06/02 01:17:16 PM | layerwise density: [15510.0, 3918.0, 3958.0, 3973.0, 3968.0, 2247.0, 2241.0, 2176.0, 2163.0, 1293.0, 1381.0, 1330.0, 1370.0, 1323.0, 1432.0, 1047.0, 630.0]
layerwise density percentage: ['0.237', '0.239', '0.242', '0.242', '0.242', '0.274', '0.274', '0.266', '0.264', '0.316', '0.337', '0.325', '0.334', '0.646', '0.699', '0.511', '0.308']
Global density: 0.2651579678058624
06/02 01:17:26 PM | Train: [ 8/80] Step 390/390 Loss 0.141 Prec@(1,5) (97.0%, 99.9%)
06/02 01:17:26 PM | layerwise density: [15454.0, 3914.0, 3949.0, 3971.0, 3958.0, 2253.0, 2238.0, 2179.0, 2166.0, 1295.0, 1383.0, 1335.0, 1365.0, 1337.0, 1446.0, 1058.0, 640.0]
layerwise density percentage: ['0.236', '0.239', '0.241', '0.242', '0.242', '0.275', '0.273', '0.266', '0.264', '0.316', '0.338', '0.326', '0.333', '0.653', '0.706', '0.517', '0.312']
Global density: 0.26505711674690247
06/02 01:17:26 PM | Train: [ 8/200] Final Prec@1 96.9600%
06/02 01:17:26 PM | Valid: [ 8/200] Step 000/078 Loss 1.217 Prec@(1,5) (73.4%, 89.1%)
06/02 01:17:28 PM | Valid: [ 8/200] Step 078/078 Loss 1.464 Prec@(1,5) (66.2%, 87.5%)
06/02 01:17:28 PM | Valid: [ 8/200] Final Prec@1 66.2100%
06/02 01:17:29 PM | Current mask training best Prec@1 = 66.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 15454.0, 0.235809326171875]
['model.relu.alpha_mask_1_0', 16384, 3914.0, 0.2388916015625]
['model.relu.alpha_mask_2_0', 16384, 3949.0, 0.24102783203125]
['model.relu.alpha_mask_3_0', 16384, 3971.0, 0.24237060546875]
['model.relu.alpha_mask_4_0', 16384, 3958.0, 0.2415771484375]
['model.relu.alpha_mask_5_0', 8192, 2253.0, 0.2750244140625]
['model.relu.alpha_mask_6_0', 8192, 2237.0, 0.2730712890625]
['model.relu.alpha_mask_7_0', 8192, 2179.0, 0.2659912109375]
['model.relu.alpha_mask_8_0', 8192, 2166.0, 0.264404296875]
['model.relu.alpha_mask_9_0', 4096, 1295.0, 0.316162109375]
['model.relu.alpha_mask_10_0', 4096, 1383.0, 0.337646484375]
['model.relu.alpha_mask_11_0', 4096, 1335.0, 0.325927734375]
['model.relu.alpha_mask_12_0', 4096, 1365.0, 0.333251953125]
['model.relu.alpha_mask_13_0', 2048, 1337.0, 0.65283203125]
['model.relu.alpha_mask_14_0', 2048, 1446.0, 0.7060546875]
['model.relu.alpha_mask_15_0', 2048, 1059.0, 0.51708984375]
['model.relu.alpha_mask_16_0', 2048, 640.0, 0.3125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49941.0, 0.26505710767663043]
########## End ###########
06/02 01:17:30 PM | Train: [ 9/80] Step 000/390 Loss 0.059 Prec@(1,5) (99.2%, 100.0%)
06/02 01:17:30 PM | layerwise density: [15454.0, 3914.0, 3949.0, 3971.0, 3958.0, 2253.0, 2237.0, 2179.0, 2166.0, 1295.0, 1383.0, 1335.0, 1365.0, 1337.0, 1446.0, 1059.0, 640.0]
layerwise density percentage: ['0.236', '0.239', '0.241', '0.242', '0.242', '0.275', '0.273', '0.266', '0.264', '0.316', '0.338', '0.326', '0.333', '0.653', '0.706', '0.517', '0.312']
Global density: 0.26505711674690247
06/02 01:17:40 PM | Train: [ 9/80] Step 100/390 Loss 0.106 Prec@(1,5) (97.9%, 100.0%)
06/02 01:17:40 PM | layerwise density: [15392.0, 3905.0, 3943.0, 3968.0, 3944.0, 2251.0, 2238.0, 2180.0, 2169.0, 1296.0, 1391.0, 1344.0, 1381.0, 1349.0, 1454.0, 1071.0, 649.0]
layerwise density percentage: ['0.235', '0.238', '0.241', '0.242', '0.241', '0.275', '0.273', '0.266', '0.265', '0.316', '0.340', '0.328', '0.337', '0.659', '0.710', '0.523', '0.317']
Global density: 0.26497218012809753
06/02 01:17:51 PM | Train: [ 9/80] Step 200/390 Loss 0.110 Prec@(1,5) (97.8%, 100.0%)
06/02 01:17:51 PM | layerwise density: [15367.0, 3906.0, 3943.0, 3966.0, 3940.0, 2260.0, 2238.0, 2184.0, 2173.0, 1309.0, 1401.0, 1355.0, 1395.0, 1360.0, 1463.0, 1082.0, 654.0]
layerwise density percentage: ['0.234', '0.238', '0.241', '0.242', '0.240', '0.276', '0.273', '0.267', '0.265', '0.320', '0.342', '0.331', '0.341', '0.664', '0.714', '0.528', '0.319']
Global density: 0.26534903049468994
06/02 01:18:02 PM | Train: [ 9/80] Step 300/390 Loss 0.111 Prec@(1,5) (97.7%, 100.0%)
06/02 01:18:02 PM | layerwise density: [15302.0, 3899.0, 3923.0, 3961.0, 3929.0, 2267.0, 2244.0, 2183.0, 2178.0, 1315.0, 1403.0, 1363.0, 1391.0, 1364.0, 1468.0, 1092.0, 661.0]
layerwise density percentage: ['0.233', '0.238', '0.239', '0.242', '0.240', '0.277', '0.274', '0.266', '0.266', '0.321', '0.343', '0.333', '0.340', '0.666', '0.717', '0.533', '0.323']
Global density: 0.2650677263736725
06/02 01:18:12 PM | Train: [ 9/80] Step 390/390 Loss 0.110 Prec@(1,5) (97.8%, 100.0%)
06/02 01:18:12 PM | layerwise density: [15241.0, 3896.0, 3916.0, 3951.0, 3911.0, 2272.0, 2241.0, 2184.0, 2178.0, 1324.0, 1412.0, 1372.0, 1404.0, 1379.0, 1482.0, 1100.0, 674.0]
layerwise density percentage: ['0.233', '0.238', '0.239', '0.241', '0.239', '0.277', '0.274', '0.267', '0.266', '0.323', '0.345', '0.335', '0.343', '0.673', '0.724', '0.537', '0.329']
Global density: 0.2650358974933624
06/02 01:18:12 PM | Train: [ 9/200] Final Prec@1 97.7620%
06/02 01:18:12 PM | Valid: [ 9/200] Step 000/078 Loss 1.242 Prec@(1,5) (73.4%, 90.6%)
06/02 01:18:15 PM | Valid: [ 9/200] Step 078/078 Loss 1.431 Prec@(1,5) (66.6%, 87.6%)
06/02 01:18:15 PM | Valid: [ 9/200] Final Prec@1 66.6000%
06/02 01:18:15 PM | Current mask training best Prec@1 = 66.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 15241.0, 0.2325592041015625]
['model.relu.alpha_mask_1_0', 16384, 3896.0, 0.23779296875]
['model.relu.alpha_mask_2_0', 16384, 3916.0, 0.239013671875]
['model.relu.alpha_mask_3_0', 16384, 3952.0, 0.2412109375]
['model.relu.alpha_mask_4_0', 16384, 3911.0, 0.23870849609375]
['model.relu.alpha_mask_5_0', 8192, 2273.0, 0.2774658203125]
['model.relu.alpha_mask_6_0', 8192, 2241.0, 0.2735595703125]
['model.relu.alpha_mask_7_0', 8192, 2185.0, 0.2667236328125]
['model.relu.alpha_mask_8_0', 8192, 2177.0, 0.2657470703125]
['model.relu.alpha_mask_9_0', 4096, 1324.0, 0.3232421875]
['model.relu.alpha_mask_10_0', 4096, 1412.0, 0.3447265625]
['model.relu.alpha_mask_11_0', 4096, 1372.0, 0.3349609375]
['model.relu.alpha_mask_12_0', 4096, 1403.0, 0.342529296875]
['model.relu.alpha_mask_13_0', 2048, 1379.0, 0.67333984375]
['model.relu.alpha_mask_14_0', 2048, 1482.0, 0.7236328125]
['model.relu.alpha_mask_15_0', 2048, 1100.0, 0.537109375]
['model.relu.alpha_mask_16_0', 2048, 675.0, 0.32958984375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49939.0, 0.26504649286684784]
########## End ###########
06/02 01:18:16 PM | Train: [10/80] Step 000/390 Loss 0.094 Prec@(1,5) (98.4%, 100.0%)
06/02 01:18:16 PM | layerwise density: [15241.0, 3896.0, 3916.0, 3952.0, 3911.0, 2273.0, 2241.0, 2185.0, 2177.0, 1324.0, 1412.0, 1372.0, 1403.0, 1379.0, 1482.0, 1100.0, 675.0]
layerwise density percentage: ['0.233', '0.238', '0.239', '0.241', '0.239', '0.277', '0.274', '0.267', '0.266', '0.323', '0.345', '0.335', '0.343', '0.673', '0.724', '0.537', '0.330']
Global density: 0.26504650712013245
06/02 01:18:27 PM | Train: [10/80] Step 100/390 Loss 0.085 Prec@(1,5) (98.5%, 100.0%)
06/02 01:18:27 PM | layerwise density: [15143.0, 3886.0, 3888.0, 3938.0, 3898.0, 2289.0, 2236.0, 2185.0, 2159.0, 1326.0, 1416.0, 1380.0, 1403.0, 1380.0, 1485.0, 1107.0, 686.0]
layerwise density percentage: ['0.231', '0.237', '0.237', '0.240', '0.238', '0.279', '0.273', '0.267', '0.264', '0.324', '0.346', '0.337', '0.343', '0.674', '0.725', '0.541', '0.335']
Global density: 0.2643353044986725
06/02 01:18:38 PM | Train: [10/80] Step 200/390 Loss 0.089 Prec@(1,5) (98.3%, 100.0%)
06/02 01:18:38 PM | layerwise density: [15105.0, 3867.0, 3867.0, 3928.0, 3885.0, 2276.0, 2229.0, 2184.0, 2152.0, 1327.0, 1418.0, 1384.0, 1404.0, 1398.0, 1495.0, 1115.0, 695.0]
layerwise density percentage: ['0.230', '0.236', '0.236', '0.240', '0.237', '0.278', '0.272', '0.267', '0.263', '0.324', '0.346', '0.338', '0.343', '0.683', '0.730', '0.544', '0.339']
Global density: 0.26393193006515503
06/02 01:18:49 PM | Train: [10/80] Step 300/390 Loss 0.090 Prec@(1,5) (98.3%, 100.0%)
06/02 01:18:49 PM | layerwise density: [15079.0, 3867.0, 3868.0, 3929.0, 3883.0, 2286.0, 2242.0, 2188.0, 2162.0, 1338.0, 1434.0, 1392.0, 1410.0, 1402.0, 1509.0, 1129.0, 700.0]
layerwise density percentage: ['0.230', '0.236', '0.236', '0.240', '0.237', '0.279', '0.274', '0.267', '0.264', '0.327', '0.350', '0.340', '0.344', '0.685', '0.737', '0.551', '0.342']
Global density: 0.264404296875
06/02 01:18:59 PM | Train: [10/80] Step 390/390 Loss 0.091 Prec@(1,5) (98.2%, 100.0%)
06/02 01:18:59 PM | layerwise density: [15058.0, 3869.0, 3871.0, 3940.0, 3892.0, 2304.0, 2259.0, 2199.0, 2175.0, 1351.0, 1447.0, 1395.0, 1426.0, 1404.0, 1518.0, 1142.0, 712.0]
layerwise density percentage: ['0.230', '0.236', '0.236', '0.240', '0.238', '0.281', '0.276', '0.268', '0.266', '0.330', '0.353', '0.341', '0.348', '0.686', '0.741', '0.558', '0.348']
Global density: 0.26516857743263245
06/02 01:18:59 PM | Train: [10/200] Final Prec@1 98.2480%
06/02 01:18:59 PM | Valid: [10/200] Step 000/078 Loss 1.232 Prec@(1,5) (71.9%, 93.0%)
06/02 01:19:02 PM | Valid: [10/200] Step 078/078 Loss 1.446 Prec@(1,5) (66.7%, 87.3%)
06/02 01:19:02 PM | Valid: [10/200] Final Prec@1 66.6600%
06/02 01:19:02 PM | Current mask training best Prec@1 = 66.6600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 15058.0, 0.229766845703125]
['model.relu.alpha_mask_1_0', 16384, 3869.0, 0.23614501953125]
['model.relu.alpha_mask_2_0', 16384, 3871.0, 0.23626708984375]
['model.relu.alpha_mask_3_0', 16384, 3940.0, 0.240478515625]
['model.relu.alpha_mask_4_0', 16384, 3892.0, 0.237548828125]
['model.relu.alpha_mask_5_0', 8192, 2304.0, 0.28125]
['model.relu.alpha_mask_6_0', 8192, 2260.0, 0.27587890625]
['model.relu.alpha_mask_7_0', 8192, 2199.0, 0.2684326171875]
['model.relu.alpha_mask_8_0', 8192, 2176.0, 0.265625]
['model.relu.alpha_mask_9_0', 4096, 1352.0, 0.330078125]
['model.relu.alpha_mask_10_0', 4096, 1446.0, 0.35302734375]
['model.relu.alpha_mask_11_0', 4096, 1395.0, 0.340576171875]
['model.relu.alpha_mask_12_0', 4096, 1427.0, 0.348388671875]
['model.relu.alpha_mask_13_0', 2048, 1404.0, 0.685546875]
['model.relu.alpha_mask_14_0', 2048, 1518.0, 0.7412109375]
['model.relu.alpha_mask_15_0', 2048, 1143.0, 0.55810546875]
['model.relu.alpha_mask_16_0', 2048, 712.0, 0.34765625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49966.0, 0.265189792798913]
########## End ###########
06/02 01:19:03 PM | Train: [11/80] Step 000/390 Loss 0.059 Prec@(1,5) (99.2%, 100.0%)
06/02 01:19:03 PM | layerwise density: [15058.0, 3869.0, 3871.0, 3940.0, 3892.0, 2304.0, 2260.0, 2199.0, 2176.0, 1352.0, 1446.0, 1395.0, 1427.0, 1404.0, 1518.0, 1143.0, 712.0]
layerwise density percentage: ['0.230', '0.236', '0.236', '0.240', '0.238', '0.281', '0.276', '0.268', '0.266', '0.330', '0.353', '0.341', '0.348', '0.686', '0.741', '0.558', '0.348']
Global density: 0.2651897966861725
06/02 01:19:14 PM | Train: [11/80] Step 100/390 Loss 0.073 Prec@(1,5) (98.7%, 100.0%)
06/02 01:19:14 PM | layerwise density: [14992.0, 3867.0, 3865.0, 3938.0, 3889.0, 2313.0, 2256.0, 2210.0, 2178.0, 1354.0, 1449.0, 1400.0, 1426.0, 1413.0, 1529.0, 1147.0, 722.0]
layerwise density percentage: ['0.229', '0.236', '0.236', '0.240', '0.237', '0.282', '0.275', '0.270', '0.266', '0.331', '0.354', '0.342', '0.348', '0.690', '0.747', '0.560', '0.353']
Global density: 0.26509425044059753
06/02 01:19:25 PM | Train: [11/80] Step 200/390 Loss 0.075 Prec@(1,5) (98.7%, 100.0%)
06/02 01:19:25 PM | layerwise density: [14918.0, 3861.0, 3848.0, 3926.0, 3875.0, 2302.0, 2262.0, 2209.0, 2184.0, 1363.0, 1446.0, 1409.0, 1426.0, 1409.0, 1536.0, 1152.0, 731.0]
layerwise density percentage: ['0.228', '0.236', '0.235', '0.240', '0.237', '0.281', '0.276', '0.270', '0.267', '0.333', '0.353', '0.344', '0.348', '0.688', '0.750', '0.562', '0.357']
Global density: 0.26461130380630493
06/02 01:19:36 PM | Train: [11/80] Step 300/390 Loss 0.077 Prec@(1,5) (98.6%, 100.0%)
06/02 01:19:36 PM | layerwise density: [14891.0, 3861.0, 3839.0, 3920.0, 3863.0, 2319.0, 2261.0, 2221.0, 2178.0, 1374.0, 1451.0, 1416.0, 1441.0, 1431.0, 1542.0, 1164.0, 740.0]
layerwise density percentage: ['0.227', '0.236', '0.234', '0.239', '0.236', '0.283', '0.276', '0.271', '0.266', '0.335', '0.354', '0.346', '0.352', '0.699', '0.753', '0.568', '0.361']
Global density: 0.26490318775177
06/02 01:19:46 PM | Train: [11/80] Step 390/390 Loss 0.077 Prec@(1,5) (98.6%, 100.0%)
06/02 01:19:46 PM | layerwise density: [14820.0, 3859.0, 3829.0, 3901.0, 3850.0, 2325.0, 2262.0, 2228.0, 2189.0, 1372.0, 1450.0, 1425.0, 1447.0, 1435.0, 1563.0, 1169.0, 750.0]
layerwise density percentage: ['0.226', '0.236', '0.234', '0.238', '0.235', '0.284', '0.276', '0.272', '0.267', '0.335', '0.354', '0.348', '0.353', '0.701', '0.763', '0.571', '0.366']
Global density: 0.2647015154361725
06/02 01:19:46 PM | Train: [11/200] Final Prec@1 98.5800%
06/02 01:19:46 PM | Valid: [11/200] Step 000/078 Loss 1.362 Prec@(1,5) (71.9%, 91.4%)
06/02 01:19:48 PM | Valid: [11/200] Step 078/078 Loss 1.464 Prec@(1,5) (66.3%, 88.0%)
06/02 01:19:48 PM | Valid: [11/200] Final Prec@1 66.2700%
06/02 01:19:48 PM | Current mask training best Prec@1 = 66.6600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 14818.0, 0.226104736328125]
['model.relu.alpha_mask_1_0', 16384, 3859.0, 0.23553466796875]
['model.relu.alpha_mask_2_0', 16384, 3828.0, 0.233642578125]
['model.relu.alpha_mask_3_0', 16384, 3900.0, 0.238037109375]
['model.relu.alpha_mask_4_0', 16384, 3850.0, 0.2349853515625]
['model.relu.alpha_mask_5_0', 8192, 2323.0, 0.2835693359375]
['model.relu.alpha_mask_6_0', 8192, 2263.0, 0.2762451171875]
['model.relu.alpha_mask_7_0', 8192, 2228.0, 0.27197265625]
['model.relu.alpha_mask_8_0', 8192, 2188.0, 0.26708984375]
['model.relu.alpha_mask_9_0', 4096, 1373.0, 0.335205078125]
['model.relu.alpha_mask_10_0', 4096, 1449.0, 0.353759765625]
['model.relu.alpha_mask_11_0', 4096, 1425.0, 0.347900390625]
['model.relu.alpha_mask_12_0', 4096, 1448.0, 0.353515625]
['model.relu.alpha_mask_13_0', 2048, 1434.0, 0.7001953125]
['model.relu.alpha_mask_14_0', 2048, 1562.0, 0.7626953125]
['model.relu.alpha_mask_15_0', 2048, 1170.0, 0.5712890625]
['model.relu.alpha_mask_16_0', 2048, 750.0, 0.3662109375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49868.0, 0.26466966711956524]
########## End ###########
06/02 01:19:49 PM | Train: [12/80] Step 000/390 Loss 0.061 Prec@(1,5) (100.0%, 100.0%)
06/02 01:19:49 PM | layerwise density: [14818.0, 3859.0, 3828.0, 3900.0, 3850.0, 2323.0, 2263.0, 2228.0, 2188.0, 1373.0, 1449.0, 1425.0, 1448.0, 1434.0, 1562.0, 1170.0, 750.0]
layerwise density percentage: ['0.226', '0.236', '0.234', '0.238', '0.235', '0.284', '0.276', '0.272', '0.267', '0.335', '0.354', '0.348', '0.354', '0.700', '0.763', '0.571', '0.366']
Global density: 0.2646696865558624
06/02 01:20:00 PM | Train: [12/80] Step 100/390 Loss 0.068 Prec@(1,5) (98.8%, 100.0%)
06/02 01:20:00 PM | layerwise density: [14785.0, 3861.0, 3813.0, 3898.0, 3840.0, 2333.0, 2266.0, 2230.0, 2178.0, 1365.0, 1458.0, 1431.0, 1443.0, 1443.0, 1573.0, 1178.0, 764.0]
layerwise density percentage: ['0.226', '0.236', '0.233', '0.238', '0.234', '0.285', '0.277', '0.272', '0.266', '0.333', '0.356', '0.349', '0.352', '0.705', '0.768', '0.575', '0.373']
Global density: 0.26462191343307495
06/02 01:20:11 PM | Train: [12/80] Step 200/390 Loss 0.068 Prec@(1,5) (98.8%, 100.0%)
06/02 01:20:11 PM | layerwise density: [14758.0, 3875.0, 3817.0, 3897.0, 3841.0, 2343.0, 2280.0, 2239.0, 2189.0, 1380.0, 1463.0, 1439.0, 1462.0, 1448.0, 1577.0, 1190.0, 778.0]
layerwise density percentage: ['0.225', '0.237', '0.233', '0.238', '0.234', '0.286', '0.278', '0.273', '0.267', '0.337', '0.357', '0.351', '0.357', '0.707', '0.770', '0.581', '0.380']
Global density: 0.26524287462234497
06/02 01:20:22 PM | Train: [12/80] Step 300/390 Loss 0.068 Prec@(1,5) (98.8%, 100.0%)
06/02 01:20:22 PM | layerwise density: [14668.0, 3866.0, 3801.0, 3886.0, 3828.0, 2328.0, 2275.0, 2239.0, 2194.0, 1381.0, 1459.0, 1445.0, 1464.0, 1452.0, 1589.0, 1200.0, 787.0]
layerwise density percentage: ['0.224', '0.236', '0.232', '0.237', '0.234', '0.284', '0.278', '0.273', '0.268', '0.337', '0.356', '0.353', '0.357', '0.709', '0.776', '0.586', '0.384']
Global density: 0.26463782787323
06/02 01:20:32 PM | Train: [12/80] Step 390/390 Loss 0.068 Prec@(1,5) (98.8%, 100.0%)
06/02 01:20:32 PM | layerwise density: [14643.0, 3869.0, 3797.0, 3884.0, 3819.0, 2347.0, 2285.0, 2255.0, 2203.0, 1391.0, 1473.0, 1450.0, 1480.0, 1466.0, 1608.0, 1205.0, 797.0]
layerwise density percentage: ['0.223', '0.236', '0.232', '0.237', '0.233', '0.286', '0.279', '0.275', '0.269', '0.340', '0.360', '0.354', '0.361', '0.716', '0.785', '0.588', '0.389']
Global density: 0.26522165536880493
06/02 01:20:32 PM | Train: [12/200] Final Prec@1 98.7660%
06/02 01:20:32 PM | Valid: [12/200] Step 000/078 Loss 1.148 Prec@(1,5) (73.4%, 91.4%)
06/02 01:20:35 PM | Valid: [12/200] Step 078/078 Loss 1.425 Prec@(1,5) (66.9%, 87.8%)
06/02 01:20:35 PM | Valid: [12/200] Final Prec@1 66.8700%
06/02 01:20:35 PM | Current mask training best Prec@1 = 66.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 14643.0, 0.2234344482421875]
['model.relu.alpha_mask_1_0', 16384, 3869.0, 0.23614501953125]
['model.relu.alpha_mask_2_0', 16384, 3797.0, 0.23175048828125]
['model.relu.alpha_mask_3_0', 16384, 3884.0, 0.237060546875]
['model.relu.alpha_mask_4_0', 16384, 3819.0, 0.23309326171875]
['model.relu.alpha_mask_5_0', 8192, 2347.0, 0.2864990234375]
['model.relu.alpha_mask_6_0', 8192, 2285.0, 0.2789306640625]
['model.relu.alpha_mask_7_0', 8192, 2255.0, 0.2752685546875]
['model.relu.alpha_mask_8_0', 8192, 2203.0, 0.2689208984375]
['model.relu.alpha_mask_9_0', 4096, 1391.0, 0.339599609375]
['model.relu.alpha_mask_10_0', 4096, 1473.0, 0.359619140625]
['model.relu.alpha_mask_11_0', 4096, 1450.0, 0.35400390625]
['model.relu.alpha_mask_12_0', 4096, 1480.0, 0.361328125]
['model.relu.alpha_mask_13_0', 2048, 1466.0, 0.7158203125]
['model.relu.alpha_mask_14_0', 2048, 1608.0, 0.78515625]
['model.relu.alpha_mask_15_0', 2048, 1205.0, 0.58837890625]
['model.relu.alpha_mask_16_0', 2048, 797.0, 0.38916015625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49972.0, 0.26522163722826086]
########## End ###########
06/02 01:20:36 PM | Train: [13/80] Step 000/390 Loss 0.067 Prec@(1,5) (99.2%, 100.0%)
06/02 01:20:36 PM | layerwise density: [14643.0, 3869.0, 3797.0, 3884.0, 3819.0, 2347.0, 2285.0, 2255.0, 2203.0, 1391.0, 1473.0, 1450.0, 1480.0, 1466.0, 1608.0, 1205.0, 797.0]
layerwise density percentage: ['0.223', '0.236', '0.232', '0.237', '0.233', '0.286', '0.279', '0.275', '0.269', '0.340', '0.360', '0.354', '0.361', '0.716', '0.785', '0.588', '0.389']
Global density: 0.26522165536880493
06/02 01:20:47 PM | Train: [13/80] Step 100/390 Loss 0.057 Prec@(1,5) (99.1%, 100.0%)
06/02 01:20:47 PM | layerwise density: [14548.0, 3852.0, 3774.0, 3867.0, 3812.0, 2346.0, 2273.0, 2246.0, 2193.0, 1396.0, 1464.0, 1457.0, 1463.0, 1466.0, 1609.0, 1210.0, 805.0]
layerwise density percentage: ['0.222', '0.235', '0.230', '0.236', '0.233', '0.286', '0.277', '0.274', '0.268', '0.341', '0.357', '0.356', '0.357', '0.716', '0.786', '0.591', '0.393']
Global density: 0.2642079293727875
06/02 01:20:58 PM | Train: [13/80] Step 200/390 Loss 0.059 Prec@(1,5) (99.0%, 100.0%)
06/02 01:20:58 PM | layerwise density: [14525.0, 3844.0, 3758.0, 3872.0, 3810.0, 2346.0, 2285.0, 2250.0, 2203.0, 1400.0, 1485.0, 1465.0, 1479.0, 1468.0, 1618.0, 1217.0, 822.0]
layerwise density percentage: ['0.222', '0.235', '0.229', '0.236', '0.233', '0.286', '0.279', '0.275', '0.269', '0.342', '0.363', '0.358', '0.361', '0.717', '0.790', '0.594', '0.401']
Global density: 0.26455822587013245
06/02 01:21:09 PM | Train: [13/80] Step 300/390 Loss 0.061 Prec@(1,5) (99.0%, 100.0%)
06/02 01:21:09 PM | layerwise density: [14497.0, 3854.0, 3758.0, 3873.0, 3813.0, 2364.0, 2292.0, 2258.0, 2224.0, 1423.0, 1494.0, 1478.0, 1493.0, 1478.0, 1635.0, 1224.0, 831.0]
layerwise density percentage: ['0.221', '0.235', '0.229', '0.236', '0.233', '0.289', '0.280', '0.276', '0.271', '0.347', '0.365', '0.361', '0.365', '0.722', '0.798', '0.598', '0.406']
Global density: 0.2653118669986725
06/02 01:21:19 PM | Train: [13/80] Step 390/390 Loss 0.061 Prec@(1,5) (99.0%, 100.0%)
06/02 01:21:19 PM | layerwise density: [14390.0, 3832.0, 3740.0, 3855.0, 3787.0, 2351.0, 2272.0, 2250.0, 2226.0, 1429.0, 1478.0, 1486.0, 1495.0, 1482.0, 1645.0, 1230.0, 837.0]
layerwise density percentage: ['0.220', '0.234', '0.228', '0.235', '0.231', '0.287', '0.277', '0.275', '0.272', '0.349', '0.361', '0.363', '0.365', '0.724', '0.803', '0.601', '0.409']
Global density: 0.2642291486263275
06/02 01:21:19 PM | Train: [13/200] Final Prec@1 98.9660%
06/02 01:21:20 PM | Valid: [13/200] Step 000/078 Loss 1.232 Prec@(1,5) (71.9%, 91.4%)
06/02 01:21:22 PM | Valid: [13/200] Step 078/078 Loss 1.406 Prec@(1,5) (67.4%, 88.0%)
06/02 01:21:22 PM | Valid: [13/200] Final Prec@1 67.4400%
06/02 01:21:23 PM | Current mask training best Prec@1 = 67.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 14390.0, 0.219573974609375]
['model.relu.alpha_mask_1_0', 16384, 3832.0, 0.23388671875]
['model.relu.alpha_mask_2_0', 16384, 3740.0, 0.228271484375]
['model.relu.alpha_mask_3_0', 16384, 3855.0, 0.23529052734375]
['model.relu.alpha_mask_4_0', 16384, 3787.0, 0.23114013671875]
['model.relu.alpha_mask_5_0', 8192, 2351.0, 0.2869873046875]
['model.relu.alpha_mask_6_0', 8192, 2272.0, 0.27734375]
['model.relu.alpha_mask_7_0', 8192, 2250.0, 0.274658203125]
['model.relu.alpha_mask_8_0', 8192, 2226.0, 0.271728515625]
['model.relu.alpha_mask_9_0', 4096, 1429.0, 0.348876953125]
['model.relu.alpha_mask_10_0', 4096, 1478.0, 0.36083984375]
['model.relu.alpha_mask_11_0', 4096, 1486.0, 0.36279296875]
['model.relu.alpha_mask_12_0', 4096, 1495.0, 0.364990234375]
['model.relu.alpha_mask_13_0', 2048, 1482.0, 0.7236328125]
['model.relu.alpha_mask_14_0', 2048, 1645.0, 0.80322265625]
['model.relu.alpha_mask_15_0', 2048, 1230.0, 0.6005859375]
['model.relu.alpha_mask_16_0', 2048, 837.0, 0.40869140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49785.0, 0.264229152513587]
########## End ###########
06/02 01:21:24 PM | Train: [14/80] Step 000/390 Loss 0.050 Prec@(1,5) (99.2%, 100.0%)
06/02 01:21:24 PM | layerwise density: [14390.0, 3832.0, 3740.0, 3855.0, 3787.0, 2351.0, 2272.0, 2250.0, 2226.0, 1429.0, 1478.0, 1486.0, 1495.0, 1482.0, 1645.0, 1230.0, 837.0]
layerwise density percentage: ['0.220', '0.234', '0.228', '0.235', '0.231', '0.287', '0.277', '0.275', '0.272', '0.349', '0.361', '0.363', '0.365', '0.724', '0.803', '0.601', '0.409']
Global density: 0.2642291486263275
06/02 01:21:34 PM | Train: [14/80] Step 100/390 Loss 0.051 Prec@(1,5) (99.2%, 100.0%)
06/02 01:21:34 PM | layerwise density: [14352.0, 3828.0, 3736.0, 3859.0, 3788.0, 2355.0, 2297.0, 2254.0, 2224.0, 1424.0, 1488.0, 1496.0, 1492.0, 1487.0, 1662.0, 1236.0, 849.0]
layerwise density percentage: ['0.219', '0.234', '0.228', '0.236', '0.231', '0.287', '0.280', '0.275', '0.271', '0.348', '0.363', '0.365', '0.364', '0.726', '0.812', '0.604', '0.415']
Global density: 0.2644520699977875
06/02 01:21:45 PM | Train: [14/80] Step 200/390 Loss 0.053 Prec@(1,5) (99.1%, 100.0%)
06/02 01:21:45 PM | layerwise density: [14346.0, 3842.0, 3752.0, 3875.0, 3802.0, 2372.0, 2308.0, 2264.0, 2235.0, 1434.0, 1498.0, 1505.0, 1504.0, 1494.0, 1661.0, 1250.0, 857.0]
layerwise density percentage: ['0.219', '0.234', '0.229', '0.237', '0.232', '0.290', '0.282', '0.276', '0.273', '0.350', '0.366', '0.367', '0.367', '0.729', '0.811', '0.610', '0.418']
Global density: 0.26536494493484497
06/02 01:21:56 PM | Train: [14/80] Step 300/390 Loss 0.054 Prec@(1,5) (99.1%, 100.0%)
06/02 01:21:56 PM | layerwise density: [14236.0, 3821.0, 3717.0, 3851.0, 3772.0, 2363.0, 2303.0, 2265.0, 2218.0, 1440.0, 1496.0, 1504.0, 1513.0, 1501.0, 1661.0, 1256.0, 865.0]
layerwise density percentage: ['0.217', '0.233', '0.227', '0.235', '0.230', '0.288', '0.281', '0.276', '0.271', '0.352', '0.365', '0.367', '0.369', '0.733', '0.811', '0.613', '0.422']
Global density: 0.2642132341861725
06/02 01:22:06 PM | Train: [14/80] Step 390/390 Loss 0.055 Prec@(1,5) (99.1%, 100.0%)
06/02 01:22:06 PM | layerwise density: [14215.0, 3825.0, 3715.0, 3850.0, 3777.0, 2377.0, 2301.0, 2277.0, 2225.0, 1452.0, 1505.0, 1517.0, 1518.0, 1503.0, 1677.0, 1261.0, 881.0]
layerwise density percentage: ['0.217', '0.233', '0.227', '0.235', '0.231', '0.290', '0.281', '0.278', '0.272', '0.354', '0.367', '0.370', '0.371', '0.734', '0.819', '0.616', '0.430']
Global density: 0.2647121250629425
06/02 01:22:06 PM | Train: [14/200] Final Prec@1 99.0700%
06/02 01:22:06 PM | Valid: [14/200] Step 000/078 Loss 1.217 Prec@(1,5) (71.9%, 89.1%)
06/02 01:22:08 PM | Valid: [14/200] Step 078/078 Loss 1.407 Prec@(1,5) (67.9%, 88.0%)
06/02 01:22:09 PM | Valid: [14/200] Final Prec@1 67.9400%
06/02 01:22:09 PM | Current mask training best Prec@1 = 67.9400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 14215.0, 0.2169036865234375]
['model.relu.alpha_mask_1_0', 16384, 3825.0, 0.23345947265625]
['model.relu.alpha_mask_2_0', 16384, 3715.0, 0.22674560546875]
['model.relu.alpha_mask_3_0', 16384, 3850.0, 0.2349853515625]
['model.relu.alpha_mask_4_0', 16384, 3777.0, 0.23052978515625]
['model.relu.alpha_mask_5_0', 8192, 2377.0, 0.2901611328125]
['model.relu.alpha_mask_6_0', 8192, 2300.0, 0.28076171875]
['model.relu.alpha_mask_7_0', 8192, 2277.0, 0.2779541015625]
['model.relu.alpha_mask_8_0', 8192, 2226.0, 0.271728515625]
['model.relu.alpha_mask_9_0', 4096, 1451.0, 0.354248046875]
['model.relu.alpha_mask_10_0', 4096, 1505.0, 0.367431640625]
['model.relu.alpha_mask_11_0', 4096, 1517.0, 0.370361328125]
['model.relu.alpha_mask_12_0', 4096, 1519.0, 0.370849609375]
['model.relu.alpha_mask_13_0', 2048, 1502.0, 0.7333984375]
['model.relu.alpha_mask_14_0', 2048, 1676.0, 0.818359375]
['model.relu.alpha_mask_15_0', 2048, 1261.0, 0.61572265625]
['model.relu.alpha_mask_16_0', 2048, 882.0, 0.4306640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49875.0, 0.2647068189538043]
########## End ###########
06/02 01:22:10 PM | Train: [15/80] Step 000/390 Loss 0.020 Prec@(1,5) (100.0%, 100.0%)
06/02 01:22:10 PM | layerwise density: [14215.0, 3825.0, 3715.0, 3850.0, 3777.0, 2377.0, 2300.0, 2277.0, 2226.0, 1451.0, 1505.0, 1517.0, 1519.0, 1502.0, 1676.0, 1261.0, 882.0]
layerwise density percentage: ['0.217', '0.233', '0.227', '0.235', '0.231', '0.290', '0.281', '0.278', '0.272', '0.354', '0.367', '0.370', '0.371', '0.733', '0.818', '0.616', '0.431']
Global density: 0.2647068202495575
06/02 01:22:21 PM | Train: [15/80] Step 100/390 Loss 0.046 Prec@(1,5) (99.3%, 100.0%)
06/02 01:22:21 PM | layerwise density: [14114.0, 3805.0, 3709.0, 3849.0, 3748.0, 2384.0, 2303.0, 2285.0, 2224.0, 1454.0, 1512.0, 1520.0, 1515.0, 1511.0, 1684.0, 1268.0, 894.0]
layerwise density percentage: ['0.215', '0.232', '0.226', '0.235', '0.229', '0.291', '0.281', '0.279', '0.271', '0.355', '0.369', '0.371', '0.370', '0.738', '0.822', '0.619', '0.437']
Global density: 0.26419731974601746
06/02 01:22:32 PM | Train: [15/80] Step 200/390 Loss 0.047 Prec@(1,5) (99.3%, 100.0%)
06/02 01:22:32 PM | layerwise density: [14061.0, 3797.0, 3697.0, 3837.0, 3731.0, 2388.0, 2307.0, 2284.0, 2217.0, 1449.0, 1511.0, 1528.0, 1516.0, 1518.0, 1688.0, 1273.0, 906.0]
layerwise density percentage: ['0.215', '0.232', '0.226', '0.234', '0.228', '0.292', '0.282', '0.279', '0.271', '0.354', '0.369', '0.373', '0.370', '0.741', '0.824', '0.622', '0.442']
Global density: 0.26382049918174744
06/02 01:22:43 PM | Train: [15/80] Step 300/390 Loss 0.048 Prec@(1,5) (99.2%, 100.0%)
06/02 01:22:43 PM | layerwise density: [14033.0, 3808.0, 3693.0, 3839.0, 3733.0, 2391.0, 2314.0, 2292.0, 2250.0, 1474.0, 1532.0, 1534.0, 1528.0, 1512.0, 1697.0, 1283.0, 918.0]
layerwise density percentage: ['0.214', '0.232', '0.225', '0.234', '0.228', '0.292', '0.282', '0.280', '0.275', '0.360', '0.374', '0.375', '0.373', '0.738', '0.829', '0.626', '0.448']
Global density: 0.2644732892513275
06/02 01:22:53 PM | Train: [15/80] Step 390/390 Loss 0.048 Prec@(1,5) (99.2%, 100.0%)
06/02 01:22:53 PM | layerwise density: [13955.0, 3799.0, 3678.0, 3852.0, 3725.0, 2408.0, 2326.0, 2295.0, 2251.0, 1484.0, 1535.0, 1544.0, 1531.0, 1528.0, 1704.0, 1286.0, 931.0]
layerwise density percentage: ['0.213', '0.232', '0.224', '0.235', '0.227', '0.294', '0.284', '0.280', '0.275', '0.362', '0.375', '0.377', '0.374', '0.746', '0.832', '0.628', '0.455']
Global density: 0.2644785940647125
06/02 01:22:53 PM | Train: [15/200] Final Prec@1 99.2420%
06/02 01:22:53 PM | Valid: [15/200] Step 000/078 Loss 1.106 Prec@(1,5) (72.7%, 92.2%)
06/02 01:22:55 PM | Valid: [15/200] Step 078/078 Loss 1.426 Prec@(1,5) (67.5%, 87.9%)
06/02 01:22:55 PM | Valid: [15/200] Final Prec@1 67.5500%
06/02 01:22:55 PM | Current mask training best Prec@1 = 67.9400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 13954.0, 0.212921142578125]
['model.relu.alpha_mask_1_0', 16384, 3798.0, 0.2318115234375]
['model.relu.alpha_mask_2_0', 16384, 3677.0, 0.22442626953125]
['model.relu.alpha_mask_3_0', 16384, 3853.0, 0.23516845703125]
['model.relu.alpha_mask_4_0', 16384, 3724.0, 0.227294921875]
['model.relu.alpha_mask_5_0', 8192, 2409.0, 0.2940673828125]
['model.relu.alpha_mask_6_0', 8192, 2325.0, 0.2838134765625]
['model.relu.alpha_mask_7_0', 8192, 2295.0, 0.2801513671875]
['model.relu.alpha_mask_8_0', 8192, 2249.0, 0.2745361328125]
['model.relu.alpha_mask_9_0', 4096, 1484.0, 0.3623046875]
['model.relu.alpha_mask_10_0', 4096, 1535.0, 0.374755859375]
['model.relu.alpha_mask_11_0', 4096, 1544.0, 0.376953125]
['model.relu.alpha_mask_12_0', 4096, 1532.0, 0.3740234375]
['model.relu.alpha_mask_13_0', 2048, 1527.0, 0.74560546875]
['model.relu.alpha_mask_14_0', 2048, 1704.0, 0.83203125]
['model.relu.alpha_mask_15_0', 2048, 1286.0, 0.6279296875]
['model.relu.alpha_mask_16_0', 2048, 931.0, 0.45458984375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49827.0, 0.26445206351902173]
########## End ###########
06/02 01:22:56 PM | Train: [16/80] Step 000/390 Loss 0.036 Prec@(1,5) (100.0%, 100.0%)
06/02 01:22:56 PM | layerwise density: [13954.0, 3798.0, 3677.0, 3853.0, 3724.0, 2409.0, 2325.0, 2295.0, 2249.0, 1484.0, 1535.0, 1544.0, 1532.0, 1527.0, 1704.0, 1286.0, 931.0]
layerwise density percentage: ['0.213', '0.232', '0.224', '0.235', '0.227', '0.294', '0.284', '0.280', '0.275', '0.362', '0.375', '0.377', '0.374', '0.746', '0.832', '0.628', '0.455']
Global density: 0.2644520699977875
06/02 01:23:07 PM | Train: [16/80] Step 100/390 Loss 0.042 Prec@(1,5) (99.4%, 100.0%)
06/02 01:23:07 PM | layerwise density: [13903.0, 3788.0, 3661.0, 3835.0, 3709.0, 2379.0, 2311.0, 2293.0, 2241.0, 1480.0, 1528.0, 1554.0, 1522.0, 1525.0, 1706.0, 1290.0, 945.0]
layerwise density percentage: ['0.212', '0.231', '0.223', '0.234', '0.226', '0.290', '0.282', '0.280', '0.274', '0.361', '0.373', '0.379', '0.372', '0.745', '0.833', '0.630', '0.461']
Global density: 0.2636187970638275
06/02 01:23:18 PM | Train: [16/80] Step 200/390 Loss 0.042 Prec@(1,5) (99.4%, 100.0%)
06/02 01:23:18 PM | layerwise density: [13875.0, 3789.0, 3667.0, 3845.0, 3700.0, 2392.0, 2308.0, 2305.0, 2251.0, 1497.0, 1537.0, 1558.0, 1530.0, 1529.0, 1721.0, 1296.0, 956.0]
layerwise density percentage: ['0.212', '0.231', '0.224', '0.235', '0.226', '0.292', '0.282', '0.281', '0.275', '0.365', '0.375', '0.380', '0.374', '0.747', '0.840', '0.633', '0.467']
Global density: 0.26407524943351746
06/02 01:23:29 PM | Train: [16/80] Step 300/390 Loss 0.042 Prec@(1,5) (99.4%, 100.0%)
06/02 01:23:29 PM | layerwise density: [13859.0, 3798.0, 3668.0, 3863.0, 3696.0, 2420.0, 2333.0, 2311.0, 2256.0, 1510.0, 1549.0, 1562.0, 1540.0, 1530.0, 1733.0, 1301.0, 968.0]
layerwise density percentage: ['0.211', '0.232', '0.224', '0.236', '0.226', '0.295', '0.285', '0.282', '0.275', '0.369', '0.378', '0.381', '0.376', '0.747', '0.846', '0.635', '0.473']
Global density: 0.2648235857486725
06/02 01:23:39 PM | Train: [16/80] Step 390/390 Loss 0.042 Prec@(1,5) (99.4%, 100.0%)
06/02 01:23:39 PM | layerwise density: [13759.0, 3791.0, 3633.0, 3850.0, 3663.0, 2404.0, 2309.0, 2305.0, 2248.0, 1499.0, 1544.0, 1564.0, 1543.0, 1542.0, 1735.0, 1307.0, 976.0]
layerwise density percentage: ['0.210', '0.231', '0.222', '0.235', '0.224', '0.293', '0.282', '0.281', '0.274', '0.366', '0.377', '0.382', '0.377', '0.753', '0.847', '0.638', '0.477']
Global density: 0.26362940669059753
06/02 01:23:39 PM | Train: [16/200] Final Prec@1 99.3920%
06/02 01:23:39 PM | Valid: [16/200] Step 000/078 Loss 1.284 Prec@(1,5) (71.9%, 89.8%)
06/02 01:23:41 PM | Valid: [16/200] Step 078/078 Loss 1.412 Prec@(1,5) (67.0%, 88.3%)
06/02 01:23:41 PM | Valid: [16/200] Final Prec@1 67.0500%
06/02 01:23:41 PM | Current mask training best Prec@1 = 67.9400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 13757.0, 0.2099151611328125]
['model.relu.alpha_mask_1_0', 16384, 3791.0, 0.23138427734375]
['model.relu.alpha_mask_2_0', 16384, 3633.0, 0.22174072265625]
['model.relu.alpha_mask_3_0', 16384, 3850.0, 0.2349853515625]
['model.relu.alpha_mask_4_0', 16384, 3662.0, 0.2235107421875]
['model.relu.alpha_mask_5_0', 8192, 2403.0, 0.2933349609375]
['model.relu.alpha_mask_6_0', 8192, 2309.0, 0.2818603515625]
['model.relu.alpha_mask_7_0', 8192, 2305.0, 0.2813720703125]
['model.relu.alpha_mask_8_0', 8192, 2247.0, 0.2742919921875]
['model.relu.alpha_mask_9_0', 4096, 1499.0, 0.365966796875]
['model.relu.alpha_mask_10_0', 4096, 1545.0, 0.377197265625]
['model.relu.alpha_mask_11_0', 4096, 1564.0, 0.3818359375]
['model.relu.alpha_mask_12_0', 4096, 1544.0, 0.376953125]
['model.relu.alpha_mask_13_0', 2048, 1543.0, 0.75341796875]
['model.relu.alpha_mask_14_0', 2048, 1735.0, 0.84716796875]
['model.relu.alpha_mask_15_0', 2048, 1307.0, 0.63818359375]
['model.relu.alpha_mask_16_0', 2048, 976.0, 0.4765625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49670.0, 0.263618800951087]
########## End ###########
06/02 01:23:42 PM | Train: [17/80] Step 000/390 Loss 0.018 Prec@(1,5) (100.0%, 100.0%)
06/02 01:23:42 PM | layerwise density: [13757.0, 3791.0, 3633.0, 3850.0, 3662.0, 2403.0, 2309.0, 2305.0, 2247.0, 1499.0, 1545.0, 1564.0, 1544.0, 1543.0, 1735.0, 1307.0, 976.0]
layerwise density percentage: ['0.210', '0.231', '0.222', '0.235', '0.224', '0.293', '0.282', '0.281', '0.274', '0.366', '0.377', '0.382', '0.377', '0.753', '0.847', '0.638', '0.477']
Global density: 0.2636187970638275
06/02 01:23:53 PM | Train: [17/80] Step 100/390 Loss 0.035 Prec@(1,5) (99.6%, 100.0%)
06/02 01:23:53 PM | layerwise density: [13705.0, 3784.0, 3621.0, 3832.0, 3640.0, 2394.0, 2310.0, 2317.0, 2248.0, 1500.0, 1537.0, 1569.0, 1548.0, 1539.0, 1738.0, 1308.0, 984.0]
layerwise density percentage: ['0.209', '0.231', '0.221', '0.234', '0.222', '0.292', '0.282', '0.283', '0.274', '0.366', '0.375', '0.383', '0.378', '0.751', '0.849', '0.639', '0.480']
Global density: 0.2631092965602875
06/02 01:24:04 PM | Train: [17/80] Step 200/390 Loss 0.035 Prec@(1,5) (99.5%, 100.0%)
06/02 01:24:04 PM | layerwise density: [13680.0, 3802.0, 3621.0, 3825.0, 3650.0, 2414.0, 2329.0, 2325.0, 2244.0, 1502.0, 1549.0, 1575.0, 1554.0, 1550.0, 1747.0, 1315.0, 998.0]
layerwise density percentage: ['0.209', '0.232', '0.221', '0.233', '0.223', '0.295', '0.284', '0.284', '0.274', '0.367', '0.378', '0.385', '0.379', '0.757', '0.853', '0.642', '0.487']
Global density: 0.263671875
06/02 01:24:15 PM | Train: [17/80] Step 300/390 Loss 0.036 Prec@(1,5) (99.5%, 100.0%)
06/02 01:24:15 PM | layerwise density: [13656.0, 3810.0, 3628.0, 3824.0, 3657.0, 2433.0, 2350.0, 2332.0, 2268.0, 1508.0, 1566.0, 1589.0, 1577.0, 1560.0, 1756.0, 1325.0, 1008.0]
layerwise density percentage: ['0.208', '0.233', '0.221', '0.233', '0.223', '0.297', '0.287', '0.285', '0.277', '0.368', '0.382', '0.388', '0.385', '0.762', '0.857', '0.647', '0.492']
Global density: 0.26455822587013245
06/02 01:24:25 PM | Train: [17/80] Step 390/390 Loss 0.037 Prec@(1,5) (99.5%, 100.0%)
06/02 01:24:25 PM | layerwise density: [13569.0, 3798.0, 3614.0, 3817.0, 3667.0, 2442.0, 2338.0, 2327.0, 2258.0, 1525.0, 1564.0, 1595.0, 1566.0, 1568.0, 1755.0, 1326.0, 1020.0]
layerwise density percentage: ['0.207', '0.232', '0.221', '0.233', '0.224', '0.298', '0.285', '0.284', '0.276', '0.372', '0.382', '0.389', '0.382', '0.766', '0.857', '0.647', '0.498']
Global density: 0.2640380859375
06/02 01:24:25 PM | Train: [17/200] Final Prec@1 99.4920%
06/02 01:24:26 PM | Valid: [17/200] Step 000/078 Loss 1.217 Prec@(1,5) (71.9%, 89.1%)
06/02 01:24:28 PM | Valid: [17/200] Step 078/078 Loss 1.418 Prec@(1,5) (67.4%, 87.8%)
06/02 01:24:28 PM | Valid: [17/200] Final Prec@1 67.4400%
06/02 01:24:28 PM | Current mask training best Prec@1 = 67.9400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 13561.0, 0.2069244384765625]
['model.relu.alpha_mask_1_0', 16384, 3793.0, 0.23150634765625]
['model.relu.alpha_mask_2_0', 16384, 3609.0, 0.22027587890625]
['model.relu.alpha_mask_3_0', 16384, 3815.0, 0.23284912109375]
['model.relu.alpha_mask_4_0', 16384, 3664.0, 0.2236328125]
['model.relu.alpha_mask_5_0', 8192, 2440.0, 0.2978515625]
['model.relu.alpha_mask_6_0', 8192, 2335.0, 0.2850341796875]
['model.relu.alpha_mask_7_0', 8192, 2326.0, 0.283935546875]
['model.relu.alpha_mask_8_0', 8192, 2254.0, 0.275146484375]
['model.relu.alpha_mask_9_0', 4096, 1523.0, 0.371826171875]
['model.relu.alpha_mask_10_0', 4096, 1564.0, 0.3818359375]
['model.relu.alpha_mask_11_0', 4096, 1595.0, 0.389404296875]
['model.relu.alpha_mask_12_0', 4096, 1564.0, 0.3818359375]
['model.relu.alpha_mask_13_0', 2048, 1568.0, 0.765625]
['model.relu.alpha_mask_14_0', 2048, 1755.0, 0.85693359375]
['model.relu.alpha_mask_15_0', 2048, 1326.0, 0.6474609375]
['model.relu.alpha_mask_16_0', 2048, 1020.0, 0.498046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49712.0, 0.26384171195652173]
########## End ###########
06/02 01:24:29 PM | Train: [18/80] Step 000/390 Loss 0.033 Prec@(1,5) (100.0%, 100.0%)
06/02 01:24:29 PM | layerwise density: [13561.0, 3793.0, 3609.0, 3815.0, 3664.0, 2440.0, 2335.0, 2326.0, 2254.0, 1523.0, 1564.0, 1595.0, 1564.0, 1568.0, 1755.0, 1326.0, 1020.0]
layerwise density percentage: ['0.207', '0.232', '0.220', '0.233', '0.224', '0.298', '0.285', '0.284', '0.275', '0.372', '0.382', '0.389', '0.382', '0.766', '0.857', '0.647', '0.498']
Global density: 0.2638417184352875
06/02 01:24:40 PM | Train: [18/80] Step 100/390 Loss 0.036 Prec@(1,5) (99.6%, 100.0%)
06/02 01:24:40 PM | layerwise density: [13487.0, 3771.0, 3587.0, 3791.0, 3636.0, 2433.0, 2320.0, 2308.0, 2240.0, 1528.0, 1548.0, 1598.0, 1557.0, 1564.0, 1761.0, 1327.0, 1031.0]
layerwise density percentage: ['0.206', '0.230', '0.219', '0.231', '0.222', '0.297', '0.283', '0.282', '0.273', '0.373', '0.378', '0.390', '0.380', '0.764', '0.860', '0.648', '0.503']
Global density: 0.2626475393772125
06/02 01:24:51 PM | Train: [18/80] Step 200/390 Loss 0.035 Prec@(1,5) (99.5%, 100.0%)
06/02 01:24:51 PM | layerwise density: [13455.0, 3774.0, 3591.0, 3794.0, 3626.0, 2447.0, 2323.0, 2314.0, 2274.0, 1542.0, 1557.0, 1617.0, 1564.0, 1577.0, 1776.0, 1335.0, 1049.0]
layerwise density percentage: ['0.205', '0.230', '0.219', '0.232', '0.221', '0.299', '0.284', '0.282', '0.278', '0.376', '0.380', '0.395', '0.382', '0.770', '0.867', '0.652', '0.512']
Global density: 0.2633269131183624
06/02 01:25:02 PM | Train: [18/80] Step 300/390 Loss 0.036 Prec@(1,5) (99.5%, 100.0%)
06/02 01:25:02 PM | layerwise density: [13424.0, 3789.0, 3592.0, 3806.0, 3636.0, 2482.0, 2338.0, 2338.0, 2288.0, 1540.0, 1565.0, 1629.0, 1601.0, 1575.0, 1766.0, 1342.0, 1070.0]
layerwise density percentage: ['0.205', '0.231', '0.219', '0.232', '0.222', '0.303', '0.285', '0.285', '0.279', '0.376', '0.382', '0.398', '0.391', '0.769', '0.862', '0.655', '0.522']
Global density: 0.2642079293727875
06/02 01:25:12 PM | Train: [18/80] Step 390/390 Loss 0.036 Prec@(1,5) (99.5%, 100.0%)
06/02 01:25:12 PM | layerwise density: [13414.0, 3801.0, 3602.0, 3822.0, 3659.0, 2493.0, 2359.0, 2342.0, 2300.0, 1551.0, 1568.0, 1634.0, 1605.0, 1573.0, 1776.0, 1349.0, 1082.0]
layerwise density percentage: ['0.205', '0.232', '0.220', '0.233', '0.223', '0.304', '0.288', '0.286', '0.281', '0.379', '0.383', '0.399', '0.392', '0.768', '0.867', '0.659', '0.528']
Global density: 0.26499873399734497
06/02 01:25:12 PM | Train: [18/200] Final Prec@1 99.5220%
06/02 01:25:12 PM | Valid: [18/200] Step 000/078 Loss 1.185 Prec@(1,5) (72.7%, 89.1%)
06/02 01:25:15 PM | Valid: [18/200] Step 078/078 Loss 1.367 Prec@(1,5) (68.4%, 89.1%)
06/02 01:25:15 PM | Valid: [18/200] Final Prec@1 68.3600%
06/02 01:25:15 PM | Current mask training best Prec@1 = 68.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 13414.0, 0.204681396484375]
['model.relu.alpha_mask_1_0', 16384, 3801.0, 0.23199462890625]
['model.relu.alpha_mask_2_0', 16384, 3602.0, 0.2198486328125]
['model.relu.alpha_mask_3_0', 16384, 3822.0, 0.2332763671875]
['model.relu.alpha_mask_4_0', 16384, 3659.0, 0.22332763671875]
['model.relu.alpha_mask_5_0', 8192, 2493.0, 0.3043212890625]
['model.relu.alpha_mask_6_0', 8192, 2359.0, 0.2879638671875]
['model.relu.alpha_mask_7_0', 8192, 2342.0, 0.285888671875]
['model.relu.alpha_mask_8_0', 8192, 2300.0, 0.28076171875]
['model.relu.alpha_mask_9_0', 4096, 1551.0, 0.378662109375]
['model.relu.alpha_mask_10_0', 4096, 1568.0, 0.3828125]
['model.relu.alpha_mask_11_0', 4096, 1634.0, 0.39892578125]
['model.relu.alpha_mask_12_0', 4096, 1605.0, 0.391845703125]
['model.relu.alpha_mask_13_0', 2048, 1573.0, 0.76806640625]
['model.relu.alpha_mask_14_0', 2048, 1776.0, 0.8671875]
['model.relu.alpha_mask_15_0', 2048, 1349.0, 0.65869140625]
['model.relu.alpha_mask_16_0', 2048, 1082.0, 0.5283203125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49930.0, 0.2649987262228261]
########## End ###########
06/02 01:25:16 PM | Train: [19/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/02 01:25:16 PM | layerwise density: [13414.0, 3801.0, 3602.0, 3822.0, 3659.0, 2493.0, 2359.0, 2342.0, 2300.0, 1551.0, 1568.0, 1634.0, 1605.0, 1573.0, 1776.0, 1349.0, 1082.0]
layerwise density percentage: ['0.205', '0.232', '0.220', '0.233', '0.223', '0.304', '0.288', '0.286', '0.281', '0.379', '0.383', '0.399', '0.392', '0.768', '0.867', '0.659', '0.528']
Global density: 0.26499873399734497
06/02 01:25:27 PM | Train: [19/80] Step 100/390 Loss 0.033 Prec@(1,5) (99.6%, 100.0%)
06/02 01:25:27 PM | layerwise density: [13070.0, 3690.0, 3506.0, 3714.0, 3510.0, 2411.0, 2293.0, 2291.0, 2220.0, 1503.0, 1555.0, 1621.0, 1554.0, 1558.0, 1765.0, 1337.0, 1092.0]
layerwise density percentage: ['0.199', '0.225', '0.214', '0.227', '0.214', '0.294', '0.280', '0.280', '0.271', '0.367', '0.380', '0.396', '0.379', '0.761', '0.862', '0.653', '0.533']
Global density: 0.258417546749115
06/02 01:25:38 PM | Train: [19/80] Step 200/390 Loss 0.035 Prec@(1,5) (99.5%, 100.0%)
06/02 01:25:38 PM | layerwise density: [13015.0, 3668.0, 3484.0, 3689.0, 3485.0, 2422.0, 2278.0, 2278.0, 2200.0, 1514.0, 1549.0, 1624.0, 1564.0, 1577.0, 1783.0, 1354.0, 1104.0]
layerwise density percentage: ['0.199', '0.224', '0.213', '0.225', '0.213', '0.296', '0.278', '0.278', '0.269', '0.370', '0.378', '0.396', '0.382', '0.770', '0.871', '0.661', '0.539']
Global density: 0.2578761875629425
06/02 01:25:49 PM | Train: [19/80] Step 300/390 Loss 0.035 Prec@(1,5) (99.5%, 100.0%)
06/02 01:25:49 PM | layerwise density: [12973.0, 3677.0, 3483.0, 3684.0, 3500.0, 2458.0, 2295.0, 2287.0, 2213.0, 1546.0, 1570.0, 1633.0, 1582.0, 1580.0, 1795.0, 1368.0, 1117.0]
layerwise density percentage: ['0.198', '0.224', '0.213', '0.225', '0.214', '0.300', '0.280', '0.279', '0.270', '0.377', '0.383', '0.399', '0.386', '0.771', '0.876', '0.668', '0.545']
Global density: 0.258794367313385
06/02 01:25:59 PM | Train: [19/80] Step 390/390 Loss 0.035 Prec@(1,5) (99.5%, 100.0%)
06/02 01:25:59 PM | layerwise density: [12945.0, 3705.0, 3492.0, 3702.0, 3504.0, 2477.0, 2327.0, 2306.0, 2250.0, 1576.0, 1582.0, 1640.0, 1581.0, 1596.0, 1797.0, 1372.0, 1128.0]
layerwise density percentage: ['0.198', '0.226', '0.213', '0.226', '0.214', '0.302', '0.284', '0.281', '0.275', '0.385', '0.386', '0.400', '0.386', '0.779', '0.877', '0.670', '0.551']
Global density: 0.2599566876888275
06/02 01:25:59 PM | Train: [19/200] Final Prec@1 99.4700%
06/02 01:25:59 PM | Valid: [19/200] Step 000/078 Loss 1.159 Prec@(1,5) (73.4%, 91.4%)
06/02 01:26:01 PM | Valid: [19/200] Step 078/078 Loss 1.380 Prec@(1,5) (68.2%, 88.7%)
06/02 01:26:01 PM | Valid: [19/200] Final Prec@1 68.1500%
06/02 01:26:01 PM | Current mask training best Prec@1 = 68.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 12945.0, 0.1975250244140625]
['model.relu.alpha_mask_1_0', 16384, 3705.0, 0.22613525390625]
['model.relu.alpha_mask_2_0', 16384, 3492.0, 0.213134765625]
['model.relu.alpha_mask_3_0', 16384, 3702.0, 0.2259521484375]
['model.relu.alpha_mask_4_0', 16384, 3504.0, 0.2138671875]
['model.relu.alpha_mask_5_0', 8192, 2477.0, 0.3023681640625]
['model.relu.alpha_mask_6_0', 8192, 2327.0, 0.2840576171875]
['model.relu.alpha_mask_7_0', 8192, 2306.0, 0.281494140625]
['model.relu.alpha_mask_8_0', 8192, 2250.0, 0.274658203125]
['model.relu.alpha_mask_9_0', 4096, 1575.0, 0.384521484375]
['model.relu.alpha_mask_10_0', 4096, 1582.0, 0.38623046875]
['model.relu.alpha_mask_11_0', 4096, 1640.0, 0.400390625]
['model.relu.alpha_mask_12_0', 4096, 1581.0, 0.385986328125]
['model.relu.alpha_mask_13_0', 2048, 1596.0, 0.779296875]
['model.relu.alpha_mask_14_0', 2048, 1797.0, 0.87744140625]
['model.relu.alpha_mask_15_0', 2048, 1372.0, 0.669921875]
['model.relu.alpha_mask_16_0', 2048, 1128.0, 0.55078125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48979.0, 0.2599513841711957]
########## End ###########
06/02 01:26:02 PM | Train: [20/80] Step 000/390 Loss 0.014 Prec@(1,5) (100.0%, 100.0%)
06/02 01:26:02 PM | layerwise density: [12945.0, 3705.0, 3492.0, 3702.0, 3504.0, 2477.0, 2327.0, 2306.0, 2250.0, 1575.0, 1582.0, 1640.0, 1581.0, 1596.0, 1797.0, 1372.0, 1128.0]
layerwise density percentage: ['0.198', '0.226', '0.213', '0.226', '0.214', '0.302', '0.284', '0.281', '0.275', '0.385', '0.386', '0.400', '0.386', '0.779', '0.877', '0.670', '0.551']
Global density: 0.2599513828754425
06/02 01:26:13 PM | Train: [20/80] Step 100/390 Loss 0.032 Prec@(1,5) (99.5%, 100.0%)
06/02 01:26:13 PM | layerwise density: [12925.0, 3724.0, 3490.0, 3729.0, 3515.0, 2482.0, 2347.0, 2326.0, 2278.0, 1581.0, 1614.0, 1653.0, 1594.0, 1594.0, 1785.0, 1377.0, 1143.0]
layerwise density percentage: ['0.197', '0.227', '0.213', '0.228', '0.215', '0.303', '0.286', '0.284', '0.278', '0.386', '0.394', '0.404', '0.389', '0.778', '0.872', '0.672', '0.558']
Global density: 0.26089611649513245
06/02 01:26:24 PM | Train: [20/80] Step 200/390 Loss 0.031 Prec@(1,5) (99.6%, 100.0%)
06/02 01:26:24 PM | layerwise density: [12926.0, 3752.0, 3494.0, 3746.0, 3523.0, 2514.0, 2350.0, 2346.0, 2294.0, 1593.0, 1613.0, 1665.0, 1612.0, 1603.0, 1803.0, 1386.0, 1152.0]
layerwise density percentage: ['0.197', '0.229', '0.213', '0.229', '0.215', '0.307', '0.287', '0.286', '0.280', '0.389', '0.394', '0.406', '0.394', '0.783', '0.880', '0.677', '0.562']
Global density: 0.2620371878147125
06/02 01:26:35 PM | Train: [20/80] Step 300/390 Loss 0.031 Prec@(1,5) (99.6%, 100.0%)
06/02 01:26:35 PM | layerwise density: [12924.0, 3778.0, 3518.0, 3771.0, 3548.0, 2525.0, 2357.0, 2362.0, 2296.0, 1612.0, 1612.0, 1680.0, 1603.0, 1603.0, 1814.0, 1394.0, 1166.0]
layerwise density percentage: ['0.197', '0.231', '0.215', '0.230', '0.217', '0.308', '0.288', '0.288', '0.280', '0.394', '0.394', '0.410', '0.391', '0.783', '0.886', '0.681', '0.569']
Global density: 0.26305091381073
06/02 01:26:45 PM | Train: [20/80] Step 390/390 Loss 0.031 Prec@(1,5) (99.6%, 100.0%)
06/02 01:26:45 PM | layerwise density: [12922.0, 3818.0, 3518.0, 3783.0, 3570.0, 2545.0, 2377.0, 2377.0, 2303.0, 1620.0, 1616.0, 1692.0, 1610.0, 1604.0, 1818.0, 1404.0, 1185.0]
layerwise density percentage: ['0.197', '0.233', '0.215', '0.231', '0.218', '0.311', '0.290', '0.290', '0.281', '0.396', '0.395', '0.413', '0.393', '0.783', '0.888', '0.686', '0.579']
Global density: 0.2641070783138275
06/02 01:26:45 PM | Train: [20/200] Final Prec@1 99.6000%
06/02 01:26:45 PM | Valid: [20/200] Step 000/078 Loss 1.116 Prec@(1,5) (73.4%, 88.3%)
06/02 01:26:48 PM | Valid: [20/200] Step 078/078 Loss 1.370 Prec@(1,5) (68.4%, 88.5%)
06/02 01:26:48 PM | Valid: [20/200] Final Prec@1 68.3700%
06/02 01:26:48 PM | Current mask training best Prec@1 = 68.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 12922.0, 0.197174072265625]
['model.relu.alpha_mask_1_0', 16384, 3818.0, 0.2330322265625]
['model.relu.alpha_mask_2_0', 16384, 3518.0, 0.2147216796875]
['model.relu.alpha_mask_3_0', 16384, 3784.0, 0.23095703125]
['model.relu.alpha_mask_4_0', 16384, 3570.0, 0.2178955078125]
['model.relu.alpha_mask_5_0', 8192, 2545.0, 0.3106689453125]
['model.relu.alpha_mask_6_0', 8192, 2377.0, 0.2901611328125]
['model.relu.alpha_mask_7_0', 8192, 2377.0, 0.2901611328125]
['model.relu.alpha_mask_8_0', 8192, 2303.0, 0.2811279296875]
['model.relu.alpha_mask_9_0', 4096, 1620.0, 0.3955078125]
['model.relu.alpha_mask_10_0', 4096, 1616.0, 0.39453125]
['model.relu.alpha_mask_11_0', 4096, 1692.0, 0.4130859375]
['model.relu.alpha_mask_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_mask_13_0', 2048, 1604.0, 0.783203125]
['model.relu.alpha_mask_14_0', 2048, 1818.0, 0.8876953125]
['model.relu.alpha_mask_15_0', 2048, 1402.0, 0.6845703125]
['model.relu.alpha_mask_16_0', 2048, 1185.0, 0.57861328125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49764.0, 0.26411769701086957]
########## End ###########
06/02 01:26:49 PM | Train: [21/80] Step 000/390 Loss 0.018 Prec@(1,5) (100.0%, 100.0%)
06/02 01:26:49 PM | layerwise density: [12922.0, 3818.0, 3518.0, 3784.0, 3570.0, 2545.0, 2377.0, 2377.0, 2303.0, 1620.0, 1616.0, 1692.0, 1613.0, 1604.0, 1818.0, 1402.0, 1185.0]
layerwise density percentage: ['0.197', '0.233', '0.215', '0.231', '0.218', '0.311', '0.290', '0.290', '0.281', '0.396', '0.395', '0.413', '0.394', '0.783', '0.888', '0.685', '0.579']
Global density: 0.26411768794059753
06/02 01:27:01 PM | Train: [21/80] Step 100/390 Loss 0.026 Prec@(1,5) (99.7%, 100.0%)
06/02 01:27:01 PM | layerwise density: [12919.0, 3818.0, 3523.0, 3802.0, 3579.0, 2563.0, 2389.0, 2388.0, 2323.0, 1613.0, 1612.0, 1700.0, 1636.0, 1617.0, 1824.0, 1400.0, 1197.0]
layerwise density percentage: ['0.197', '0.233', '0.215', '0.232', '0.218', '0.313', '0.292', '0.292', '0.284', '0.394', '0.394', '0.415', '0.399', '0.790', '0.891', '0.684', '0.584']
Global density: 0.26485544443130493
06/02 01:27:12 PM | Train: [21/80] Step 200/390 Loss 0.026 Prec@(1,5) (99.7%, 100.0%)
06/02 01:27:12 PM | layerwise density: [12744.0, 3797.0, 3491.0, 3783.0, 3532.0, 2523.0, 2359.0, 2373.0, 2313.0, 1598.0, 1610.0, 1701.0, 1614.0, 1611.0, 1833.0, 1406.0, 1207.0]
layerwise density percentage: ['0.194', '0.232', '0.213', '0.231', '0.216', '0.308', '0.288', '0.290', '0.282', '0.390', '0.393', '0.415', '0.394', '0.787', '0.895', '0.687', '0.589']
Global density: 0.262690007686615
06/02 01:27:23 PM | Train: [21/80] Step 300/390 Loss 0.027 Prec@(1,5) (99.6%, 100.0%)
06/02 01:27:23 PM | layerwise density: [12674.0, 3776.0, 3465.0, 3756.0, 3509.0, 2502.0, 2349.0, 2347.0, 2294.0, 1592.0, 1612.0, 1704.0, 1608.0, 1621.0, 1832.0, 1401.0, 1212.0]
layerwise density percentage: ['0.193', '0.230', '0.211', '0.229', '0.214', '0.305', '0.287', '0.286', '0.280', '0.389', '0.394', '0.416', '0.393', '0.792', '0.895', '0.684', '0.592']
Global density: 0.2614109218120575
06/02 01:27:32 PM | Train: [21/80] Step 390/390 Loss 0.028 Prec@(1,5) (99.6%, 100.0%)
06/02 01:27:32 PM | layerwise density: [12635.0, 3768.0, 3463.0, 3758.0, 3515.0, 2519.0, 2345.0, 2355.0, 2292.0, 1612.0, 1610.0, 1708.0, 1610.0, 1625.0, 1829.0, 1410.0, 1228.0]
layerwise density percentage: ['0.193', '0.230', '0.211', '0.229', '0.215', '0.307', '0.286', '0.287', '0.280', '0.394', '0.393', '0.417', '0.393', '0.793', '0.893', '0.688', '0.600']
Global density: 0.26155954599380493
06/02 01:27:32 PM | Train: [21/200] Final Prec@1 99.6180%
06/02 01:27:33 PM | Valid: [21/200] Step 000/078 Loss 1.132 Prec@(1,5) (75.0%, 93.0%)
06/02 01:27:35 PM | Valid: [21/200] Step 078/078 Loss 1.376 Prec@(1,5) (68.3%, 88.4%)
06/02 01:27:35 PM | Valid: [21/200] Final Prec@1 68.3400%
06/02 01:27:35 PM | Current mask training best Prec@1 = 68.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 12636.0, 0.19281005859375]
['model.relu.alpha_mask_1_0', 16384, 3767.0, 0.22991943359375]
['model.relu.alpha_mask_2_0', 16384, 3464.0, 0.21142578125]
['model.relu.alpha_mask_3_0', 16384, 3758.0, 0.2293701171875]
['model.relu.alpha_mask_4_0', 16384, 3514.0, 0.2144775390625]
['model.relu.alpha_mask_5_0', 8192, 2520.0, 0.3076171875]
['model.relu.alpha_mask_6_0', 8192, 2345.0, 0.2862548828125]
['model.relu.alpha_mask_7_0', 8192, 2355.0, 0.2874755859375]
['model.relu.alpha_mask_8_0', 8192, 2292.0, 0.27978515625]
['model.relu.alpha_mask_9_0', 4096, 1612.0, 0.3935546875]
['model.relu.alpha_mask_10_0', 4096, 1608.0, 0.392578125]
['model.relu.alpha_mask_11_0', 4096, 1708.0, 0.4169921875]
['model.relu.alpha_mask_12_0', 4096, 1611.0, 0.393310546875]
['model.relu.alpha_mask_13_0', 2048, 1625.0, 0.79345703125]
['model.relu.alpha_mask_14_0', 2048, 1829.0, 0.89306640625]
['model.relu.alpha_mask_15_0', 2048, 1410.0, 0.6884765625]
['model.relu.alpha_mask_16_0', 2048, 1228.0, 0.599609375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49282.0, 0.26155952785326086]
########## End ###########
06/02 01:27:36 PM | Train: [22/80] Step 000/390 Loss 0.026 Prec@(1,5) (99.2%, 100.0%)
06/02 01:27:36 PM | layerwise density: [12636.0, 3767.0, 3464.0, 3758.0, 3514.0, 2520.0, 2345.0, 2355.0, 2292.0, 1612.0, 1608.0, 1708.0, 1611.0, 1625.0, 1829.0, 1410.0, 1228.0]
layerwise density percentage: ['0.193', '0.230', '0.211', '0.229', '0.214', '0.308', '0.286', '0.287', '0.280', '0.394', '0.393', '0.417', '0.393', '0.793', '0.893', '0.688', '0.600']
Global density: 0.26155954599380493
06/02 01:27:47 PM | Train: [22/80] Step 100/390 Loss 0.025 Prec@(1,5) (99.7%, 100.0%)
06/02 01:27:47 PM | layerwise density: [12603.0, 3783.0, 3467.0, 3772.0, 3511.0, 2559.0, 2375.0, 2382.0, 2298.0, 1638.0, 1614.0, 1712.0, 1609.0, 1631.0, 1838.0, 1411.0, 1237.0]
layerwise density percentage: ['0.192', '0.231', '0.212', '0.230', '0.214', '0.312', '0.290', '0.291', '0.281', '0.400', '0.394', '0.418', '0.393', '0.796', '0.897', '0.689', '0.604']
Global density: 0.2623980939388275
06/02 01:27:57 PM | Train: [22/80] Step 200/390 Loss 0.026 Prec@(1,5) (99.7%, 100.0%)
06/02 01:27:57 PM | layerwise density: [12589.0, 3800.0, 3465.0, 3780.0, 3507.0, 2567.0, 2392.0, 2398.0, 2310.0, 1640.0, 1618.0, 1731.0, 1630.0, 1640.0, 1857.0, 1415.0, 1247.0]
layerwise density percentage: ['0.192', '0.232', '0.211', '0.231', '0.214', '0.313', '0.292', '0.293', '0.282', '0.400', '0.395', '0.423', '0.398', '0.801', '0.907', '0.691', '0.609']
Global density: 0.26317298412323
06/02 01:28:08 PM | Train: [22/80] Step 300/390 Loss 0.026 Prec@(1,5) (99.7%, 100.0%)
06/02 01:28:08 PM | layerwise density: [12599.0, 3812.0, 3472.0, 3791.0, 3521.0, 2594.0, 2397.0, 2405.0, 2324.0, 1652.0, 1636.0, 1738.0, 1630.0, 1639.0, 1855.0, 1418.0, 1257.0]
layerwise density percentage: ['0.192', '0.233', '0.212', '0.231', '0.215', '0.317', '0.293', '0.294', '0.284', '0.403', '0.399', '0.424', '0.398', '0.800', '0.906', '0.692', '0.614']
Global density: 0.2639903128147125
06/02 01:28:18 PM | Train: [22/80] Step 390/390 Loss 0.026 Prec@(1,5) (99.7%, 100.0%)
06/02 01:28:18 PM | layerwise density: [12572.0, 3812.0, 3491.0, 3800.0, 3537.0, 2602.0, 2415.0, 2412.0, 2333.0, 1668.0, 1640.0, 1746.0, 1644.0, 1639.0, 1860.0, 1429.0, 1264.0]
layerwise density percentage: ['0.192', '0.233', '0.213', '0.232', '0.216', '0.318', '0.295', '0.294', '0.285', '0.407', '0.400', '0.426', '0.401', '0.800', '0.908', '0.698', '0.617']
Global density: 0.2646484375
06/02 01:28:18 PM | Train: [22/200] Final Prec@1 99.6560%
06/02 01:28:18 PM | Valid: [22/200] Step 000/078 Loss 1.151 Prec@(1,5) (72.7%, 91.4%)
06/02 01:28:20 PM | Valid: [22/200] Step 078/078 Loss 1.370 Prec@(1,5) (68.2%, 88.5%)
06/02 01:28:20 PM | Valid: [22/200] Final Prec@1 68.2100%
06/02 01:28:20 PM | Current mask training best Prec@1 = 68.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 12573.0, 0.1918487548828125]
['model.relu.alpha_mask_1_0', 16384, 3813.0, 0.23272705078125]
['model.relu.alpha_mask_2_0', 16384, 3490.0, 0.2130126953125]
['model.relu.alpha_mask_3_0', 16384, 3801.0, 0.23199462890625]
['model.relu.alpha_mask_4_0', 16384, 3538.0, 0.2159423828125]
['model.relu.alpha_mask_5_0', 8192, 2602.0, 0.317626953125]
['model.relu.alpha_mask_6_0', 8192, 2416.0, 0.294921875]
['model.relu.alpha_mask_7_0', 8192, 2412.0, 0.29443359375]
['model.relu.alpha_mask_8_0', 8192, 2333.0, 0.2847900390625]
['model.relu.alpha_mask_9_0', 4096, 1668.0, 0.4072265625]
['model.relu.alpha_mask_10_0', 4096, 1644.0, 0.4013671875]
['model.relu.alpha_mask_11_0', 4096, 1747.0, 0.426513671875]
['model.relu.alpha_mask_12_0', 4096, 1645.0, 0.401611328125]
['model.relu.alpha_mask_13_0', 2048, 1639.0, 0.80029296875]
['model.relu.alpha_mask_14_0', 2048, 1861.0, 0.90869140625]
['model.relu.alpha_mask_15_0', 2048, 1429.0, 0.69775390625]
['model.relu.alpha_mask_16_0', 2048, 1264.0, 0.6171875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49875.0, 0.2647068189538043]
########## End ###########
06/02 01:28:21 PM | Train: [23/80] Step 000/390 Loss 0.030 Prec@(1,5) (99.2%, 100.0%)
06/02 01:28:21 PM | layerwise density: [12573.0, 3813.0, 3490.0, 3801.0, 3538.0, 2602.0, 2416.0, 2412.0, 2333.0, 1668.0, 1644.0, 1747.0, 1645.0, 1639.0, 1861.0, 1429.0, 1264.0]
layerwise density percentage: ['0.192', '0.233', '0.213', '0.232', '0.216', '0.318', '0.295', '0.294', '0.285', '0.407', '0.401', '0.427', '0.402', '0.800', '0.909', '0.698', '0.617']
Global density: 0.2647068202495575
06/02 01:28:32 PM | Train: [23/80] Step 100/390 Loss 0.023 Prec@(1,5) (99.8%, 100.0%)
06/02 01:28:32 PM | layerwise density: [12361.0, 3786.0, 3462.0, 3769.0, 3505.0, 2581.0, 2399.0, 2389.0, 2290.0, 1659.0, 1637.0, 1750.0, 1639.0, 1641.0, 1855.0, 1431.0, 1276.0]
layerwise density percentage: ['0.189', '0.231', '0.211', '0.230', '0.214', '0.315', '0.293', '0.292', '0.280', '0.405', '0.400', '0.427', '0.400', '0.801', '0.906', '0.699', '0.623']
Global density: 0.26234501600265503
06/02 01:28:43 PM | Train: [23/80] Step 200/390 Loss 0.023 Prec@(1,5) (99.7%, 100.0%)
06/02 01:28:43 PM | layerwise density: [12246.0, 3735.0, 3422.0, 3713.0, 3451.0, 2563.0, 2349.0, 2371.0, 2261.0, 1651.0, 1619.0, 1743.0, 1625.0, 1640.0, 1850.0, 1429.0, 1285.0]
layerwise density percentage: ['0.187', '0.228', '0.209', '0.227', '0.211', '0.313', '0.287', '0.289', '0.276', '0.403', '0.395', '0.426', '0.397', '0.801', '0.903', '0.698', '0.627']
Global density: 0.2598133981227875
06/02 01:28:54 PM | Train: [23/80] Step 300/390 Loss 0.023 Prec@(1,5) (99.7%, 100.0%)
06/02 01:28:54 PM | layerwise density: [12219.0, 3753.0, 3408.0, 3719.0, 3438.0, 2560.0, 2369.0, 2382.0, 2271.0, 1647.0, 1625.0, 1753.0, 1641.0, 1649.0, 1865.0, 1435.0, 1293.0]
layerwise density percentage: ['0.186', '0.229', '0.208', '0.227', '0.210', '0.312', '0.289', '0.291', '0.277', '0.402', '0.397', '0.428', '0.401', '0.805', '0.911', '0.701', '0.631']
Global density: 0.2602061331272125
06/02 01:29:04 PM | Train: [23/80] Step 390/390 Loss 0.024 Prec@(1,5) (99.7%, 100.0%)
06/02 01:29:04 PM | layerwise density: [12215.0, 3743.0, 3415.0, 3730.0, 3450.0, 2589.0, 2400.0, 2394.0, 2300.0, 1672.0, 1639.0, 1758.0, 1655.0, 1655.0, 1881.0, 1443.0, 1307.0]
layerwise density percentage: ['0.186', '0.228', '0.208', '0.228', '0.211', '0.316', '0.293', '0.292', '0.281', '0.408', '0.400', '0.429', '0.404', '0.808', '0.918', '0.705', '0.638']
Global density: 0.26136845350265503
06/02 01:29:04 PM | Train: [23/200] Final Prec@1 99.6980%
06/02 01:29:04 PM | Valid: [23/200] Step 000/078 Loss 1.175 Prec@(1,5) (71.1%, 92.2%)
06/02 01:29:07 PM | Valid: [23/200] Step 078/078 Loss 1.409 Prec@(1,5) (67.6%, 88.4%)
06/02 01:29:07 PM | Valid: [23/200] Final Prec@1 67.5800%
06/02 01:29:07 PM | Current mask training best Prec@1 = 68.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 12214.0, 0.186370849609375]
['model.relu.alpha_mask_1_0', 16384, 3747.0, 0.22869873046875]
['model.relu.alpha_mask_2_0', 16384, 3415.0, 0.20843505859375]
['model.relu.alpha_mask_3_0', 16384, 3730.0, 0.2276611328125]
['model.relu.alpha_mask_4_0', 16384, 3451.0, 0.21063232421875]
['model.relu.alpha_mask_5_0', 8192, 2589.0, 0.3160400390625]
['model.relu.alpha_mask_6_0', 8192, 2401.0, 0.2930908203125]
['model.relu.alpha_mask_7_0', 8192, 2394.0, 0.292236328125]
['model.relu.alpha_mask_8_0', 8192, 2301.0, 0.2808837890625]
['model.relu.alpha_mask_9_0', 4096, 1672.0, 0.408203125]
['model.relu.alpha_mask_10_0', 4096, 1639.0, 0.400146484375]
['model.relu.alpha_mask_11_0', 4096, 1758.0, 0.42919921875]
['model.relu.alpha_mask_12_0', 4096, 1656.0, 0.404296875]
['model.relu.alpha_mask_13_0', 2048, 1655.0, 0.80810546875]
['model.relu.alpha_mask_14_0', 2048, 1880.0, 0.91796875]
['model.relu.alpha_mask_15_0', 2048, 1443.0, 0.70458984375]
['model.relu.alpha_mask_16_0', 2048, 1307.0, 0.63818359375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49252.0, 0.26140030570652173]
########## End ###########
06/02 01:29:08 PM | Train: [24/80] Step 000/390 Loss 0.013 Prec@(1,5) (100.0%, 100.0%)
06/02 01:29:08 PM | layerwise density: [12214.0, 3747.0, 3415.0, 3730.0, 3451.0, 2589.0, 2401.0, 2394.0, 2301.0, 1672.0, 1639.0, 1758.0, 1656.0, 1655.0, 1880.0, 1443.0, 1307.0]
layerwise density percentage: ['0.186', '0.229', '0.208', '0.228', '0.211', '0.316', '0.293', '0.292', '0.281', '0.408', '0.400', '0.429', '0.404', '0.808', '0.918', '0.705', '0.638']
Global density: 0.2614003121852875
06/02 01:29:18 PM | Train: [24/80] Step 100/390 Loss 0.024 Prec@(1,5) (99.7%, 100.0%)
06/02 01:29:18 PM | layerwise density: [12202.0, 3762.0, 3435.0, 3745.0, 3456.0, 2595.0, 2410.0, 2405.0, 2325.0, 1677.0, 1646.0, 1767.0, 1659.0, 1649.0, 1874.0, 1440.0, 1322.0]
layerwise density percentage: ['0.186', '0.230', '0.210', '0.229', '0.211', '0.317', '0.294', '0.294', '0.284', '0.409', '0.402', '0.431', '0.405', '0.805', '0.915', '0.703', '0.646']
Global density: 0.2620212733745575
06/02 01:29:29 PM | Train: [24/80] Step 200/390 Loss 0.023 Prec@(1,5) (99.7%, 100.0%)
06/02 01:29:29 PM | layerwise density: [12187.0, 3800.0, 3445.0, 3745.0, 3477.0, 2604.0, 2404.0, 2417.0, 2338.0, 1675.0, 1655.0, 1781.0, 1661.0, 1658.0, 1887.0, 1448.0, 1327.0]
layerwise density percentage: ['0.186', '0.232', '0.210', '0.229', '0.212', '0.318', '0.293', '0.295', '0.285', '0.409', '0.404', '0.435', '0.406', '0.810', '0.921', '0.707', '0.648']
Global density: 0.2627643048763275
06/02 01:29:41 PM | Train: [24/80] Step 300/390 Loss 0.023 Prec@(1,5) (99.7%, 100.0%)
06/02 01:29:41 PM | layerwise density: [12157.0, 3823.0, 3456.0, 3779.0, 3514.0, 2652.0, 2420.0, 2423.0, 2362.0, 1679.0, 1662.0, 1786.0, 1677.0, 1674.0, 1886.0, 1452.0, 1339.0]
layerwise density percentage: ['0.186', '0.233', '0.211', '0.231', '0.214', '0.324', '0.295', '0.296', '0.288', '0.410', '0.406', '0.436', '0.409', '0.817', '0.921', '0.709', '0.654']
Global density: 0.26399561762809753
06/02 01:29:51 PM | Train: [24/80] Step 390/390 Loss 0.023 Prec@(1,5) (99.7%, 100.0%)
06/02 01:29:51 PM | layerwise density: [12149.0, 3835.0, 3475.0, 3793.0, 3520.0, 2647.0, 2435.0, 2444.0, 2358.0, 1694.0, 1666.0, 1794.0, 1691.0, 1675.0, 1883.0, 1463.0, 1344.0]
layerwise density percentage: ['0.185', '0.234', '0.212', '0.232', '0.215', '0.323', '0.297', '0.298', '0.288', '0.414', '0.407', '0.438', '0.413', '0.818', '0.919', '0.714', '0.656']
Global density: 0.26465904712677
06/02 01:29:51 PM | Train: [24/200] Final Prec@1 99.6920%
06/02 01:29:52 PM | Valid: [24/200] Step 000/078 Loss 1.214 Prec@(1,5) (72.7%, 90.6%)
06/02 01:29:54 PM | Valid: [24/200] Step 078/078 Loss 1.363 Prec@(1,5) (68.6%, 88.7%)
06/02 01:29:54 PM | Valid: [24/200] Final Prec@1 68.6000%
06/02 01:29:54 PM | Current mask training best Prec@1 = 68.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 12149.0, 0.1853790283203125]
['model.relu.alpha_mask_1_0', 16384, 3835.0, 0.23406982421875]
['model.relu.alpha_mask_2_0', 16384, 3475.0, 0.21209716796875]
['model.relu.alpha_mask_3_0', 16384, 3793.0, 0.23150634765625]
['model.relu.alpha_mask_4_0', 16384, 3520.0, 0.21484375]
['model.relu.alpha_mask_5_0', 8192, 2647.0, 0.3231201171875]
['model.relu.alpha_mask_6_0', 8192, 2435.0, 0.2972412109375]
['model.relu.alpha_mask_7_0', 8192, 2444.0, 0.29833984375]
['model.relu.alpha_mask_8_0', 8192, 2358.0, 0.287841796875]
['model.relu.alpha_mask_9_0', 4096, 1694.0, 0.41357421875]
['model.relu.alpha_mask_10_0', 4096, 1666.0, 0.40673828125]
['model.relu.alpha_mask_11_0', 4096, 1794.0, 0.43798828125]
['model.relu.alpha_mask_12_0', 4096, 1691.0, 0.412841796875]
['model.relu.alpha_mask_13_0', 2048, 1675.0, 0.81787109375]
['model.relu.alpha_mask_14_0', 2048, 1883.0, 0.91943359375]
['model.relu.alpha_mask_15_0', 2048, 1463.0, 0.71435546875]
['model.relu.alpha_mask_16_0', 2048, 1344.0, 0.65625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49866.0, 0.2646590523097826]
########## End ###########
06/02 01:29:56 PM | Train: [25/80] Step 000/390 Loss 0.015 Prec@(1,5) (100.0%, 100.0%)
06/02 01:29:56 PM | layerwise density: [12149.0, 3835.0, 3475.0, 3793.0, 3520.0, 2647.0, 2435.0, 2444.0, 2358.0, 1694.0, 1666.0, 1794.0, 1691.0, 1675.0, 1883.0, 1463.0, 1344.0]
layerwise density percentage: ['0.185', '0.234', '0.212', '0.232', '0.215', '0.323', '0.297', '0.298', '0.288', '0.414', '0.407', '0.438', '0.413', '0.818', '0.919', '0.714', '0.656']
Global density: 0.26465904712677
06/02 01:30:07 PM | Train: [25/80] Step 100/390 Loss 0.019 Prec@(1,5) (99.8%, 100.0%)
06/02 01:30:07 PM | layerwise density: [12135.0, 3843.0, 3494.0, 3811.0, 3528.0, 2654.0, 2433.0, 2454.0, 2372.0, 1712.0, 1673.0, 1798.0, 1691.0, 1671.0, 1878.0, 1465.0, 1352.0]
layerwise density percentage: ['0.185', '0.235', '0.213', '0.233', '0.215', '0.324', '0.297', '0.300', '0.290', '0.418', '0.408', '0.439', '0.413', '0.816', '0.917', '0.715', '0.660']
Global density: 0.26517918705940247
06/02 01:30:17 PM | Train: [25/80] Step 200/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/02 01:30:17 PM | layerwise density: [11859.0, 3728.0, 3392.0, 3710.0, 3406.0, 2575.0, 2373.0, 2414.0, 2298.0, 1675.0, 1638.0, 1791.0, 1654.0, 1669.0, 1883.0, 1456.0, 1358.0]
layerwise density percentage: ['0.181', '0.228', '0.207', '0.226', '0.208', '0.314', '0.290', '0.295', '0.281', '0.409', '0.400', '0.437', '0.404', '0.815', '0.919', '0.711', '0.663']
Global density: 0.2594206631183624
06/02 01:30:28 PM | Train: [25/80] Step 300/390 Loss 0.021 Prec@(1,5) (99.8%, 100.0%)
06/02 01:30:28 PM | layerwise density: [11805.0, 3718.0, 3370.0, 3700.0, 3384.0, 2593.0, 2361.0, 2414.0, 2304.0, 1691.0, 1654.0, 1797.0, 1668.0, 1674.0, 1896.0, 1469.0, 1367.0]
layerwise density percentage: ['0.180', '0.227', '0.206', '0.226', '0.207', '0.317', '0.288', '0.295', '0.281', '0.413', '0.404', '0.439', '0.407', '0.817', '0.926', '0.717', '0.667']
Global density: 0.2593463361263275
06/02 01:30:38 PM | Train: [25/80] Step 390/390 Loss 0.021 Prec@(1,5) (99.8%, 100.0%)
06/02 01:30:38 PM | layerwise density: [11785.0, 3730.0, 3366.0, 3701.0, 3391.0, 2620.0, 2365.0, 2423.0, 2327.0, 1696.0, 1662.0, 1799.0, 1685.0, 1680.0, 1907.0, 1475.0, 1382.0]
layerwise density percentage: ['0.180', '0.228', '0.205', '0.226', '0.207', '0.320', '0.289', '0.296', '0.284', '0.414', '0.406', '0.439', '0.411', '0.820', '0.931', '0.720', '0.675']
Global density: 0.2600310146808624
06/02 01:30:38 PM | Train: [25/200] Final Prec@1 99.7640%
06/02 01:30:38 PM | Valid: [25/200] Step 000/078 Loss 1.119 Prec@(1,5) (73.4%, 91.4%)
06/02 01:30:40 PM | Valid: [25/200] Step 078/078 Loss 1.356 Prec@(1,5) (68.3%, 88.9%)
06/02 01:30:40 PM | Valid: [25/200] Final Prec@1 68.3200%
06/02 01:30:40 PM | Current mask training best Prec@1 = 68.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 11785.0, 0.1798248291015625]
['model.relu.alpha_mask_1_0', 16384, 3730.0, 0.2276611328125]
['model.relu.alpha_mask_2_0', 16384, 3366.0, 0.2054443359375]
['model.relu.alpha_mask_3_0', 16384, 3701.0, 0.22589111328125]
['model.relu.alpha_mask_4_0', 16384, 3391.0, 0.20697021484375]
['model.relu.alpha_mask_5_0', 8192, 2620.0, 0.31982421875]
['model.relu.alpha_mask_6_0', 8192, 2365.0, 0.2886962890625]
['model.relu.alpha_mask_7_0', 8192, 2423.0, 0.2957763671875]
['model.relu.alpha_mask_8_0', 8192, 2327.0, 0.2840576171875]
['model.relu.alpha_mask_9_0', 4096, 1697.0, 0.414306640625]
['model.relu.alpha_mask_10_0', 4096, 1662.0, 0.40576171875]
['model.relu.alpha_mask_11_0', 4096, 1799.0, 0.439208984375]
['model.relu.alpha_mask_12_0', 4096, 1685.0, 0.411376953125]
['model.relu.alpha_mask_13_0', 2048, 1680.0, 0.8203125]
['model.relu.alpha_mask_14_0', 2048, 1907.0, 0.93115234375]
['model.relu.alpha_mask_15_0', 2048, 1475.0, 0.72021484375]
['model.relu.alpha_mask_16_0', 2048, 1382.0, 0.6748046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48995.0, 0.26003630264945654]
########## End ###########
06/02 01:30:41 PM | Train: [26/80] Step 000/390 Loss 0.015 Prec@(1,5) (100.0%, 100.0%)
06/02 01:30:41 PM | layerwise density: [11785.0, 3730.0, 3366.0, 3701.0, 3391.0, 2620.0, 2365.0, 2423.0, 2327.0, 1697.0, 1662.0, 1799.0, 1685.0, 1680.0, 1907.0, 1475.0, 1382.0]
layerwise density percentage: ['0.180', '0.228', '0.205', '0.226', '0.207', '0.320', '0.289', '0.296', '0.284', '0.414', '0.406', '0.439', '0.411', '0.820', '0.931', '0.720', '0.675']
Global density: 0.26003631949424744
06/02 01:30:52 PM | Train: [26/80] Step 100/390 Loss 0.019 Prec@(1,5) (99.9%, 100.0%)
06/02 01:30:52 PM | layerwise density: [11774.0, 3741.0, 3377.0, 3724.0, 3413.0, 2634.0, 2391.0, 2437.0, 2345.0, 1709.0, 1662.0, 1806.0, 1693.0, 1690.0, 1901.0, 1483.0, 1393.0]
layerwise density percentage: ['0.180', '0.228', '0.206', '0.227', '0.208', '0.322', '0.292', '0.297', '0.286', '0.417', '0.406', '0.441', '0.413', '0.825', '0.928', '0.724', '0.680']
Global density: 0.260981023311615
06/02 01:31:03 PM | Train: [26/80] Step 200/390 Loss 0.019 Prec@(1,5) (99.8%, 100.0%)
06/02 01:31:03 PM | layerwise density: [11764.0, 3757.0, 3387.0, 3732.0, 3436.0, 2661.0, 2417.0, 2447.0, 2364.0, 1719.0, 1669.0, 1810.0, 1707.0, 1691.0, 1904.0, 1482.0, 1398.0]
layerwise density percentage: ['0.180', '0.229', '0.207', '0.228', '0.210', '0.325', '0.295', '0.299', '0.289', '0.420', '0.407', '0.442', '0.417', '0.826', '0.930', '0.724', '0.683']
Global density: 0.2618938982486725
06/02 01:31:14 PM | Train: [26/80] Step 300/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/02 01:31:14 PM | layerwise density: [11756.0, 3786.0, 3404.0, 3759.0, 3452.0, 2667.0, 2444.0, 2459.0, 2381.0, 1716.0, 1674.0, 1823.0, 1718.0, 1689.0, 1897.0, 1493.0, 1406.0]
layerwise density percentage: ['0.179', '0.231', '0.208', '0.229', '0.211', '0.326', '0.298', '0.300', '0.291', '0.419', '0.409', '0.445', '0.419', '0.825', '0.926', '0.729', '0.687']
Global density: 0.26284393668174744
06/02 01:31:23 PM | Train: [26/80] Step 390/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/02 01:31:23 PM | layerwise density: [11774.0, 3803.0, 3413.0, 3777.0, 3457.0, 2672.0, 2460.0, 2464.0, 2397.0, 1725.0, 1685.0, 1832.0, 1711.0, 1693.0, 1908.0, 1496.0, 1416.0]
layerwise density percentage: ['0.180', '0.232', '0.208', '0.231', '0.211', '0.326', '0.300', '0.301', '0.293', '0.421', '0.411', '0.447', '0.418', '0.827', '0.932', '0.730', '0.691']
Global density: 0.26368778944015503
06/02 01:31:23 PM | Train: [26/200] Final Prec@1 99.7740%
06/02 01:31:24 PM | Valid: [26/200] Step 000/078 Loss 1.193 Prec@(1,5) (73.4%, 90.6%)
06/02 01:31:26 PM | Valid: [26/200] Step 078/078 Loss 1.344 Prec@(1,5) (69.0%, 88.8%)
06/02 01:31:26 PM | Valid: [26/200] Final Prec@1 68.9900%
06/02 01:31:27 PM | Current mask training best Prec@1 = 68.9900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 11774.0, 0.179656982421875]
['model.relu.alpha_mask_1_0', 16384, 3803.0, 0.23211669921875]
['model.relu.alpha_mask_2_0', 16384, 3412.0, 0.208251953125]
['model.relu.alpha_mask_3_0', 16384, 3777.0, 0.23052978515625]
['model.relu.alpha_mask_4_0', 16384, 3457.0, 0.21099853515625]
['model.relu.alpha_mask_5_0', 8192, 2673.0, 0.3262939453125]
['model.relu.alpha_mask_6_0', 8192, 2461.0, 0.3004150390625]
['model.relu.alpha_mask_7_0', 8192, 2465.0, 0.3009033203125]
['model.relu.alpha_mask_8_0', 8192, 2398.0, 0.292724609375]
['model.relu.alpha_mask_9_0', 4096, 1725.0, 0.421142578125]
['model.relu.alpha_mask_10_0', 4096, 1686.0, 0.41162109375]
['model.relu.alpha_mask_11_0', 4096, 1832.0, 0.447265625]
['model.relu.alpha_mask_12_0', 4096, 1711.0, 0.417724609375]
['model.relu.alpha_mask_13_0', 2048, 1694.0, 0.8271484375]
['model.relu.alpha_mask_14_0', 2048, 1908.0, 0.931640625]
['model.relu.alpha_mask_15_0', 2048, 1496.0, 0.73046875]
['model.relu.alpha_mask_16_0', 2048, 1416.0, 0.69140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49688.0, 0.26371433423913043]
########## End ###########
06/02 01:31:28 PM | Train: [27/80] Step 000/390 Loss 0.016 Prec@(1,5) (100.0%, 100.0%)
06/02 01:31:28 PM | layerwise density: [11774.0, 3803.0, 3412.0, 3777.0, 3457.0, 2673.0, 2461.0, 2465.0, 2398.0, 1725.0, 1686.0, 1832.0, 1711.0, 1694.0, 1908.0, 1496.0, 1416.0]
layerwise density percentage: ['0.180', '0.232', '0.208', '0.231', '0.211', '0.326', '0.300', '0.301', '0.293', '0.421', '0.412', '0.447', '0.418', '0.827', '0.932', '0.730', '0.691']
Global density: 0.26371434330940247
06/02 01:31:39 PM | Train: [27/80] Step 100/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/02 01:31:39 PM | layerwise density: [11776.0, 3806.0, 3412.0, 3796.0, 3475.0, 2712.0, 2468.0, 2475.0, 2402.0, 1736.0, 1693.0, 1842.0, 1710.0, 1696.0, 1916.0, 1503.0, 1422.0]
layerwise density percentage: ['0.180', '0.232', '0.208', '0.232', '0.212', '0.331', '0.301', '0.302', '0.293', '0.424', '0.413', '0.450', '0.417', '0.828', '0.936', '0.734', '0.694']
Global density: 0.264521062374115
06/02 01:31:49 PM | Train: [27/80] Step 200/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 01:31:49 PM | layerwise density: [11792.0, 3817.0, 3414.0, 3815.0, 3492.0, 2721.0, 2459.0, 2482.0, 2410.0, 1739.0, 1693.0, 1849.0, 1727.0, 1695.0, 1915.0, 1507.0, 1435.0]
layerwise density percentage: ['0.180', '0.233', '0.208', '0.233', '0.213', '0.332', '0.300', '0.303', '0.294', '0.425', '0.413', '0.451', '0.422', '0.828', '0.935', '0.736', '0.701']
Global density: 0.26516857743263245
06/02 01:32:00 PM | Train: [27/80] Step 300/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/02 01:32:00 PM | layerwise density: [11444.0, 3697.0, 3324.0, 3702.0, 3357.0, 2623.0, 2386.0, 2423.0, 2333.0, 1721.0, 1655.0, 1834.0, 1668.0, 1678.0, 1895.0, 1487.0, 1439.0]
layerwise density percentage: ['0.175', '0.226', '0.203', '0.226', '0.205', '0.320', '0.291', '0.296', '0.285', '0.420', '0.404', '0.448', '0.407', '0.819', '0.925', '0.726', '0.703']
Global density: 0.25829017162323
06/02 01:32:10 PM | Train: [27/80] Step 390/390 Loss 0.018 Prec@(1,5) (99.8%, 100.0%)
06/02 01:32:10 PM | layerwise density: [11387.0, 3679.0, 3299.0, 3668.0, 3342.0, 2630.0, 2388.0, 2419.0, 2314.0, 1725.0, 1662.0, 1833.0, 1680.0, 1693.0, 1911.0, 1500.0, 1442.0]
layerwise density percentage: ['0.174', '0.225', '0.201', '0.224', '0.204', '0.321', '0.292', '0.295', '0.282', '0.421', '0.406', '0.448', '0.410', '0.827', '0.933', '0.732', '0.704']
Global density: 0.25779128074645996
06/02 01:32:10 PM | Train: [27/200] Final Prec@1 99.7960%
06/02 01:32:10 PM | Valid: [27/200] Step 000/078 Loss 1.151 Prec@(1,5) (71.1%, 89.8%)
06/02 01:32:12 PM | Valid: [27/200] Step 078/078 Loss 1.349 Prec@(1,5) (68.7%, 88.6%)
06/02 01:32:12 PM | Valid: [27/200] Final Prec@1 68.7000%
06/02 01:32:12 PM | Current mask training best Prec@1 = 68.9900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 11387.0, 0.1737518310546875]
['model.relu.alpha_mask_1_0', 16384, 3678.0, 0.2244873046875]
['model.relu.alpha_mask_2_0', 16384, 3299.0, 0.20135498046875]
['model.relu.alpha_mask_3_0', 16384, 3669.0, 0.22393798828125]
['model.relu.alpha_mask_4_0', 16384, 3343.0, 0.20404052734375]
['model.relu.alpha_mask_5_0', 8192, 2630.0, 0.321044921875]
['model.relu.alpha_mask_6_0', 8192, 2389.0, 0.2916259765625]
['model.relu.alpha_mask_7_0', 8192, 2418.0, 0.295166015625]
['model.relu.alpha_mask_8_0', 8192, 2316.0, 0.28271484375]
['model.relu.alpha_mask_9_0', 4096, 1725.0, 0.421142578125]
['model.relu.alpha_mask_10_0', 4096, 1662.0, 0.40576171875]
['model.relu.alpha_mask_11_0', 4096, 1832.0, 0.447265625]
['model.relu.alpha_mask_12_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_mask_13_0', 2048, 1694.0, 0.8271484375]
['model.relu.alpha_mask_14_0', 2048, 1911.0, 0.93310546875]
['model.relu.alpha_mask_15_0', 2048, 1500.0, 0.732421875]
['model.relu.alpha_mask_16_0', 2048, 1442.0, 0.7041015625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48575.0, 0.2578071925951087]
########## End ###########
06/02 01:32:13 PM | Train: [28/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 01:32:13 PM | layerwise density: [11387.0, 3678.0, 3299.0, 3669.0, 3343.0, 2630.0, 2389.0, 2418.0, 2316.0, 1725.0, 1662.0, 1832.0, 1680.0, 1694.0, 1911.0, 1500.0, 1442.0]
layerwise density percentage: ['0.174', '0.224', '0.201', '0.224', '0.204', '0.321', '0.292', '0.295', '0.283', '0.421', '0.406', '0.447', '0.410', '0.827', '0.933', '0.732', '0.704']
Global density: 0.257807195186615
06/02 01:32:24 PM | Train: [28/80] Step 100/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 01:32:24 PM | layerwise density: [11354.0, 3696.0, 3301.0, 3683.0, 3340.0, 2668.0, 2401.0, 2434.0, 2343.0, 1734.0, 1671.0, 1845.0, 1702.0, 1705.0, 1929.0, 1510.0, 1448.0]
layerwise density percentage: ['0.173', '0.226', '0.201', '0.225', '0.204', '0.326', '0.293', '0.297', '0.286', '0.423', '0.408', '0.450', '0.416', '0.833', '0.942', '0.737', '0.707']
Global density: 0.2588103115558624
06/02 01:32:35 PM | Train: [28/80] Step 200/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/02 01:32:35 PM | layerwise density: [11330.0, 3724.0, 3317.0, 3697.0, 3343.0, 2703.0, 2406.0, 2444.0, 2367.0, 1753.0, 1678.0, 1851.0, 1718.0, 1703.0, 1920.0, 1514.0, 1463.0]
layerwise density percentage: ['0.173', '0.227', '0.202', '0.226', '0.204', '0.330', '0.294', '0.298', '0.289', '0.428', '0.410', '0.452', '0.419', '0.832', '0.938', '0.739', '0.714']
Global density: 0.2596966326236725
06/02 01:32:46 PM | Train: [28/80] Step 300/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/02 01:32:46 PM | layerwise density: [11334.0, 3740.0, 3336.0, 3718.0, 3353.0, 2723.0, 2429.0, 2451.0, 2377.0, 1746.0, 1690.0, 1864.0, 1728.0, 1711.0, 1923.0, 1517.0, 1469.0]
layerwise density percentage: ['0.173', '0.228', '0.204', '0.227', '0.205', '0.332', '0.297', '0.299', '0.290', '0.426', '0.413', '0.455', '0.422', '0.835', '0.939', '0.741', '0.717']
Global density: 0.2606413662433624
06/02 01:32:55 PM | Train: [28/80] Step 390/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/02 01:32:55 PM | layerwise density: [11329.0, 3756.0, 3347.0, 3726.0, 3344.0, 2738.0, 2437.0, 2464.0, 2383.0, 1758.0, 1700.0, 1879.0, 1731.0, 1707.0, 1922.0, 1521.0, 1476.0]
layerwise density percentage: ['0.173', '0.229', '0.204', '0.227', '0.204', '0.334', '0.297', '0.301', '0.291', '0.429', '0.415', '0.459', '0.423', '0.833', '0.938', '0.743', '0.721']
Global density: 0.26121985912323
06/02 01:32:55 PM | Train: [28/200] Final Prec@1 99.8060%
06/02 01:32:56 PM | Valid: [28/200] Step 000/078 Loss 1.112 Prec@(1,5) (75.0%, 89.1%)
06/02 01:32:58 PM | Valid: [28/200] Step 078/078 Loss 1.355 Prec@(1,5) (69.0%, 88.8%)
06/02 01:32:58 PM | Valid: [28/200] Final Prec@1 69.0000%
06/02 01:32:58 PM | Current mask training best Prec@1 = 69.0000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 11330.0, 0.172882080078125]
['model.relu.alpha_mask_1_0', 16384, 3757.0, 0.22930908203125]
['model.relu.alpha_mask_2_0', 16384, 3347.0, 0.20428466796875]
['model.relu.alpha_mask_3_0', 16384, 3726.0, 0.2274169921875]
['model.relu.alpha_mask_4_0', 16384, 3346.0, 0.2042236328125]
['model.relu.alpha_mask_5_0', 8192, 2736.0, 0.333984375]
['model.relu.alpha_mask_6_0', 8192, 2437.0, 0.2974853515625]
['model.relu.alpha_mask_7_0', 8192, 2463.0, 0.3006591796875]
['model.relu.alpha_mask_8_0', 8192, 2385.0, 0.2911376953125]
['model.relu.alpha_mask_9_0', 4096, 1759.0, 0.429443359375]
['model.relu.alpha_mask_10_0', 4096, 1701.0, 0.415283203125]
['model.relu.alpha_mask_11_0', 4096, 1879.0, 0.458740234375]
['model.relu.alpha_mask_12_0', 4096, 1731.0, 0.422607421875]
['model.relu.alpha_mask_13_0', 2048, 1708.0, 0.833984375]
['model.relu.alpha_mask_14_0', 2048, 1921.0, 0.93798828125]
['model.relu.alpha_mask_15_0', 2048, 1522.0, 0.7431640625]
['model.relu.alpha_mask_16_0', 2048, 1476.0, 0.720703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49224.0, 0.26125169836956524]
########## End ###########
06/02 01:33:00 PM | Train: [29/80] Step 000/390 Loss 0.015 Prec@(1,5) (99.2%, 100.0%)
06/02 01:33:00 PM | layerwise density: [11330.0, 3757.0, 3347.0, 3726.0, 3346.0, 2736.0, 2437.0, 2463.0, 2385.0, 1759.0, 1701.0, 1879.0, 1731.0, 1708.0, 1921.0, 1522.0, 1476.0]
layerwise density percentage: ['0.173', '0.229', '0.204', '0.227', '0.204', '0.334', '0.297', '0.301', '0.291', '0.429', '0.415', '0.459', '0.423', '0.834', '0.938', '0.743', '0.721']
Global density: 0.2612517178058624
06/02 01:33:10 PM | Train: [29/80] Step 100/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 01:33:10 PM | layerwise density: [11326.0, 3774.0, 3354.0, 3753.0, 3357.0, 2741.0, 2461.0, 2478.0, 2386.0, 1760.0, 1703.0, 1883.0, 1732.0, 1702.0, 1927.0, 1529.0, 1483.0]
layerwise density percentage: ['0.173', '0.230', '0.205', '0.229', '0.205', '0.335', '0.300', '0.302', '0.291', '0.430', '0.416', '0.460', '0.423', '0.831', '0.941', '0.747', '0.724']
Global density: 0.2619151175022125
06/02 01:33:21 PM | Train: [29/80] Step 200/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 01:33:21 PM | layerwise density: [11311.0, 3785.0, 3351.0, 3770.0, 3383.0, 2761.0, 2475.0, 2484.0, 2404.0, 1767.0, 1719.0, 1886.0, 1724.0, 1708.0, 1936.0, 1528.0, 1489.0]
layerwise density percentage: ['0.173', '0.231', '0.205', '0.230', '0.206', '0.337', '0.302', '0.303', '0.293', '0.431', '0.420', '0.460', '0.421', '0.834', '0.945', '0.746', '0.727']
Global density: 0.26261571049690247
06/02 01:33:32 PM | Train: [29/80] Step 300/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 01:33:32 PM | layerwise density: [11312.0, 3796.0, 3357.0, 3786.0, 3390.0, 2774.0, 2478.0, 2496.0, 2421.0, 1774.0, 1714.0, 1895.0, 1720.0, 1712.0, 1943.0, 1529.0, 1498.0]
layerwise density percentage: ['0.173', '0.232', '0.205', '0.231', '0.207', '0.339', '0.302', '0.305', '0.296', '0.433', '0.418', '0.463', '0.420', '0.836', '0.949', '0.747', '0.731']
Global density: 0.26322075724601746
06/02 01:33:42 PM | Train: [29/80] Step 390/390 Loss 0.015 Prec@(1,5) (99.9%, 100.0%)
06/02 01:33:42 PM | layerwise density: [11309.0, 3807.0, 3374.0, 3816.0, 3391.0, 2790.0, 2504.0, 2508.0, 2433.0, 1782.0, 1722.0, 1901.0, 1732.0, 1712.0, 1941.0, 1532.0, 1506.0]
layerwise density percentage: ['0.173', '0.232', '0.206', '0.233', '0.207', '0.341', '0.306', '0.306', '0.297', '0.435', '0.420', '0.464', '0.423', '0.836', '0.948', '0.748', '0.735']
Global density: 0.2640964686870575
06/02 01:33:42 PM | Train: [29/200] Final Prec@1 99.8500%
06/02 01:33:42 PM | Valid: [29/200] Step 000/078 Loss 1.146 Prec@(1,5) (71.1%, 91.4%)
06/02 01:33:45 PM | Valid: [29/200] Step 078/078 Loss 1.355 Prec@(1,5) (69.1%, 88.9%)
06/02 01:33:45 PM | Valid: [29/200] Final Prec@1 69.1100%
06/02 01:33:45 PM | Current mask training best Prec@1 = 69.1100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 11309.0, 0.1725616455078125]
['model.relu.alpha_mask_1_0', 16384, 3808.0, 0.232421875]
['model.relu.alpha_mask_2_0', 16384, 3374.0, 0.2059326171875]
['model.relu.alpha_mask_3_0', 16384, 3818.0, 0.2330322265625]
['model.relu.alpha_mask_4_0', 16384, 3391.0, 0.20697021484375]
['model.relu.alpha_mask_5_0', 8192, 2790.0, 0.340576171875]
['model.relu.alpha_mask_6_0', 8192, 2503.0, 0.3055419921875]
['model.relu.alpha_mask_7_0', 8192, 2507.0, 0.3060302734375]
['model.relu.alpha_mask_8_0', 8192, 2433.0, 0.2969970703125]
['model.relu.alpha_mask_9_0', 4096, 1782.0, 0.43505859375]
['model.relu.alpha_mask_10_0', 4096, 1720.0, 0.419921875]
['model.relu.alpha_mask_11_0', 4096, 1901.0, 0.464111328125]
['model.relu.alpha_mask_12_0', 4096, 1732.0, 0.4228515625]
['model.relu.alpha_mask_13_0', 2048, 1713.0, 0.83642578125]
['model.relu.alpha_mask_14_0', 2048, 1940.0, 0.947265625]
['model.relu.alpha_mask_15_0', 2048, 1533.0, 0.74853515625]
['model.relu.alpha_mask_16_0', 2048, 1506.0, 0.7353515625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49760.0, 0.2640964673913043]
########## End ###########
06/02 01:33:46 PM | Train: [30/80] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/02 01:33:46 PM | layerwise density: [11309.0, 3808.0, 3374.0, 3818.0, 3391.0, 2790.0, 2503.0, 2507.0, 2433.0, 1782.0, 1720.0, 1901.0, 1732.0, 1713.0, 1940.0, 1533.0, 1506.0]
layerwise density percentage: ['0.173', '0.232', '0.206', '0.233', '0.207', '0.341', '0.306', '0.306', '0.297', '0.435', '0.420', '0.464', '0.423', '0.836', '0.947', '0.749', '0.735']
Global density: 0.2640964686870575
06/02 01:33:57 PM | Train: [30/80] Step 100/390 Loss 0.014 Prec@(1,5) (99.9%, 100.0%)
06/02 01:33:57 PM | layerwise density: [11317.0, 3821.0, 3373.0, 3835.0, 3414.0, 2800.0, 2509.0, 2519.0, 2424.0, 1791.0, 1729.0, 1909.0, 1745.0, 1705.0, 1951.0, 1544.0, 1517.0]
layerwise density percentage: ['0.173', '0.233', '0.206', '0.234', '0.208', '0.342', '0.306', '0.307', '0.296', '0.437', '0.422', '0.466', '0.426', '0.833', '0.953', '0.754', '0.741']
Global density: 0.26485544443130493
06/02 01:34:07 PM | Train: [30/80] Step 200/390 Loss 0.014 Prec@(1,5) (99.9%, 100.0%)
06/02 01:34:07 PM | layerwise density: [11074.0, 3750.0, 3320.0, 3765.0, 3322.0, 2758.0, 2463.0, 2481.0, 2386.0, 1778.0, 1713.0, 1908.0, 1722.0, 1706.0, 1922.0, 1524.0, 1530.0]
layerwise density percentage: ['0.169', '0.229', '0.203', '0.230', '0.203', '0.337', '0.301', '0.303', '0.291', '0.434', '0.418', '0.466', '0.420', '0.833', '0.938', '0.744', '0.747']
Global density: 0.26071035861968994
06/02 01:34:18 PM | Train: [30/80] Step 300/390 Loss 0.015 Prec@(1,5) (99.9%, 100.0%)
06/02 01:34:18 PM | layerwise density: [10952.0, 3688.0, 3252.0, 3708.0, 3243.0, 2691.0, 2413.0, 2458.0, 2306.0, 1746.0, 1707.0, 1906.0, 1716.0, 1707.0, 1936.0, 1524.0, 1533.0]
layerwise density percentage: ['0.167', '0.225', '0.198', '0.226', '0.198', '0.328', '0.295', '0.300', '0.281', '0.426', '0.417', '0.465', '0.419', '0.833', '0.945', '0.744', '0.749']
Global density: 0.25733482837677
06/02 01:34:27 PM | Train: [30/80] Step 390/390 Loss 0.015 Prec@(1,5) (99.9%, 100.0%)
06/02 01:34:27 PM | layerwise density: [10913.0, 3675.0, 3241.0, 3695.0, 3238.0, 2703.0, 2416.0, 2465.0, 2315.0, 1772.0, 1706.0, 1904.0, 1718.0, 1713.0, 1939.0, 1540.0, 1543.0]
layerwise density percentage: ['0.167', '0.224', '0.198', '0.226', '0.198', '0.330', '0.295', '0.301', '0.283', '0.433', '0.417', '0.465', '0.419', '0.836', '0.947', '0.752', '0.753']
Global density: 0.2573879063129425
06/02 01:34:27 PM | Train: [30/200] Final Prec@1 99.8520%
06/02 01:34:27 PM | Valid: [30/200] Step 000/078 Loss 1.077 Prec@(1,5) (75.0%, 92.2%)
06/02 01:34:30 PM | Valid: [30/200] Step 078/078 Loss 1.332 Prec@(1,5) (69.2%, 89.2%)
06/02 01:34:30 PM | Valid: [30/200] Final Prec@1 69.1800%
06/02 01:34:30 PM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 10913.0, 0.1665191650390625]
['model.relu.alpha_mask_1_0', 16384, 3675.0, 0.22430419921875]
['model.relu.alpha_mask_2_0', 16384, 3241.0, 0.19781494140625]
['model.relu.alpha_mask_3_0', 16384, 3695.0, 0.22552490234375]
['model.relu.alpha_mask_4_0', 16384, 3238.0, 0.1976318359375]
['model.relu.alpha_mask_5_0', 8192, 2703.0, 0.3299560546875]
['model.relu.alpha_mask_6_0', 8192, 2416.0, 0.294921875]
['model.relu.alpha_mask_7_0', 8192, 2465.0, 0.3009033203125]
['model.relu.alpha_mask_8_0', 8192, 2315.0, 0.2825927734375]
['model.relu.alpha_mask_9_0', 4096, 1772.0, 0.4326171875]
['model.relu.alpha_mask_10_0', 4096, 1706.0, 0.41650390625]
['model.relu.alpha_mask_11_0', 4096, 1904.0, 0.46484375]
['model.relu.alpha_mask_12_0', 4096, 1718.0, 0.41943359375]
['model.relu.alpha_mask_13_0', 2048, 1713.0, 0.83642578125]
['model.relu.alpha_mask_14_0', 2048, 1939.0, 0.94677734375]
['model.relu.alpha_mask_15_0', 2048, 1540.0, 0.751953125]
['model.relu.alpha_mask_16_0', 2048, 1543.0, 0.75341796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48496.0, 0.2573879076086957]
########## End ###########
06/02 01:34:31 PM | Train: [31/80] Step 000/390 Loss 0.016 Prec@(1,5) (100.0%, 100.0%)
06/02 01:34:31 PM | layerwise density: [10913.0, 3675.0, 3241.0, 3695.0, 3238.0, 2703.0, 2416.0, 2465.0, 2315.0, 1772.0, 1706.0, 1904.0, 1718.0, 1713.0, 1939.0, 1540.0, 1543.0]
layerwise density percentage: ['0.167', '0.224', '0.198', '0.226', '0.198', '0.330', '0.295', '0.301', '0.283', '0.433', '0.417', '0.465', '0.419', '0.836', '0.947', '0.752', '0.753']
Global density: 0.2573879063129425
06/02 01:34:42 PM | Train: [31/80] Step 100/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 01:34:42 PM | layerwise density: [10903.0, 3687.0, 3235.0, 3694.0, 3255.0, 2728.0, 2423.0, 2482.0, 2355.0, 1782.0, 1723.0, 1912.0, 1744.0, 1721.0, 1946.0, 1544.0, 1548.0]
layerwise density percentage: ['0.166', '0.225', '0.197', '0.225', '0.199', '0.333', '0.296', '0.303', '0.287', '0.435', '0.421', '0.467', '0.426', '0.840', '0.950', '0.754', '0.756']
Global density: 0.2583750784397125
06/02 01:34:53 PM | Train: [31/80] Step 200/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 01:34:53 PM | layerwise density: [10890.0, 3710.0, 3255.0, 3702.0, 3271.0, 2753.0, 2444.0, 2488.0, 2384.0, 1795.0, 1727.0, 1925.0, 1767.0, 1725.0, 1958.0, 1551.0, 1551.0]
layerwise density percentage: ['0.166', '0.226', '0.199', '0.226', '0.200', '0.336', '0.298', '0.304', '0.291', '0.438', '0.422', '0.470', '0.431', '0.842', '0.956', '0.757', '0.757']
Global density: 0.25951087474823
06/02 01:35:04 PM | Train: [31/80] Step 300/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 01:35:04 PM | layerwise density: [10874.0, 3723.0, 3266.0, 3722.0, 3272.0, 2765.0, 2465.0, 2495.0, 2385.0, 1791.0, 1732.0, 1931.0, 1761.0, 1717.0, 1952.0, 1553.0, 1559.0]
layerwise density percentage: ['0.166', '0.227', '0.199', '0.227', '0.200', '0.338', '0.301', '0.305', '0.291', '0.437', '0.423', '0.471', '0.430', '0.838', '0.953', '0.758', '0.761']
Global density: 0.25986647605895996
06/02 01:35:13 PM | Train: [31/80] Step 390/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 01:35:13 PM | layerwise density: [10875.0, 3737.0, 3271.0, 3742.0, 3288.0, 2772.0, 2480.0, 2503.0, 2387.0, 1789.0, 1731.0, 1932.0, 1753.0, 1724.0, 1948.0, 1554.0, 1563.0]
layerwise density percentage: ['0.166', '0.228', '0.200', '0.228', '0.201', '0.338', '0.303', '0.306', '0.291', '0.437', '0.423', '0.472', '0.428', '0.842', '0.951', '0.759', '0.763']
Global density: 0.2603228986263275
06/02 01:35:13 PM | Train: [31/200] Final Prec@1 99.8140%
06/02 01:35:14 PM | Valid: [31/200] Step 000/078 Loss 1.133 Prec@(1,5) (75.0%, 89.1%)
06/02 01:35:16 PM | Valid: [31/200] Step 078/078 Loss 1.329 Prec@(1,5) (69.1%, 89.1%)
06/02 01:35:16 PM | Valid: [31/200] Final Prec@1 69.1000%
06/02 01:35:16 PM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 10875.0, 0.1659393310546875]
['model.relu.alpha_mask_1_0', 16384, 3737.0, 0.22808837890625]
['model.relu.alpha_mask_2_0', 16384, 3272.0, 0.19970703125]
['model.relu.alpha_mask_3_0', 16384, 3743.0, 0.22845458984375]
['model.relu.alpha_mask_4_0', 16384, 3288.0, 0.20068359375]
['model.relu.alpha_mask_5_0', 8192, 2772.0, 0.33837890625]
['model.relu.alpha_mask_6_0', 8192, 2480.0, 0.302734375]
['model.relu.alpha_mask_7_0', 8192, 2504.0, 0.3056640625]
['model.relu.alpha_mask_8_0', 8192, 2388.0, 0.29150390625]
['model.relu.alpha_mask_9_0', 4096, 1789.0, 0.436767578125]
['model.relu.alpha_mask_10_0', 4096, 1731.0, 0.422607421875]
['model.relu.alpha_mask_11_0', 4096, 1932.0, 0.4716796875]
['model.relu.alpha_mask_12_0', 4096, 1754.0, 0.42822265625]
['model.relu.alpha_mask_13_0', 2048, 1724.0, 0.841796875]
['model.relu.alpha_mask_14_0', 2048, 1948.0, 0.951171875]
['model.relu.alpha_mask_15_0', 2048, 1554.0, 0.7587890625]
['model.relu.alpha_mask_16_0', 2048, 1563.0, 0.76318359375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49054.0, 0.26034943953804346]
########## End ###########
06/02 01:35:17 PM | Train: [32/80] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/02 01:35:17 PM | layerwise density: [10875.0, 3737.0, 3272.0, 3743.0, 3288.0, 2772.0, 2480.0, 2504.0, 2388.0, 1789.0, 1731.0, 1932.0, 1754.0, 1724.0, 1948.0, 1554.0, 1563.0]
layerwise density percentage: ['0.166', '0.228', '0.200', '0.228', '0.201', '0.338', '0.303', '0.306', '0.292', '0.437', '0.423', '0.472', '0.428', '0.842', '0.951', '0.759', '0.763']
Global density: 0.26034945249557495
06/02 01:35:28 PM | Train: [32/80] Step 100/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 01:35:28 PM | layerwise density: [10883.0, 3753.0, 3273.0, 3752.0, 3303.0, 2775.0, 2498.0, 2517.0, 2404.0, 1811.0, 1725.0, 1936.0, 1762.0, 1733.0, 1947.0, 1554.0, 1573.0]
layerwise density percentage: ['0.166', '0.229', '0.200', '0.229', '0.202', '0.339', '0.305', '0.307', '0.293', '0.442', '0.421', '0.473', '0.430', '0.846', '0.951', '0.759', '0.768']
Global density: 0.26111900806427
06/02 01:35:38 PM | Train: [32/80] Step 200/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 01:35:38 PM | layerwise density: [10872.0, 3762.0, 3280.0, 3762.0, 3309.0, 2800.0, 2504.0, 2524.0, 2406.0, 1814.0, 1732.0, 1945.0, 1759.0, 1740.0, 1962.0, 1555.0, 1581.0]
layerwise density percentage: ['0.166', '0.230', '0.200', '0.230', '0.202', '0.342', '0.306', '0.308', '0.294', '0.443', '0.423', '0.475', '0.429', '0.850', '0.958', '0.759', '0.772']
Global density: 0.26169222593307495
06/02 01:35:49 PM | Train: [32/80] Step 300/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/02 01:35:49 PM | layerwise density: [10874.0, 3792.0, 3289.0, 3773.0, 3324.0, 2809.0, 2523.0, 2522.0, 2417.0, 1822.0, 1743.0, 1947.0, 1764.0, 1737.0, 1958.0, 1559.0, 1585.0]
layerwise density percentage: ['0.166', '0.231', '0.201', '0.230', '0.203', '0.343', '0.308', '0.308', '0.295', '0.445', '0.426', '0.475', '0.431', '0.848', '0.956', '0.761', '0.774']
Global density: 0.2623874843120575
06/02 01:35:59 PM | Train: [32/80] Step 390/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/02 01:35:59 PM | layerwise density: [10874.0, 3819.0, 3310.0, 3789.0, 3339.0, 2826.0, 2518.0, 2525.0, 2430.0, 1804.0, 1755.0, 1952.0, 1774.0, 1741.0, 1960.0, 1565.0, 1590.0]
layerwise density percentage: ['0.166', '0.233', '0.202', '0.231', '0.204', '0.345', '0.307', '0.308', '0.297', '0.440', '0.428', '0.477', '0.433', '0.850', '0.957', '0.764', '0.776']
Global density: 0.26309338212013245
06/02 01:35:59 PM | Train: [32/200] Final Prec@1 99.8920%
06/02 01:35:59 PM | Valid: [32/200] Step 000/078 Loss 1.107 Prec@(1,5) (75.0%, 91.4%)
06/02 01:36:01 PM | Valid: [32/200] Step 078/078 Loss 1.345 Prec@(1,5) (69.2%, 89.1%)
06/02 01:36:01 PM | Valid: [32/200] Final Prec@1 69.1500%
06/02 01:36:01 PM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 10874.0, 0.165924072265625]
['model.relu.alpha_mask_1_0', 16384, 3818.0, 0.2330322265625]
['model.relu.alpha_mask_2_0', 16384, 3310.0, 0.2020263671875]
['model.relu.alpha_mask_3_0', 16384, 3789.0, 0.23126220703125]
['model.relu.alpha_mask_4_0', 16384, 3338.0, 0.2037353515625]
['model.relu.alpha_mask_5_0', 8192, 2825.0, 0.3448486328125]
['model.relu.alpha_mask_6_0', 8192, 2518.0, 0.307373046875]
['model.relu.alpha_mask_7_0', 8192, 2526.0, 0.308349609375]
['model.relu.alpha_mask_8_0', 8192, 2431.0, 0.2967529296875]
['model.relu.alpha_mask_9_0', 4096, 1805.0, 0.440673828125]
['model.relu.alpha_mask_10_0', 4096, 1756.0, 0.4287109375]
['model.relu.alpha_mask_11_0', 4096, 1952.0, 0.4765625]
['model.relu.alpha_mask_12_0', 4096, 1773.0, 0.432861328125]
['model.relu.alpha_mask_13_0', 2048, 1741.0, 0.85009765625]
['model.relu.alpha_mask_14_0', 2048, 1960.0, 0.95703125]
['model.relu.alpha_mask_15_0', 2048, 1565.0, 0.76416015625]
['model.relu.alpha_mask_16_0', 2048, 1590.0, 0.7763671875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49571.0, 0.26309336786684784]
########## End ###########
06/02 01:36:02 PM | Train: [33/80] Step 000/390 Loss 0.015 Prec@(1,5) (100.0%, 100.0%)
06/02 01:36:02 PM | layerwise density: [10874.0, 3818.0, 3310.0, 3789.0, 3338.0, 2825.0, 2518.0, 2526.0, 2431.0, 1805.0, 1756.0, 1952.0, 1773.0, 1741.0, 1960.0, 1565.0, 1590.0]
layerwise density percentage: ['0.166', '0.233', '0.202', '0.231', '0.204', '0.345', '0.307', '0.308', '0.297', '0.441', '0.429', '0.477', '0.433', '0.850', '0.957', '0.764', '0.776']
Global density: 0.26309338212013245
06/02 01:36:13 PM | Train: [33/80] Step 100/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 01:36:13 PM | layerwise density: [10879.0, 3830.0, 3334.0, 3801.0, 3354.0, 2841.0, 2506.0, 2545.0, 2443.0, 1817.0, 1755.0, 1958.0, 1778.0, 1744.0, 1960.0, 1576.0, 1594.0]
layerwise density percentage: ['0.166', '0.234', '0.203', '0.232', '0.205', '0.347', '0.306', '0.311', '0.298', '0.444', '0.428', '0.478', '0.434', '0.852', '0.957', '0.770', '0.778']
Global density: 0.2638576328754425
06/02 01:36:24 PM | Train: [33/80] Step 200/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 01:36:24 PM | layerwise density: [10889.0, 3830.0, 3348.0, 3809.0, 3357.0, 2839.0, 2525.0, 2559.0, 2449.0, 1824.0, 1764.0, 1965.0, 1758.0, 1739.0, 1955.0, 1569.0, 1598.0]
layerwise density percentage: ['0.166', '0.234', '0.204', '0.232', '0.205', '0.347', '0.308', '0.312', '0.299', '0.445', '0.431', '0.480', '0.429', '0.849', '0.955', '0.766', '0.780']
Global density: 0.26418671011924744
06/02 01:36:35 PM | Train: [33/80] Step 300/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 01:36:35 PM | layerwise density: [10889.0, 3839.0, 3358.0, 3833.0, 3363.0, 2854.0, 2537.0, 2568.0, 2456.0, 1839.0, 1774.0, 1967.0, 1776.0, 1738.0, 1960.0, 1576.0, 1610.0]
layerwise density percentage: ['0.166', '0.234', '0.205', '0.234', '0.205', '0.348', '0.310', '0.313', '0.300', '0.449', '0.433', '0.480', '0.434', '0.849', '0.957', '0.770', '0.786']
Global density: 0.2650358974933624
06/02 01:36:45 PM | Train: [33/80] Step 390/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 01:36:45 PM | layerwise density: [10868.0, 3848.0, 3359.0, 3853.0, 3367.0, 2868.0, 2538.0, 2575.0, 2459.0, 1842.0, 1776.0, 1972.0, 1780.0, 1731.0, 1955.0, 1584.0, 1615.0]
layerwise density percentage: ['0.166', '0.235', '0.205', '0.235', '0.206', '0.350', '0.310', '0.314', '0.300', '0.450', '0.434', '0.481', '0.435', '0.845', '0.955', '0.773', '0.789']
Global density: 0.2653171718120575
06/02 01:36:45 PM | Train: [33/200] Final Prec@1 99.8920%
06/02 01:36:45 PM | Valid: [33/200] Step 000/078 Loss 1.124 Prec@(1,5) (73.4%, 92.2%)
06/02 01:36:47 PM | Valid: [33/200] Step 078/078 Loss 1.333 Prec@(1,5) (69.0%, 89.1%)
06/02 01:36:47 PM | Valid: [33/200] Final Prec@1 68.9700%
06/02 01:36:47 PM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 10869.0, 0.1658477783203125]
['model.relu.alpha_mask_1_0', 16384, 3850.0, 0.2349853515625]
['model.relu.alpha_mask_2_0', 16384, 3359.0, 0.20501708984375]
['model.relu.alpha_mask_3_0', 16384, 3853.0, 0.23516845703125]
['model.relu.alpha_mask_4_0', 16384, 3366.0, 0.2054443359375]
['model.relu.alpha_mask_5_0', 8192, 2867.0, 0.3499755859375]
['model.relu.alpha_mask_6_0', 8192, 2538.0, 0.309814453125]
['model.relu.alpha_mask_7_0', 8192, 2575.0, 0.3143310546875]
['model.relu.alpha_mask_8_0', 8192, 2459.0, 0.3001708984375]
['model.relu.alpha_mask_9_0', 4096, 1843.0, 0.449951171875]
['model.relu.alpha_mask_10_0', 4096, 1776.0, 0.43359375]
['model.relu.alpha_mask_11_0', 4096, 1972.0, 0.4814453125]
['model.relu.alpha_mask_12_0', 4096, 1779.0, 0.434326171875]
['model.relu.alpha_mask_13_0', 2048, 1732.0, 0.845703125]
['model.relu.alpha_mask_14_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_15_0', 2048, 1584.0, 0.7734375]
['model.relu.alpha_mask_16_0', 2048, 1615.0, 0.78857421875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49992.0, 0.265327785326087]
########## End ###########
06/02 01:36:48 PM | Train: [34/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 01:36:48 PM | layerwise density: [10869.0, 3850.0, 3359.0, 3853.0, 3366.0, 2867.0, 2538.0, 2575.0, 2459.0, 1843.0, 1776.0, 1972.0, 1779.0, 1732.0, 1955.0, 1584.0, 1615.0]
layerwise density percentage: ['0.166', '0.235', '0.205', '0.235', '0.205', '0.350', '0.310', '0.314', '0.300', '0.450', '0.434', '0.481', '0.434', '0.846', '0.955', '0.773', '0.789']
Global density: 0.2653277814388275
06/02 01:36:59 PM | Train: [34/80] Step 100/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
06/02 01:36:59 PM | layerwise density: [10503.0, 3698.0, 3211.0, 3735.0, 3205.0, 2758.0, 2449.0, 2508.0, 2337.0, 1796.0, 1716.0, 1951.0, 1736.0, 1726.0, 1933.0, 1549.0, 1616.0]
layerwise density percentage: ['0.160', '0.226', '0.196', '0.228', '0.196', '0.337', '0.299', '0.306', '0.285', '0.438', '0.419', '0.476', '0.424', '0.843', '0.944', '0.756', '0.789']
Global density: 0.2570216953754425
06/02 01:37:10 PM | Train: [34/80] Step 200/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:37:10 PM | layerwise density: [10437.0, 3679.0, 3195.0, 3719.0, 3182.0, 2740.0, 2423.0, 2493.0, 2322.0, 1801.0, 1721.0, 1953.0, 1737.0, 1720.0, 1948.0, 1558.0, 1618.0]
layerwise density percentage: ['0.159', '0.225', '0.195', '0.227', '0.194', '0.334', '0.296', '0.304', '0.283', '0.440', '0.420', '0.477', '0.424', '0.840', '0.951', '0.761', '0.790']
Global density: 0.25606104731559753
06/02 01:37:21 PM | Train: [34/80] Step 300/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:37:21 PM | layerwise density: [10395.0, 3677.0, 3204.0, 3717.0, 3169.0, 2771.0, 2428.0, 2497.0, 2344.0, 1821.0, 1732.0, 1964.0, 1758.0, 1735.0, 1958.0, 1578.0, 1628.0]
layerwise density percentage: ['0.159', '0.224', '0.196', '0.227', '0.193', '0.338', '0.296', '0.305', '0.286', '0.445', '0.423', '0.479', '0.429', '0.847', '0.956', '0.771', '0.795']
Global density: 0.25675103068351746
06/02 01:37:30 PM | Train: [34/80] Step 390/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 01:37:30 PM | layerwise density: [10382.0, 3704.0, 3209.0, 3718.0, 3198.0, 2790.0, 2445.0, 2514.0, 2361.0, 1834.0, 1734.0, 1969.0, 1777.0, 1745.0, 1954.0, 1586.0, 1631.0]
layerwise density percentage: ['0.158', '0.226', '0.196', '0.227', '0.195', '0.341', '0.298', '0.307', '0.288', '0.448', '0.423', '0.481', '0.434', '0.852', '0.954', '0.774', '0.796']
Global density: 0.25767982006073
06/02 01:37:30 PM | Train: [34/200] Final Prec@1 99.8900%
06/02 01:37:31 PM | Valid: [34/200] Step 000/078 Loss 1.125 Prec@(1,5) (75.0%, 93.0%)
06/02 01:37:33 PM | Valid: [34/200] Step 078/078 Loss 1.338 Prec@(1,5) (68.8%, 89.2%)
06/02 01:37:33 PM | Valid: [34/200] Final Prec@1 68.7700%
06/02 01:37:33 PM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 10383.0, 0.1584320068359375]
['model.relu.alpha_mask_1_0', 16384, 3702.0, 0.2259521484375]
['model.relu.alpha_mask_2_0', 16384, 3209.0, 0.19586181640625]
['model.relu.alpha_mask_3_0', 16384, 3718.0, 0.2269287109375]
['model.relu.alpha_mask_4_0', 16384, 3196.0, 0.195068359375]
['model.relu.alpha_mask_5_0', 8192, 2790.0, 0.340576171875]
['model.relu.alpha_mask_6_0', 8192, 2448.0, 0.298828125]
['model.relu.alpha_mask_7_0', 8192, 2515.0, 0.3070068359375]
['model.relu.alpha_mask_8_0', 8192, 2359.0, 0.2879638671875]
['model.relu.alpha_mask_9_0', 4096, 1835.0, 0.447998046875]
['model.relu.alpha_mask_10_0', 4096, 1733.0, 0.423095703125]
['model.relu.alpha_mask_11_0', 4096, 1969.0, 0.480712890625]
['model.relu.alpha_mask_12_0', 4096, 1778.0, 0.43408203125]
['model.relu.alpha_mask_13_0', 2048, 1745.0, 0.85205078125]
['model.relu.alpha_mask_14_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_15_0', 2048, 1587.0, 0.77490234375]
['model.relu.alpha_mask_16_0', 2048, 1632.0, 0.796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48554.0, 0.2576957370923913]
########## End ###########
06/02 01:37:34 PM | Train: [35/80] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/02 01:37:34 PM | layerwise density: [10383.0, 3702.0, 3209.0, 3718.0, 3196.0, 2790.0, 2448.0, 2515.0, 2359.0, 1835.0, 1733.0, 1969.0, 1778.0, 1745.0, 1955.0, 1587.0, 1632.0]
layerwise density percentage: ['0.158', '0.226', '0.196', '0.227', '0.195', '0.341', '0.299', '0.307', '0.288', '0.448', '0.423', '0.481', '0.434', '0.852', '0.955', '0.775', '0.797']
Global density: 0.257695734500885
06/02 01:37:44 PM | Train: [35/80] Step 100/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:37:44 PM | layerwise density: [10376.0, 3704.0, 3209.0, 3721.0, 3210.0, 2783.0, 2478.0, 2529.0, 2399.0, 1845.0, 1735.0, 1971.0, 1779.0, 1744.0, 1957.0, 1592.0, 1639.0]
layerwise density percentage: ['0.158', '0.226', '0.196', '0.227', '0.196', '0.340', '0.302', '0.309', '0.293', '0.450', '0.424', '0.481', '0.434', '0.852', '0.956', '0.777', '0.800']
Global density: 0.25831669569015503
06/02 01:37:55 PM | Train: [35/80] Step 200/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:37:55 PM | layerwise density: [10379.0, 3722.0, 3212.0, 3732.0, 3228.0, 2793.0, 2478.0, 2539.0, 2408.0, 1841.0, 1756.0, 1974.0, 1782.0, 1746.0, 1969.0, 1588.0, 1642.0]
layerwise density percentage: ['0.158', '0.227', '0.196', '0.228', '0.197', '0.341', '0.302', '0.310', '0.294', '0.449', '0.429', '0.482', '0.435', '0.853', '0.961', '0.775', '0.802']
Global density: 0.25894299149513245
06/02 01:38:06 PM | Train: [35/80] Step 300/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:38:06 PM | layerwise density: [10374.0, 3751.0, 3223.0, 3739.0, 3235.0, 2823.0, 2485.0, 2546.0, 2421.0, 1847.0, 1767.0, 1984.0, 1784.0, 1755.0, 1962.0, 1588.0, 1650.0]
layerwise density percentage: ['0.158', '0.229', '0.197', '0.228', '0.197', '0.345', '0.303', '0.311', '0.296', '0.451', '0.431', '0.484', '0.436', '0.857', '0.958', '0.775', '0.806']
Global density: 0.2597125470638275
06/02 01:38:16 PM | Train: [35/80] Step 390/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:38:16 PM | layerwise density: [10379.0, 3765.0, 3230.0, 3755.0, 3258.0, 2842.0, 2501.0, 2554.0, 2440.0, 1849.0, 1771.0, 1992.0, 1799.0, 1751.0, 1955.0, 1589.0, 1652.0]
layerwise density percentage: ['0.158', '0.230', '0.197', '0.229', '0.199', '0.347', '0.305', '0.312', '0.298', '0.451', '0.432', '0.486', '0.439', '0.855', '0.955', '0.776', '0.807']
Global density: 0.260498046875
06/02 01:38:16 PM | Train: [35/200] Final Prec@1 99.8960%
06/02 01:38:16 PM | Valid: [35/200] Step 000/078 Loss 1.146 Prec@(1,5) (75.8%, 90.6%)
06/02 01:38:18 PM | Valid: [35/200] Step 078/078 Loss 1.337 Prec@(1,5) (69.1%, 89.3%)
06/02 01:38:19 PM | Valid: [35/200] Final Prec@1 69.1200%
06/02 01:38:19 PM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 10379.0, 0.1583709716796875]
['model.relu.alpha_mask_1_0', 16384, 3764.0, 0.229736328125]
['model.relu.alpha_mask_2_0', 16384, 3229.0, 0.19708251953125]
['model.relu.alpha_mask_3_0', 16384, 3756.0, 0.229248046875]
['model.relu.alpha_mask_4_0', 16384, 3258.0, 0.1988525390625]
['model.relu.alpha_mask_5_0', 8192, 2844.0, 0.34716796875]
['model.relu.alpha_mask_6_0', 8192, 2501.0, 0.3052978515625]
['model.relu.alpha_mask_7_0', 8192, 2553.0, 0.3116455078125]
['model.relu.alpha_mask_8_0', 8192, 2443.0, 0.2982177734375]
['model.relu.alpha_mask_9_0', 4096, 1850.0, 0.45166015625]
['model.relu.alpha_mask_10_0', 4096, 1770.0, 0.43212890625]
['model.relu.alpha_mask_11_0', 4096, 1992.0, 0.486328125]
['model.relu.alpha_mask_12_0', 4096, 1799.0, 0.439208984375]
['model.relu.alpha_mask_13_0', 2048, 1751.0, 0.85498046875]
['model.relu.alpha_mask_14_0', 2048, 1956.0, 0.955078125]
['model.relu.alpha_mask_15_0', 2048, 1589.0, 0.77587890625]
['model.relu.alpha_mask_16_0', 2048, 1652.0, 0.806640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49086.0, 0.26051927649456524]
########## End ###########
06/02 01:38:20 PM | Train: [36/80] Step 000/390 Loss 0.041 Prec@(1,5) (99.2%, 100.0%)
06/02 01:38:20 PM | layerwise density: [10379.0, 3764.0, 3229.0, 3756.0, 3258.0, 2844.0, 2501.0, 2553.0, 2443.0, 1850.0, 1770.0, 1992.0, 1799.0, 1751.0, 1956.0, 1589.0, 1652.0]
layerwise density percentage: ['0.158', '0.230', '0.197', '0.229', '0.199', '0.347', '0.305', '0.312', '0.298', '0.452', '0.432', '0.486', '0.439', '0.855', '0.955', '0.776', '0.807']
Global density: 0.2605192959308624
06/02 01:38:30 PM | Train: [36/80] Step 100/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:38:30 PM | layerwise density: [10391.0, 3785.0, 3247.0, 3768.0, 3266.0, 2844.0, 2508.0, 2564.0, 2461.0, 1857.0, 1780.0, 1998.0, 1809.0, 1758.0, 1971.0, 1589.0, 1658.0]
layerwise density percentage: ['0.159', '0.231', '0.198', '0.230', '0.199', '0.347', '0.306', '0.313', '0.300', '0.453', '0.435', '0.488', '0.442', '0.858', '0.962', '0.776', '0.810']
Global density: 0.2614109218120575
06/02 01:38:41 PM | Train: [36/80] Step 200/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 01:38:41 PM | layerwise density: [10389.0, 3794.0, 3259.0, 3762.0, 3269.0, 2865.0, 2529.0, 2573.0, 2457.0, 1857.0, 1782.0, 2005.0, 1818.0, 1758.0, 1973.0, 1592.0, 1663.0]
layerwise density percentage: ['0.159', '0.232', '0.199', '0.230', '0.200', '0.350', '0.309', '0.314', '0.300', '0.453', '0.435', '0.490', '0.444', '0.858', '0.963', '0.777', '0.812']
Global density: 0.2618938982486725
06/02 01:38:51 PM | Train: [36/80] Step 300/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:38:51 PM | layerwise density: [10394.0, 3808.0, 3273.0, 3781.0, 3286.0, 2861.0, 2541.0, 2588.0, 2460.0, 1867.0, 1786.0, 2017.0, 1817.0, 1757.0, 1968.0, 1588.0, 1665.0]
layerwise density percentage: ['0.159', '0.232', '0.200', '0.231', '0.201', '0.349', '0.310', '0.316', '0.300', '0.456', '0.436', '0.492', '0.444', '0.858', '0.961', '0.775', '0.813']
Global density: 0.26248833537101746
06/02 01:39:01 PM | Train: [36/80] Step 390/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:39:01 PM | layerwise density: [10393.0, 3813.0, 3285.0, 3801.0, 3301.0, 2874.0, 2559.0, 2588.0, 2472.0, 1865.0, 1788.0, 2019.0, 1813.0, 1760.0, 1976.0, 1598.0, 1668.0]
layerwise density percentage: ['0.159', '0.233', '0.201', '0.232', '0.201', '0.351', '0.312', '0.316', '0.302', '0.455', '0.437', '0.493', '0.443', '0.859', '0.965', '0.780', '0.814']
Global density: 0.26310399174690247
06/02 01:39:01 PM | Train: [36/200] Final Prec@1 99.8800%
06/02 01:39:01 PM | Valid: [36/200] Step 000/078 Loss 1.145 Prec@(1,5) (77.3%, 91.4%)
06/02 01:39:04 PM | Valid: [36/200] Step 078/078 Loss 1.308 Prec@(1,5) (69.6%, 89.7%)
06/02 01:39:04 PM | Valid: [36/200] Final Prec@1 69.6400%
06/02 01:39:04 PM | Current mask training best Prec@1 = 69.6400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 10393.0, 0.1585845947265625]
['model.relu.alpha_mask_1_0', 16384, 3813.0, 0.23272705078125]
['model.relu.alpha_mask_2_0', 16384, 3285.0, 0.20050048828125]
['model.relu.alpha_mask_3_0', 16384, 3801.0, 0.23199462890625]
['model.relu.alpha_mask_4_0', 16384, 3301.0, 0.20147705078125]
['model.relu.alpha_mask_5_0', 8192, 2874.0, 0.350830078125]
['model.relu.alpha_mask_6_0', 8192, 2559.0, 0.3123779296875]
['model.relu.alpha_mask_7_0', 8192, 2588.0, 0.31591796875]
['model.relu.alpha_mask_8_0', 8192, 2472.0, 0.3017578125]
['model.relu.alpha_mask_9_0', 4096, 1865.0, 0.455322265625]
['model.relu.alpha_mask_10_0', 4096, 1788.0, 0.4365234375]
['model.relu.alpha_mask_11_0', 4096, 2019.0, 0.492919921875]
['model.relu.alpha_mask_12_0', 4096, 1813.0, 0.442626953125]
['model.relu.alpha_mask_13_0', 2048, 1760.0, 0.859375]
['model.relu.alpha_mask_14_0', 2048, 1976.0, 0.96484375]
['model.relu.alpha_mask_15_0', 2048, 1598.0, 0.7802734375]
['model.relu.alpha_mask_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49573.0, 0.26310398267663043]
########## End ###########
06/02 01:39:05 PM | Train: [37/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:39:05 PM | layerwise density: [10393.0, 3813.0, 3285.0, 3801.0, 3301.0, 2874.0, 2559.0, 2588.0, 2472.0, 1865.0, 1788.0, 2019.0, 1813.0, 1760.0, 1976.0, 1598.0, 1668.0]
layerwise density percentage: ['0.159', '0.233', '0.201', '0.232', '0.201', '0.351', '0.312', '0.316', '0.302', '0.455', '0.437', '0.493', '0.443', '0.859', '0.965', '0.780', '0.814']
Global density: 0.26310399174690247
06/02 01:39:16 PM | Train: [37/80] Step 100/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:39:16 PM | layerwise density: [10394.0, 3844.0, 3278.0, 3820.0, 3304.0, 2880.0, 2578.0, 2586.0, 2475.0, 1865.0, 1802.0, 2021.0, 1811.0, 1755.0, 1983.0, 1594.0, 1671.0]
layerwise density percentage: ['0.159', '0.235', '0.200', '0.233', '0.202', '0.352', '0.315', '0.316', '0.302', '0.455', '0.440', '0.493', '0.442', '0.857', '0.968', '0.778', '0.816']
Global density: 0.2635710537433624
06/02 01:39:27 PM | Train: [37/80] Step 200/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:39:27 PM | layerwise density: [10387.0, 3856.0, 3280.0, 3830.0, 3307.0, 2893.0, 2577.0, 2583.0, 2472.0, 1872.0, 1811.0, 2027.0, 1823.0, 1770.0, 1975.0, 1601.0, 1676.0]
layerwise density percentage: ['0.158', '0.235', '0.200', '0.234', '0.202', '0.353', '0.315', '0.315', '0.302', '0.457', '0.442', '0.495', '0.445', '0.864', '0.964', '0.782', '0.818']
Global density: 0.2639903128147125
06/02 01:39:37 PM | Train: [37/80] Step 300/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:39:37 PM | layerwise density: [10376.0, 3869.0, 3285.0, 3836.0, 3319.0, 2892.0, 2580.0, 2586.0, 2466.0, 1877.0, 1806.0, 2031.0, 1829.0, 1765.0, 1970.0, 1603.0, 1678.0]
layerwise density percentage: ['0.158', '0.236', '0.201', '0.234', '0.203', '0.353', '0.315', '0.316', '0.301', '0.458', '0.441', '0.496', '0.447', '0.862', '0.962', '0.783', '0.819']
Global density: 0.26413893699645996
06/02 01:39:47 PM | Train: [37/80] Step 390/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 01:39:47 PM | layerwise density: [10367.0, 3870.0, 3297.0, 3858.0, 3323.0, 2922.0, 2564.0, 2596.0, 2470.0, 1878.0, 1795.0, 2037.0, 1837.0, 1762.0, 1972.0, 1599.0, 1682.0]
layerwise density percentage: ['0.158', '0.236', '0.201', '0.235', '0.203', '0.357', '0.313', '0.317', '0.302', '0.458', '0.438', '0.497', '0.448', '0.860', '0.963', '0.781', '0.821']
Global density: 0.2644626796245575
06/02 01:39:47 PM | Train: [37/200] Final Prec@1 99.8840%
06/02 01:39:47 PM | Valid: [37/200] Step 000/078 Loss 1.093 Prec@(1,5) (75.0%, 93.0%)
06/02 01:39:49 PM | Valid: [37/200] Step 078/078 Loss 1.312 Prec@(1,5) (70.0%, 89.7%)
06/02 01:39:50 PM | Valid: [37/200] Final Prec@1 69.9500%
06/02 01:39:50 PM | Current mask training best Prec@1 = 69.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 10367.0, 0.1581878662109375]
['model.relu.alpha_mask_1_0', 16384, 3870.0, 0.2362060546875]
['model.relu.alpha_mask_2_0', 16384, 3297.0, 0.20123291015625]
['model.relu.alpha_mask_3_0', 16384, 3858.0, 0.2354736328125]
['model.relu.alpha_mask_4_0', 16384, 3323.0, 0.20281982421875]
['model.relu.alpha_mask_5_0', 8192, 2922.0, 0.356689453125]
['model.relu.alpha_mask_6_0', 8192, 2564.0, 0.31298828125]
['model.relu.alpha_mask_7_0', 8192, 2596.0, 0.31689453125]
['model.relu.alpha_mask_8_0', 8192, 2470.0, 0.301513671875]
['model.relu.alpha_mask_9_0', 4096, 1878.0, 0.45849609375]
['model.relu.alpha_mask_10_0', 4096, 1795.0, 0.438232421875]
['model.relu.alpha_mask_11_0', 4096, 2037.0, 0.497314453125]
['model.relu.alpha_mask_12_0', 4096, 1837.0, 0.448486328125]
['model.relu.alpha_mask_13_0', 2048, 1762.0, 0.8603515625]
['model.relu.alpha_mask_14_0', 2048, 1972.0, 0.962890625]
['model.relu.alpha_mask_15_0', 2048, 1599.0, 0.78076171875]
['model.relu.alpha_mask_16_0', 2048, 1682.0, 0.8212890625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49829.0, 0.2644626783288043]
########## End ###########
06/02 01:39:51 PM | Train: [38/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/02 01:39:51 PM | layerwise density: [10367.0, 3870.0, 3297.0, 3858.0, 3323.0, 2922.0, 2564.0, 2596.0, 2470.0, 1878.0, 1795.0, 2037.0, 1837.0, 1762.0, 1972.0, 1599.0, 1682.0]
layerwise density percentage: ['0.158', '0.236', '0.201', '0.235', '0.203', '0.357', '0.313', '0.317', '0.302', '0.458', '0.438', '0.497', '0.448', '0.860', '0.963', '0.781', '0.821']
Global density: 0.2644626796245575
06/02 01:40:02 PM | Train: [38/80] Step 100/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 01:40:02 PM | layerwise density: [10372.0, 3889.0, 3308.0, 3865.0, 3343.0, 2916.0, 2561.0, 2619.0, 2481.0, 1875.0, 1809.0, 2043.0, 1844.0, 1768.0, 1972.0, 1605.0, 1687.0]
layerwise density percentage: ['0.158', '0.237', '0.202', '0.236', '0.204', '0.356', '0.313', '0.320', '0.303', '0.458', '0.442', '0.499', '0.450', '0.863', '0.963', '0.784', '0.824']
Global density: 0.265142023563385
06/02 01:40:13 PM | Train: [38/80] Step 200/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 01:40:13 PM | layerwise density: [10050.0, 3739.0, 3190.0, 3740.0, 3210.0, 2811.0, 2464.0, 2562.0, 2392.0, 1848.0, 1770.0, 2027.0, 1792.0, 1747.0, 1949.0, 1570.0, 1689.0]
layerwise density percentage: ['0.153', '0.228', '0.195', '0.228', '0.196', '0.343', '0.301', '0.313', '0.292', '0.451', '0.432', '0.495', '0.438', '0.853', '0.952', '0.767', '0.825']
Global density: 0.25767451524734497
06/02 01:40:24 PM | Train: [38/80] Step 300/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 01:40:24 PM | layerwise density: [10005.0, 3692.0, 3149.0, 3704.0, 3161.0, 2770.0, 2443.0, 2548.0, 2357.0, 1838.0, 1755.0, 2020.0, 1778.0, 1752.0, 1959.0, 1571.0, 1688.0]
layerwise density percentage: ['0.153', '0.225', '0.192', '0.226', '0.193', '0.338', '0.298', '0.311', '0.288', '0.449', '0.428', '0.493', '0.434', '0.855', '0.957', '0.767', '0.824']
Global density: 0.25576385855674744
06/02 01:40:34 PM | Train: [38/80] Step 390/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 01:40:34 PM | layerwise density: [9976.0, 3678.0, 3147.0, 3690.0, 3144.0, 2786.0, 2456.0, 2544.0, 2366.0, 1840.0, 1765.0, 2017.0, 1777.0, 1765.0, 1970.0, 1589.0, 1688.0]
layerwise density percentage: ['0.152', '0.224', '0.192', '0.225', '0.192', '0.340', '0.300', '0.311', '0.289', '0.449', '0.431', '0.492', '0.434', '0.862', '0.962', '0.776', '0.824']
Global density: 0.2558062970638275
06/02 01:40:34 PM | Train: [38/200] Final Prec@1 99.8980%
06/02 01:40:34 PM | Valid: [38/200] Step 000/078 Loss 1.153 Prec@(1,5) (73.4%, 92.2%)
06/02 01:40:37 PM | Valid: [38/200] Step 078/078 Loss 1.317 Prec@(1,5) (69.6%, 89.3%)
06/02 01:40:37 PM | Valid: [38/200] Final Prec@1 69.5700%
06/02 01:40:37 PM | Current mask training best Prec@1 = 69.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9976.0, 0.1522216796875]
['model.relu.alpha_mask_1_0', 16384, 3678.0, 0.2244873046875]
['model.relu.alpha_mask_2_0', 16384, 3147.0, 0.19207763671875]
['model.relu.alpha_mask_3_0', 16384, 3690.0, 0.2252197265625]
['model.relu.alpha_mask_4_0', 16384, 3144.0, 0.19189453125]
['model.relu.alpha_mask_5_0', 8192, 2786.0, 0.340087890625]
['model.relu.alpha_mask_6_0', 8192, 2457.0, 0.2999267578125]
['model.relu.alpha_mask_7_0', 8192, 2544.0, 0.310546875]
['model.relu.alpha_mask_8_0', 8192, 2366.0, 0.288818359375]
['model.relu.alpha_mask_9_0', 4096, 1840.0, 0.44921875]
['model.relu.alpha_mask_10_0', 4096, 1764.0, 0.4306640625]
['model.relu.alpha_mask_11_0', 4096, 2017.0, 0.492431640625]
['model.relu.alpha_mask_12_0', 4096, 1777.0, 0.433837890625]
['model.relu.alpha_mask_13_0', 2048, 1765.0, 0.86181640625]
['model.relu.alpha_mask_14_0', 2048, 1970.0, 0.9619140625]
['model.relu.alpha_mask_15_0', 2048, 1589.0, 0.77587890625]
['model.relu.alpha_mask_16_0', 2048, 1688.0, 0.82421875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48198.0, 0.255806300951087]
########## End ###########
06/02 01:40:38 PM | Train: [39/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 01:40:38 PM | layerwise density: [9976.0, 3678.0, 3147.0, 3690.0, 3144.0, 2786.0, 2457.0, 2544.0, 2366.0, 1840.0, 1764.0, 2017.0, 1777.0, 1765.0, 1970.0, 1589.0, 1688.0]
layerwise density percentage: ['0.152', '0.224', '0.192', '0.225', '0.192', '0.340', '0.300', '0.311', '0.289', '0.449', '0.431', '0.492', '0.434', '0.862', '0.962', '0.776', '0.824']
Global density: 0.2558062970638275
06/02 01:40:49 PM | Train: [39/80] Step 100/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 01:40:49 PM | layerwise density: [9961.0, 3689.0, 3140.0, 3689.0, 3144.0, 2810.0, 2475.0, 2552.0, 2383.0, 1852.0, 1763.0, 2022.0, 1779.0, 1762.0, 1978.0, 1603.0, 1690.0]
layerwise density percentage: ['0.152', '0.225', '0.192', '0.225', '0.192', '0.343', '0.302', '0.312', '0.291', '0.452', '0.430', '0.494', '0.434', '0.860', '0.966', '0.783', '0.825']
Global density: 0.25630518794059753
06/02 01:41:00 PM | Train: [39/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 01:41:00 PM | layerwise density: [9950.0, 3706.0, 3139.0, 3690.0, 3129.0, 2850.0, 2500.0, 2559.0, 2378.0, 1881.0, 1769.0, 2026.0, 1789.0, 1771.0, 1976.0, 1613.0, 1690.0]
layerwise density percentage: ['0.152', '0.226', '0.192', '0.225', '0.191', '0.348', '0.305', '0.312', '0.290', '0.459', '0.432', '0.495', '0.437', '0.865', '0.965', '0.788', '0.825']
Global density: 0.256963312625885
06/02 01:41:11 PM | Train: [39/80] Step 300/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 01:41:11 PM | layerwise density: [9954.0, 3719.0, 3142.0, 3703.0, 3138.0, 2867.0, 2517.0, 2577.0, 2409.0, 1885.0, 1778.0, 2032.0, 1806.0, 1779.0, 1972.0, 1612.0, 1697.0]
layerwise density percentage: ['0.152', '0.227', '0.192', '0.226', '0.192', '0.350', '0.307', '0.315', '0.294', '0.460', '0.434', '0.496', '0.441', '0.869', '0.963', '0.787', '0.829']
Global density: 0.2578708827495575
06/02 01:41:22 PM | Train: [39/80] Step 390/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 01:41:22 PM | layerwise density: [9954.0, 3729.0, 3154.0, 3721.0, 3151.0, 2880.0, 2535.0, 2582.0, 2422.0, 1888.0, 1791.0, 2035.0, 1815.0, 1777.0, 1971.0, 1613.0, 1702.0]
layerwise density percentage: ['0.152', '0.228', '0.193', '0.227', '0.192', '0.352', '0.309', '0.315', '0.296', '0.461', '0.437', '0.497', '0.443', '0.868', '0.962', '0.788', '0.831']
Global density: 0.25857678055763245
06/02 01:41:22 PM | Train: [39/200] Final Prec@1 99.9240%
06/02 01:41:22 PM | Valid: [39/200] Step 000/078 Loss 1.046 Prec@(1,5) (77.3%, 90.6%)
06/02 01:41:24 PM | Valid: [39/200] Step 078/078 Loss 1.307 Prec@(1,5) (69.9%, 89.5%)
06/02 01:41:24 PM | Valid: [39/200] Final Prec@1 69.9300%
06/02 01:41:24 PM | Current mask training best Prec@1 = 69.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9952.0, 0.15185546875]
['model.relu.alpha_mask_1_0', 16384, 3727.0, 0.22747802734375]
['model.relu.alpha_mask_2_0', 16384, 3154.0, 0.1925048828125]
['model.relu.alpha_mask_3_0', 16384, 3722.0, 0.2271728515625]
['model.relu.alpha_mask_4_0', 16384, 3151.0, 0.19232177734375]
['model.relu.alpha_mask_5_0', 8192, 2881.0, 0.3516845703125]
['model.relu.alpha_mask_6_0', 8192, 2535.0, 0.3094482421875]
['model.relu.alpha_mask_7_0', 8192, 2583.0, 0.3153076171875]
['model.relu.alpha_mask_8_0', 8192, 2420.0, 0.29541015625]
['model.relu.alpha_mask_9_0', 4096, 1887.0, 0.460693359375]
['model.relu.alpha_mask_10_0', 4096, 1791.0, 0.437255859375]
['model.relu.alpha_mask_11_0', 4096, 2035.0, 0.496826171875]
['model.relu.alpha_mask_12_0', 4096, 1816.0, 0.443359375]
['model.relu.alpha_mask_13_0', 2048, 1776.0, 0.8671875]
['model.relu.alpha_mask_14_0', 2048, 1973.0, 0.96337890625]
['model.relu.alpha_mask_15_0', 2048, 1612.0, 0.787109375]
['model.relu.alpha_mask_16_0', 2048, 1702.0, 0.8310546875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48717.0, 0.2585608440896739]
########## End ###########
06/02 01:41:25 PM | Train: [40/80] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/02 01:41:25 PM | layerwise density: [9952.0, 3727.0, 3154.0, 3722.0, 3151.0, 2881.0, 2535.0, 2583.0, 2420.0, 1887.0, 1791.0, 2035.0, 1816.0, 1776.0, 1973.0, 1612.0, 1702.0]
layerwise density percentage: ['0.152', '0.227', '0.193', '0.227', '0.192', '0.352', '0.309', '0.315', '0.295', '0.461', '0.437', '0.497', '0.443', '0.867', '0.963', '0.787', '0.831']
Global density: 0.25856083631515503
06/02 01:41:36 PM | Train: [40/80] Step 100/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 01:41:36 PM | layerwise density: [9948.0, 3737.0, 3166.0, 3732.0, 3158.0, 2884.0, 2547.0, 2588.0, 2439.0, 1901.0, 1803.0, 2039.0, 1820.0, 1777.0, 1971.0, 1606.0, 1707.0]
layerwise density percentage: ['0.152', '0.228', '0.193', '0.228', '0.193', '0.352', '0.311', '0.316', '0.298', '0.464', '0.440', '0.498', '0.444', '0.868', '0.962', '0.784', '0.833']
Global density: 0.25912344455718994
06/02 01:41:47 PM | Train: [40/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:41:47 PM | layerwise density: [9947.0, 3759.0, 3176.0, 3745.0, 3166.0, 2894.0, 2548.0, 2594.0, 2460.0, 1919.0, 1814.0, 2048.0, 1823.0, 1774.0, 1982.0, 1608.0, 1709.0]
layerwise density percentage: ['0.152', '0.229', '0.194', '0.229', '0.193', '0.353', '0.311', '0.317', '0.300', '0.469', '0.443', '0.500', '0.445', '0.866', '0.968', '0.785', '0.834']
Global density: 0.259882390499115
06/02 01:41:58 PM | Train: [40/80] Step 300/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 01:41:58 PM | layerwise density: [9946.0, 3775.0, 3182.0, 3752.0, 3180.0, 2913.0, 2567.0, 2610.0, 2473.0, 1923.0, 1808.0, 2052.0, 1824.0, 1782.0, 1975.0, 1607.0, 1713.0]
layerwise density percentage: ['0.152', '0.230', '0.194', '0.229', '0.194', '0.356', '0.313', '0.319', '0.302', '0.469', '0.441', '0.501', '0.445', '0.870', '0.964', '0.785', '0.836']
Global density: 0.260498046875
06/02 01:42:07 PM | Train: [40/80] Step 390/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 01:42:07 PM | layerwise density: [9946.0, 3782.0, 3204.0, 3765.0, 3182.0, 2919.0, 2567.0, 2612.0, 2472.0, 1912.0, 1813.0, 2060.0, 1826.0, 1784.0, 1978.0, 1614.0, 1716.0]
layerwise density percentage: ['0.152', '0.231', '0.196', '0.230', '0.194', '0.356', '0.313', '0.319', '0.302', '0.467', '0.443', '0.503', '0.446', '0.871', '0.966', '0.788', '0.838']
Global density: 0.260869562625885
06/02 01:42:07 PM | Train: [40/200] Final Prec@1 99.9180%
06/02 01:42:08 PM | Valid: [40/200] Step 000/078 Loss 1.113 Prec@(1,5) (75.8%, 89.8%)
06/02 01:42:10 PM | Valid: [40/200] Step 078/078 Loss 1.323 Prec@(1,5) (69.5%, 89.3%)
06/02 01:42:10 PM | Valid: [40/200] Final Prec@1 69.5000%
06/02 01:42:10 PM | Current mask training best Prec@1 = 69.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9947.0, 0.1517791748046875]
['model.relu.alpha_mask_1_0', 16384, 3783.0, 0.23089599609375]
['model.relu.alpha_mask_2_0', 16384, 3204.0, 0.195556640625]
['model.relu.alpha_mask_3_0', 16384, 3764.0, 0.229736328125]
['model.relu.alpha_mask_4_0', 16384, 3185.0, 0.19439697265625]
['model.relu.alpha_mask_5_0', 8192, 2918.0, 0.356201171875]
['model.relu.alpha_mask_6_0', 8192, 2568.0, 0.3134765625]
['model.relu.alpha_mask_7_0', 8192, 2612.0, 0.31884765625]
['model.relu.alpha_mask_8_0', 8192, 2472.0, 0.3017578125]
['model.relu.alpha_mask_9_0', 4096, 1914.0, 0.46728515625]
['model.relu.alpha_mask_10_0', 4096, 1811.0, 0.442138671875]
['model.relu.alpha_mask_11_0', 4096, 2062.0, 0.50341796875]
['model.relu.alpha_mask_12_0', 4096, 1826.0, 0.44580078125]
['model.relu.alpha_mask_13_0', 2048, 1782.0, 0.8701171875]
['model.relu.alpha_mask_14_0', 2048, 1978.0, 0.9658203125]
['model.relu.alpha_mask_15_0', 2048, 1614.0, 0.7880859375]
['model.relu.alpha_mask_16_0', 2048, 1716.0, 0.837890625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49156.0, 0.26089079483695654]
########## End ###########
06/02 01:42:11 PM | Train: [41/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:42:11 PM | layerwise density: [9947.0, 3783.0, 3204.0, 3764.0, 3185.0, 2918.0, 2568.0, 2612.0, 2472.0, 1914.0, 1811.0, 2062.0, 1826.0, 1782.0, 1978.0, 1614.0, 1716.0]
layerwise density percentage: ['0.152', '0.231', '0.196', '0.230', '0.194', '0.356', '0.313', '0.319', '0.302', '0.467', '0.442', '0.503', '0.446', '0.870', '0.966', '0.788', '0.838']
Global density: 0.26089081168174744
06/02 01:42:22 PM | Train: [41/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:42:22 PM | layerwise density: [9929.0, 3799.0, 3192.0, 3777.0, 3192.0, 2929.0, 2572.0, 2619.0, 2474.0, 1909.0, 1818.0, 2066.0, 1820.0, 1778.0, 1981.0, 1615.0, 1717.0]
layerwise density percentage: ['0.152', '0.232', '0.195', '0.231', '0.195', '0.358', '0.314', '0.320', '0.302', '0.466', '0.444', '0.504', '0.444', '0.868', '0.967', '0.789', '0.838']
Global density: 0.2610553205013275
06/02 01:42:33 PM | Train: [41/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:42:33 PM | layerwise density: [9937.0, 3807.0, 3190.0, 3792.0, 3181.0, 2942.0, 2576.0, 2636.0, 2485.0, 1928.0, 1806.0, 2066.0, 1833.0, 1781.0, 1978.0, 1620.0, 1719.0]
layerwise density percentage: ['0.152', '0.232', '0.195', '0.231', '0.194', '0.359', '0.314', '0.322', '0.303', '0.471', '0.441', '0.504', '0.448', '0.870', '0.966', '0.791', '0.839']
Global density: 0.2615329921245575
06/02 01:42:44 PM | Train: [41/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:42:44 PM | layerwise density: [9942.0, 3829.0, 3197.0, 3815.0, 3190.0, 2950.0, 2573.0, 2641.0, 2482.0, 1923.0, 1817.0, 2071.0, 1823.0, 1779.0, 1979.0, 1615.0, 1725.0]
layerwise density percentage: ['0.152', '0.234', '0.195', '0.233', '0.195', '0.360', '0.314', '0.322', '0.303', '0.469', '0.444', '0.506', '0.445', '0.869', '0.966', '0.789', '0.842']
Global density: 0.26192575693130493
06/02 01:42:53 PM | Train: [41/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:42:53 PM | layerwise density: [9945.0, 3841.0, 3187.0, 3821.0, 3210.0, 2952.0, 2575.0, 2647.0, 2480.0, 1928.0, 1816.0, 2072.0, 1834.0, 1780.0, 1982.0, 1613.0, 1728.0]
layerwise density percentage: ['0.152', '0.234', '0.195', '0.233', '0.196', '0.360', '0.314', '0.323', '0.303', '0.471', '0.443', '0.506', '0.448', '0.869', '0.968', '0.788', '0.844']
Global density: 0.26224419474601746
06/02 01:42:53 PM | Train: [41/200] Final Prec@1 99.9360%
06/02 01:42:54 PM | Valid: [41/200] Step 000/078 Loss 1.038 Prec@(1,5) (74.2%, 90.6%)
06/02 01:42:56 PM | Valid: [41/200] Step 078/078 Loss 1.321 Prec@(1,5) (69.8%, 89.5%)
06/02 01:42:56 PM | Valid: [41/200] Final Prec@1 69.8300%
06/02 01:42:56 PM | Current mask training best Prec@1 = 69.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9948.0, 0.15179443359375]
['model.relu.alpha_mask_1_0', 16384, 3843.0, 0.23455810546875]
['model.relu.alpha_mask_2_0', 16384, 3186.0, 0.1944580078125]
['model.relu.alpha_mask_3_0', 16384, 3821.0, 0.23321533203125]
['model.relu.alpha_mask_4_0', 16384, 3209.0, 0.19586181640625]
['model.relu.alpha_mask_5_0', 8192, 2952.0, 0.3603515625]
['model.relu.alpha_mask_6_0', 8192, 2577.0, 0.3145751953125]
['model.relu.alpha_mask_7_0', 8192, 2647.0, 0.3231201171875]
['model.relu.alpha_mask_8_0', 8192, 2480.0, 0.302734375]
['model.relu.alpha_mask_9_0', 4096, 1929.0, 0.470947265625]
['model.relu.alpha_mask_10_0', 4096, 1817.0, 0.443603515625]
['model.relu.alpha_mask_11_0', 4096, 2072.0, 0.505859375]
['model.relu.alpha_mask_12_0', 4096, 1834.0, 0.44775390625]
['model.relu.alpha_mask_13_0', 2048, 1780.0, 0.869140625]
['model.relu.alpha_mask_14_0', 2048, 1982.0, 0.9677734375]
['model.relu.alpha_mask_15_0', 2048, 1613.0, 0.78759765625]
['model.relu.alpha_mask_16_0', 2048, 1728.0, 0.84375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49418.0, 0.26228133491847827]
########## End ###########
06/02 01:42:57 PM | Train: [42/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/02 01:42:57 PM | layerwise density: [9948.0, 3843.0, 3186.0, 3821.0, 3209.0, 2952.0, 2577.0, 2647.0, 2480.0, 1929.0, 1817.0, 2072.0, 1834.0, 1780.0, 1982.0, 1613.0, 1728.0]
layerwise density percentage: ['0.152', '0.235', '0.194', '0.233', '0.196', '0.360', '0.315', '0.323', '0.303', '0.471', '0.444', '0.506', '0.448', '0.869', '0.968', '0.788', '0.844']
Global density: 0.2622813284397125
06/02 01:43:08 PM | Train: [42/80] Step 100/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 01:43:08 PM | layerwise density: [9960.0, 3858.0, 3192.0, 3825.0, 3221.0, 2964.0, 2585.0, 2653.0, 2485.0, 1931.0, 1825.0, 2073.0, 1823.0, 1788.0, 1983.0, 1612.0, 1731.0]
layerwise density percentage: ['0.152', '0.235', '0.195', '0.233', '0.197', '0.362', '0.316', '0.324', '0.303', '0.471', '0.446', '0.506', '0.445', '0.873', '0.968', '0.787', '0.845']
Global density: 0.2627643048763275
06/02 01:43:18 PM | Train: [42/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:43:18 PM | layerwise density: [9948.0, 3860.0, 3195.0, 3827.0, 3222.0, 2970.0, 2585.0, 2652.0, 2500.0, 1943.0, 1833.0, 2076.0, 1839.0, 1788.0, 1985.0, 1608.0, 1732.0]
layerwise density percentage: ['0.152', '0.236', '0.195', '0.234', '0.197', '0.363', '0.316', '0.324', '0.305', '0.474', '0.448', '0.507', '0.449', '0.873', '0.969', '0.785', '0.846']
Global density: 0.26305091381073
06/02 01:43:29 PM | Train: [42/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:43:29 PM | layerwise density: [9942.0, 3871.0, 3199.0, 3839.0, 3236.0, 2983.0, 2586.0, 2666.0, 2495.0, 1954.0, 1829.0, 2084.0, 1848.0, 1795.0, 1986.0, 1615.0, 1734.0]
layerwise density percentage: ['0.152', '0.236', '0.195', '0.234', '0.198', '0.364', '0.316', '0.325', '0.305', '0.477', '0.447', '0.509', '0.451', '0.876', '0.970', '0.789', '0.847']
Global density: 0.26357635855674744
06/02 01:43:39 PM | Train: [42/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:43:39 PM | layerwise density: [9938.0, 3884.0, 3203.0, 3849.0, 3226.0, 2975.0, 2590.0, 2668.0, 2507.0, 1953.0, 1829.0, 2085.0, 1839.0, 1787.0, 1986.0, 1626.0, 1736.0]
layerwise density percentage: ['0.152', '0.237', '0.195', '0.235', '0.197', '0.363', '0.316', '0.326', '0.306', '0.477', '0.447', '0.509', '0.449', '0.873', '0.970', '0.794', '0.848']
Global density: 0.263677179813385
06/02 01:43:39 PM | Train: [42/200] Final Prec@1 99.9160%
06/02 01:43:40 PM | Valid: [42/200] Step 000/078 Loss 1.122 Prec@(1,5) (74.2%, 91.4%)
06/02 01:43:42 PM | Valid: [42/200] Step 078/078 Loss 1.322 Prec@(1,5) (69.4%, 89.4%)
06/02 01:43:42 PM | Valid: [42/200] Final Prec@1 69.4000%
06/02 01:43:42 PM | Current mask training best Prec@1 = 69.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9938.0, 0.151641845703125]
['model.relu.alpha_mask_1_0', 16384, 3884.0, 0.237060546875]
['model.relu.alpha_mask_2_0', 16384, 3203.0, 0.19549560546875]
['model.relu.alpha_mask_3_0', 16384, 3849.0, 0.23492431640625]
['model.relu.alpha_mask_4_0', 16384, 3226.0, 0.1968994140625]
['model.relu.alpha_mask_5_0', 8192, 2975.0, 0.3631591796875]
['model.relu.alpha_mask_6_0', 8192, 2590.0, 0.316162109375]
['model.relu.alpha_mask_7_0', 8192, 2668.0, 0.32568359375]
['model.relu.alpha_mask_8_0', 8192, 2507.0, 0.3060302734375]
['model.relu.alpha_mask_9_0', 4096, 1953.0, 0.476806640625]
['model.relu.alpha_mask_10_0', 4096, 1829.0, 0.446533203125]
['model.relu.alpha_mask_11_0', 4096, 2085.0, 0.509033203125]
['model.relu.alpha_mask_12_0', 4096, 1839.0, 0.448974609375]
['model.relu.alpha_mask_13_0', 2048, 1787.0, 0.87255859375]
['model.relu.alpha_mask_14_0', 2048, 1986.0, 0.9697265625]
['model.relu.alpha_mask_15_0', 2048, 1626.0, 0.7939453125]
['model.relu.alpha_mask_16_0', 2048, 1736.0, 0.84765625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49681.0, 0.2636771824048913]
########## End ###########
06/02 01:43:43 PM | Train: [43/80] Step 000/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
06/02 01:43:43 PM | layerwise density: [9938.0, 3884.0, 3203.0, 3849.0, 3226.0, 2975.0, 2590.0, 2668.0, 2507.0, 1953.0, 1829.0, 2085.0, 1839.0, 1787.0, 1986.0, 1626.0, 1736.0]
layerwise density percentage: ['0.152', '0.237', '0.195', '0.235', '0.197', '0.363', '0.316', '0.326', '0.306', '0.477', '0.447', '0.509', '0.449', '0.873', '0.970', '0.794', '0.848']
Global density: 0.263677179813385
06/02 01:43:54 PM | Train: [43/80] Step 100/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 01:43:54 PM | layerwise density: [9947.0, 3893.0, 3210.0, 3845.0, 3218.0, 2992.0, 2586.0, 2680.0, 2516.0, 1946.0, 1833.0, 2086.0, 1847.0, 1790.0, 1981.0, 1625.0, 1740.0]
layerwise density percentage: ['0.152', '0.238', '0.196', '0.235', '0.196', '0.365', '0.316', '0.327', '0.307', '0.475', '0.448', '0.509', '0.451', '0.874', '0.967', '0.793', '0.850']
Global density: 0.2639637887477875
06/02 01:44:04 PM | Train: [43/80] Step 200/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 01:44:04 PM | layerwise density: [9952.0, 3887.0, 3218.0, 3856.0, 3232.0, 2996.0, 2590.0, 2684.0, 2523.0, 1953.0, 1827.0, 2087.0, 1832.0, 1785.0, 1980.0, 1616.0, 1743.0]
layerwise density percentage: ['0.152', '0.237', '0.196', '0.235', '0.197', '0.366', '0.316', '0.328', '0.308', '0.477', '0.446', '0.510', '0.447', '0.872', '0.967', '0.789', '0.851']
Global density: 0.2641017735004425
06/02 01:44:15 PM | Train: [43/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:44:15 PM | layerwise density: [9957.0, 3888.0, 3225.0, 3874.0, 3237.0, 2990.0, 2601.0, 2689.0, 2522.0, 1947.0, 1829.0, 2088.0, 1841.0, 1797.0, 1985.0, 1617.0, 1744.0]
layerwise density percentage: ['0.152', '0.237', '0.197', '0.236', '0.198', '0.365', '0.318', '0.328', '0.308', '0.475', '0.447', '0.510', '0.449', '0.877', '0.969', '0.790', '0.852']
Global density: 0.2644732892513275
06/02 01:44:24 PM | Train: [43/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:44:24 PM | layerwise density: [9970.0, 3904.0, 3228.0, 3881.0, 3241.0, 2997.0, 2630.0, 2701.0, 2539.0, 1951.0, 1834.0, 2092.0, 1844.0, 1800.0, 1981.0, 1626.0, 1744.0]
layerwise density percentage: ['0.152', '0.238', '0.197', '0.237', '0.198', '0.366', '0.321', '0.330', '0.310', '0.476', '0.448', '0.511', '0.450', '0.879', '0.967', '0.794', '0.852']
Global density: 0.26517388224601746
06/02 01:44:24 PM | Train: [43/200] Final Prec@1 99.9320%
06/02 01:44:24 PM | Valid: [43/200] Step 000/078 Loss 1.105 Prec@(1,5) (75.0%, 90.6%)
06/02 01:44:26 PM | Valid: [43/200] Step 078/078 Loss 1.304 Prec@(1,5) (70.0%, 89.6%)
06/02 01:44:27 PM | Valid: [43/200] Final Prec@1 70.0300%
06/02 01:44:27 PM | Current mask training best Prec@1 = 70.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9970.0, 0.152130126953125]
['model.relu.alpha_mask_1_0', 16384, 3904.0, 0.23828125]
['model.relu.alpha_mask_2_0', 16384, 3228.0, 0.197021484375]
['model.relu.alpha_mask_3_0', 16384, 3882.0, 0.2369384765625]
['model.relu.alpha_mask_4_0', 16384, 3241.0, 0.19781494140625]
['model.relu.alpha_mask_5_0', 8192, 2996.0, 0.36572265625]
['model.relu.alpha_mask_6_0', 8192, 2630.0, 0.321044921875]
['model.relu.alpha_mask_7_0', 8192, 2701.0, 0.3297119140625]
['model.relu.alpha_mask_8_0', 8192, 2539.0, 0.3099365234375]
['model.relu.alpha_mask_9_0', 4096, 1951.0, 0.476318359375]
['model.relu.alpha_mask_10_0', 4096, 1834.0, 0.44775390625]
['model.relu.alpha_mask_11_0', 4096, 2092.0, 0.5107421875]
['model.relu.alpha_mask_12_0', 4096, 1844.0, 0.4501953125]
['model.relu.alpha_mask_13_0', 2048, 1800.0, 0.87890625]
['model.relu.alpha_mask_14_0', 2048, 1981.0, 0.96728515625]
['model.relu.alpha_mask_15_0', 2048, 1626.0, 0.7939453125]
['model.relu.alpha_mask_16_0', 2048, 1744.0, 0.8515625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49963.0, 0.26517387058423914]
########## End ###########
06/02 01:44:28 PM | Train: [44/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/02 01:44:28 PM | layerwise density: [9970.0, 3904.0, 3228.0, 3882.0, 3241.0, 2996.0, 2630.0, 2701.0, 2539.0, 1951.0, 1834.0, 2092.0, 1844.0, 1800.0, 1981.0, 1626.0, 1744.0]
layerwise density percentage: ['0.152', '0.238', '0.197', '0.237', '0.198', '0.366', '0.321', '0.330', '0.310', '0.476', '0.448', '0.511', '0.450', '0.879', '0.967', '0.794', '0.852']
Global density: 0.26517388224601746
06/02 01:44:39 PM | Train: [44/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:44:39 PM | layerwise density: [9713.0, 3783.0, 3130.0, 3784.0, 3121.0, 2888.0, 2556.0, 2637.0, 2446.0, 1906.0, 1798.0, 2087.0, 1792.0, 1782.0, 1974.0, 1592.0, 1744.0]
layerwise density percentage: ['0.148', '0.231', '0.191', '0.231', '0.190', '0.353', '0.312', '0.322', '0.299', '0.465', '0.439', '0.510', '0.438', '0.870', '0.964', '0.777', '0.852']
Global density: 0.25864577293395996
06/02 01:44:50 PM | Train: [44/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:44:50 PM | layerwise density: [9647.0, 3728.0, 3091.0, 3751.0, 3084.0, 2834.0, 2524.0, 2613.0, 2406.0, 1886.0, 1789.0, 2079.0, 1786.0, 1773.0, 1970.0, 1586.0, 1744.0]
layerwise density percentage: ['0.147', '0.228', '0.189', '0.229', '0.188', '0.346', '0.308', '0.319', '0.294', '0.460', '0.437', '0.508', '0.436', '0.866', '0.962', '0.774', '0.852']
Global density: 0.2562998831272125
06/02 01:45:01 PM | Train: [44/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:45:01 PM | layerwise density: [9609.0, 3708.0, 3080.0, 3732.0, 3067.0, 2825.0, 2509.0, 2605.0, 2399.0, 1882.0, 1789.0, 2078.0, 1787.0, 1774.0, 1973.0, 1592.0, 1744.0]
layerwise density percentage: ['0.147', '0.226', '0.188', '0.228', '0.187', '0.345', '0.306', '0.318', '0.293', '0.459', '0.437', '0.507', '0.436', '0.866', '0.963', '0.777', '0.852']
Global density: 0.2555674612522125
06/02 01:45:10 PM | Train: [44/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:45:10 PM | layerwise density: [9587.0, 3693.0, 3065.0, 3722.0, 3050.0, 2862.0, 2498.0, 2599.0, 2395.0, 1896.0, 1794.0, 2076.0, 1806.0, 1784.0, 1978.0, 1612.0, 1748.0]
layerwise density percentage: ['0.146', '0.225', '0.187', '0.227', '0.186', '0.349', '0.305', '0.317', '0.292', '0.463', '0.438', '0.507', '0.441', '0.871', '0.966', '0.787', '0.854']
Global density: 0.25563114881515503
06/02 01:45:10 PM | Train: [44/200] Final Prec@1 99.9100%
06/02 01:45:11 PM | Valid: [44/200] Step 000/078 Loss 1.120 Prec@(1,5) (74.2%, 91.4%)
06/02 01:45:13 PM | Valid: [44/200] Step 078/078 Loss 1.322 Prec@(1,5) (69.8%, 89.3%)
06/02 01:45:13 PM | Valid: [44/200] Final Prec@1 69.8300%
06/02 01:45:13 PM | Current mask training best Prec@1 = 70.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9587.0, 0.1462860107421875]
['model.relu.alpha_mask_1_0', 16384, 3694.0, 0.2254638671875]
['model.relu.alpha_mask_2_0', 16384, 3065.0, 0.18707275390625]
['model.relu.alpha_mask_3_0', 16384, 3722.0, 0.2271728515625]
['model.relu.alpha_mask_4_0', 16384, 3050.0, 0.1861572265625]
['model.relu.alpha_mask_5_0', 8192, 2862.0, 0.349365234375]
['model.relu.alpha_mask_6_0', 8192, 2499.0, 0.3050537109375]
['model.relu.alpha_mask_7_0', 8192, 2598.0, 0.317138671875]
['model.relu.alpha_mask_8_0', 8192, 2394.0, 0.292236328125]
['model.relu.alpha_mask_9_0', 4096, 1896.0, 0.462890625]
['model.relu.alpha_mask_10_0', 4096, 1794.0, 0.43798828125]
['model.relu.alpha_mask_11_0', 4096, 2076.0, 0.5068359375]
['model.relu.alpha_mask_12_0', 4096, 1806.0, 0.44091796875]
['model.relu.alpha_mask_13_0', 2048, 1784.0, 0.87109375]
['model.relu.alpha_mask_14_0', 2048, 1978.0, 0.9658203125]
['model.relu.alpha_mask_15_0', 2048, 1612.0, 0.787109375]
['model.relu.alpha_mask_16_0', 2048, 1748.0, 0.853515625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48165.0, 0.2556311565896739]
########## End ###########
06/02 01:45:14 PM | Train: [45/80] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/02 01:45:14 PM | layerwise density: [9587.0, 3694.0, 3065.0, 3722.0, 3050.0, 2862.0, 2499.0, 2598.0, 2394.0, 1896.0, 1794.0, 2076.0, 1806.0, 1784.0, 1978.0, 1612.0, 1748.0]
layerwise density percentage: ['0.146', '0.225', '0.187', '0.227', '0.186', '0.349', '0.305', '0.317', '0.292', '0.463', '0.438', '0.507', '0.441', '0.871', '0.966', '0.787', '0.854']
Global density: 0.25563114881515503
06/02 01:45:25 PM | Train: [45/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:45:25 PM | layerwise density: [9565.0, 3699.0, 3061.0, 3721.0, 3052.0, 2878.0, 2496.0, 2605.0, 2413.0, 1917.0, 1808.0, 2073.0, 1824.0, 1796.0, 1984.0, 1618.0, 1750.0]
layerwise density percentage: ['0.146', '0.226', '0.187', '0.227', '0.186', '0.351', '0.305', '0.318', '0.295', '0.468', '0.441', '0.506', '0.445', '0.877', '0.969', '0.790', '0.854']
Global density: 0.25613537430763245
06/02 01:45:36 PM | Train: [45/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:45:36 PM | layerwise density: [9558.0, 3719.0, 3052.0, 3726.0, 3057.0, 2897.0, 2497.0, 2615.0, 2408.0, 1931.0, 1827.0, 2084.0, 1821.0, 1806.0, 1985.0, 1630.0, 1753.0]
layerwise density percentage: ['0.146', '0.227', '0.186', '0.227', '0.187', '0.354', '0.305', '0.319', '0.294', '0.471', '0.446', '0.509', '0.445', '0.882', '0.969', '0.796', '0.856']
Global density: 0.25669795274734497
06/02 01:45:46 PM | Train: [45/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:45:46 PM | layerwise density: [9553.0, 3740.0, 3063.0, 3739.0, 3068.0, 2901.0, 2516.0, 2624.0, 2414.0, 1950.0, 1828.0, 2088.0, 1837.0, 1804.0, 1986.0, 1628.0, 1755.0]
layerwise density percentage: ['0.146', '0.228', '0.187', '0.228', '0.187', '0.354', '0.307', '0.320', '0.295', '0.476', '0.446', '0.510', '0.448', '0.881', '0.970', '0.795', '0.857']
Global density: 0.2573772966861725
06/02 01:45:57 PM | Train: [45/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 01:45:57 PM | layerwise density: [9549.0, 3754.0, 3075.0, 3755.0, 3078.0, 2916.0, 2528.0, 2634.0, 2423.0, 1952.0, 1820.0, 2093.0, 1839.0, 1801.0, 1992.0, 1614.0, 1756.0]
layerwise density percentage: ['0.146', '0.229', '0.188', '0.229', '0.188', '0.356', '0.309', '0.322', '0.296', '0.477', '0.444', '0.511', '0.449', '0.879', '0.973', '0.788', '0.857']
Global density: 0.25782841444015503
06/02 01:45:57 PM | Train: [45/200] Final Prec@1 99.9200%
06/02 01:45:57 PM | Valid: [45/200] Step 000/078 Loss 1.192 Prec@(1,5) (71.9%, 92.2%)
06/02 01:45:59 PM | Valid: [45/200] Step 078/078 Loss 1.313 Prec@(1,5) (69.8%, 89.5%)
06/02 01:45:59 PM | Valid: [45/200] Final Prec@1 69.7600%
06/02 01:45:59 PM | Current mask training best Prec@1 = 70.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9549.0, 0.1457061767578125]
['model.relu.alpha_mask_1_0', 16384, 3754.0, 0.2291259765625]
['model.relu.alpha_mask_2_0', 16384, 3075.0, 0.18768310546875]
['model.relu.alpha_mask_3_0', 16384, 3755.0, 0.22918701171875]
['model.relu.alpha_mask_4_0', 16384, 3078.0, 0.1878662109375]
['model.relu.alpha_mask_5_0', 8192, 2917.0, 0.3560791015625]
['model.relu.alpha_mask_6_0', 8192, 2529.0, 0.3087158203125]
['model.relu.alpha_mask_7_0', 8192, 2634.0, 0.321533203125]
['model.relu.alpha_mask_8_0', 8192, 2424.0, 0.2958984375]
['model.relu.alpha_mask_9_0', 4096, 1951.0, 0.476318359375]
['model.relu.alpha_mask_10_0', 4096, 1821.0, 0.444580078125]
['model.relu.alpha_mask_11_0', 4096, 2094.0, 0.51123046875]
['model.relu.alpha_mask_12_0', 4096, 1839.0, 0.448974609375]
['model.relu.alpha_mask_13_0', 2048, 1801.0, 0.87939453125]
['model.relu.alpha_mask_14_0', 2048, 1992.0, 0.97265625]
['model.relu.alpha_mask_15_0', 2048, 1614.0, 0.7880859375]
['model.relu.alpha_mask_16_0', 2048, 1756.0, 0.857421875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48583.0, 0.25784965183423914]
########## End ###########
06/02 01:46:00 PM | Train: [46/80] Step 000/390 Loss 0.012 Prec@(1,5) (99.2%, 100.0%)
06/02 01:46:00 PM | layerwise density: [9549.0, 3754.0, 3075.0, 3755.0, 3078.0, 2917.0, 2529.0, 2634.0, 2424.0, 1951.0, 1821.0, 2094.0, 1839.0, 1801.0, 1992.0, 1614.0, 1756.0]
layerwise density percentage: ['0.146', '0.229', '0.188', '0.229', '0.188', '0.356', '0.309', '0.322', '0.296', '0.476', '0.445', '0.511', '0.449', '0.879', '0.973', '0.788', '0.857']
Global density: 0.25784966349601746
06/02 01:46:11 PM | Train: [46/80] Step 100/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:46:11 PM | layerwise density: [9549.0, 3764.0, 3088.0, 3759.0, 3079.0, 2929.0, 2536.0, 2641.0, 2442.0, 1952.0, 1819.0, 2097.0, 1841.0, 1800.0, 1987.0, 1624.0, 1757.0]
layerwise density percentage: ['0.146', '0.230', '0.188', '0.229', '0.188', '0.358', '0.310', '0.322', '0.298', '0.477', '0.444', '0.512', '0.449', '0.879', '0.970', '0.793', '0.858']
Global density: 0.25827956199645996
06/02 01:46:22 PM | Train: [46/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:46:22 PM | layerwise density: [9557.0, 3775.0, 3108.0, 3774.0, 3099.0, 2928.0, 2548.0, 2651.0, 2467.0, 1958.0, 1825.0, 2100.0, 1853.0, 1799.0, 1985.0, 1627.0, 1758.0]
layerwise density percentage: ['0.146', '0.230', '0.190', '0.230', '0.189', '0.357', '0.311', '0.324', '0.301', '0.478', '0.446', '0.513', '0.452', '0.878', '0.969', '0.794', '0.858']
Global density: 0.25906506180763245
06/02 01:46:32 PM | Train: [46/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:46:32 PM | layerwise density: [9560.0, 3768.0, 3119.0, 3780.0, 3109.0, 2950.0, 2549.0, 2657.0, 2468.0, 1974.0, 1819.0, 2100.0, 1853.0, 1800.0, 1992.0, 1622.0, 1758.0]
layerwise density percentage: ['0.146', '0.230', '0.190', '0.231', '0.190', '0.360', '0.311', '0.324', '0.301', '0.482', '0.444', '0.513', '0.452', '0.879', '0.973', '0.792', '0.858']
Global density: 0.25941532850265503
06/02 01:46:41 PM | Train: [46/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:46:41 PM | layerwise density: [9565.0, 3781.0, 3126.0, 3790.0, 3123.0, 2948.0, 2563.0, 2660.0, 2475.0, 1971.0, 1830.0, 2103.0, 1868.0, 1805.0, 1993.0, 1622.0, 1760.0]
layerwise density percentage: ['0.146', '0.231', '0.191', '0.231', '0.191', '0.360', '0.313', '0.325', '0.302', '0.481', '0.447', '0.513', '0.456', '0.881', '0.973', '0.792', '0.859']
Global density: 0.25997263193130493
06/02 01:46:42 PM | Train: [46/200] Final Prec@1 99.9220%
06/02 01:46:42 PM | Valid: [46/200] Step 000/078 Loss 1.160 Prec@(1,5) (74.2%, 92.2%)
06/02 01:46:44 PM | Valid: [46/200] Step 078/078 Loss 1.308 Prec@(1,5) (69.9%, 89.8%)
06/02 01:46:44 PM | Valid: [46/200] Final Prec@1 69.8600%
06/02 01:46:44 PM | Current mask training best Prec@1 = 70.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9565.0, 0.1459503173828125]
['model.relu.alpha_mask_1_0', 16384, 3781.0, 0.23077392578125]
['model.relu.alpha_mask_2_0', 16384, 3127.0, 0.19085693359375]
['model.relu.alpha_mask_3_0', 16384, 3791.0, 0.23138427734375]
['model.relu.alpha_mask_4_0', 16384, 3124.0, 0.190673828125]
['model.relu.alpha_mask_5_0', 8192, 2949.0, 0.3599853515625]
['model.relu.alpha_mask_6_0', 8192, 2564.0, 0.31298828125]
['model.relu.alpha_mask_7_0', 8192, 2660.0, 0.32470703125]
['model.relu.alpha_mask_8_0', 8192, 2475.0, 0.3021240234375]
['model.relu.alpha_mask_9_0', 4096, 1970.0, 0.48095703125]
['model.relu.alpha_mask_10_0', 4096, 1828.0, 0.4462890625]
['model.relu.alpha_mask_11_0', 4096, 2103.0, 0.513427734375]
['model.relu.alpha_mask_12_0', 4096, 1868.0, 0.4560546875]
['model.relu.alpha_mask_13_0', 2048, 1807.0, 0.88232421875]
['model.relu.alpha_mask_14_0', 2048, 1993.0, 0.97314453125]
['model.relu.alpha_mask_15_0', 2048, 1620.0, 0.791015625]
['model.relu.alpha_mask_16_0', 2048, 1760.0, 0.859375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48985.0, 0.25998322860054346]
########## End ###########
06/02 01:46:45 PM | Train: [47/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:46:45 PM | layerwise density: [9565.0, 3781.0, 3127.0, 3791.0, 3124.0, 2949.0, 2564.0, 2660.0, 2475.0, 1970.0, 1828.0, 2103.0, 1868.0, 1807.0, 1993.0, 1620.0, 1760.0]
layerwise density percentage: ['0.146', '0.231', '0.191', '0.231', '0.191', '0.360', '0.313', '0.325', '0.302', '0.481', '0.446', '0.513', '0.456', '0.882', '0.973', '0.791', '0.859']
Global density: 0.25998324155807495
06/02 01:46:56 PM | Train: [47/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:46:56 PM | layerwise density: [9563.0, 3798.0, 3133.0, 3795.0, 3132.0, 2967.0, 2576.0, 2664.0, 2487.0, 1958.0, 1833.0, 2100.0, 1864.0, 1808.0, 1998.0, 1623.0, 1764.0]
layerwise density percentage: ['0.146', '0.232', '0.191', '0.232', '0.191', '0.362', '0.314', '0.325', '0.304', '0.478', '0.448', '0.513', '0.455', '0.883', '0.976', '0.792', '0.861']
Global density: 0.2603972256183624
06/02 01:47:07 PM | Train: [47/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:47:07 PM | layerwise density: [9569.0, 3808.0, 3124.0, 3796.0, 3141.0, 2984.0, 2581.0, 2662.0, 2491.0, 1960.0, 1840.0, 2103.0, 1866.0, 1809.0, 1995.0, 1625.0, 1767.0]
layerwise density percentage: ['0.146', '0.232', '0.191', '0.232', '0.192', '0.364', '0.315', '0.325', '0.304', '0.479', '0.449', '0.513', '0.456', '0.883', '0.974', '0.793', '0.863']
Global density: 0.26070505380630493
06/02 01:47:18 PM | Train: [47/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:47:18 PM | layerwise density: [9568.0, 3822.0, 3129.0, 3792.0, 3152.0, 2986.0, 2590.0, 2665.0, 2503.0, 1954.0, 1852.0, 2112.0, 1859.0, 1813.0, 1990.0, 1618.0, 1768.0]
layerwise density percentage: ['0.146', '0.233', '0.191', '0.231', '0.192', '0.365', '0.316', '0.325', '0.306', '0.477', '0.452', '0.516', '0.454', '0.885', '0.972', '0.790', '0.863']
Global density: 0.260981023311615
06/02 01:47:27 PM | Train: [47/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:47:27 PM | layerwise density: [9566.0, 3834.0, 3135.0, 3795.0, 3154.0, 2973.0, 2596.0, 2672.0, 2503.0, 1958.0, 1846.0, 2114.0, 1862.0, 1816.0, 1987.0, 1627.0, 1768.0]
layerwise density percentage: ['0.146', '0.234', '0.191', '0.232', '0.193', '0.363', '0.317', '0.326', '0.306', '0.478', '0.451', '0.516', '0.455', '0.887', '0.970', '0.794', '0.863']
Global density: 0.2611561715602875
06/02 01:47:28 PM | Train: [47/200] Final Prec@1 99.9320%
06/02 01:47:28 PM | Valid: [47/200] Step 000/078 Loss 1.143 Prec@(1,5) (73.4%, 91.4%)
06/02 01:47:30 PM | Valid: [47/200] Step 078/078 Loss 1.313 Prec@(1,5) (69.6%, 89.6%)
06/02 01:47:30 PM | Valid: [47/200] Final Prec@1 69.6000%
06/02 01:47:30 PM | Current mask training best Prec@1 = 70.0300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9566.0, 0.145965576171875]
['model.relu.alpha_mask_1_0', 16384, 3834.0, 0.2340087890625]
['model.relu.alpha_mask_2_0', 16384, 3136.0, 0.19140625]
['model.relu.alpha_mask_3_0', 16384, 3796.0, 0.231689453125]
['model.relu.alpha_mask_4_0', 16384, 3155.0, 0.19256591796875]
['model.relu.alpha_mask_5_0', 8192, 2973.0, 0.3629150390625]
['model.relu.alpha_mask_6_0', 8192, 2595.0, 0.3167724609375]
['model.relu.alpha_mask_7_0', 8192, 2672.0, 0.326171875]
['model.relu.alpha_mask_8_0', 8192, 2503.0, 0.3055419921875]
['model.relu.alpha_mask_9_0', 4096, 1958.0, 0.47802734375]
['model.relu.alpha_mask_10_0', 4096, 1849.0, 0.451416015625]
['model.relu.alpha_mask_11_0', 4096, 2114.0, 0.51611328125]
['model.relu.alpha_mask_12_0', 4096, 1862.0, 0.45458984375]
['model.relu.alpha_mask_13_0', 2048, 1816.0, 0.88671875]
['model.relu.alpha_mask_14_0', 2048, 1986.0, 0.9697265625]
['model.relu.alpha_mask_15_0', 2048, 1628.0, 0.794921875]
['model.relu.alpha_mask_16_0', 2048, 1768.0, 0.86328125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49211.0, 0.26118270210597827]
########## End ###########
06/02 01:47:31 PM | Train: [48/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:47:31 PM | layerwise density: [9566.0, 3834.0, 3136.0, 3796.0, 3155.0, 2973.0, 2595.0, 2672.0, 2503.0, 1958.0, 1849.0, 2114.0, 1862.0, 1816.0, 1986.0, 1628.0, 1768.0]
layerwise density percentage: ['0.146', '0.234', '0.191', '0.232', '0.193', '0.363', '0.317', '0.326', '0.306', '0.478', '0.451', '0.516', '0.455', '0.887', '0.970', '0.795', '0.863']
Global density: 0.2611826956272125
06/02 01:47:42 PM | Train: [48/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:47:42 PM | layerwise density: [9561.0, 3830.0, 3141.0, 3801.0, 3154.0, 2987.0, 2597.0, 2676.0, 2499.0, 1961.0, 1850.0, 2113.0, 1857.0, 1811.0, 1987.0, 1622.0, 1768.0]
layerwise density percentage: ['0.146', '0.234', '0.192', '0.232', '0.193', '0.365', '0.317', '0.327', '0.305', '0.479', '0.452', '0.516', '0.453', '0.884', '0.970', '0.792', '0.863']
Global density: 0.26120394468307495
06/02 01:47:53 PM | Train: [48/80] Step 200/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 01:47:53 PM | layerwise density: [9562.0, 3829.0, 3147.0, 3801.0, 3155.0, 2994.0, 2601.0, 2680.0, 2496.0, 1973.0, 1858.0, 2117.0, 1861.0, 1807.0, 1985.0, 1623.0, 1771.0]
layerwise density percentage: ['0.146', '0.234', '0.192', '0.232', '0.193', '0.365', '0.318', '0.327', '0.305', '0.482', '0.454', '0.517', '0.454', '0.882', '0.969', '0.792', '0.865']
Global density: 0.26144278049468994
06/02 01:48:03 PM | Train: [48/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:48:03 PM | layerwise density: [9564.0, 3837.0, 3150.0, 3813.0, 3157.0, 3012.0, 2609.0, 2685.0, 2503.0, 1981.0, 1854.0, 2118.0, 1874.0, 1807.0, 1995.0, 1623.0, 1771.0]
layerwise density percentage: ['0.146', '0.234', '0.192', '0.233', '0.193', '0.368', '0.318', '0.328', '0.306', '0.484', '0.453', '0.517', '0.458', '0.882', '0.974', '0.792', '0.865']
Global density: 0.26193636655807495
06/02 01:48:13 PM | Train: [48/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:48:13 PM | layerwise density: [9570.0, 3847.0, 3156.0, 3824.0, 3167.0, 3016.0, 2597.0, 2693.0, 2507.0, 1986.0, 1856.0, 2120.0, 1868.0, 1807.0, 1996.0, 1626.0, 1772.0]
layerwise density percentage: ['0.146', '0.235', '0.193', '0.233', '0.193', '0.368', '0.317', '0.329', '0.306', '0.485', '0.453', '0.518', '0.456', '0.882', '0.975', '0.794', '0.865']
Global density: 0.2622282803058624
06/02 01:48:13 PM | Train: [48/200] Final Prec@1 99.9400%
06/02 01:48:13 PM | Valid: [48/200] Step 000/078 Loss 1.147 Prec@(1,5) (73.4%, 92.2%)
06/02 01:48:16 PM | Valid: [48/200] Step 078/078 Loss 1.296 Prec@(1,5) (70.4%, 89.9%)
06/02 01:48:16 PM | Valid: [48/200] Final Prec@1 70.4300%
06/02 01:48:16 PM | Current mask training best Prec@1 = 70.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9570.0, 0.146026611328125]
['model.relu.alpha_mask_1_0', 16384, 3847.0, 0.23480224609375]
['model.relu.alpha_mask_2_0', 16384, 3156.0, 0.192626953125]
['model.relu.alpha_mask_3_0', 16384, 3824.0, 0.2333984375]
['model.relu.alpha_mask_4_0', 16384, 3167.0, 0.19329833984375]
['model.relu.alpha_mask_5_0', 8192, 3016.0, 0.3681640625]
['model.relu.alpha_mask_6_0', 8192, 2597.0, 0.3170166015625]
['model.relu.alpha_mask_7_0', 8192, 2693.0, 0.3287353515625]
['model.relu.alpha_mask_8_0', 8192, 2507.0, 0.3060302734375]
['model.relu.alpha_mask_9_0', 4096, 1986.0, 0.48486328125]
['model.relu.alpha_mask_10_0', 4096, 1856.0, 0.453125]
['model.relu.alpha_mask_11_0', 4096, 2120.0, 0.517578125]
['model.relu.alpha_mask_12_0', 4096, 1868.0, 0.4560546875]
['model.relu.alpha_mask_13_0', 2048, 1807.0, 0.88232421875]
['model.relu.alpha_mask_14_0', 2048, 1996.0, 0.974609375]
['model.relu.alpha_mask_15_0', 2048, 1626.0, 0.7939453125]
['model.relu.alpha_mask_16_0', 2048, 1772.0, 0.865234375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49408.0, 0.26222826086956524]
########## End ###########
06/02 01:48:17 PM | Train: [49/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:48:17 PM | layerwise density: [9570.0, 3847.0, 3156.0, 3824.0, 3167.0, 3016.0, 2597.0, 2693.0, 2507.0, 1986.0, 1856.0, 2120.0, 1868.0, 1807.0, 1996.0, 1626.0, 1772.0]
layerwise density percentage: ['0.146', '0.235', '0.193', '0.233', '0.193', '0.368', '0.317', '0.329', '0.306', '0.485', '0.453', '0.518', '0.456', '0.882', '0.975', '0.794', '0.865']
Global density: 0.2622282803058624
06/02 01:48:28 PM | Train: [49/80] Step 100/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:48:28 PM | layerwise density: [9567.0, 3856.0, 3155.0, 3836.0, 3166.0, 3025.0, 2597.0, 2690.0, 2518.0, 1981.0, 1858.0, 2124.0, 1867.0, 1817.0, 1988.0, 1628.0, 1774.0]
layerwise density percentage: ['0.146', '0.235', '0.193', '0.234', '0.193', '0.369', '0.317', '0.328', '0.307', '0.484', '0.454', '0.519', '0.456', '0.887', '0.971', '0.795', '0.866']
Global density: 0.26243525743484497
06/02 01:48:39 PM | Train: [49/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:48:39 PM | layerwise density: [9578.0, 3867.0, 3153.0, 3843.0, 3175.0, 3028.0, 2604.0, 2697.0, 2522.0, 1983.0, 1864.0, 2126.0, 1869.0, 1814.0, 1994.0, 1621.0, 1776.0]
layerwise density percentage: ['0.146', '0.236', '0.192', '0.235', '0.194', '0.370', '0.318', '0.329', '0.308', '0.484', '0.455', '0.519', '0.456', '0.886', '0.974', '0.792', '0.867']
Global density: 0.26279085874557495
06/02 01:48:50 PM | Train: [49/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 01:48:50 PM | layerwise density: [9581.0, 3883.0, 3172.0, 3839.0, 3182.0, 3044.0, 2610.0, 2705.0, 2526.0, 1993.0, 1863.0, 2129.0, 1882.0, 1800.0, 1990.0, 1628.0, 1776.0]
layerwise density percentage: ['0.146', '0.237', '0.194', '0.234', '0.194', '0.372', '0.319', '0.330', '0.308', '0.487', '0.455', '0.520', '0.459', '0.879', '0.972', '0.795', '0.867']
Global density: 0.26326319575309753
06/02 01:49:00 PM | Train: [49/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:49:00 PM | layerwise density: [9576.0, 3893.0, 3165.0, 3832.0, 3185.0, 3047.0, 2606.0, 2703.0, 2540.0, 1994.0, 1869.0, 2131.0, 1884.0, 1800.0, 1992.0, 1629.0, 1777.0]
layerwise density percentage: ['0.146', '0.238', '0.193', '0.234', '0.194', '0.372', '0.318', '0.330', '0.310', '0.487', '0.456', '0.520', '0.460', '0.879', '0.973', '0.795', '0.868']
Global density: 0.2633693516254425
06/02 01:49:00 PM | Train: [49/200] Final Prec@1 99.9380%
06/02 01:49:00 PM | Valid: [49/200] Step 000/078 Loss 1.092 Prec@(1,5) (75.0%, 89.8%)
06/02 01:49:02 PM | Valid: [49/200] Step 078/078 Loss 1.288 Prec@(1,5) (70.2%, 90.0%)
06/02 01:49:02 PM | Valid: [49/200] Final Prec@1 70.1600%
06/02 01:49:02 PM | Current mask training best Prec@1 = 70.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9576.0, 0.1461181640625]
['model.relu.alpha_mask_1_0', 16384, 3893.0, 0.23760986328125]
['model.relu.alpha_mask_2_0', 16384, 3165.0, 0.19317626953125]
['model.relu.alpha_mask_3_0', 16384, 3832.0, 0.23388671875]
['model.relu.alpha_mask_4_0', 16384, 3185.0, 0.19439697265625]
['model.relu.alpha_mask_5_0', 8192, 3047.0, 0.3719482421875]
['model.relu.alpha_mask_6_0', 8192, 2606.0, 0.318115234375]
['model.relu.alpha_mask_7_0', 8192, 2703.0, 0.3299560546875]
['model.relu.alpha_mask_8_0', 8192, 2540.0, 0.31005859375]
['model.relu.alpha_mask_9_0', 4096, 1994.0, 0.48681640625]
['model.relu.alpha_mask_10_0', 4096, 1869.0, 0.456298828125]
['model.relu.alpha_mask_11_0', 4096, 2131.0, 0.520263671875]
['model.relu.alpha_mask_12_0', 4096, 1884.0, 0.4599609375]
['model.relu.alpha_mask_13_0', 2048, 1800.0, 0.87890625]
['model.relu.alpha_mask_14_0', 2048, 1992.0, 0.97265625]
['model.relu.alpha_mask_15_0', 2048, 1629.0, 0.79541015625]
['model.relu.alpha_mask_16_0', 2048, 1777.0, 0.86767578125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49623.0, 0.2633693529211957]
########## End ###########
06/02 01:49:03 PM | Train: [50/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/02 01:49:03 PM | layerwise density: [9576.0, 3893.0, 3165.0, 3832.0, 3185.0, 3047.0, 2606.0, 2703.0, 2540.0, 1994.0, 1869.0, 2131.0, 1884.0, 1800.0, 1992.0, 1629.0, 1777.0]
layerwise density percentage: ['0.146', '0.238', '0.193', '0.234', '0.194', '0.372', '0.318', '0.330', '0.310', '0.487', '0.456', '0.520', '0.460', '0.879', '0.973', '0.795', '0.868']
Global density: 0.2633693516254425
06/02 01:49:14 PM | Train: [50/80] Step 100/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:49:14 PM | layerwise density: [9575.0, 3895.0, 3166.0, 3835.0, 3185.0, 3043.0, 2608.0, 2710.0, 2543.0, 1999.0, 1876.0, 2134.0, 1896.0, 1808.0, 1996.0, 1630.0, 1778.0]
layerwise density percentage: ['0.146', '0.238', '0.193', '0.234', '0.194', '0.371', '0.318', '0.331', '0.310', '0.488', '0.458', '0.521', '0.463', '0.883', '0.975', '0.796', '0.868']
Global density: 0.26365596055984497
06/02 01:49:25 PM | Train: [50/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:49:25 PM | layerwise density: [9571.0, 3895.0, 3177.0, 3841.0, 3184.0, 3051.0, 2600.0, 2709.0, 2542.0, 2007.0, 1878.0, 2137.0, 1888.0, 1814.0, 1992.0, 1638.0, 1778.0]
layerwise density percentage: ['0.146', '0.238', '0.194', '0.234', '0.194', '0.372', '0.317', '0.331', '0.310', '0.490', '0.458', '0.522', '0.461', '0.886', '0.973', '0.800', '0.868']
Global density: 0.263788640499115
06/02 01:49:36 PM | Train: [50/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:49:36 PM | layerwise density: [9581.0, 3902.0, 3178.0, 3848.0, 3185.0, 3053.0, 2606.0, 2712.0, 2556.0, 2000.0, 1883.0, 2138.0, 1888.0, 1815.0, 1991.0, 1639.0, 1779.0]
layerwise density percentage: ['0.146', '0.238', '0.194', '0.235', '0.194', '0.373', '0.318', '0.331', '0.312', '0.488', '0.460', '0.522', '0.461', '0.886', '0.972', '0.800', '0.869']
Global density: 0.26406463980674744
06/02 01:49:45 PM | Train: [50/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:49:45 PM | layerwise density: [9588.0, 3900.0, 3180.0, 3857.0, 3195.0, 3044.0, 2615.0, 2715.0, 2566.0, 2000.0, 1885.0, 2141.0, 1899.0, 1810.0, 1991.0, 1636.0, 1780.0]
layerwise density percentage: ['0.146', '0.238', '0.194', '0.235', '0.195', '0.372', '0.319', '0.331', '0.313', '0.488', '0.460', '0.523', '0.464', '0.884', '0.972', '0.799', '0.869']
Global density: 0.26431939005851746
06/02 01:49:45 PM | Train: [50/200] Final Prec@1 99.9440%
06/02 01:49:46 PM | Valid: [50/200] Step 000/078 Loss 1.087 Prec@(1,5) (75.8%, 91.4%)
06/02 01:49:48 PM | Valid: [50/200] Step 078/078 Loss 1.292 Prec@(1,5) (70.2%, 89.9%)
06/02 01:49:48 PM | Valid: [50/200] Final Prec@1 70.2000%
06/02 01:49:48 PM | Current mask training best Prec@1 = 70.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9588.0, 0.14630126953125]
['model.relu.alpha_mask_1_0', 16384, 3900.0, 0.238037109375]
['model.relu.alpha_mask_2_0', 16384, 3180.0, 0.194091796875]
['model.relu.alpha_mask_3_0', 16384, 3857.0, 0.23541259765625]
['model.relu.alpha_mask_4_0', 16384, 3195.0, 0.19500732421875]
['model.relu.alpha_mask_5_0', 8192, 3044.0, 0.37158203125]
['model.relu.alpha_mask_6_0', 8192, 2615.0, 0.3192138671875]
['model.relu.alpha_mask_7_0', 8192, 2715.0, 0.3314208984375]
['model.relu.alpha_mask_8_0', 8192, 2567.0, 0.3133544921875]
['model.relu.alpha_mask_9_0', 4096, 2000.0, 0.48828125]
['model.relu.alpha_mask_10_0', 4096, 1884.0, 0.4599609375]
['model.relu.alpha_mask_11_0', 4096, 2141.0, 0.522705078125]
['model.relu.alpha_mask_12_0', 4096, 1899.0, 0.463623046875]
['model.relu.alpha_mask_13_0', 2048, 1810.0, 0.8837890625]
['model.relu.alpha_mask_14_0', 2048, 1991.0, 0.97216796875]
['model.relu.alpha_mask_15_0', 2048, 1636.0, 0.798828125]
['model.relu.alpha_mask_16_0', 2048, 1780.0, 0.869140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49802.0, 0.26431937839673914]
########## End ###########
06/02 01:49:49 PM | Train: [51/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 01:49:49 PM | layerwise density: [9588.0, 3900.0, 3180.0, 3857.0, 3195.0, 3044.0, 2615.0, 2715.0, 2567.0, 2000.0, 1884.0, 2141.0, 1899.0, 1810.0, 1991.0, 1636.0, 1780.0]
layerwise density percentage: ['0.146', '0.238', '0.194', '0.235', '0.195', '0.372', '0.319', '0.331', '0.313', '0.488', '0.460', '0.523', '0.464', '0.884', '0.972', '0.799', '0.869']
Global density: 0.26431939005851746
06/02 01:50:00 PM | Train: [51/80] Step 100/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:50:00 PM | layerwise density: [9585.0, 3911.0, 3180.0, 3860.0, 3199.0, 3046.0, 2624.0, 2719.0, 2575.0, 1995.0, 1890.0, 2141.0, 1890.0, 1809.0, 1992.0, 1635.0, 1781.0]
layerwise density percentage: ['0.146', '0.239', '0.194', '0.236', '0.195', '0.372', '0.320', '0.332', '0.314', '0.487', '0.461', '0.523', '0.461', '0.883', '0.973', '0.798', '0.870']
Global density: 0.2644785940647125
06/02 01:50:10 PM | Train: [51/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:50:10 PM | layerwise density: [9580.0, 3917.0, 3176.0, 3875.0, 3209.0, 3050.0, 2624.0, 2723.0, 2574.0, 1992.0, 1888.0, 2138.0, 1888.0, 1808.0, 1992.0, 1629.0, 1782.0]
layerwise density percentage: ['0.146', '0.239', '0.194', '0.237', '0.196', '0.372', '0.320', '0.332', '0.314', '0.486', '0.461', '0.522', '0.461', '0.883', '0.973', '0.795', '0.870']
Global density: 0.2645476162433624
06/02 01:50:21 PM | Train: [51/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:50:21 PM | layerwise density: [9577.0, 3925.0, 3176.0, 3880.0, 3227.0, 3057.0, 2624.0, 2719.0, 2566.0, 1994.0, 1869.0, 2142.0, 1881.0, 1814.0, 1992.0, 1636.0, 1784.0]
layerwise density percentage: ['0.146', '0.240', '0.194', '0.237', '0.197', '0.373', '0.320', '0.332', '0.313', '0.487', '0.456', '0.523', '0.459', '0.886', '0.973', '0.799', '0.871']
Global density: 0.264643132686615
06/02 01:50:31 PM | Train: [51/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:50:31 PM | layerwise density: [9574.0, 3932.0, 3181.0, 3893.0, 3228.0, 3073.0, 2623.0, 2724.0, 2566.0, 2000.0, 1857.0, 2144.0, 1886.0, 1811.0, 1996.0, 1633.0, 1784.0]
layerwise density percentage: ['0.146', '0.240', '0.194', '0.238', '0.197', '0.375', '0.320', '0.333', '0.313', '0.488', '0.453', '0.523', '0.460', '0.884', '0.975', '0.797', '0.871']
Global density: 0.26486605405807495
06/02 01:50:31 PM | Train: [51/200] Final Prec@1 99.9440%
06/02 01:50:31 PM | Valid: [51/200] Step 000/078 Loss 1.127 Prec@(1,5) (74.2%, 93.0%)
06/02 01:50:33 PM | Valid: [51/200] Step 078/078 Loss 1.295 Prec@(1,5) (70.0%, 89.8%)
06/02 01:50:33 PM | Valid: [51/200] Final Prec@1 69.9600%
06/02 01:50:33 PM | Current mask training best Prec@1 = 70.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9574.0, 0.146087646484375]
['model.relu.alpha_mask_1_0', 16384, 3932.0, 0.239990234375]
['model.relu.alpha_mask_2_0', 16384, 3182.0, 0.1942138671875]
['model.relu.alpha_mask_3_0', 16384, 3893.0, 0.23760986328125]
['model.relu.alpha_mask_4_0', 16384, 3229.0, 0.19708251953125]
['model.relu.alpha_mask_5_0', 8192, 3073.0, 0.3751220703125]
['model.relu.alpha_mask_6_0', 8192, 2623.0, 0.3201904296875]
['model.relu.alpha_mask_7_0', 8192, 2724.0, 0.33251953125]
['model.relu.alpha_mask_8_0', 8192, 2566.0, 0.313232421875]
['model.relu.alpha_mask_9_0', 4096, 2000.0, 0.48828125]
['model.relu.alpha_mask_10_0', 4096, 1858.0, 0.45361328125]
['model.relu.alpha_mask_11_0', 4096, 2144.0, 0.5234375]
['model.relu.alpha_mask_12_0', 4096, 1886.0, 0.46044921875]
['model.relu.alpha_mask_13_0', 2048, 1811.0, 0.88427734375]
['model.relu.alpha_mask_14_0', 2048, 1997.0, 0.97509765625]
['model.relu.alpha_mask_15_0', 2048, 1632.0, 0.796875]
['model.relu.alpha_mask_16_0', 2048, 1784.0, 0.87109375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49908.0, 0.2648819633152174]
########## End ###########
06/02 01:50:35 PM | Train: [52/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:50:35 PM | layerwise density: [9574.0, 3932.0, 3182.0, 3893.0, 3229.0, 3073.0, 2623.0, 2724.0, 2566.0, 2000.0, 1858.0, 2144.0, 1886.0, 1811.0, 1997.0, 1632.0, 1784.0]
layerwise density percentage: ['0.146', '0.240', '0.194', '0.238', '0.197', '0.375', '0.320', '0.333', '0.313', '0.488', '0.454', '0.523', '0.460', '0.884', '0.975', '0.797', '0.871']
Global density: 0.26488196849823
06/02 01:50:45 PM | Train: [52/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:50:45 PM | layerwise density: [9560.0, 3923.0, 3195.0, 3904.0, 3225.0, 3080.0, 2608.0, 2728.0, 2569.0, 2005.0, 1860.0, 2146.0, 1886.0, 1816.0, 1997.0, 1633.0, 1785.0]
layerwise density percentage: ['0.146', '0.239', '0.195', '0.238', '0.197', '0.376', '0.318', '0.333', '0.314', '0.490', '0.454', '0.524', '0.460', '0.887', '0.975', '0.797', '0.872']
Global density: 0.2649456560611725
06/02 01:50:56 PM | Train: [52/80] Step 200/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:50:56 PM | layerwise density: [9559.0, 3924.0, 3201.0, 3902.0, 3223.0, 3075.0, 2610.0, 2736.0, 2565.0, 2013.0, 1860.0, 2148.0, 1888.0, 1818.0, 1998.0, 1639.0, 1785.0]
layerwise density percentage: ['0.146', '0.240', '0.195', '0.238', '0.197', '0.375', '0.319', '0.334', '0.313', '0.491', '0.454', '0.524', '0.461', '0.888', '0.976', '0.800', '0.872']
Global density: 0.2650730311870575
06/02 01:51:07 PM | Train: [52/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:51:07 PM | layerwise density: [9564.0, 3930.0, 3202.0, 3906.0, 3227.0, 3076.0, 2620.0, 2743.0, 2567.0, 2006.0, 1863.0, 2156.0, 1883.0, 1823.0, 2000.0, 1639.0, 1787.0]
layerwise density percentage: ['0.146', '0.240', '0.195', '0.238', '0.197', '0.375', '0.320', '0.335', '0.313', '0.490', '0.455', '0.526', '0.460', '0.890', '0.977', '0.800', '0.873']
Global density: 0.2653277814388275
06/02 01:51:17 PM | Train: [52/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:51:17 PM | layerwise density: [9353.0, 3821.0, 3105.0, 3814.0, 3125.0, 2954.0, 2553.0, 2706.0, 2476.0, 1948.0, 1823.0, 2140.0, 1837.0, 1806.0, 1980.0, 1602.0, 1787.0]
layerwise density percentage: ['0.143', '0.233', '0.190', '0.233', '0.191', '0.361', '0.312', '0.330', '0.302', '0.476', '0.445', '0.522', '0.448', '0.882', '0.967', '0.782', '0.873']
Global density: 0.259160578250885
06/02 01:51:17 PM | Train: [52/200] Final Prec@1 99.9480%
06/02 01:51:17 PM | Valid: [52/200] Step 000/078 Loss 1.146 Prec@(1,5) (76.6%, 92.2%)
06/02 01:51:19 PM | Valid: [52/200] Step 078/078 Loss 1.302 Prec@(1,5) (70.4%, 89.7%)
06/02 01:51:19 PM | Valid: [52/200] Final Prec@1 70.3800%
06/02 01:51:19 PM | Current mask training best Prec@1 = 70.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9353.0, 0.1427154541015625]
['model.relu.alpha_mask_1_0', 16384, 3820.0, 0.233154296875]
['model.relu.alpha_mask_2_0', 16384, 3105.0, 0.18951416015625]
['model.relu.alpha_mask_3_0', 16384, 3813.0, 0.23272705078125]
['model.relu.alpha_mask_4_0', 16384, 3123.0, 0.19061279296875]
['model.relu.alpha_mask_5_0', 8192, 2952.0, 0.3603515625]
['model.relu.alpha_mask_6_0', 8192, 2552.0, 0.3115234375]
['model.relu.alpha_mask_7_0', 8192, 2705.0, 0.3302001953125]
['model.relu.alpha_mask_8_0', 8192, 2475.0, 0.3021240234375]
['model.relu.alpha_mask_9_0', 4096, 1947.0, 0.475341796875]
['model.relu.alpha_mask_10_0', 4096, 1824.0, 0.4453125]
['model.relu.alpha_mask_11_0', 4096, 2140.0, 0.5224609375]
['model.relu.alpha_mask_12_0', 4096, 1833.0, 0.447509765625]
['model.relu.alpha_mask_13_0', 2048, 1805.0, 0.88134765625]
['model.relu.alpha_mask_14_0', 2048, 1980.0, 0.966796875]
['model.relu.alpha_mask_15_0', 2048, 1603.0, 0.78271484375]
['model.relu.alpha_mask_16_0', 2048, 1787.0, 0.87255859375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48817.0, 0.2590915845788043]
########## End ###########
06/02 01:51:20 PM | Train: [53/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:51:20 PM | layerwise density: [9353.0, 3820.0, 3105.0, 3813.0, 3123.0, 2952.0, 2552.0, 2705.0, 2475.0, 1947.0, 1824.0, 2140.0, 1833.0, 1805.0, 1980.0, 1603.0, 1787.0]
layerwise density percentage: ['0.143', '0.233', '0.190', '0.233', '0.191', '0.360', '0.312', '0.330', '0.302', '0.475', '0.445', '0.522', '0.448', '0.881', '0.967', '0.783', '0.873']
Global density: 0.2590915858745575
06/02 01:51:31 PM | Train: [53/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:51:31 PM | layerwise density: [9324.0, 3802.0, 3090.0, 3796.0, 3098.0, 2924.0, 2519.0, 2697.0, 2457.0, 1940.0, 1824.0, 2136.0, 1827.0, 1800.0, 1977.0, 1595.0, 1787.0]
layerwise density percentage: ['0.142', '0.232', '0.189', '0.232', '0.189', '0.357', '0.307', '0.329', '0.300', '0.474', '0.445', '0.521', '0.446', '0.879', '0.965', '0.779', '0.873']
Global density: 0.25790274143218994
06/02 01:51:42 PM | Train: [53/80] Step 200/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:51:42 PM | layerwise density: [9302.0, 3780.0, 3077.0, 3777.0, 3082.0, 2903.0, 2503.0, 2686.0, 2443.0, 1934.0, 1817.0, 2133.0, 1828.0, 1799.0, 1978.0, 1595.0, 1787.0]
layerwise density percentage: ['0.142', '0.231', '0.188', '0.231', '0.188', '0.354', '0.306', '0.328', '0.298', '0.472', '0.444', '0.521', '0.446', '0.878', '0.966', '0.779', '0.873']
Global density: 0.2570057809352875
06/02 01:51:53 PM | Train: [53/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:51:53 PM | layerwise density: [9284.0, 3762.0, 3065.0, 3770.0, 3070.0, 2892.0, 2501.0, 2677.0, 2436.0, 1941.0, 1821.0, 2135.0, 1833.0, 1796.0, 1979.0, 1605.0, 1787.0]
layerwise density percentage: ['0.142', '0.230', '0.187', '0.230', '0.187', '0.353', '0.305', '0.327', '0.297', '0.474', '0.445', '0.521', '0.448', '0.877', '0.966', '0.784', '0.873']
Global density: 0.25663426518440247
06/02 01:52:03 PM | Train: [53/80] Step 390/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:52:03 PM | layerwise density: [9266.0, 3762.0, 3059.0, 3761.0, 3056.0, 2905.0, 2508.0, 2675.0, 2422.0, 1945.0, 1815.0, 2136.0, 1833.0, 1805.0, 1982.0, 1618.0, 1787.0]
layerwise density percentage: ['0.141', '0.230', '0.187', '0.230', '0.187', '0.355', '0.306', '0.327', '0.296', '0.475', '0.443', '0.521', '0.448', '0.881', '0.968', '0.790', '0.873']
Global density: 0.2565334141254425
06/02 01:52:03 PM | Train: [53/200] Final Prec@1 99.9560%
06/02 01:52:03 PM | Valid: [53/200] Step 000/078 Loss 1.143 Prec@(1,5) (74.2%, 92.2%)
06/02 01:52:05 PM | Valid: [53/200] Step 078/078 Loss 1.299 Prec@(1,5) (69.6%, 89.8%)
06/02 01:52:05 PM | Valid: [53/200] Final Prec@1 69.6000%
06/02 01:52:05 PM | Current mask training best Prec@1 = 70.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9266.0, 0.141387939453125]
['model.relu.alpha_mask_1_0', 16384, 3762.0, 0.2296142578125]
['model.relu.alpha_mask_2_0', 16384, 3058.0, 0.1866455078125]
['model.relu.alpha_mask_3_0', 16384, 3761.0, 0.22955322265625]
['model.relu.alpha_mask_4_0', 16384, 3057.0, 0.18658447265625]
['model.relu.alpha_mask_5_0', 8192, 2905.0, 0.3546142578125]
['model.relu.alpha_mask_6_0', 8192, 2509.0, 0.3062744140625]
['model.relu.alpha_mask_7_0', 8192, 2675.0, 0.3265380859375]
['model.relu.alpha_mask_8_0', 8192, 2421.0, 0.2955322265625]
['model.relu.alpha_mask_9_0', 4096, 1945.0, 0.474853515625]
['model.relu.alpha_mask_10_0', 4096, 1815.0, 0.443115234375]
['model.relu.alpha_mask_11_0', 4096, 2136.0, 0.521484375]
['model.relu.alpha_mask_12_0', 4096, 1833.0, 0.447509765625]
['model.relu.alpha_mask_13_0', 2048, 1805.0, 0.88134765625]
['model.relu.alpha_mask_14_0', 2048, 1982.0, 0.9677734375]
['model.relu.alpha_mask_15_0', 2048, 1617.0, 0.78955078125]
['model.relu.alpha_mask_16_0', 2048, 1787.0, 0.87255859375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48334.0, 0.2565281080163043]
########## End ###########
06/02 01:52:06 PM | Train: [54/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:52:06 PM | layerwise density: [9266.0, 3762.0, 3058.0, 3761.0, 3057.0, 2905.0, 2509.0, 2675.0, 2421.0, 1945.0, 1815.0, 2136.0, 1833.0, 1805.0, 1982.0, 1617.0, 1787.0]
layerwise density percentage: ['0.141', '0.230', '0.187', '0.230', '0.187', '0.355', '0.306', '0.327', '0.296', '0.475', '0.443', '0.521', '0.448', '0.881', '0.968', '0.790', '0.873']
Global density: 0.2565281093120575
06/02 01:52:17 PM | Train: [54/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:52:17 PM | layerwise density: [9258.0, 3757.0, 3048.0, 3757.0, 3053.0, 2902.0, 2502.0, 2678.0, 2418.0, 1958.0, 1820.0, 2135.0, 1835.0, 1804.0, 1987.0, 1631.0, 1787.0]
layerwise density percentage: ['0.141', '0.229', '0.186', '0.229', '0.186', '0.354', '0.305', '0.327', '0.295', '0.478', '0.444', '0.521', '0.448', '0.881', '0.970', '0.796', '0.873']
Global density: 0.25650689005851746
06/02 01:52:28 PM | Train: [54/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 01:52:28 PM | layerwise density: [9244.0, 3752.0, 3043.0, 3757.0, 3044.0, 2898.0, 2505.0, 2683.0, 2429.0, 1959.0, 1829.0, 2136.0, 1840.0, 1811.0, 1995.0, 1641.0, 1788.0]
layerwise density percentage: ['0.141', '0.229', '0.186', '0.229', '0.186', '0.354', '0.306', '0.328', '0.297', '0.478', '0.447', '0.521', '0.449', '0.884', '0.974', '0.801', '0.873']
Global density: 0.25663426518440247
06/02 01:52:39 PM | Train: [54/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:52:39 PM | layerwise density: [9226.0, 3753.0, 3034.0, 3756.0, 3041.0, 2911.0, 2527.0, 2686.0, 2430.0, 1966.0, 1846.0, 2131.0, 1853.0, 1810.0, 1995.0, 1639.0, 1788.0]
layerwise density percentage: ['0.141', '0.229', '0.185', '0.229', '0.186', '0.355', '0.308', '0.328', '0.297', '0.480', '0.451', '0.520', '0.452', '0.884', '0.974', '0.800', '0.873']
Global density: 0.2568359375
06/02 01:52:48 PM | Train: [54/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:52:48 PM | layerwise density: [9226.0, 3756.0, 3031.0, 3755.0, 3035.0, 2926.0, 2541.0, 2683.0, 2444.0, 1976.0, 1852.0, 2128.0, 1861.0, 1810.0, 1997.0, 1642.0, 1789.0]
layerwise density percentage: ['0.141', '0.229', '0.185', '0.229', '0.185', '0.357', '0.310', '0.328', '0.298', '0.482', '0.452', '0.520', '0.454', '0.884', '0.975', '0.802', '0.874']
Global density: 0.2571543753147125
06/02 01:52:48 PM | Train: [54/200] Final Prec@1 99.9320%
06/02 01:52:48 PM | Valid: [54/200] Step 000/078 Loss 1.093 Prec@(1,5) (75.8%, 91.4%)
06/02 01:52:51 PM | Valid: [54/200] Step 078/078 Loss 1.289 Prec@(1,5) (70.1%, 90.0%)
06/02 01:52:51 PM | Valid: [54/200] Final Prec@1 70.1400%
06/02 01:52:51 PM | Current mask training best Prec@1 = 70.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9226.0, 0.140777587890625]
['model.relu.alpha_mask_1_0', 16384, 3756.0, 0.229248046875]
['model.relu.alpha_mask_2_0', 16384, 3031.0, 0.18499755859375]
['model.relu.alpha_mask_3_0', 16384, 3755.0, 0.22918701171875]
['model.relu.alpha_mask_4_0', 16384, 3035.0, 0.18524169921875]
['model.relu.alpha_mask_5_0', 8192, 2926.0, 0.357177734375]
['model.relu.alpha_mask_6_0', 8192, 2541.0, 0.3101806640625]
['model.relu.alpha_mask_7_0', 8192, 2683.0, 0.3275146484375]
['model.relu.alpha_mask_8_0', 8192, 2444.0, 0.29833984375]
['model.relu.alpha_mask_9_0', 4096, 1976.0, 0.482421875]
['model.relu.alpha_mask_10_0', 4096, 1852.0, 0.4521484375]
['model.relu.alpha_mask_11_0', 4096, 2128.0, 0.51953125]
['model.relu.alpha_mask_12_0', 4096, 1861.0, 0.454345703125]
['model.relu.alpha_mask_13_0', 2048, 1810.0, 0.8837890625]
['model.relu.alpha_mask_14_0', 2048, 1997.0, 0.97509765625]
['model.relu.alpha_mask_15_0', 2048, 1642.0, 0.8017578125]
['model.relu.alpha_mask_16_0', 2048, 1789.0, 0.87353515625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48452.0, 0.25715438179347827]
########## End ###########
06/02 01:52:52 PM | Train: [55/80] Step 000/390 Loss 0.015 Prec@(1,5) (99.2%, 100.0%)
06/02 01:52:52 PM | layerwise density: [9226.0, 3756.0, 3031.0, 3755.0, 3035.0, 2926.0, 2541.0, 2683.0, 2444.0, 1976.0, 1852.0, 2128.0, 1861.0, 1810.0, 1997.0, 1642.0, 1789.0]
layerwise density percentage: ['0.141', '0.229', '0.185', '0.229', '0.185', '0.357', '0.310', '0.328', '0.298', '0.482', '0.452', '0.520', '0.454', '0.884', '0.975', '0.802', '0.874']
Global density: 0.2571543753147125
06/02 01:53:02 PM | Train: [55/80] Step 100/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:53:02 PM | layerwise density: [9223.0, 3767.0, 3031.0, 3752.0, 3046.0, 2940.0, 2552.0, 2689.0, 2454.0, 1978.0, 1861.0, 2128.0, 1860.0, 1817.0, 1995.0, 1638.0, 1789.0]
layerwise density percentage: ['0.141', '0.230', '0.185', '0.229', '0.186', '0.359', '0.312', '0.328', '0.300', '0.483', '0.454', '0.520', '0.454', '0.887', '0.974', '0.800', '0.874']
Global density: 0.2575152814388275
06/02 01:53:13 PM | Train: [55/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:53:13 PM | layerwise density: [9216.0, 3770.0, 3037.0, 3753.0, 3051.0, 2957.0, 2555.0, 2692.0, 2466.0, 1983.0, 1872.0, 2131.0, 1862.0, 1814.0, 1988.0, 1630.0, 1789.0]
layerwise density percentage: ['0.141', '0.230', '0.185', '0.229', '0.186', '0.361', '0.312', '0.329', '0.301', '0.484', '0.457', '0.520', '0.455', '0.886', '0.971', '0.796', '0.874']
Global density: 0.2577594220638275
06/02 01:53:24 PM | Train: [55/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 01:53:24 PM | layerwise density: [9209.0, 3777.0, 3037.0, 3751.0, 3060.0, 2974.0, 2560.0, 2691.0, 2465.0, 1991.0, 1884.0, 2131.0, 1868.0, 1816.0, 1993.0, 1628.0, 1791.0]
layerwise density percentage: ['0.141', '0.231', '0.185', '0.229', '0.187', '0.363', '0.312', '0.328', '0.301', '0.486', '0.460', '0.520', '0.456', '0.887', '0.973', '0.795', '0.875']
Global density: 0.2580778896808624
06/02 01:53:34 PM | Train: [55/80] Step 390/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 01:53:34 PM | layerwise density: [9201.0, 3781.0, 3046.0, 3757.0, 3065.0, 2990.0, 2565.0, 2694.0, 2469.0, 1998.0, 1889.0, 2133.0, 1870.0, 1822.0, 1999.0, 1629.0, 1791.0]
layerwise density percentage: ['0.140', '0.231', '0.186', '0.229', '0.187', '0.365', '0.313', '0.329', '0.301', '0.488', '0.461', '0.521', '0.457', '0.890', '0.976', '0.795', '0.875']
Global density: 0.25846531987190247
06/02 01:53:34 PM | Train: [55/200] Final Prec@1 99.9520%
06/02 01:53:34 PM | Valid: [55/200] Step 000/078 Loss 1.061 Prec@(1,5) (75.0%, 93.0%)
06/02 01:53:37 PM | Valid: [55/200] Step 078/078 Loss 1.285 Prec@(1,5) (70.5%, 90.0%)
06/02 01:53:37 PM | Valid: [55/200] Final Prec@1 70.5300%
06/02 01:53:38 PM | Current mask training best Prec@1 = 70.5300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9201.0, 0.1403961181640625]
['model.relu.alpha_mask_1_0', 16384, 3781.0, 0.23077392578125]
['model.relu.alpha_mask_2_0', 16384, 3046.0, 0.1859130859375]
['model.relu.alpha_mask_3_0', 16384, 3757.0, 0.22930908203125]
['model.relu.alpha_mask_4_0', 16384, 3065.0, 0.18707275390625]
['model.relu.alpha_mask_5_0', 8192, 2990.0, 0.364990234375]
['model.relu.alpha_mask_6_0', 8192, 2565.0, 0.3131103515625]
['model.relu.alpha_mask_7_0', 8192, 2694.0, 0.328857421875]
['model.relu.alpha_mask_8_0', 8192, 2469.0, 0.3013916015625]
['model.relu.alpha_mask_9_0', 4096, 1998.0, 0.48779296875]
['model.relu.alpha_mask_10_0', 4096, 1889.0, 0.461181640625]
['model.relu.alpha_mask_11_0', 4096, 2133.0, 0.520751953125]
['model.relu.alpha_mask_12_0', 4096, 1870.0, 0.45654296875]
['model.relu.alpha_mask_13_0', 2048, 1822.0, 0.8896484375]
['model.relu.alpha_mask_14_0', 2048, 1999.0, 0.97607421875]
['model.relu.alpha_mask_15_0', 2048, 1629.0, 0.79541015625]
['model.relu.alpha_mask_16_0', 2048, 1791.0, 0.87451171875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48699.0, 0.25846531080163043]
########## End ###########
06/02 01:53:39 PM | Train: [56/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:53:39 PM | layerwise density: [9201.0, 3781.0, 3046.0, 3757.0, 3065.0, 2990.0, 2565.0, 2694.0, 2469.0, 1998.0, 1889.0, 2133.0, 1870.0, 1822.0, 1999.0, 1629.0, 1791.0]
layerwise density percentage: ['0.140', '0.231', '0.186', '0.229', '0.187', '0.365', '0.313', '0.329', '0.301', '0.488', '0.461', '0.521', '0.457', '0.890', '0.976', '0.795', '0.875']
Global density: 0.25846531987190247
06/02 01:53:50 PM | Train: [56/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:53:50 PM | layerwise density: [9189.0, 3793.0, 3045.0, 3763.0, 3069.0, 3007.0, 2574.0, 2703.0, 2477.0, 2002.0, 1889.0, 2134.0, 1872.0, 1822.0, 1996.0, 1635.0, 1791.0]
layerwise density percentage: ['0.140', '0.232', '0.186', '0.230', '0.187', '0.367', '0.314', '0.330', '0.302', '0.489', '0.461', '0.521', '0.457', '0.890', '0.975', '0.798', '0.875']
Global density: 0.258794367313385
06/02 01:54:01 PM | Train: [56/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 01:54:01 PM | layerwise density: [9189.0, 3794.0, 3046.0, 3772.0, 3082.0, 3018.0, 2572.0, 2702.0, 2478.0, 2006.0, 1884.0, 2138.0, 1881.0, 1820.0, 2002.0, 1639.0, 1792.0]
layerwise density percentage: ['0.140', '0.232', '0.186', '0.230', '0.188', '0.368', '0.314', '0.330', '0.302', '0.490', '0.460', '0.522', '0.459', '0.889', '0.978', '0.800', '0.875']
Global density: 0.2590809762477875
06/02 01:54:12 PM | Train: [56/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:54:12 PM | layerwise density: [9190.0, 3795.0, 3046.0, 3773.0, 3097.0, 3035.0, 2574.0, 2703.0, 2477.0, 2009.0, 1886.0, 2138.0, 1886.0, 1820.0, 1995.0, 1646.0, 1792.0]
layerwise density percentage: ['0.140', '0.232', '0.186', '0.230', '0.189', '0.370', '0.314', '0.330', '0.302', '0.490', '0.460', '0.522', '0.460', '0.889', '0.974', '0.804', '0.875']
Global density: 0.2593304216861725
06/02 01:54:22 PM | Train: [56/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:54:22 PM | layerwise density: [9189.0, 3803.0, 3045.0, 3777.0, 3105.0, 3047.0, 2575.0, 2710.0, 2473.0, 2008.0, 1886.0, 2139.0, 1893.0, 1824.0, 1992.0, 1643.0, 1792.0]
layerwise density percentage: ['0.140', '0.232', '0.186', '0.231', '0.190', '0.372', '0.314', '0.331', '0.302', '0.490', '0.460', '0.522', '0.462', '0.891', '0.973', '0.802', '0.875']
Global density: 0.25953739881515503
06/02 01:54:22 PM | Train: [56/200] Final Prec@1 99.9500%
06/02 01:54:22 PM | Valid: [56/200] Step 000/078 Loss 1.075 Prec@(1,5) (75.0%, 93.0%)
06/02 01:54:24 PM | Valid: [56/200] Step 078/078 Loss 1.290 Prec@(1,5) (70.1%, 90.1%)
06/02 01:54:24 PM | Valid: [56/200] Final Prec@1 70.1000%
06/02 01:54:24 PM | Current mask training best Prec@1 = 70.5300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9190.0, 0.140228271484375]
['model.relu.alpha_mask_1_0', 16384, 3803.0, 0.23211669921875]
['model.relu.alpha_mask_2_0', 16384, 3045.0, 0.18585205078125]
['model.relu.alpha_mask_3_0', 16384, 3777.0, 0.23052978515625]
['model.relu.alpha_mask_4_0', 16384, 3105.0, 0.18951416015625]
['model.relu.alpha_mask_5_0', 8192, 3048.0, 0.3720703125]
['model.relu.alpha_mask_6_0', 8192, 2575.0, 0.3143310546875]
['model.relu.alpha_mask_7_0', 8192, 2710.0, 0.330810546875]
['model.relu.alpha_mask_8_0', 8192, 2473.0, 0.3018798828125]
['model.relu.alpha_mask_9_0', 4096, 2008.0, 0.490234375]
['model.relu.alpha_mask_10_0', 4096, 1886.0, 0.46044921875]
['model.relu.alpha_mask_11_0', 4096, 2139.0, 0.522216796875]
['model.relu.alpha_mask_12_0', 4096, 1892.0, 0.4619140625]
['model.relu.alpha_mask_13_0', 2048, 1824.0, 0.890625]
['model.relu.alpha_mask_14_0', 2048, 1992.0, 0.97265625]
['model.relu.alpha_mask_15_0', 2048, 1644.0, 0.802734375]
['model.relu.alpha_mask_16_0', 2048, 1792.0, 0.875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 48903.0, 0.25954802139945654]
########## End ###########
06/02 01:54:25 PM | Train: [57/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:54:25 PM | layerwise density: [9190.0, 3803.0, 3045.0, 3777.0, 3105.0, 3048.0, 2575.0, 2710.0, 2473.0, 2008.0, 1886.0, 2139.0, 1892.0, 1824.0, 1992.0, 1644.0, 1792.0]
layerwise density percentage: ['0.140', '0.232', '0.186', '0.231', '0.190', '0.372', '0.314', '0.331', '0.302', '0.490', '0.460', '0.522', '0.462', '0.891', '0.973', '0.803', '0.875']
Global density: 0.25954803824424744
06/02 01:54:36 PM | Train: [57/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:54:36 PM | layerwise density: [9180.0, 3814.0, 3047.0, 3782.0, 3109.0, 3045.0, 2584.0, 2711.0, 2487.0, 2013.0, 1888.0, 2142.0, 1886.0, 1827.0, 1994.0, 1643.0, 1792.0]
layerwise density percentage: ['0.140', '0.233', '0.186', '0.231', '0.190', '0.372', '0.315', '0.331', '0.304', '0.491', '0.461', '0.523', '0.460', '0.892', '0.974', '0.802', '0.875']
Global density: 0.259765625
06/02 01:54:47 PM | Train: [57/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:54:47 PM | layerwise density: [9178.0, 3830.0, 3050.0, 3786.0, 3115.0, 3055.0, 2591.0, 2712.0, 2496.0, 2007.0, 1888.0, 2145.0, 1884.0, 1824.0, 1993.0, 1643.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.186', '0.231', '0.190', '0.373', '0.316', '0.331', '0.305', '0.490', '0.461', '0.524', '0.460', '0.891', '0.973', '0.802', '0.875']
Global density: 0.260004460811615
06/02 01:54:58 PM | Train: [57/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:54:58 PM | layerwise density: [9173.0, 3830.0, 3053.0, 3794.0, 3120.0, 3069.0, 2594.0, 2717.0, 2508.0, 2010.0, 1876.0, 2148.0, 1885.0, 1823.0, 1992.0, 1637.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.186', '0.232', '0.190', '0.375', '0.317', '0.332', '0.306', '0.491', '0.458', '0.524', '0.460', '0.890', '0.973', '0.799', '0.875']
Global density: 0.26017430424690247
06/02 01:55:08 PM | Train: [57/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:55:08 PM | layerwise density: [9170.0, 3830.0, 3052.0, 3802.0, 3128.0, 3079.0, 2599.0, 2723.0, 2524.0, 2017.0, 1875.0, 2149.0, 1889.0, 1825.0, 1992.0, 1627.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.186', '0.232', '0.191', '0.376', '0.317', '0.332', '0.308', '0.492', '0.458', '0.525', '0.461', '0.891', '0.973', '0.794', '0.875']
Global density: 0.2604502737522125
06/02 01:55:08 PM | Train: [57/200] Final Prec@1 99.9560%
06/02 01:55:08 PM | Valid: [57/200] Step 000/078 Loss 1.131 Prec@(1,5) (71.9%, 92.2%)
06/02 01:55:10 PM | Valid: [57/200] Step 078/078 Loss 1.289 Prec@(1,5) (70.3%, 89.9%)
06/02 01:55:10 PM | Valid: [57/200] Final Prec@1 70.3300%
06/02 01:55:10 PM | Current mask training best Prec@1 = 70.5300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9170.0, 0.139923095703125]
['model.relu.alpha_mask_1_0', 16384, 3829.0, 0.23370361328125]
['model.relu.alpha_mask_2_0', 16384, 3052.0, 0.186279296875]
['model.relu.alpha_mask_3_0', 16384, 3802.0, 0.2320556640625]
['model.relu.alpha_mask_4_0', 16384, 3128.0, 0.19091796875]
['model.relu.alpha_mask_5_0', 8192, 3078.0, 0.375732421875]
['model.relu.alpha_mask_6_0', 8192, 2600.0, 0.3173828125]
['model.relu.alpha_mask_7_0', 8192, 2723.0, 0.3323974609375]
['model.relu.alpha_mask_8_0', 8192, 2524.0, 0.30810546875]
['model.relu.alpha_mask_9_0', 4096, 2018.0, 0.49267578125]
['model.relu.alpha_mask_10_0', 4096, 1876.0, 0.4580078125]
['model.relu.alpha_mask_11_0', 4096, 2149.0, 0.524658203125]
['model.relu.alpha_mask_12_0', 4096, 1888.0, 0.4609375]
['model.relu.alpha_mask_13_0', 2048, 1825.0, 0.89111328125]
['model.relu.alpha_mask_14_0', 2048, 1992.0, 0.97265625]
['model.relu.alpha_mask_15_0', 2048, 1627.0, 0.79443359375]
['model.relu.alpha_mask_16_0', 2048, 1792.0, 0.875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49073.0, 0.26045028023097827]
########## End ###########
06/02 01:55:11 PM | Train: [58/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:55:11 PM | layerwise density: [9170.0, 3829.0, 3052.0, 3802.0, 3128.0, 3078.0, 2600.0, 2723.0, 2524.0, 2018.0, 1876.0, 2149.0, 1888.0, 1825.0, 1992.0, 1627.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.186', '0.232', '0.191', '0.376', '0.317', '0.332', '0.308', '0.493', '0.458', '0.525', '0.461', '0.891', '0.973', '0.794', '0.875']
Global density: 0.2604502737522125
06/02 01:55:22 PM | Train: [58/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:55:22 PM | layerwise density: [9172.0, 3830.0, 3057.0, 3805.0, 3121.0, 3078.0, 2602.0, 2734.0, 2538.0, 2002.0, 1883.0, 2152.0, 1884.0, 1827.0, 1992.0, 1625.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.187', '0.232', '0.190', '0.376', '0.318', '0.334', '0.310', '0.489', '0.460', '0.525', '0.460', '0.892', '0.973', '0.793', '0.875']
Global density: 0.2605617344379425
06/02 01:55:33 PM | Train: [58/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:55:33 PM | layerwise density: [9161.0, 3834.0, 3058.0, 3803.0, 3127.0, 3075.0, 2597.0, 2735.0, 2550.0, 2004.0, 1888.0, 2150.0, 1888.0, 1821.0, 1991.0, 1621.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.187', '0.232', '0.191', '0.375', '0.317', '0.334', '0.311', '0.489', '0.461', '0.525', '0.461', '0.889', '0.972', '0.792', '0.875']
Global density: 0.2605670392513275
06/02 01:55:44 PM | Train: [58/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:55:44 PM | layerwise density: [9154.0, 3836.0, 3060.0, 3808.0, 3133.0, 3074.0, 2601.0, 2741.0, 2554.0, 2004.0, 1887.0, 2151.0, 1893.0, 1819.0, 1991.0, 1626.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.187', '0.232', '0.191', '0.375', '0.318', '0.335', '0.312', '0.489', '0.461', '0.525', '0.462', '0.888', '0.972', '0.794', '0.875']
Global density: 0.26072096824645996
06/02 01:55:53 PM | Train: [58/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:55:53 PM | layerwise density: [9150.0, 3838.0, 3059.0, 3814.0, 3135.0, 3085.0, 2604.0, 2747.0, 2557.0, 2011.0, 1889.0, 2152.0, 1898.0, 1820.0, 1993.0, 1635.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.187', '0.233', '0.191', '0.377', '0.318', '0.335', '0.312', '0.491', '0.461', '0.525', '0.463', '0.889', '0.973', '0.798', '0.875']
Global density: 0.26101288199424744
06/02 01:55:53 PM | Train: [58/200] Final Prec@1 99.9600%
06/02 01:55:54 PM | Valid: [58/200] Step 000/078 Loss 1.126 Prec@(1,5) (73.4%, 91.4%)
06/02 01:55:56 PM | Valid: [58/200] Step 078/078 Loss 1.291 Prec@(1,5) (70.2%, 89.8%)
06/02 01:55:56 PM | Valid: [58/200] Final Prec@1 70.2500%
06/02 01:55:56 PM | Current mask training best Prec@1 = 70.5300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9150.0, 0.139617919921875]
['model.relu.alpha_mask_1_0', 16384, 3837.0, 0.23419189453125]
['model.relu.alpha_mask_2_0', 16384, 3060.0, 0.186767578125]
['model.relu.alpha_mask_3_0', 16384, 3815.0, 0.23284912109375]
['model.relu.alpha_mask_4_0', 16384, 3137.0, 0.19146728515625]
['model.relu.alpha_mask_5_0', 8192, 3084.0, 0.37646484375]
['model.relu.alpha_mask_6_0', 8192, 2604.0, 0.31787109375]
['model.relu.alpha_mask_7_0', 8192, 2747.0, 0.3353271484375]
['model.relu.alpha_mask_8_0', 8192, 2557.0, 0.3121337890625]
['model.relu.alpha_mask_9_0', 4096, 2011.0, 0.490966796875]
['model.relu.alpha_mask_10_0', 4096, 1890.0, 0.46142578125]
['model.relu.alpha_mask_11_0', 4096, 2152.0, 0.525390625]
['model.relu.alpha_mask_12_0', 4096, 1897.0, 0.463134765625]
['model.relu.alpha_mask_13_0', 2048, 1820.0, 0.888671875]
['model.relu.alpha_mask_14_0', 2048, 1993.0, 0.97314453125]
['model.relu.alpha_mask_15_0', 2048, 1635.0, 0.79833984375]
['model.relu.alpha_mask_16_0', 2048, 1792.0, 0.875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49181.0, 0.26102347995923914]
########## End ###########
06/02 01:55:57 PM | Train: [59/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:55:57 PM | layerwise density: [9150.0, 3837.0, 3060.0, 3815.0, 3137.0, 3084.0, 2604.0, 2747.0, 2557.0, 2011.0, 1890.0, 2152.0, 1897.0, 1820.0, 1993.0, 1635.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.187', '0.233', '0.191', '0.376', '0.318', '0.335', '0.312', '0.491', '0.461', '0.525', '0.463', '0.889', '0.973', '0.798', '0.875']
Global density: 0.26102349162101746
06/02 01:56:08 PM | Train: [59/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:56:08 PM | layerwise density: [9152.0, 3837.0, 3067.0, 3820.0, 3139.0, 3093.0, 2603.0, 2749.0, 2555.0, 2004.0, 1896.0, 2151.0, 1902.0, 1821.0, 1987.0, 1624.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.187', '0.233', '0.192', '0.378', '0.318', '0.336', '0.312', '0.489', '0.463', '0.525', '0.464', '0.889', '0.970', '0.793', '0.875']
Global density: 0.26108187437057495
06/02 01:56:19 PM | Train: [59/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 01:56:19 PM | layerwise density: [9154.0, 3842.0, 3066.0, 3829.0, 3139.0, 3090.0, 2605.0, 2747.0, 2561.0, 2003.0, 1909.0, 2149.0, 1908.0, 1824.0, 1986.0, 1629.0, 1792.0]
layerwise density percentage: ['0.140', '0.234', '0.187', '0.234', '0.192', '0.377', '0.318', '0.335', '0.313', '0.489', '0.466', '0.525', '0.466', '0.891', '0.970', '0.795', '0.875']
Global density: 0.2612994611263275
06/02 01:56:30 PM | Train: [59/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:56:30 PM | layerwise density: [9148.0, 3856.0, 3072.0, 3833.0, 3140.0, 3090.0, 2608.0, 2752.0, 2560.0, 2006.0, 1918.0, 2153.0, 1903.0, 1826.0, 1993.0, 1626.0, 1793.0]
layerwise density percentage: ['0.140', '0.235', '0.188', '0.234', '0.192', '0.377', '0.318', '0.336', '0.312', '0.490', '0.468', '0.526', '0.465', '0.892', '0.973', '0.794', '0.875']
Global density: 0.2615329921245575
06/02 01:56:39 PM | Train: [59/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 01:56:39 PM | layerwise density: [9149.0, 3866.0, 3076.0, 3831.0, 3147.0, 3099.0, 2603.0, 2757.0, 2566.0, 2002.0, 1922.0, 2156.0, 1907.0, 1828.0, 1997.0, 1630.0, 1794.0]
layerwise density percentage: ['0.140', '0.236', '0.188', '0.234', '0.192', '0.378', '0.318', '0.337', '0.313', '0.489', '0.469', '0.526', '0.466', '0.893', '0.975', '0.796', '0.876']
Global density: 0.26181429624557495
06/02 01:56:39 PM | Train: [59/200] Final Prec@1 99.9460%
06/02 01:56:40 PM | Valid: [59/200] Step 000/078 Loss 1.070 Prec@(1,5) (74.2%, 92.2%)
06/02 01:56:42 PM | Valid: [59/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.5%, 90.1%)
06/02 01:56:42 PM | Valid: [59/200] Final Prec@1 70.5200%
06/02 01:56:42 PM | Current mask training best Prec@1 = 70.5300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9149.0, 0.1396026611328125]
['model.relu.alpha_mask_1_0', 16384, 3866.0, 0.2359619140625]
['model.relu.alpha_mask_2_0', 16384, 3076.0, 0.187744140625]
['model.relu.alpha_mask_3_0', 16384, 3832.0, 0.23388671875]
['model.relu.alpha_mask_4_0', 16384, 3147.0, 0.19207763671875]
['model.relu.alpha_mask_5_0', 8192, 3099.0, 0.3782958984375]
['model.relu.alpha_mask_6_0', 8192, 2603.0, 0.3177490234375]
['model.relu.alpha_mask_7_0', 8192, 2757.0, 0.3365478515625]
['model.relu.alpha_mask_8_0', 8192, 2566.0, 0.313232421875]
['model.relu.alpha_mask_9_0', 4096, 2002.0, 0.48876953125]
['model.relu.alpha_mask_10_0', 4096, 1923.0, 0.469482421875]
['model.relu.alpha_mask_11_0', 4096, 2156.0, 0.5263671875]
['model.relu.alpha_mask_12_0', 4096, 1909.0, 0.466064453125]
['model.relu.alpha_mask_13_0', 2048, 1828.0, 0.892578125]
['model.relu.alpha_mask_14_0', 2048, 1997.0, 0.97509765625]
['model.relu.alpha_mask_15_0', 2048, 1631.0, 0.79638671875]
['model.relu.alpha_mask_16_0', 2048, 1794.0, 0.8759765625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49335.0, 0.2618408203125]
########## End ###########
06/02 01:56:43 PM | Train: [60/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:56:43 PM | layerwise density: [9149.0, 3866.0, 3076.0, 3832.0, 3147.0, 3099.0, 2603.0, 2757.0, 2566.0, 2002.0, 1923.0, 2156.0, 1909.0, 1828.0, 1997.0, 1631.0, 1794.0]
layerwise density percentage: ['0.140', '0.236', '0.188', '0.234', '0.192', '0.378', '0.318', '0.337', '0.313', '0.489', '0.469', '0.526', '0.466', '0.893', '0.975', '0.796', '0.876']
Global density: 0.2618408203125
06/02 01:56:53 PM | Train: [60/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:56:53 PM | layerwise density: [9154.0, 3869.0, 3080.0, 3841.0, 3144.0, 3106.0, 2606.0, 2754.0, 2567.0, 2003.0, 1925.0, 2159.0, 1903.0, 1836.0, 2001.0, 1634.0, 1795.0]
layerwise density percentage: ['0.140', '0.236', '0.188', '0.234', '0.192', '0.379', '0.318', '0.336', '0.313', '0.489', '0.470', '0.527', '0.465', '0.896', '0.977', '0.798', '0.876']
Global density: 0.26206374168395996
06/02 01:57:04 PM | Train: [60/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:57:04 PM | layerwise density: [9151.0, 3873.0, 3085.0, 3845.0, 3142.0, 3111.0, 2602.0, 2754.0, 2565.0, 2005.0, 1921.0, 2159.0, 1901.0, 1831.0, 2003.0, 1638.0, 1795.0]
layerwise density percentage: ['0.140', '0.236', '0.188', '0.235', '0.192', '0.380', '0.318', '0.336', '0.313', '0.490', '0.469', '0.527', '0.464', '0.894', '0.978', '0.800', '0.876']
Global density: 0.2620849609375
06/02 01:57:15 PM | Train: [60/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:57:15 PM | layerwise density: [9148.0, 3882.0, 3090.0, 3853.0, 3147.0, 3117.0, 2607.0, 2751.0, 2556.0, 2007.0, 1919.0, 2158.0, 1906.0, 1831.0, 2003.0, 1639.0, 1795.0]
layerwise density percentage: ['0.140', '0.237', '0.189', '0.235', '0.192', '0.380', '0.318', '0.336', '0.312', '0.490', '0.469', '0.527', '0.465', '0.894', '0.978', '0.800', '0.876']
Global density: 0.26223358511924744
06/02 01:57:25 PM | Train: [60/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 01:57:25 PM | layerwise density: [9151.0, 3885.0, 3095.0, 3855.0, 3149.0, 3123.0, 2609.0, 2754.0, 2562.0, 2009.0, 1915.0, 2157.0, 1911.0, 1830.0, 2000.0, 1637.0, 1796.0]
layerwise density percentage: ['0.140', '0.237', '0.189', '0.235', '0.192', '0.381', '0.318', '0.336', '0.313', '0.490', '0.468', '0.527', '0.467', '0.894', '0.977', '0.799', '0.877']
Global density: 0.2623874843120575
06/02 01:57:25 PM | Train: [60/200] Final Prec@1 99.9480%
06/02 01:57:25 PM | Valid: [60/200] Step 000/078 Loss 1.069 Prec@(1,5) (77.3%, 93.0%)
06/02 01:57:28 PM | Valid: [60/200] Step 078/078 Loss 1.285 Prec@(1,5) (70.5%, 90.0%)
06/02 01:57:28 PM | Valid: [60/200] Final Prec@1 70.5100%
06/02 01:57:28 PM | Current mask training best Prec@1 = 70.5300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9151.0, 0.1396331787109375]
['model.relu.alpha_mask_1_0', 16384, 3885.0, 0.23712158203125]
['model.relu.alpha_mask_2_0', 16384, 3095.0, 0.18890380859375]
['model.relu.alpha_mask_3_0', 16384, 3855.0, 0.23529052734375]
['model.relu.alpha_mask_4_0', 16384, 3149.0, 0.19219970703125]
['model.relu.alpha_mask_5_0', 8192, 3123.0, 0.3812255859375]
['model.relu.alpha_mask_6_0', 8192, 2609.0, 0.3184814453125]
['model.relu.alpha_mask_7_0', 8192, 2754.0, 0.336181640625]
['model.relu.alpha_mask_8_0', 8192, 2562.0, 0.312744140625]
['model.relu.alpha_mask_9_0', 4096, 2009.0, 0.490478515625]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2157.0, 0.526611328125]
['model.relu.alpha_mask_12_0', 4096, 1911.0, 0.466552734375]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1637.0, 0.79931640625]
['model.relu.alpha_mask_16_0', 2048, 1796.0, 0.876953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49438.0, 0.2623874830163043]
########## End ###########
06/02 01:57:29 PM | Train: [61/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:57:29 PM | layerwise density: [9151.0, 3885.0, 3095.0, 3855.0, 3149.0, 3123.0, 2609.0, 2754.0, 2562.0, 2009.0, 1915.0, 2157.0, 1911.0, 1830.0, 2000.0, 1637.0, 1796.0]
layerwise density percentage: ['0.140', '0.237', '0.189', '0.235', '0.192', '0.381', '0.318', '0.336', '0.313', '0.490', '0.468', '0.527', '0.467', '0.894', '0.977', '0.799', '0.877']
Global density: 0.2623874843120575
06/02 01:57:40 PM | Train: [61/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:57:40 PM | layerwise density: [9157.0, 3878.0, 3098.0, 3851.0, 3152.0, 3113.0, 2613.0, 2755.0, 2557.0, 2008.0, 1914.0, 2157.0, 1910.0, 1827.0, 2001.0, 1643.0, 1796.0]
layerwise density percentage: ['0.140', '0.237', '0.189', '0.235', '0.192', '0.380', '0.319', '0.336', '0.312', '0.490', '0.467', '0.527', '0.466', '0.892', '0.977', '0.802', '0.877']
Global density: 0.26234501600265503
06/02 01:57:52 PM | Train: [61/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:57:52 PM | layerwise density: [9155.0, 3882.0, 3104.0, 3853.0, 3154.0, 3117.0, 2611.0, 2755.0, 2558.0, 2010.0, 1914.0, 2160.0, 1913.0, 1828.0, 2001.0, 1641.0, 1796.0]
layerwise density percentage: ['0.140', '0.237', '0.189', '0.235', '0.193', '0.380', '0.319', '0.336', '0.312', '0.491', '0.467', '0.527', '0.467', '0.893', '0.977', '0.801', '0.877']
Global density: 0.26246178150177
06/02 01:58:03 PM | Train: [61/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:58:03 PM | layerwise density: [9158.0, 3891.0, 3105.0, 3851.0, 3150.0, 3118.0, 2619.0, 2757.0, 2561.0, 2013.0, 1919.0, 2156.0, 1912.0, 1829.0, 2000.0, 1641.0, 1796.0]
layerwise density percentage: ['0.140', '0.237', '0.190', '0.235', '0.192', '0.381', '0.320', '0.337', '0.313', '0.491', '0.469', '0.526', '0.467', '0.893', '0.977', '0.801', '0.877']
Global density: 0.26258915662765503
06/02 01:58:13 PM | Train: [61/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:58:13 PM | layerwise density: [9162.0, 3899.0, 3108.0, 3856.0, 3156.0, 3122.0, 2625.0, 2760.0, 2560.0, 2024.0, 1907.0, 2159.0, 1904.0, 1829.0, 2000.0, 1637.0, 1797.0]
layerwise density percentage: ['0.140', '0.238', '0.190', '0.235', '0.193', '0.381', '0.320', '0.337', '0.312', '0.494', '0.466', '0.527', '0.465', '0.893', '0.977', '0.799', '0.877']
Global density: 0.2627430856227875
06/02 01:58:13 PM | Train: [61/200] Final Prec@1 99.9560%
06/02 01:58:13 PM | Valid: [61/200] Step 000/078 Loss 1.106 Prec@(1,5) (74.2%, 92.2%)
06/02 01:58:16 PM | Valid: [61/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.4%, 90.0%)
06/02 01:58:16 PM | Valid: [61/200] Final Prec@1 70.4200%
06/02 01:58:16 PM | Current mask training best Prec@1 = 70.5300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9162.0, 0.139801025390625]
['model.relu.alpha_mask_1_0', 16384, 3899.0, 0.23797607421875]
['model.relu.alpha_mask_2_0', 16384, 3108.0, 0.189697265625]
['model.relu.alpha_mask_3_0', 16384, 3856.0, 0.2353515625]
['model.relu.alpha_mask_4_0', 16384, 3156.0, 0.192626953125]
['model.relu.alpha_mask_5_0', 8192, 3122.0, 0.381103515625]
['model.relu.alpha_mask_6_0', 8192, 2625.0, 0.3204345703125]
['model.relu.alpha_mask_7_0', 8192, 2760.0, 0.3369140625]
['model.relu.alpha_mask_8_0', 8192, 2560.0, 0.3125]
['model.relu.alpha_mask_9_0', 4096, 2024.0, 0.494140625]
['model.relu.alpha_mask_10_0', 4096, 1907.0, 0.465576171875]
['model.relu.alpha_mask_11_0', 4096, 2159.0, 0.527099609375]
['model.relu.alpha_mask_12_0', 4096, 1904.0, 0.46484375]
['model.relu.alpha_mask_13_0', 2048, 1829.0, 0.89306640625]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1637.0, 0.79931640625]
['model.relu.alpha_mask_16_0', 2048, 1797.0, 0.87744140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49505.0, 0.26274307914402173]
########## End ###########
06/02 01:58:17 PM | Train: [62/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:58:17 PM | layerwise density: [9162.0, 3899.0, 3108.0, 3856.0, 3156.0, 3122.0, 2625.0, 2760.0, 2560.0, 2024.0, 1907.0, 2159.0, 1904.0, 1829.0, 2000.0, 1637.0, 1797.0]
layerwise density percentage: ['0.140', '0.238', '0.190', '0.235', '0.193', '0.381', '0.320', '0.337', '0.312', '0.494', '0.466', '0.527', '0.465', '0.893', '0.977', '0.799', '0.877']
Global density: 0.2627430856227875
06/02 01:58:28 PM | Train: [62/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:58:28 PM | layerwise density: [9157.0, 3905.0, 3111.0, 3859.0, 3161.0, 3123.0, 2625.0, 2764.0, 2557.0, 2034.0, 1903.0, 2160.0, 1905.0, 1833.0, 1999.0, 1632.0, 1797.0]
layerwise density percentage: ['0.140', '0.238', '0.190', '0.236', '0.193', '0.381', '0.320', '0.337', '0.312', '0.497', '0.465', '0.527', '0.465', '0.895', '0.976', '0.797', '0.877']
Global density: 0.26284924149513245
06/02 01:58:39 PM | Train: [62/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:58:39 PM | layerwise density: [9156.0, 3908.0, 3112.0, 3860.0, 3161.0, 3121.0, 2628.0, 2764.0, 2561.0, 2041.0, 1899.0, 2159.0, 1909.0, 1830.0, 1997.0, 1630.0, 1798.0]
layerwise density percentage: ['0.140', '0.239', '0.190', '0.236', '0.193', '0.381', '0.321', '0.337', '0.313', '0.498', '0.464', '0.527', '0.466', '0.894', '0.975', '0.796', '0.878']
Global density: 0.26289698481559753
06/02 01:58:51 PM | Train: [62/80] Step 300/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 01:58:51 PM | layerwise density: [9150.0, 3910.0, 3116.0, 3862.0, 3166.0, 3119.0, 2642.0, 2768.0, 2570.0, 2040.0, 1890.0, 2159.0, 1905.0, 1827.0, 1996.0, 1627.0, 1798.0]
layerwise density percentage: ['0.140', '0.239', '0.190', '0.236', '0.193', '0.381', '0.323', '0.338', '0.314', '0.498', '0.461', '0.527', '0.465', '0.892', '0.975', '0.794', '0.878']
Global density: 0.26295536756515503
06/02 01:59:01 PM | Train: [62/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 01:59:01 PM | layerwise density: [9152.0, 3912.0, 3116.0, 3863.0, 3163.0, 3125.0, 2649.0, 2768.0, 2568.0, 2036.0, 1894.0, 2160.0, 1915.0, 1827.0, 1995.0, 1634.0, 1798.0]
layerwise density percentage: ['0.140', '0.239', '0.190', '0.236', '0.193', '0.381', '0.323', '0.338', '0.313', '0.497', '0.462', '0.527', '0.468', '0.892', '0.974', '0.798', '0.878']
Global density: 0.2631146013736725
06/02 01:59:01 PM | Train: [62/200] Final Prec@1 99.9480%
06/02 01:59:01 PM | Valid: [62/200] Step 000/078 Loss 1.045 Prec@(1,5) (76.6%, 93.0%)
06/02 01:59:03 PM | Valid: [62/200] Step 078/078 Loss 1.282 Prec@(1,5) (70.6%, 90.0%)
06/02 01:59:04 PM | Valid: [62/200] Final Prec@1 70.6000%
06/02 01:59:04 PM | Current mask training best Prec@1 = 70.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9152.0, 0.1396484375]
['model.relu.alpha_mask_1_0', 16384, 3912.0, 0.23876953125]
['model.relu.alpha_mask_2_0', 16384, 3116.0, 0.190185546875]
['model.relu.alpha_mask_3_0', 16384, 3863.0, 0.23577880859375]
['model.relu.alpha_mask_4_0', 16384, 3163.0, 0.19305419921875]
['model.relu.alpha_mask_5_0', 8192, 3125.0, 0.3814697265625]
['model.relu.alpha_mask_6_0', 8192, 2649.0, 0.3233642578125]
['model.relu.alpha_mask_7_0', 8192, 2768.0, 0.337890625]
['model.relu.alpha_mask_8_0', 8192, 2568.0, 0.3134765625]
['model.relu.alpha_mask_9_0', 4096, 2036.0, 0.4970703125]
['model.relu.alpha_mask_10_0', 4096, 1894.0, 0.46240234375]
['model.relu.alpha_mask_11_0', 4096, 2159.0, 0.527099609375]
['model.relu.alpha_mask_12_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_13_0', 2048, 1827.0, 0.89208984375]
['model.relu.alpha_mask_14_0', 2048, 1995.0, 0.97412109375]
['model.relu.alpha_mask_15_0', 2048, 1634.0, 0.7978515625]
['model.relu.alpha_mask_16_0', 2048, 1798.0, 0.8779296875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49574.0, 0.26310929008152173]
########## End ###########
06/02 01:59:05 PM | Train: [63/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:59:05 PM | layerwise density: [9152.0, 3912.0, 3116.0, 3863.0, 3163.0, 3125.0, 2649.0, 2768.0, 2568.0, 2036.0, 1894.0, 2159.0, 1915.0, 1827.0, 1995.0, 1634.0, 1798.0]
layerwise density percentage: ['0.140', '0.239', '0.190', '0.236', '0.193', '0.381', '0.323', '0.338', '0.313', '0.497', '0.462', '0.527', '0.468', '0.892', '0.974', '0.798', '0.878']
Global density: 0.2631092965602875
06/02 01:59:16 PM | Train: [63/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:59:16 PM | layerwise density: [9156.0, 3918.0, 3118.0, 3864.0, 3164.0, 3113.0, 2654.0, 2776.0, 2571.0, 2038.0, 1895.0, 2161.0, 1918.0, 1821.0, 1995.0, 1627.0, 1798.0]
layerwise density percentage: ['0.140', '0.239', '0.190', '0.236', '0.193', '0.380', '0.324', '0.339', '0.314', '0.498', '0.463', '0.528', '0.468', '0.889', '0.974', '0.794', '0.878']
Global density: 0.263178288936615
06/02 01:59:27 PM | Train: [63/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:59:27 PM | layerwise density: [9156.0, 3920.0, 3117.0, 3863.0, 3170.0, 3107.0, 2656.0, 2777.0, 2570.0, 2035.0, 1899.0, 2159.0, 1921.0, 1819.0, 1994.0, 1627.0, 1799.0]
layerwise density percentage: ['0.140', '0.239', '0.190', '0.236', '0.193', '0.379', '0.324', '0.339', '0.314', '0.497', '0.464', '0.527', '0.469', '0.888', '0.974', '0.794', '0.878']
Global density: 0.263188898563385
06/02 01:59:37 PM | Train: [63/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 01:59:37 PM | layerwise density: [9154.0, 3922.0, 3117.0, 3870.0, 3179.0, 3105.0, 2666.0, 2777.0, 2571.0, 2034.0, 1894.0, 2159.0, 1928.0, 1819.0, 1995.0, 1627.0, 1799.0]
layerwise density percentage: ['0.140', '0.239', '0.190', '0.236', '0.194', '0.379', '0.325', '0.339', '0.314', '0.497', '0.462', '0.527', '0.471', '0.888', '0.974', '0.794', '0.878']
Global density: 0.26333221793174744
06/02 01:59:47 PM | Train: [63/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 01:59:47 PM | layerwise density: [9149.0, 3922.0, 3123.0, 3864.0, 3176.0, 3099.0, 2668.0, 2775.0, 2575.0, 2037.0, 1892.0, 2161.0, 1927.0, 1821.0, 1994.0, 1630.0, 1799.0]
layerwise density percentage: ['0.140', '0.239', '0.191', '0.236', '0.194', '0.378', '0.326', '0.339', '0.314', '0.497', '0.462', '0.528', '0.470', '0.889', '0.974', '0.796', '0.878']
Global density: 0.263310968875885
06/02 01:59:47 PM | Train: [63/200] Final Prec@1 99.9600%
06/02 01:59:47 PM | Valid: [63/200] Step 000/078 Loss 1.080 Prec@(1,5) (76.6%, 91.4%)
06/02 01:59:50 PM | Valid: [63/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.6%, 90.1%)
06/02 01:59:50 PM | Valid: [63/200] Final Prec@1 70.5900%
06/02 01:59:50 PM | Current mask training best Prec@1 = 70.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9149.0, 0.1396026611328125]
['model.relu.alpha_mask_1_0', 16384, 3922.0, 0.2393798828125]
['model.relu.alpha_mask_2_0', 16384, 3123.0, 0.19061279296875]
['model.relu.alpha_mask_3_0', 16384, 3864.0, 0.23583984375]
['model.relu.alpha_mask_4_0', 16384, 3176.0, 0.19384765625]
['model.relu.alpha_mask_5_0', 8192, 3098.0, 0.378173828125]
['model.relu.alpha_mask_6_0', 8192, 2668.0, 0.32568359375]
['model.relu.alpha_mask_7_0', 8192, 2775.0, 0.3387451171875]
['model.relu.alpha_mask_8_0', 8192, 2575.0, 0.3143310546875]
['model.relu.alpha_mask_9_0', 4096, 2035.0, 0.496826171875]
['model.relu.alpha_mask_10_0', 4096, 1892.0, 0.4619140625]
['model.relu.alpha_mask_11_0', 4096, 2161.0, 0.527587890625]
['model.relu.alpha_mask_12_0', 4096, 1927.0, 0.470458984375]
['model.relu.alpha_mask_13_0', 2048, 1821.0, 0.88916015625]
['model.relu.alpha_mask_14_0', 2048, 1994.0, 0.9736328125]
['model.relu.alpha_mask_15_0', 2048, 1629.0, 0.79541015625]
['model.relu.alpha_mask_16_0', 2048, 1799.0, 0.87841796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49608.0, 0.2632897418478261]
########## End ###########
06/02 01:59:51 PM | Train: [64/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 01:59:51 PM | layerwise density: [9149.0, 3922.0, 3123.0, 3864.0, 3176.0, 3098.0, 2668.0, 2775.0, 2575.0, 2035.0, 1892.0, 2161.0, 1927.0, 1821.0, 1994.0, 1629.0, 1799.0]
layerwise density percentage: ['0.140', '0.239', '0.191', '0.236', '0.194', '0.378', '0.326', '0.339', '0.314', '0.497', '0.462', '0.528', '0.470', '0.889', '0.974', '0.795', '0.878']
Global density: 0.26328974962234497
06/02 02:00:01 PM | Train: [64/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:00:01 PM | layerwise density: [9142.0, 3920.0, 3121.0, 3863.0, 3180.0, 3103.0, 2667.0, 2777.0, 2573.0, 2038.0, 1891.0, 2161.0, 1922.0, 1822.0, 1997.0, 1629.0, 1799.0]
layerwise density percentage: ['0.139', '0.239', '0.190', '0.236', '0.194', '0.379', '0.326', '0.339', '0.314', '0.498', '0.462', '0.528', '0.469', '0.890', '0.975', '0.795', '0.878']
Global density: 0.26327383518218994
06/02 02:00:12 PM | Train: [64/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:00:12 PM | layerwise density: [9140.0, 3925.0, 3122.0, 3871.0, 3180.0, 3105.0, 2666.0, 2777.0, 2576.0, 2040.0, 1892.0, 2160.0, 1921.0, 1822.0, 1997.0, 1630.0, 1799.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.236', '0.194', '0.379', '0.325', '0.339', '0.314', '0.498', '0.462', '0.527', '0.469', '0.890', '0.975', '0.796', '0.878']
Global density: 0.2633693516254425
06/02 02:00:23 PM | Train: [64/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:00:23 PM | layerwise density: [9139.0, 3921.0, 3126.0, 3875.0, 3180.0, 3101.0, 2665.0, 2782.0, 2576.0, 2044.0, 1897.0, 2163.0, 1925.0, 1827.0, 2000.0, 1633.0, 1799.0]
layerwise density percentage: ['0.139', '0.239', '0.191', '0.237', '0.194', '0.379', '0.325', '0.340', '0.314', '0.499', '0.463', '0.528', '0.470', '0.892', '0.977', '0.797', '0.878']
Global density: 0.26352858543395996
06/02 02:00:33 PM | Train: [64/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:00:33 PM | layerwise density: [9139.0, 3925.0, 3127.0, 3880.0, 3176.0, 3105.0, 2672.0, 2785.0, 2575.0, 2048.0, 1893.0, 2163.0, 1924.0, 1826.0, 1998.0, 1631.0, 1800.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.237', '0.194', '0.379', '0.326', '0.340', '0.314', '0.500', '0.462', '0.528', '0.470', '0.892', '0.976', '0.796', '0.879']
Global density: 0.2636028826236725
06/02 02:00:33 PM | Train: [64/200] Final Prec@1 99.9520%
06/02 02:00:33 PM | Valid: [64/200] Step 000/078 Loss 1.042 Prec@(1,5) (76.6%, 92.2%)
06/02 02:00:35 PM | Valid: [64/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.7%, 90.3%)
06/02 02:00:35 PM | Valid: [64/200] Final Prec@1 70.7300%
06/02 02:00:36 PM | Current mask training best Prec@1 = 70.7300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9139.0, 0.1394500732421875]
['model.relu.alpha_mask_1_0', 16384, 3925.0, 0.23956298828125]
['model.relu.alpha_mask_2_0', 16384, 3128.0, 0.19091796875]
['model.relu.alpha_mask_3_0', 16384, 3880.0, 0.23681640625]
['model.relu.alpha_mask_4_0', 16384, 3176.0, 0.19384765625]
['model.relu.alpha_mask_5_0', 8192, 3105.0, 0.3790283203125]
['model.relu.alpha_mask_6_0', 8192, 2672.0, 0.326171875]
['model.relu.alpha_mask_7_0', 8192, 2785.0, 0.3399658203125]
['model.relu.alpha_mask_8_0', 8192, 2575.0, 0.3143310546875]
['model.relu.alpha_mask_9_0', 4096, 2048.0, 0.5]
['model.relu.alpha_mask_10_0', 4096, 1893.0, 0.462158203125]
['model.relu.alpha_mask_11_0', 4096, 2163.0, 0.528076171875]
['model.relu.alpha_mask_12_0', 4096, 1924.0, 0.4697265625]
['model.relu.alpha_mask_13_0', 2048, 1826.0, 0.8916015625]
['model.relu.alpha_mask_14_0', 2048, 1998.0, 0.9755859375]
['model.relu.alpha_mask_15_0', 2048, 1631.0, 0.79638671875]
['model.relu.alpha_mask_16_0', 2048, 1800.0, 0.87890625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49668.0, 0.2636081861413043]
########## End ###########
06/02 02:00:37 PM | Train: [65/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:00:37 PM | layerwise density: [9139.0, 3925.0, 3128.0, 3880.0, 3176.0, 3105.0, 2672.0, 2785.0, 2575.0, 2048.0, 1893.0, 2163.0, 1924.0, 1826.0, 1998.0, 1631.0, 1800.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.237', '0.194', '0.379', '0.326', '0.340', '0.314', '0.500', '0.462', '0.528', '0.470', '0.892', '0.976', '0.796', '0.879']
Global density: 0.2636081874370575
06/02 02:00:48 PM | Train: [65/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:00:48 PM | layerwise density: [9139.0, 3926.0, 3132.0, 3884.0, 3176.0, 3113.0, 2670.0, 2781.0, 2580.0, 2048.0, 1898.0, 2162.0, 1920.0, 1824.0, 2001.0, 1630.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.237', '0.194', '0.380', '0.326', '0.339', '0.315', '0.500', '0.463', '0.528', '0.469', '0.891', '0.977', '0.796', '0.879']
Global density: 0.26369842886924744
06/02 02:00:59 PM | Train: [65/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:00:59 PM | layerwise density: [9140.0, 3930.0, 3129.0, 3885.0, 3181.0, 3113.0, 2667.0, 2785.0, 2583.0, 2049.0, 1899.0, 2160.0, 1917.0, 1829.0, 1998.0, 1631.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.237', '0.194', '0.380', '0.326', '0.340', '0.315', '0.500', '0.464', '0.527', '0.468', '0.893', '0.976', '0.796', '0.879']
Global density: 0.26376211643218994
06/02 02:01:10 PM | Train: [65/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:01:10 PM | layerwise density: [9138.0, 3926.0, 3130.0, 3891.0, 3181.0, 3112.0, 2672.0, 2785.0, 2579.0, 2046.0, 1901.0, 2160.0, 1924.0, 1836.0, 1999.0, 1629.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.237', '0.194', '0.380', '0.326', '0.340', '0.315', '0.500', '0.464', '0.527', '0.470', '0.896', '0.976', '0.795', '0.879']
Global density: 0.26383110880851746
06/02 02:01:20 PM | Train: [65/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:01:20 PM | layerwise density: [9133.0, 3926.0, 3130.0, 3891.0, 3184.0, 3109.0, 2672.0, 2785.0, 2580.0, 2047.0, 1905.0, 2161.0, 1915.0, 1837.0, 1995.0, 1631.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.237', '0.194', '0.380', '0.326', '0.340', '0.315', '0.500', '0.465', '0.528', '0.468', '0.897', '0.974', '0.796', '0.879']
Global density: 0.263788640499115
06/02 02:01:20 PM | Train: [65/200] Final Prec@1 99.9620%
06/02 02:01:20 PM | Valid: [65/200] Step 000/078 Loss 1.069 Prec@(1,5) (78.1%, 92.2%)
06/02 02:01:22 PM | Valid: [65/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.5%, 90.3%)
06/02 02:01:22 PM | Valid: [65/200] Final Prec@1 70.5200%
06/02 02:01:22 PM | Current mask training best Prec@1 = 70.7300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9134.0, 0.139373779296875]
['model.relu.alpha_mask_1_0', 16384, 3926.0, 0.2396240234375]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3891.0, 0.23748779296875]
['model.relu.alpha_mask_4_0', 16384, 3184.0, 0.1943359375]
['model.relu.alpha_mask_5_0', 8192, 3109.0, 0.3795166015625]
['model.relu.alpha_mask_6_0', 8192, 2673.0, 0.3262939453125]
['model.relu.alpha_mask_7_0', 8192, 2785.0, 0.3399658203125]
['model.relu.alpha_mask_8_0', 8192, 2580.0, 0.31494140625]
['model.relu.alpha_mask_9_0', 4096, 2047.0, 0.499755859375]
['model.relu.alpha_mask_10_0', 4096, 1905.0, 0.465087890625]
['model.relu.alpha_mask_11_0', 4096, 2161.0, 0.527587890625]
['model.relu.alpha_mask_12_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_13_0', 2048, 1837.0, 0.89697265625]
['model.relu.alpha_mask_14_0', 2048, 1995.0, 0.97412109375]
['model.relu.alpha_mask_15_0', 2048, 1631.0, 0.79638671875]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49704.0, 0.2637992527173913]
########## End ###########
06/02 02:01:23 PM | Train: [66/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:01:23 PM | layerwise density: [9134.0, 3926.0, 3130.0, 3891.0, 3184.0, 3109.0, 2673.0, 2785.0, 2580.0, 2047.0, 1905.0, 2161.0, 1915.0, 1837.0, 1995.0, 1631.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.237', '0.194', '0.380', '0.326', '0.340', '0.315', '0.500', '0.465', '0.528', '0.468', '0.897', '0.974', '0.796', '0.879']
Global density: 0.263799250125885
06/02 02:01:34 PM | Train: [66/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:01:34 PM | layerwise density: [9132.0, 3927.0, 3129.0, 3889.0, 3185.0, 3109.0, 2671.0, 2788.0, 2580.0, 2045.0, 1906.0, 2162.0, 1916.0, 1841.0, 1993.0, 1632.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.237', '0.194', '0.380', '0.326', '0.340', '0.315', '0.499', '0.465', '0.528', '0.468', '0.899', '0.973', '0.797', '0.879']
Global density: 0.26380985975265503
06/02 02:01:45 PM | Train: [66/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:01:45 PM | layerwise density: [9138.0, 3931.0, 3125.0, 3892.0, 3179.0, 3106.0, 2675.0, 2786.0, 2577.0, 2050.0, 1902.0, 2164.0, 1917.0, 1841.0, 1997.0, 1636.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.238', '0.194', '0.379', '0.327', '0.340', '0.315', '0.500', '0.464', '0.528', '0.468', '0.899', '0.975', '0.799', '0.879']
Global density: 0.2638682425022125
06/02 02:01:56 PM | Train: [66/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:01:56 PM | layerwise density: [9141.0, 3931.0, 3129.0, 3892.0, 3175.0, 3109.0, 2673.0, 2788.0, 2582.0, 2052.0, 1903.0, 2164.0, 1916.0, 1839.0, 1999.0, 1628.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.238', '0.194', '0.380', '0.326', '0.340', '0.315', '0.501', '0.465', '0.528', '0.468', '0.898', '0.976', '0.795', '0.879']
Global density: 0.26389479637145996
06/02 02:02:06 PM | Train: [66/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:02:06 PM | layerwise density: [9140.0, 3936.0, 3131.0, 3896.0, 3179.0, 3114.0, 2675.0, 2788.0, 2585.0, 2052.0, 1907.0, 2164.0, 1918.0, 1840.0, 1997.0, 1629.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.238', '0.194', '0.380', '0.327', '0.340', '0.316', '0.501', '0.466', '0.528', '0.468', '0.898', '0.975', '0.795', '0.879']
Global density: 0.26405400037765503
06/02 02:02:06 PM | Train: [66/200] Final Prec@1 99.9640%
06/02 02:02:06 PM | Valid: [66/200] Step 000/078 Loss 1.128 Prec@(1,5) (77.3%, 92.2%)
06/02 02:02:09 PM | Valid: [66/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.7%, 90.2%)
06/02 02:02:09 PM | Valid: [66/200] Final Prec@1 70.7000%
06/02 02:02:09 PM | Current mask training best Prec@1 = 70.7300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9140.0, 0.13946533203125]
['model.relu.alpha_mask_1_0', 16384, 3936.0, 0.240234375]
['model.relu.alpha_mask_2_0', 16384, 3131.0, 0.19110107421875]
['model.relu.alpha_mask_3_0', 16384, 3896.0, 0.23779296875]
['model.relu.alpha_mask_4_0', 16384, 3179.0, 0.19403076171875]
['model.relu.alpha_mask_5_0', 8192, 3114.0, 0.380126953125]
['model.relu.alpha_mask_6_0', 8192, 2675.0, 0.3265380859375]
['model.relu.alpha_mask_7_0', 8192, 2788.0, 0.34033203125]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2052.0, 0.5009765625]
['model.relu.alpha_mask_10_0', 4096, 1907.0, 0.465576171875]
['model.relu.alpha_mask_11_0', 4096, 2164.0, 0.5283203125]
['model.relu.alpha_mask_12_0', 4096, 1918.0, 0.46826171875]
['model.relu.alpha_mask_13_0', 2048, 1840.0, 0.8984375]
['model.relu.alpha_mask_14_0', 2048, 1997.0, 0.97509765625]
['model.relu.alpha_mask_15_0', 2048, 1629.0, 0.79541015625]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49752.0, 0.2640540081521739]
########## End ###########
06/02 02:02:10 PM | Train: [67/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:02:10 PM | layerwise density: [9140.0, 3936.0, 3131.0, 3896.0, 3179.0, 3114.0, 2675.0, 2788.0, 2585.0, 2052.0, 1907.0, 2164.0, 1918.0, 1840.0, 1997.0, 1629.0, 1801.0]
layerwise density percentage: ['0.139', '0.240', '0.191', '0.238', '0.194', '0.380', '0.327', '0.340', '0.316', '0.501', '0.466', '0.528', '0.468', '0.898', '0.975', '0.795', '0.879']
Global density: 0.26405400037765503
06/02 02:02:21 PM | Train: [67/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:02:21 PM | layerwise density: [9141.0, 3943.0, 3135.0, 3902.0, 3178.0, 3115.0, 2673.0, 2788.0, 2586.0, 2054.0, 1912.0, 2164.0, 1923.0, 1843.0, 1994.0, 1629.0, 1801.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.238', '0.194', '0.380', '0.326', '0.340', '0.316', '0.501', '0.467', '0.528', '0.469', '0.900', '0.974', '0.795', '0.879']
Global density: 0.2642079293727875
06/02 02:02:32 PM | Train: [67/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:02:32 PM | layerwise density: [9142.0, 3945.0, 3138.0, 3904.0, 3180.0, 3121.0, 2675.0, 2788.0, 2583.0, 2054.0, 1911.0, 2165.0, 1919.0, 1841.0, 1997.0, 1626.0, 1801.0]
layerwise density percentage: ['0.139', '0.241', '0.192', '0.238', '0.194', '0.381', '0.327', '0.340', '0.315', '0.501', '0.467', '0.529', '0.469', '0.899', '0.975', '0.794', '0.879']
Global density: 0.26425570249557495
06/02 02:02:43 PM | Train: [67/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:02:43 PM | layerwise density: [9145.0, 3946.0, 3136.0, 3906.0, 3183.0, 3124.0, 2682.0, 2787.0, 2581.0, 2055.0, 1912.0, 2165.0, 1910.0, 1839.0, 1999.0, 1625.0, 1801.0]
layerwise density percentage: ['0.140', '0.241', '0.191', '0.238', '0.194', '0.381', '0.327', '0.340', '0.315', '0.502', '0.467', '0.529', '0.466', '0.898', '0.976', '0.793', '0.879']
Global density: 0.264287531375885
06/02 02:02:52 PM | Train: [67/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:02:52 PM | layerwise density: [9143.0, 3946.0, 3138.0, 3908.0, 3184.0, 3123.0, 2681.0, 2788.0, 2584.0, 2055.0, 1916.0, 2166.0, 1910.0, 1839.0, 1998.0, 1625.0, 1801.0]
layerwise density percentage: ['0.140', '0.241', '0.192', '0.239', '0.194', '0.381', '0.327', '0.340', '0.315', '0.502', '0.468', '0.529', '0.466', '0.898', '0.976', '0.793', '0.879']
Global density: 0.2643353044986725
06/02 02:02:53 PM | Train: [67/200] Final Prec@1 99.9660%
06/02 02:02:53 PM | Valid: [67/200] Step 000/078 Loss 1.040 Prec@(1,5) (77.3%, 92.2%)
06/02 02:02:55 PM | Valid: [67/200] Step 078/078 Loss 1.276 Prec@(1,5) (70.8%, 90.0%)
06/02 02:02:55 PM | Valid: [67/200] Final Prec@1 70.8000%
06/02 02:02:56 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9143.0, 0.1395111083984375]
['model.relu.alpha_mask_1_0', 16384, 3946.0, 0.2408447265625]
['model.relu.alpha_mask_2_0', 16384, 3138.0, 0.1915283203125]
['model.relu.alpha_mask_3_0', 16384, 3908.0, 0.238525390625]
['model.relu.alpha_mask_4_0', 16384, 3184.0, 0.1943359375]
['model.relu.alpha_mask_5_0', 8192, 3123.0, 0.3812255859375]
['model.relu.alpha_mask_6_0', 8192, 2681.0, 0.3272705078125]
['model.relu.alpha_mask_7_0', 8192, 2788.0, 0.34033203125]
['model.relu.alpha_mask_8_0', 8192, 2584.0, 0.3154296875]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1916.0, 0.4677734375]
['model.relu.alpha_mask_11_0', 4096, 2166.0, 0.52880859375]
['model.relu.alpha_mask_12_0', 4096, 1910.0, 0.46630859375]
['model.relu.alpha_mask_13_0', 2048, 1839.0, 0.89794921875]
['model.relu.alpha_mask_14_0', 2048, 1998.0, 0.9755859375]
['model.relu.alpha_mask_15_0', 2048, 1625.0, 0.79345703125]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:02:57 PM | Train: [68/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:02:57 PM | layerwise density: [9143.0, 3946.0, 3138.0, 3908.0, 3184.0, 3123.0, 2681.0, 2788.0, 2584.0, 2055.0, 1916.0, 2166.0, 1910.0, 1839.0, 1998.0, 1625.0, 1801.0]
layerwise density percentage: ['0.140', '0.241', '0.192', '0.239', '0.194', '0.381', '0.327', '0.340', '0.315', '0.502', '0.468', '0.529', '0.466', '0.898', '0.976', '0.793', '0.879']
Global density: 0.2643353044986725
06/02 02:03:08 PM | Train: [68/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:03:08 PM | layerwise density: [9141.0, 3950.0, 3137.0, 3912.0, 3185.0, 3131.0, 2681.0, 2785.0, 2587.0, 2060.0, 1917.0, 2167.0, 1908.0, 1837.0, 1997.0, 1626.0, 1801.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.194', '0.382', '0.327', '0.340', '0.316', '0.503', '0.468', '0.529', '0.466', '0.897', '0.975', '0.794', '0.879']
Global density: 0.2644255459308624
06/02 02:03:19 PM | Train: [68/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:03:19 PM | layerwise density: [9136.0, 3954.0, 3137.0, 3914.0, 3189.0, 3136.0, 2681.0, 2784.0, 2590.0, 2066.0, 1915.0, 2168.0, 1907.0, 1837.0, 1996.0, 1628.0, 1801.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.383', '0.327', '0.340', '0.316', '0.504', '0.468', '0.529', '0.466', '0.897', '0.975', '0.795', '0.879']
Global density: 0.26451575756073
06/02 02:03:29 PM | Train: [68/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:03:29 PM | layerwise density: [9138.0, 3959.0, 3139.0, 3917.0, 3190.0, 3135.0, 2682.0, 2783.0, 2594.0, 2066.0, 1914.0, 2168.0, 1909.0, 1838.0, 1997.0, 1626.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.195', '0.383', '0.327', '0.340', '0.317', '0.504', '0.467', '0.529', '0.466', '0.897', '0.975', '0.794', '0.879']
Global density: 0.26460596919059753
06/02 02:03:39 PM | Train: [68/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:03:39 PM | layerwise density: [9141.0, 3958.0, 3139.0, 3917.0, 3189.0, 3132.0, 2680.0, 2782.0, 2587.0, 2061.0, 1913.0, 2168.0, 1906.0, 1837.0, 1993.0, 1628.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.195', '0.382', '0.327', '0.340', '0.316', '0.503', '0.467', '0.529', '0.465', '0.897', '0.973', '0.795', '0.879']
Global density: 0.2644785940647125
06/02 02:03:39 PM | Train: [68/200] Final Prec@1 99.9520%
06/02 02:03:39 PM | Valid: [68/200] Step 000/078 Loss 1.090 Prec@(1,5) (75.8%, 93.0%)
06/02 02:03:42 PM | Valid: [68/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.8%, 90.0%)
06/02 02:03:42 PM | Valid: [68/200] Final Prec@1 70.7800%
06/02 02:03:42 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9141.0, 0.1394805908203125]
['model.relu.alpha_mask_1_0', 16384, 3958.0, 0.2415771484375]
['model.relu.alpha_mask_2_0', 16384, 3139.0, 0.19158935546875]
['model.relu.alpha_mask_3_0', 16384, 3917.0, 0.23907470703125]
['model.relu.alpha_mask_4_0', 16384, 3189.0, 0.19464111328125]
['model.relu.alpha_mask_5_0', 8192, 3132.0, 0.38232421875]
['model.relu.alpha_mask_6_0', 8192, 2680.0, 0.3271484375]
['model.relu.alpha_mask_7_0', 8192, 2782.0, 0.339599609375]
['model.relu.alpha_mask_8_0', 8192, 2587.0, 0.3157958984375]
['model.relu.alpha_mask_9_0', 4096, 2061.0, 0.503173828125]
['model.relu.alpha_mask_10_0', 4096, 1913.0, 0.467041015625]
['model.relu.alpha_mask_11_0', 4096, 2168.0, 0.529296875]
['model.relu.alpha_mask_12_0', 4096, 1906.0, 0.46533203125]
['model.relu.alpha_mask_13_0', 2048, 1837.0, 0.89697265625]
['model.relu.alpha_mask_14_0', 2048, 1993.0, 0.97314453125]
['model.relu.alpha_mask_15_0', 2048, 1628.0, 0.794921875]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49832.0, 0.26447860054347827]
########## End ###########
06/02 02:03:43 PM | Train: [69/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:03:43 PM | layerwise density: [9141.0, 3958.0, 3139.0, 3917.0, 3189.0, 3132.0, 2680.0, 2782.0, 2587.0, 2061.0, 1913.0, 2168.0, 1906.0, 1837.0, 1993.0, 1628.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.195', '0.382', '0.327', '0.340', '0.316', '0.503', '0.467', '0.529', '0.465', '0.897', '0.973', '0.795', '0.879']
Global density: 0.2644785940647125
06/02 02:03:54 PM | Train: [69/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:03:54 PM | layerwise density: [9141.0, 3959.0, 3138.0, 3916.0, 3190.0, 3134.0, 2679.0, 2783.0, 2586.0, 2062.0, 1915.0, 2168.0, 1913.0, 1839.0, 1998.0, 1633.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.195', '0.383', '0.327', '0.340', '0.316', '0.503', '0.468', '0.529', '0.467', '0.898', '0.976', '0.797', '0.879']
Global density: 0.2646006643772125
06/02 02:04:05 PM | Train: [69/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 02:04:05 PM | layerwise density: [9141.0, 3963.0, 3136.0, 3915.0, 3193.0, 3134.0, 2678.0, 2783.0, 2584.0, 2063.0, 1916.0, 2168.0, 1911.0, 1838.0, 1997.0, 1634.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.383', '0.327', '0.340', '0.315', '0.504', '0.468', '0.529', '0.467', '0.897', '0.975', '0.798', '0.879']
Global density: 0.2646006643772125
06/02 02:04:15 PM | Train: [69/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:04:15 PM | layerwise density: [9145.0, 3966.0, 3138.0, 3915.0, 3195.0, 3137.0, 2676.0, 2784.0, 2587.0, 2070.0, 1920.0, 2168.0, 1912.0, 1838.0, 1997.0, 1633.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.192', '0.239', '0.195', '0.383', '0.327', '0.340', '0.316', '0.505', '0.469', '0.529', '0.467', '0.897', '0.975', '0.797', '0.879']
Global density: 0.26474398374557495
06/02 02:04:25 PM | Train: [69/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:04:25 PM | layerwise density: [9145.0, 3966.0, 3134.0, 3916.0, 3198.0, 3140.0, 2672.0, 2785.0, 2592.0, 2072.0, 1917.0, 2169.0, 1913.0, 1833.0, 1995.0, 1633.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.191', '0.239', '0.195', '0.383', '0.326', '0.340', '0.316', '0.506', '0.468', '0.530', '0.467', '0.895', '0.974', '0.797', '0.879']
Global density: 0.26473867893218994
06/02 02:04:25 PM | Train: [69/200] Final Prec@1 99.9600%
06/02 02:04:26 PM | Valid: [69/200] Step 000/078 Loss 1.064 Prec@(1,5) (75.8%, 92.2%)
06/02 02:04:28 PM | Valid: [69/200] Step 078/078 Loss 1.270 Prec@(1,5) (70.6%, 90.1%)
06/02 02:04:28 PM | Valid: [69/200] Final Prec@1 70.6000%
06/02 02:04:28 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9145.0, 0.1395416259765625]
['model.relu.alpha_mask_1_0', 16384, 3966.0, 0.2420654296875]
['model.relu.alpha_mask_2_0', 16384, 3134.0, 0.1912841796875]
['model.relu.alpha_mask_3_0', 16384, 3916.0, 0.239013671875]
['model.relu.alpha_mask_4_0', 16384, 3198.0, 0.1951904296875]
['model.relu.alpha_mask_5_0', 8192, 3140.0, 0.38330078125]
['model.relu.alpha_mask_6_0', 8192, 2672.0, 0.326171875]
['model.relu.alpha_mask_7_0', 8192, 2785.0, 0.3399658203125]
['model.relu.alpha_mask_8_0', 8192, 2592.0, 0.31640625]
['model.relu.alpha_mask_9_0', 4096, 2072.0, 0.505859375]
['model.relu.alpha_mask_10_0', 4096, 1918.0, 0.46826171875]
['model.relu.alpha_mask_11_0', 4096, 2169.0, 0.529541015625]
['model.relu.alpha_mask_12_0', 4096, 1913.0, 0.467041015625]
['model.relu.alpha_mask_13_0', 2048, 1833.0, 0.89501953125]
['model.relu.alpha_mask_14_0', 2048, 1994.0, 0.9736328125]
['model.relu.alpha_mask_15_0', 2048, 1633.0, 0.79736328125]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49881.0, 0.26473866338315216]
########## End ###########
06/02 02:04:29 PM | Train: [70/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:04:29 PM | layerwise density: [9145.0, 3966.0, 3134.0, 3916.0, 3198.0, 3140.0, 2672.0, 2785.0, 2592.0, 2072.0, 1918.0, 2169.0, 1913.0, 1833.0, 1994.0, 1633.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.191', '0.239', '0.195', '0.383', '0.326', '0.340', '0.316', '0.506', '0.468', '0.530', '0.467', '0.895', '0.974', '0.797', '0.879']
Global density: 0.26473867893218994
06/02 02:04:40 PM | Train: [70/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:04:40 PM | layerwise density: [9144.0, 3965.0, 3135.0, 3921.0, 3199.0, 3143.0, 2670.0, 2786.0, 2596.0, 2074.0, 1913.0, 2169.0, 1913.0, 1831.0, 1993.0, 1630.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.191', '0.239', '0.195', '0.384', '0.326', '0.340', '0.317', '0.506', '0.467', '0.530', '0.467', '0.894', '0.973', '0.796', '0.879']
Global density: 0.26474928855895996
06/02 02:04:51 PM | Train: [70/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:04:51 PM | layerwise density: [9144.0, 3965.0, 3134.0, 3923.0, 3198.0, 3147.0, 2672.0, 2786.0, 2595.0, 2072.0, 1914.0, 2169.0, 1912.0, 1831.0, 1997.0, 1630.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.191', '0.239', '0.195', '0.384', '0.326', '0.340', '0.317', '0.506', '0.467', '0.530', '0.467', '0.894', '0.975', '0.796', '0.879']
Global density: 0.26478642225265503
06/02 02:05:02 PM | Train: [70/80] Step 300/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 02:05:02 PM | layerwise density: [9144.0, 3964.0, 3133.0, 3925.0, 3199.0, 3149.0, 2670.0, 2786.0, 2594.0, 2073.0, 1917.0, 2170.0, 1911.0, 1833.0, 2001.0, 1635.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.191', '0.240', '0.195', '0.384', '0.326', '0.340', '0.317', '0.506', '0.468', '0.530', '0.467', '0.895', '0.977', '0.798', '0.879']
Global density: 0.26486605405807495
06/02 02:05:12 PM | Train: [70/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 02:05:12 PM | layerwise density: [9144.0, 3967.0, 3136.0, 3925.0, 3201.0, 3150.0, 2667.0, 2785.0, 2595.0, 2075.0, 1918.0, 2170.0, 1913.0, 1832.0, 2001.0, 1635.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.191', '0.240', '0.195', '0.385', '0.326', '0.340', '0.317', '0.507', '0.468', '0.530', '0.467', '0.895', '0.977', '0.798', '0.879']
Global density: 0.26491913199424744
06/02 02:05:12 PM | Train: [70/200] Final Prec@1 99.9440%
06/02 02:05:12 PM | Valid: [70/200] Step 000/078 Loss 1.082 Prec@(1,5) (76.6%, 92.2%)
06/02 02:05:14 PM | Valid: [70/200] Step 078/078 Loss 1.273 Prec@(1,5) (70.8%, 90.1%)
06/02 02:05:14 PM | Valid: [70/200] Final Prec@1 70.7600%
06/02 02:05:14 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9144.0, 0.1395263671875]
['model.relu.alpha_mask_1_0', 16384, 3967.0, 0.24212646484375]
['model.relu.alpha_mask_2_0', 16384, 3136.0, 0.19140625]
['model.relu.alpha_mask_3_0', 16384, 3925.0, 0.23956298828125]
['model.relu.alpha_mask_4_0', 16384, 3201.0, 0.19537353515625]
['model.relu.alpha_mask_5_0', 8192, 3150.0, 0.384521484375]
['model.relu.alpha_mask_6_0', 8192, 2666.0, 0.325439453125]
['model.relu.alpha_mask_7_0', 8192, 2785.0, 0.3399658203125]
['model.relu.alpha_mask_8_0', 8192, 2595.0, 0.3167724609375]
['model.relu.alpha_mask_9_0', 4096, 2075.0, 0.506591796875]
['model.relu.alpha_mask_10_0', 4096, 1918.0, 0.46826171875]
['model.relu.alpha_mask_11_0', 4096, 2170.0, 0.52978515625]
['model.relu.alpha_mask_12_0', 4096, 1913.0, 0.467041015625]
['model.relu.alpha_mask_13_0', 2048, 1832.0, 0.89453125]
['model.relu.alpha_mask_14_0', 2048, 2001.0, 0.97705078125]
['model.relu.alpha_mask_15_0', 2048, 1635.0, 0.79833984375]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49914.0, 0.26491380774456524]
########## End ###########
06/02 02:05:15 PM | Train: [71/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:05:15 PM | layerwise density: [9144.0, 3967.0, 3136.0, 3925.0, 3201.0, 3150.0, 2666.0, 2785.0, 2595.0, 2075.0, 1918.0, 2170.0, 1913.0, 1832.0, 2001.0, 1635.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.191', '0.240', '0.195', '0.385', '0.325', '0.340', '0.317', '0.507', '0.468', '0.530', '0.467', '0.895', '0.977', '0.798', '0.879']
Global density: 0.2649138271808624
06/02 02:05:26 PM | Train: [71/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:05:26 PM | layerwise density: [9145.0, 3966.0, 3139.0, 3924.0, 3201.0, 3150.0, 2663.0, 2785.0, 2597.0, 2073.0, 1920.0, 2170.0, 1913.0, 1830.0, 1999.0, 1634.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.192', '0.240', '0.195', '0.385', '0.325', '0.340', '0.317', '0.506', '0.469', '0.530', '0.467', '0.894', '0.976', '0.798', '0.879']
Global density: 0.264892578125
06/02 02:05:37 PM | Train: [71/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:05:37 PM | layerwise density: [9143.0, 3965.0, 3138.0, 3925.0, 3200.0, 3150.0, 2666.0, 2786.0, 2600.0, 2074.0, 1920.0, 2170.0, 1916.0, 1830.0, 1994.0, 1634.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.192', '0.240', '0.195', '0.385', '0.325', '0.340', '0.317', '0.506', '0.469', '0.530', '0.468', '0.894', '0.974', '0.798', '0.879']
Global density: 0.26490318775177
06/02 02:05:48 PM | Train: [71/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:05:48 PM | layerwise density: [9144.0, 3967.0, 3138.0, 3926.0, 3200.0, 3151.0, 2666.0, 2785.0, 2606.0, 2073.0, 1922.0, 2170.0, 1918.0, 1831.0, 1993.0, 1636.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.192', '0.240', '0.195', '0.385', '0.325', '0.340', '0.318', '0.506', '0.469', '0.530', '0.468', '0.894', '0.973', '0.799', '0.879']
Global density: 0.26498281955718994
06/02 02:05:58 PM | Train: [71/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:05:58 PM | layerwise density: [9143.0, 3963.0, 3139.0, 3927.0, 3201.0, 3153.0, 2665.0, 2783.0, 2605.0, 2075.0, 1924.0, 2169.0, 1918.0, 1831.0, 1996.0, 1636.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.192', '0.240', '0.195', '0.385', '0.325', '0.340', '0.318', '0.507', '0.470', '0.530', '0.468', '0.894', '0.975', '0.799', '0.879']
Global density: 0.26499342918395996
06/02 02:05:58 PM | Train: [71/200] Final Prec@1 99.9620%
06/02 02:05:58 PM | Valid: [71/200] Step 000/078 Loss 1.058 Prec@(1,5) (77.3%, 92.2%)
06/02 02:06:00 PM | Valid: [71/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.4%, 90.0%)
06/02 02:06:00 PM | Valid: [71/200] Final Prec@1 70.4000%
06/02 02:06:00 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9143.0, 0.1395111083984375]
['model.relu.alpha_mask_1_0', 16384, 3963.0, 0.24188232421875]
['model.relu.alpha_mask_2_0', 16384, 3139.0, 0.19158935546875]
['model.relu.alpha_mask_3_0', 16384, 3927.0, 0.23968505859375]
['model.relu.alpha_mask_4_0', 16384, 3201.0, 0.19537353515625]
['model.relu.alpha_mask_5_0', 8192, 3153.0, 0.3848876953125]
['model.relu.alpha_mask_6_0', 8192, 2666.0, 0.325439453125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2605.0, 0.3179931640625]
['model.relu.alpha_mask_9_0', 4096, 2075.0, 0.506591796875]
['model.relu.alpha_mask_10_0', 4096, 1924.0, 0.4697265625]
['model.relu.alpha_mask_11_0', 4096, 2169.0, 0.529541015625]
['model.relu.alpha_mask_12_0', 4096, 1918.0, 0.46826171875]
['model.relu.alpha_mask_13_0', 2048, 1831.0, 0.89404296875]
['model.relu.alpha_mask_14_0', 2048, 1996.0, 0.974609375]
['model.relu.alpha_mask_15_0', 2048, 1636.0, 0.798828125]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49930.0, 0.2649987262228261]
########## End ###########
06/02 02:06:01 PM | Train: [72/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:06:01 PM | layerwise density: [9143.0, 3963.0, 3139.0, 3927.0, 3201.0, 3153.0, 2666.0, 2783.0, 2605.0, 2075.0, 1924.0, 2169.0, 1918.0, 1831.0, 1996.0, 1636.0, 1801.0]
layerwise density percentage: ['0.140', '0.242', '0.192', '0.240', '0.195', '0.385', '0.325', '0.340', '0.318', '0.507', '0.470', '0.530', '0.468', '0.894', '0.975', '0.799', '0.879']
Global density: 0.26499873399734497
06/02 02:06:12 PM | Train: [72/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:06:12 PM | layerwise density: [9142.0, 3962.0, 3137.0, 3926.0, 3202.0, 3154.0, 2663.0, 2783.0, 2606.0, 2075.0, 1925.0, 2169.0, 1919.0, 1831.0, 1996.0, 1633.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.240', '0.195', '0.385', '0.325', '0.340', '0.318', '0.507', '0.470', '0.530', '0.469', '0.894', '0.975', '0.797', '0.879']
Global density: 0.2649668753147125
06/02 02:06:23 PM | Train: [72/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:06:23 PM | layerwise density: [9141.0, 3962.0, 3138.0, 3924.0, 3203.0, 3157.0, 2665.0, 2783.0, 2601.0, 2076.0, 1926.0, 2169.0, 1918.0, 1832.0, 1995.0, 1634.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.240', '0.195', '0.385', '0.325', '0.340', '0.318', '0.507', '0.470', '0.530', '0.468', '0.895', '0.974', '0.798', '0.879']
Global density: 0.26497218012809753
06/02 02:06:34 PM | Train: [72/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:06:34 PM | layerwise density: [9140.0, 3965.0, 3139.0, 3923.0, 3203.0, 3155.0, 2666.0, 2783.0, 2599.0, 2076.0, 1927.0, 2169.0, 1923.0, 1833.0, 1996.0, 1635.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.195', '0.385', '0.325', '0.340', '0.317', '0.507', '0.470', '0.530', '0.469', '0.895', '0.975', '0.798', '0.879']
Global density: 0.2650146484375
06/02 02:06:44 PM | Train: [72/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:06:44 PM | layerwise density: [9138.0, 3962.0, 3141.0, 3923.0, 3205.0, 3159.0, 2667.0, 2782.0, 2604.0, 2071.0, 1926.0, 2170.0, 1923.0, 1834.0, 1996.0, 1634.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.196', '0.386', '0.326', '0.340', '0.318', '0.506', '0.470', '0.530', '0.469', '0.896', '0.975', '0.798', '0.879']
Global density: 0.26503056287765503
06/02 02:06:44 PM | Train: [72/200] Final Prec@1 99.9680%
06/02 02:06:44 PM | Valid: [72/200] Step 000/078 Loss 1.068 Prec@(1,5) (75.0%, 91.4%)
06/02 02:06:46 PM | Valid: [72/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.5%, 90.1%)
06/02 02:06:46 PM | Valid: [72/200] Final Prec@1 70.5100%
06/02 02:06:46 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9138.0, 0.139434814453125]
['model.relu.alpha_mask_1_0', 16384, 3962.0, 0.2418212890625]
['model.relu.alpha_mask_2_0', 16384, 3141.0, 0.19171142578125]
['model.relu.alpha_mask_3_0', 16384, 3923.0, 0.23944091796875]
['model.relu.alpha_mask_4_0', 16384, 3205.0, 0.19561767578125]
['model.relu.alpha_mask_5_0', 8192, 3159.0, 0.3856201171875]
['model.relu.alpha_mask_6_0', 8192, 2667.0, 0.3255615234375]
['model.relu.alpha_mask_7_0', 8192, 2782.0, 0.339599609375]
['model.relu.alpha_mask_8_0', 8192, 2604.0, 0.31787109375]
['model.relu.alpha_mask_9_0', 4096, 2071.0, 0.505615234375]
['model.relu.alpha_mask_10_0', 4096, 1926.0, 0.47021484375]
['model.relu.alpha_mask_11_0', 4096, 2170.0, 0.52978515625]
['model.relu.alpha_mask_12_0', 4096, 1923.0, 0.469482421875]
['model.relu.alpha_mask_13_0', 2048, 1834.0, 0.8955078125]
['model.relu.alpha_mask_14_0', 2048, 1996.0, 0.974609375]
['model.relu.alpha_mask_15_0', 2048, 1634.0, 0.7978515625]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49936.0, 0.2650305706521739]
########## End ###########
06/02 02:06:47 PM | Train: [73/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:06:47 PM | layerwise density: [9138.0, 3962.0, 3141.0, 3923.0, 3205.0, 3159.0, 2667.0, 2782.0, 2604.0, 2071.0, 1926.0, 2170.0, 1923.0, 1834.0, 1996.0, 1634.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.196', '0.386', '0.326', '0.340', '0.318', '0.506', '0.470', '0.530', '0.469', '0.896', '0.975', '0.798', '0.879']
Global density: 0.26503056287765503
06/02 02:06:58 PM | Train: [73/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:06:58 PM | layerwise density: [9140.0, 3965.0, 3141.0, 3923.0, 3206.0, 3159.0, 2665.0, 2782.0, 2607.0, 2070.0, 1925.0, 2171.0, 1922.0, 1835.0, 1996.0, 1629.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.196', '0.386', '0.325', '0.340', '0.318', '0.505', '0.470', '0.530', '0.469', '0.896', '0.975', '0.795', '0.879']
Global density: 0.2650358974933624
06/02 02:07:09 PM | Train: [73/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:07:09 PM | layerwise density: [9140.0, 3965.0, 3142.0, 3923.0, 3204.0, 3161.0, 2667.0, 2782.0, 2611.0, 2067.0, 1928.0, 2172.0, 1921.0, 1834.0, 1996.0, 1630.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.196', '0.386', '0.326', '0.340', '0.319', '0.505', '0.471', '0.530', '0.469', '0.896', '0.975', '0.796', '0.879']
Global density: 0.2650730311870575
06/02 02:07:20 PM | Train: [73/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:07:20 PM | layerwise density: [9137.0, 3964.0, 3140.0, 3924.0, 3205.0, 3164.0, 2667.0, 2782.0, 2609.0, 2069.0, 1928.0, 2171.0, 1925.0, 1831.0, 2000.0, 1630.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.240', '0.196', '0.386', '0.326', '0.340', '0.318', '0.505', '0.471', '0.530', '0.470', '0.894', '0.977', '0.796', '0.879']
Global density: 0.2650889456272125
06/02 02:07:30 PM | Train: [73/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:07:30 PM | layerwise density: [9139.0, 3968.0, 3141.0, 3921.0, 3205.0, 3166.0, 2666.0, 2785.0, 2609.0, 2069.0, 1928.0, 2171.0, 1928.0, 1832.0, 1999.0, 1629.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.196', '0.386', '0.325', '0.340', '0.318', '0.505', '0.471', '0.530', '0.471', '0.895', '0.976', '0.795', '0.879']
Global density: 0.265142023563385
06/02 02:07:30 PM | Train: [73/200] Final Prec@1 99.9620%
06/02 02:07:30 PM | Valid: [73/200] Step 000/078 Loss 1.073 Prec@(1,5) (75.8%, 91.4%)
06/02 02:07:32 PM | Valid: [73/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.7%, 90.2%)
06/02 02:07:32 PM | Valid: [73/200] Final Prec@1 70.7400%
06/02 02:07:32 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9139.0, 0.1394500732421875]
['model.relu.alpha_mask_1_0', 16384, 3968.0, 0.2421875]
['model.relu.alpha_mask_2_0', 16384, 3141.0, 0.19171142578125]
['model.relu.alpha_mask_3_0', 16384, 3921.0, 0.23931884765625]
['model.relu.alpha_mask_4_0', 16384, 3205.0, 0.19561767578125]
['model.relu.alpha_mask_5_0', 8192, 3166.0, 0.386474609375]
['model.relu.alpha_mask_6_0', 8192, 2666.0, 0.325439453125]
['model.relu.alpha_mask_7_0', 8192, 2785.0, 0.3399658203125]
['model.relu.alpha_mask_8_0', 8192, 2609.0, 0.3184814453125]
['model.relu.alpha_mask_9_0', 4096, 2069.0, 0.505126953125]
['model.relu.alpha_mask_10_0', 4096, 1928.0, 0.470703125]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1928.0, 0.470703125]
['model.relu.alpha_mask_13_0', 2048, 1832.0, 0.89453125]
['model.relu.alpha_mask_14_0', 2048, 1999.0, 0.97607421875]
['model.relu.alpha_mask_15_0', 2048, 1629.0, 0.79541015625]
['model.relu.alpha_mask_16_0', 2048, 1801.0, 0.87939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49957.0, 0.2651420261548913]
########## End ###########
06/02 02:07:33 PM | Train: [74/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:07:33 PM | layerwise density: [9139.0, 3968.0, 3141.0, 3921.0, 3205.0, 3166.0, 2666.0, 2785.0, 2609.0, 2069.0, 1928.0, 2171.0, 1928.0, 1832.0, 1999.0, 1629.0, 1801.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.196', '0.386', '0.325', '0.340', '0.318', '0.505', '0.471', '0.530', '0.471', '0.895', '0.976', '0.795', '0.879']
Global density: 0.265142023563385
06/02 02:07:44 PM | Train: [74/80] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 02:07:44 PM | layerwise density: [9139.0, 3972.0, 3141.0, 3921.0, 3207.0, 3173.0, 2667.0, 2787.0, 2609.0, 2072.0, 1928.0, 2173.0, 1929.0, 1832.0, 2001.0, 1632.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.192', '0.239', '0.196', '0.387', '0.326', '0.340', '0.318', '0.506', '0.471', '0.531', '0.471', '0.895', '0.977', '0.797', '0.880']
Global density: 0.26529064774513245
06/02 02:07:55 PM | Train: [74/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:07:55 PM | layerwise density: [9139.0, 3974.0, 3142.0, 3921.0, 3208.0, 3170.0, 2669.0, 2787.0, 2611.0, 2072.0, 1928.0, 2174.0, 1931.0, 1832.0, 2002.0, 1629.0, 1802.0]
layerwise density percentage: ['0.139', '0.243', '0.192', '0.239', '0.196', '0.387', '0.326', '0.340', '0.319', '0.506', '0.471', '0.531', '0.471', '0.895', '0.978', '0.795', '0.880']
Global density: 0.2653224766254425
06/02 02:08:05 PM | Train: [74/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:08:05 PM | layerwise density: [9127.0, 3966.0, 3136.0, 3917.0, 3200.0, 3173.0, 2662.0, 2785.0, 2599.0, 2065.0, 1925.0, 2172.0, 1928.0, 1829.0, 1999.0, 1623.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.325', '0.340', '0.317', '0.504', '0.470', '0.530', '0.471', '0.893', '0.976', '0.792', '0.880']
Global density: 0.26488196849823
06/02 02:08:15 PM | Train: [74/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:08:15 PM | layerwise density: [9123.0, 3964.0, 3133.0, 3917.0, 3196.0, 3171.0, 2656.0, 2785.0, 2596.0, 2062.0, 1922.0, 2172.0, 1926.0, 1827.0, 1998.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.317', '0.503', '0.469', '0.530', '0.470', '0.892', '0.976', '0.792', '0.880']
Global density: 0.26468560099601746
06/02 02:08:15 PM | Train: [74/200] Final Prec@1 99.9580%
06/02 02:08:16 PM | Valid: [74/200] Step 000/078 Loss 1.101 Prec@(1,5) (76.6%, 92.2%)
06/02 02:08:18 PM | Valid: [74/200] Step 078/078 Loss 1.272 Prec@(1,5) (70.7%, 90.0%)
06/02 02:08:18 PM | Valid: [74/200] Final Prec@1 70.6800%
06/02 02:08:18 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9123.0, 0.1392059326171875]
['model.relu.alpha_mask_1_0', 16384, 3964.0, 0.241943359375]
['model.relu.alpha_mask_2_0', 16384, 3133.0, 0.19122314453125]
['model.relu.alpha_mask_3_0', 16384, 3917.0, 0.23907470703125]
['model.relu.alpha_mask_4_0', 16384, 3196.0, 0.195068359375]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2656.0, 0.32421875]
['model.relu.alpha_mask_7_0', 8192, 2785.0, 0.3399658203125]
['model.relu.alpha_mask_8_0', 8192, 2596.0, 0.31689453125]
['model.relu.alpha_mask_9_0', 4096, 2062.0, 0.50341796875]
['model.relu.alpha_mask_10_0', 4096, 1922.0, 0.46923828125]
['model.relu.alpha_mask_11_0', 4096, 2172.0, 0.5302734375]
['model.relu.alpha_mask_12_0', 4096, 1926.0, 0.47021484375]
['model.relu.alpha_mask_13_0', 2048, 1827.0, 0.89208984375]
['model.relu.alpha_mask_14_0', 2048, 1998.0, 0.9755859375]
['model.relu.alpha_mask_15_0', 2048, 1621.0, 0.79150390625]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49871.0, 0.26468558933423914]
########## End ###########
06/02 02:08:19 PM | Train: [75/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:08:19 PM | layerwise density: [9123.0, 3964.0, 3133.0, 3917.0, 3196.0, 3171.0, 2656.0, 2785.0, 2596.0, 2062.0, 1922.0, 2172.0, 1926.0, 1827.0, 1998.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.317', '0.503', '0.469', '0.530', '0.470', '0.892', '0.976', '0.792', '0.880']
Global density: 0.26468560099601746
06/02 02:08:30 PM | Train: [75/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:08:30 PM | layerwise density: [9123.0, 3963.0, 3133.0, 3917.0, 3194.0, 3171.0, 2656.0, 2785.0, 2595.0, 2061.0, 1922.0, 2172.0, 1925.0, 1827.0, 1996.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.317', '0.503', '0.469', '0.530', '0.470', '0.892', '0.975', '0.792', '0.880']
Global density: 0.264643132686615
06/02 02:08:41 PM | Train: [75/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:08:41 PM | layerwise density: [9123.0, 3962.0, 3132.0, 3917.0, 3194.0, 3170.0, 2654.0, 2785.0, 2595.0, 2061.0, 1921.0, 2172.0, 1926.0, 1827.0, 1998.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.317', '0.503', '0.469', '0.530', '0.470', '0.892', '0.976', '0.792', '0.880']
Global density: 0.26462721824645996
06/02 02:08:51 PM | Train: [75/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:08:51 PM | layerwise density: [9123.0, 3962.0, 3132.0, 3917.0, 3194.0, 3169.0, 2653.0, 2785.0, 2590.0, 2062.0, 1920.0, 2172.0, 1924.0, 1827.0, 1999.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.469', '0.530', '0.470', '0.892', '0.976', '0.791', '0.880']
Global density: 0.2645741403102875
06/02 02:09:01 PM | Train: [75/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:09:01 PM | layerwise density: [9123.0, 3961.0, 3132.0, 3915.0, 3193.0, 3169.0, 2653.0, 2785.0, 2588.0, 2061.0, 1919.0, 2172.0, 1921.0, 1828.0, 1999.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.469', '0.530', '0.469', '0.893', '0.976', '0.791', '0.880']
Global density: 0.264521062374115
06/02 02:09:01 PM | Train: [75/200] Final Prec@1 99.9700%
06/02 02:09:01 PM | Valid: [75/200] Step 000/078 Loss 1.097 Prec@(1,5) (75.0%, 90.6%)
06/02 02:09:04 PM | Valid: [75/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.8%, 90.2%)
06/02 02:09:04 PM | Valid: [75/200] Final Prec@1 70.8000%
06/02 02:09:04 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9123.0, 0.1392059326171875]
['model.relu.alpha_mask_1_0', 16384, 3961.0, 0.24176025390625]
['model.relu.alpha_mask_2_0', 16384, 3132.0, 0.191162109375]
['model.relu.alpha_mask_3_0', 16384, 3915.0, 0.23895263671875]
['model.relu.alpha_mask_4_0', 16384, 3193.0, 0.19488525390625]
['model.relu.alpha_mask_5_0', 8192, 3169.0, 0.3868408203125]
['model.relu.alpha_mask_6_0', 8192, 2653.0, 0.3238525390625]
['model.relu.alpha_mask_7_0', 8192, 2785.0, 0.3399658203125]
['model.relu.alpha_mask_8_0', 8192, 2588.0, 0.31591796875]
['model.relu.alpha_mask_9_0', 4096, 2061.0, 0.503173828125]
['model.relu.alpha_mask_10_0', 4096, 1919.0, 0.468505859375]
['model.relu.alpha_mask_11_0', 4096, 2172.0, 0.5302734375]
['model.relu.alpha_mask_12_0', 4096, 1921.0, 0.468994140625]
['model.relu.alpha_mask_13_0', 2048, 1828.0, 0.892578125]
['model.relu.alpha_mask_14_0', 2048, 1999.0, 0.97607421875]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49840.0, 0.2645210597826087]
########## End ###########
06/02 02:09:05 PM | Train: [76/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:09:05 PM | layerwise density: [9123.0, 3961.0, 3132.0, 3915.0, 3193.0, 3169.0, 2653.0, 2785.0, 2588.0, 2061.0, 1919.0, 2172.0, 1921.0, 1828.0, 1999.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.469', '0.530', '0.469', '0.893', '0.976', '0.791', '0.880']
Global density: 0.264521062374115
06/02 02:09:16 PM | Train: [76/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:09:16 PM | layerwise density: [9123.0, 3960.0, 3132.0, 3915.0, 3193.0, 3168.0, 2653.0, 2784.0, 2589.0, 2061.0, 1919.0, 2172.0, 1920.0, 1829.0, 1999.0, 1620.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.469', '0.530', '0.469', '0.893', '0.976', '0.791', '0.880']
Global density: 0.26451575756073
06/02 02:09:27 PM | Train: [76/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:09:27 PM | layerwise density: [9123.0, 3960.0, 3132.0, 3915.0, 3192.0, 3168.0, 2653.0, 2784.0, 2589.0, 2061.0, 1916.0, 2172.0, 1918.0, 1828.0, 1999.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.468', '0.530', '0.468', '0.893', '0.976', '0.791', '0.880']
Global density: 0.2644732892513275
06/02 02:09:38 PM | Train: [76/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:09:38 PM | layerwise density: [9123.0, 3960.0, 3133.0, 3915.0, 3192.0, 3169.0, 2651.0, 2784.0, 2588.0, 2061.0, 1916.0, 2172.0, 1917.0, 1829.0, 1997.0, 1618.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.468', '0.530', '0.468', '0.893', '0.975', '0.790', '0.880']
Global density: 0.2644520699977875
06/02 02:09:47 PM | Train: [76/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:09:48 PM | layerwise density: [9123.0, 3959.0, 3133.0, 3915.0, 3191.0, 3169.0, 2651.0, 2784.0, 2587.0, 2062.0, 1916.0, 2172.0, 1917.0, 1830.0, 1997.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.468', '0.530', '0.468', '0.894', '0.975', '0.791', '0.880']
Global density: 0.2644520699977875
06/02 02:09:48 PM | Train: [76/200] Final Prec@1 99.9700%
06/02 02:09:48 PM | Valid: [76/200] Step 000/078 Loss 1.098 Prec@(1,5) (75.0%, 92.2%)
06/02 02:09:50 PM | Valid: [76/200] Step 078/078 Loss 1.277 Prec@(1,5) (70.5%, 90.1%)
06/02 02:09:50 PM | Valid: [76/200] Final Prec@1 70.5300%
06/02 02:09:50 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9123.0, 0.1392059326171875]
['model.relu.alpha_mask_1_0', 16384, 3959.0, 0.24163818359375]
['model.relu.alpha_mask_2_0', 16384, 3133.0, 0.19122314453125]
['model.relu.alpha_mask_3_0', 16384, 3915.0, 0.23895263671875]
['model.relu.alpha_mask_4_0', 16384, 3191.0, 0.19476318359375]
['model.relu.alpha_mask_5_0', 8192, 3169.0, 0.3868408203125]
['model.relu.alpha_mask_6_0', 8192, 2651.0, 0.3236083984375]
['model.relu.alpha_mask_7_0', 8192, 2784.0, 0.33984375]
['model.relu.alpha_mask_8_0', 8192, 2587.0, 0.3157958984375]
['model.relu.alpha_mask_9_0', 4096, 2062.0, 0.50341796875]
['model.relu.alpha_mask_10_0', 4096, 1916.0, 0.4677734375]
['model.relu.alpha_mask_11_0', 4096, 2172.0, 0.5302734375]
['model.relu.alpha_mask_12_0', 4096, 1917.0, 0.468017578125]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 1997.0, 0.97509765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49827.0, 0.26445206351902173]
########## End ###########
06/02 02:09:51 PM | Train: [77/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:09:51 PM | layerwise density: [9123.0, 3959.0, 3133.0, 3915.0, 3191.0, 3169.0, 2651.0, 2784.0, 2587.0, 2062.0, 1916.0, 2172.0, 1917.0, 1830.0, 1997.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.468', '0.530', '0.468', '0.894', '0.975', '0.791', '0.880']
Global density: 0.2644520699977875
06/02 02:10:02 PM | Train: [77/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:10:02 PM | layerwise density: [9123.0, 3959.0, 3133.0, 3915.0, 3191.0, 3167.0, 2651.0, 2784.0, 2587.0, 2064.0, 1915.0, 2172.0, 1916.0, 1830.0, 1997.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.504', '0.468', '0.530', '0.468', '0.894', '0.975', '0.791', '0.880']
Global density: 0.26444146037101746
06/02 02:10:13 PM | Train: [77/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:10:13 PM | layerwise density: [9123.0, 3956.0, 3133.0, 3915.0, 3191.0, 3168.0, 2652.0, 2784.0, 2586.0, 2062.0, 1916.0, 2172.0, 1916.0, 1829.0, 1997.0, 1620.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.468', '0.530', '0.468', '0.893', '0.975', '0.791', '0.880']
Global density: 0.2644255459308624
06/02 02:10:24 PM | Train: [77/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:10:24 PM | layerwise density: [9122.0, 3956.0, 3133.0, 3915.0, 3191.0, 3166.0, 2652.0, 2784.0, 2587.0, 2060.0, 1915.0, 2172.0, 1916.0, 1829.0, 1997.0, 1620.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.386', '0.324', '0.340', '0.316', '0.503', '0.468', '0.530', '0.468', '0.893', '0.975', '0.791', '0.880']
Global density: 0.264398992061615
06/02 02:10:34 PM | Train: [77/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:10:34 PM | layerwise density: [9122.0, 3956.0, 3133.0, 3915.0, 3190.0, 3167.0, 2652.0, 2784.0, 2587.0, 2059.0, 1914.0, 2172.0, 1916.0, 1829.0, 1997.0, 1620.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.467', '0.530', '0.468', '0.893', '0.975', '0.791', '0.880']
Global density: 0.26438838243484497
06/02 02:10:34 PM | Train: [77/200] Final Prec@1 99.9500%
06/02 02:10:34 PM | Valid: [77/200] Step 000/078 Loss 1.103 Prec@(1,5) (75.0%, 91.4%)
06/02 02:10:36 PM | Valid: [77/200] Step 078/078 Loss 1.281 Prec@(1,5) (70.6%, 90.2%)
06/02 02:10:36 PM | Valid: [77/200] Final Prec@1 70.6100%
06/02 02:10:36 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9122.0, 0.139190673828125]
['model.relu.alpha_mask_1_0', 16384, 3956.0, 0.241455078125]
['model.relu.alpha_mask_2_0', 16384, 3133.0, 0.19122314453125]
['model.relu.alpha_mask_3_0', 16384, 3915.0, 0.23895263671875]
['model.relu.alpha_mask_4_0', 16384, 3190.0, 0.1947021484375]
['model.relu.alpha_mask_5_0', 8192, 3167.0, 0.3865966796875]
['model.relu.alpha_mask_6_0', 8192, 2652.0, 0.32373046875]
['model.relu.alpha_mask_7_0', 8192, 2784.0, 0.33984375]
['model.relu.alpha_mask_8_0', 8192, 2587.0, 0.3157958984375]
['model.relu.alpha_mask_9_0', 4096, 2059.0, 0.502685546875]
['model.relu.alpha_mask_10_0', 4096, 1914.0, 0.46728515625]
['model.relu.alpha_mask_11_0', 4096, 2172.0, 0.5302734375]
['model.relu.alpha_mask_12_0', 4096, 1916.0, 0.4677734375]
['model.relu.alpha_mask_13_0', 2048, 1829.0, 0.89306640625]
['model.relu.alpha_mask_14_0', 2048, 1997.0, 0.97509765625]
['model.relu.alpha_mask_15_0', 2048, 1620.0, 0.791015625]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49815.0, 0.2643883746603261]
########## End ###########
06/02 02:10:37 PM | Train: [78/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:10:37 PM | layerwise density: [9122.0, 3956.0, 3133.0, 3915.0, 3190.0, 3167.0, 2652.0, 2784.0, 2587.0, 2059.0, 1914.0, 2172.0, 1916.0, 1829.0, 1997.0, 1620.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.503', '0.467', '0.530', '0.468', '0.893', '0.975', '0.791', '0.880']
Global density: 0.26438838243484497
06/02 02:10:48 PM | Train: [78/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:10:48 PM | layerwise density: [9120.0, 3956.0, 3131.0, 3916.0, 3190.0, 3168.0, 2653.0, 2784.0, 2587.0, 2058.0, 1915.0, 2172.0, 1914.0, 1829.0, 1997.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.467', '0.893', '0.975', '0.792', '0.880']
Global density: 0.26437777280807495
06/02 02:10:59 PM | Train: [78/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:10:59 PM | layerwise density: [9120.0, 3956.0, 3132.0, 3916.0, 3191.0, 3168.0, 2653.0, 2784.0, 2586.0, 2058.0, 1916.0, 2171.0, 1915.0, 1829.0, 1997.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.893', '0.975', '0.792', '0.880']
Global density: 0.26438838243484497
06/02 02:11:10 PM | Train: [78/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:11:10 PM | layerwise density: [9120.0, 3956.0, 3132.0, 3916.0, 3191.0, 3168.0, 2652.0, 2784.0, 2586.0, 2057.0, 1916.0, 2171.0, 1915.0, 1829.0, 1996.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.893', '0.975', '0.792', '0.880']
Global density: 0.26437246799468994
06/02 02:11:20 PM | Train: [78/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:11:20 PM | layerwise density: [9120.0, 3956.0, 3133.0, 3916.0, 3193.0, 3169.0, 2652.0, 2784.0, 2586.0, 2058.0, 1916.0, 2171.0, 1915.0, 1829.0, 1996.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.893', '0.975', '0.792', '0.880']
Global density: 0.264398992061615
06/02 02:11:20 PM | Train: [78/200] Final Prec@1 99.9560%
06/02 02:11:20 PM | Valid: [78/200] Step 000/078 Loss 1.071 Prec@(1,5) (76.6%, 91.4%)
06/02 02:11:22 PM | Valid: [78/200] Step 078/078 Loss 1.270 Prec@(1,5) (70.7%, 90.2%)
06/02 02:11:22 PM | Valid: [78/200] Final Prec@1 70.7000%
06/02 02:11:23 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3956.0, 0.241455078125]
['model.relu.alpha_mask_2_0', 16384, 3133.0, 0.19122314453125]
['model.relu.alpha_mask_3_0', 16384, 3916.0, 0.239013671875]
['model.relu.alpha_mask_4_0', 16384, 3193.0, 0.19488525390625]
['model.relu.alpha_mask_5_0', 8192, 3169.0, 0.3868408203125]
['model.relu.alpha_mask_6_0', 8192, 2652.0, 0.32373046875]
['model.relu.alpha_mask_7_0', 8192, 2784.0, 0.33984375]
['model.relu.alpha_mask_8_0', 8192, 2586.0, 0.315673828125]
['model.relu.alpha_mask_9_0', 4096, 2058.0, 0.50244140625]
['model.relu.alpha_mask_10_0', 4096, 1916.0, 0.4677734375]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_13_0', 2048, 1829.0, 0.89306640625]
['model.relu.alpha_mask_14_0', 2048, 1996.0, 0.974609375]
['model.relu.alpha_mask_15_0', 2048, 1621.0, 0.79150390625]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49817.0, 0.2643989894701087]
########## End ###########
06/02 02:11:23 PM | Train: [79/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:11:23 PM | layerwise density: [9120.0, 3956.0, 3133.0, 3916.0, 3193.0, 3169.0, 2652.0, 2784.0, 2586.0, 2058.0, 1916.0, 2171.0, 1915.0, 1829.0, 1996.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.893', '0.975', '0.792', '0.880']
Global density: 0.264398992061615
06/02 02:11:34 PM | Train: [79/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:11:34 PM | layerwise density: [9120.0, 3956.0, 3133.0, 3916.0, 3192.0, 3169.0, 2651.0, 2784.0, 2585.0, 2057.0, 1916.0, 2171.0, 1916.0, 1829.0, 1996.0, 1622.0, 1802.0]
layerwise density percentage: ['0.139', '0.241', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.893', '0.975', '0.792', '0.880']
Global density: 0.26438838243484497
06/02 02:11:45 PM | Train: [79/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:11:45 PM | layerwise density: [9120.0, 3957.0, 3132.0, 3914.0, 3192.0, 3170.0, 2651.0, 2784.0, 2586.0, 2057.0, 1915.0, 2171.0, 1915.0, 1829.0, 1996.0, 1622.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.893', '0.975', '0.792', '0.880']
Global density: 0.26437777280807495
06/02 02:11:56 PM | Train: [79/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:11:56 PM | layerwise density: [9120.0, 3958.0, 3132.0, 3913.0, 3191.0, 3171.0, 2652.0, 2784.0, 2586.0, 2057.0, 1915.0, 2171.0, 1915.0, 1830.0, 1996.0, 1622.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.894', '0.975', '0.792', '0.880']
Global density: 0.26438838243484497
06/02 02:12:06 PM | Train: [79/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:12:06 PM | layerwise density: [9120.0, 3957.0, 3132.0, 3913.0, 3192.0, 3170.0, 2652.0, 2784.0, 2586.0, 2057.0, 1916.0, 2171.0, 1915.0, 1830.0, 1996.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.894', '0.975', '0.792', '0.880']
Global density: 0.26438307762145996
06/02 02:12:06 PM | Train: [79/200] Final Prec@1 99.9660%
06/02 02:12:06 PM | Valid: [79/200] Step 000/078 Loss 1.069 Prec@(1,5) (75.0%, 91.4%)
06/02 02:12:09 PM | Valid: [79/200] Step 078/078 Loss 1.271 Prec@(1,5) (70.4%, 90.3%)
06/02 02:12:09 PM | Valid: [79/200] Final Prec@1 70.4400%
06/02 02:12:09 PM | Current mask training best Prec@1 = 70.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3132.0, 0.191162109375]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_mask_6_0', 8192, 2652.0, 0.32373046875]
['model.relu.alpha_mask_7_0', 8192, 2784.0, 0.33984375]
['model.relu.alpha_mask_8_0', 8192, 2586.0, 0.315673828125]
['model.relu.alpha_mask_9_0', 4096, 2057.0, 0.502197265625]
['model.relu.alpha_mask_10_0', 4096, 1916.0, 0.4677734375]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 1996.0, 0.974609375]
['model.relu.alpha_mask_15_0', 2048, 1621.0, 0.79150390625]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49814.0, 0.26438306725543476]
########## End ###########
06/02 02:12:10 PM | Train: [80/80] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 02:12:10 PM | layerwise density: [9120.0, 3957.0, 3132.0, 3913.0, 3192.0, 3170.0, 2652.0, 2784.0, 2586.0, 2057.0, 1916.0, 2171.0, 1915.0, 1830.0, 1996.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.894', '0.975', '0.792', '0.880']
Global density: 0.26438307762145996
06/02 02:12:21 PM | Train: [80/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:12:21 PM | layerwise density: [9120.0, 3957.0, 3132.0, 3913.0, 3192.0, 3171.0, 2652.0, 2784.0, 2587.0, 2056.0, 1915.0, 2171.0, 1915.0, 1830.0, 1998.0, 1621.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.468', '0.894', '0.976', '0.792', '0.880']
Global density: 0.26439368724823
06/02 02:12:31 PM | Train: [80/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:12:31 PM | layerwise density: [9120.0, 3957.0, 3132.0, 3913.0, 3192.0, 3171.0, 2652.0, 2783.0, 2587.0, 2056.0, 1916.0, 2171.0, 1914.0, 1829.0, 2000.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.467', '0.893', '0.977', '0.791', '0.880']
Global density: 0.26438307762145996
06/02 02:12:42 PM | Train: [80/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:12:42 PM | layerwise density: [9120.0, 3957.0, 3130.0, 3913.0, 3192.0, 3171.0, 2651.0, 2783.0, 2585.0, 2055.0, 1916.0, 2171.0, 1912.0, 1830.0, 2000.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.324', '0.340', '0.316', '0.502', '0.468', '0.530', '0.467', '0.894', '0.977', '0.791', '0.880']
Global density: 0.2643459141254425
06/02 02:12:52 PM | Train: [80/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:12:52 PM | layerwise density: [9120.0, 3957.0, 3130.0, 3913.0, 3192.0, 3171.0, 2650.0, 2783.0, 2585.0, 2055.0, 1915.0, 2171.0, 1912.0, 1830.0, 2000.0, 1619.0, 1802.0]
layerwise density percentage: ['0.139', '0.242', '0.191', '0.239', '0.195', '0.387', '0.323', '0.340', '0.316', '0.502', '0.468', '0.530', '0.467', '0.894', '0.977', '0.791', '0.880']
Global density: 0.2643353044986725
06/02 02:12:52 PM | Train: [80/200] Final Prec@1 99.9560%
06/02 02:12:53 PM | Valid: [80/200] Step 000/078 Loss 1.097 Prec@(1,5) (74.2%, 92.2%)
06/02 02:12:55 PM | Valid: [80/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.8%, 90.3%)
06/02 02:12:55 PM | Valid: [80/200] Final Prec@1 70.8100%
06/02 02:12:56 PM | Current mask training best Prec@1 = 70.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:12:57 PM | Train: [ 1/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:13:07 PM | Train: [ 1/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:13:17 PM | Train: [ 1/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:13:28 PM | Train: [ 1/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:13:37 PM | Train: [ 1/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:13:37 PM | Train: [ 1/200] Final Prec@1 99.9720%
06/02 02:13:38 PM | Valid: [ 1/200] Step 000/078 Loss 1.086 Prec@(1,5) (75.8%, 92.2%)
06/02 02:13:40 PM | Valid: [ 1/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.8%, 90.1%)
06/02 02:13:40 PM | Valid: [ 1/200] Final Prec@1 70.7700%
06/02 02:13:40 PM | Current best Prec@1 = 70.7700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:13:41 PM | Train: [ 2/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:13:52 PM | Train: [ 2/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:14:02 PM | Train: [ 2/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:14:13 PM | Train: [ 2/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:14:22 PM | Train: [ 2/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:14:22 PM | Train: [ 2/200] Final Prec@1 99.9740%
06/02 02:14:23 PM | Valid: [ 2/200] Step 000/078 Loss 1.077 Prec@(1,5) (75.8%, 92.2%)
06/02 02:14:25 PM | Valid: [ 2/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.9%, 90.2%)
06/02 02:14:25 PM | Valid: [ 2/200] Final Prec@1 70.8700%
06/02 02:14:26 PM | Current best Prec@1 = 70.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:14:26 PM | Train: [ 3/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:14:36 PM | Train: [ 3/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:14:47 PM | Train: [ 3/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:14:57 PM | Train: [ 3/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:15:06 PM | Train: [ 3/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:15:06 PM | Train: [ 3/200] Final Prec@1 99.9680%
06/02 02:15:07 PM | Valid: [ 3/200] Step 000/078 Loss 1.085 Prec@(1,5) (74.2%, 92.2%)
06/02 02:15:09 PM | Valid: [ 3/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.8%, 90.2%)
06/02 02:15:09 PM | Valid: [ 3/200] Final Prec@1 70.7500%
06/02 02:15:10 PM | Current best Prec@1 = 70.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:15:10 PM | Train: [ 4/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:15:20 PM | Train: [ 4/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:15:31 PM | Train: [ 4/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:15:41 PM | Train: [ 4/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:15:50 PM | Train: [ 4/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:15:50 PM | Train: [ 4/200] Final Prec@1 99.9660%
06/02 02:15:51 PM | Valid: [ 4/200] Step 000/078 Loss 1.066 Prec@(1,5) (75.0%, 91.4%)
06/02 02:15:53 PM | Valid: [ 4/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.8%, 90.3%)
06/02 02:15:53 PM | Valid: [ 4/200] Final Prec@1 70.7800%
06/02 02:15:54 PM | Current best Prec@1 = 70.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:15:54 PM | Train: [ 5/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:16:04 PM | Train: [ 5/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:16:15 PM | Train: [ 5/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:16:25 PM | Train: [ 5/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:16:35 PM | Train: [ 5/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:16:35 PM | Train: [ 5/200] Final Prec@1 99.9720%
06/02 02:16:35 PM | Valid: [ 5/200] Step 000/078 Loss 1.098 Prec@(1,5) (75.0%, 92.2%)
06/02 02:16:38 PM | Valid: [ 5/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.9%, 90.3%)
06/02 02:16:38 PM | Valid: [ 5/200] Final Prec@1 70.8700%
06/02 02:16:38 PM | Current best Prec@1 = 70.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:16:39 PM | Train: [ 6/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:16:49 PM | Train: [ 6/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 02:16:59 PM | Train: [ 6/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:17:10 PM | Train: [ 6/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:17:20 PM | Train: [ 6/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:17:20 PM | Train: [ 6/200] Final Prec@1 99.9740%
06/02 02:17:20 PM | Valid: [ 6/200] Step 000/078 Loss 1.058 Prec@(1,5) (75.0%, 92.2%)
06/02 02:17:22 PM | Valid: [ 6/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.8%, 90.3%)
06/02 02:17:23 PM | Valid: [ 6/200] Final Prec@1 70.7900%
06/02 02:17:23 PM | Current best Prec@1 = 70.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:17:24 PM | Train: [ 7/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:17:33 PM | Train: [ 7/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 02:17:43 PM | Train: [ 7/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:17:54 PM | Train: [ 7/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:18:03 PM | Train: [ 7/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:18:03 PM | Train: [ 7/200] Final Prec@1 99.9640%
06/02 02:18:03 PM | Valid: [ 7/200] Step 000/078 Loss 1.069 Prec@(1,5) (75.8%, 92.2%)
06/02 02:18:06 PM | Valid: [ 7/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.9%, 90.2%)
06/02 02:18:06 PM | Valid: [ 7/200] Final Prec@1 70.9300%
06/02 02:18:06 PM | Current best Prec@1 = 70.9300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:18:07 PM | Train: [ 8/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:18:17 PM | Train: [ 8/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:18:27 PM | Train: [ 8/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:18:37 PM | Train: [ 8/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:18:46 PM | Train: [ 8/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:18:47 PM | Train: [ 8/200] Final Prec@1 99.9540%
06/02 02:18:47 PM | Valid: [ 8/200] Step 000/078 Loss 1.071 Prec@(1,5) (77.3%, 91.4%)
06/02 02:18:49 PM | Valid: [ 8/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.7%, 90.2%)
06/02 02:18:49 PM | Valid: [ 8/200] Final Prec@1 70.7300%
06/02 02:18:50 PM | Current best Prec@1 = 70.9300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:18:50 PM | Train: [ 9/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:19:01 PM | Train: [ 9/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:19:11 PM | Train: [ 9/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:19:22 PM | Train: [ 9/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:19:31 PM | Train: [ 9/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:19:32 PM | Train: [ 9/200] Final Prec@1 99.9660%
06/02 02:19:32 PM | Valid: [ 9/200] Step 000/078 Loss 1.065 Prec@(1,5) (74.2%, 92.2%)
06/02 02:19:34 PM | Valid: [ 9/200] Step 078/078 Loss 1.272 Prec@(1,5) (70.9%, 90.1%)
06/02 02:19:34 PM | Valid: [ 9/200] Final Prec@1 70.8500%
06/02 02:19:35 PM | Current best Prec@1 = 70.9300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:19:35 PM | Train: [10/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:19:45 PM | Train: [10/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:19:56 PM | Train: [10/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:20:06 PM | Train: [10/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:20:16 PM | Train: [10/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:20:16 PM | Train: [10/200] Final Prec@1 99.9760%
06/02 02:20:16 PM | Valid: [10/200] Step 000/078 Loss 1.065 Prec@(1,5) (75.8%, 92.2%)
06/02 02:20:18 PM | Valid: [10/200] Step 078/078 Loss 1.268 Prec@(1,5) (71.0%, 90.2%)
06/02 02:20:18 PM | Valid: [10/200] Final Prec@1 70.9500%
06/02 02:20:19 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:20:19 PM | Train: [11/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:20:30 PM | Train: [11/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:20:40 PM | Train: [11/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:20:50 PM | Train: [11/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:21:00 PM | Train: [11/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:21:00 PM | Train: [11/200] Final Prec@1 99.9700%
06/02 02:21:00 PM | Valid: [11/200] Step 000/078 Loss 1.051 Prec@(1,5) (76.6%, 92.2%)
06/02 02:21:03 PM | Valid: [11/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.3%)
06/02 02:21:03 PM | Valid: [11/200] Final Prec@1 70.8100%
06/02 02:21:03 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:21:04 PM | Train: [12/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:21:14 PM | Train: [12/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:21:25 PM | Train: [12/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:21:35 PM | Train: [12/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:21:44 PM | Train: [12/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:21:44 PM | Train: [12/200] Final Prec@1 99.9600%
06/02 02:21:45 PM | Valid: [12/200] Step 000/078 Loss 1.058 Prec@(1,5) (75.8%, 92.2%)
06/02 02:21:47 PM | Valid: [12/200] Step 078/078 Loss 1.269 Prec@(1,5) (71.0%, 90.4%)
06/02 02:21:47 PM | Valid: [12/200] Final Prec@1 70.9500%
06/02 02:21:48 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:21:48 PM | Train: [13/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:21:59 PM | Train: [13/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:22:09 PM | Train: [13/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:22:20 PM | Train: [13/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:22:29 PM | Train: [13/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:22:29 PM | Train: [13/200] Final Prec@1 99.9660%
06/02 02:22:29 PM | Valid: [13/200] Step 000/078 Loss 1.078 Prec@(1,5) (75.0%, 91.4%)
06/02 02:22:32 PM | Valid: [13/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.2%)
06/02 02:22:32 PM | Valid: [13/200] Final Prec@1 70.8000%
06/02 02:22:32 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:22:33 PM | Train: [14/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:22:43 PM | Train: [14/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:22:54 PM | Train: [14/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:23:04 PM | Train: [14/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:23:13 PM | Train: [14/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:23:13 PM | Train: [14/200] Final Prec@1 99.9680%
06/02 02:23:14 PM | Valid: [14/200] Step 000/078 Loss 1.078 Prec@(1,5) (74.2%, 91.4%)
06/02 02:23:16 PM | Valid: [14/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.3%)
06/02 02:23:16 PM | Valid: [14/200] Final Prec@1 70.8000%
06/02 02:23:17 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:23:17 PM | Train: [15/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:23:27 PM | Train: [15/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:23:38 PM | Train: [15/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:23:49 PM | Train: [15/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:23:58 PM | Train: [15/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:23:58 PM | Train: [15/200] Final Prec@1 99.9680%
06/02 02:23:58 PM | Valid: [15/200] Step 000/078 Loss 1.054 Prec@(1,5) (75.0%, 92.2%)
06/02 02:24:01 PM | Valid: [15/200] Step 078/078 Loss 1.270 Prec@(1,5) (70.9%, 90.2%)
06/02 02:24:01 PM | Valid: [15/200] Final Prec@1 70.8600%
06/02 02:24:01 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:24:02 PM | Train: [16/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:24:12 PM | Train: [16/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:24:23 PM | Train: [16/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:24:33 PM | Train: [16/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:24:43 PM | Train: [16/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:24:43 PM | Train: [16/200] Final Prec@1 99.9740%
06/02 02:24:43 PM | Valid: [16/200] Step 000/078 Loss 1.073 Prec@(1,5) (75.0%, 92.2%)
06/02 02:24:45 PM | Valid: [16/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.8%, 90.2%)
06/02 02:24:45 PM | Valid: [16/200] Final Prec@1 70.8400%
06/02 02:24:46 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:24:46 PM | Train: [17/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:24:56 PM | Train: [17/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:25:07 PM | Train: [17/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:25:17 PM | Train: [17/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:25:27 PM | Train: [17/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:25:27 PM | Train: [17/200] Final Prec@1 99.9840%
06/02 02:25:27 PM | Valid: [17/200] Step 000/078 Loss 1.073 Prec@(1,5) (75.8%, 92.2%)
06/02 02:25:30 PM | Valid: [17/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.9%, 90.1%)
06/02 02:25:30 PM | Valid: [17/200] Final Prec@1 70.9000%
06/02 02:25:30 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:25:31 PM | Train: [18/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:25:41 PM | Train: [18/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:25:52 PM | Train: [18/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:26:02 PM | Train: [18/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:26:12 PM | Train: [18/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:26:12 PM | Train: [18/200] Final Prec@1 99.9680%
06/02 02:26:12 PM | Valid: [18/200] Step 000/078 Loss 1.057 Prec@(1,5) (75.0%, 92.2%)
06/02 02:26:15 PM | Valid: [18/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.8%, 90.3%)
06/02 02:26:15 PM | Valid: [18/200] Final Prec@1 70.7500%
06/02 02:26:15 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:26:16 PM | Train: [19/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:26:27 PM | Train: [19/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:26:37 PM | Train: [19/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:26:48 PM | Train: [19/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:26:58 PM | Train: [19/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:26:58 PM | Train: [19/200] Final Prec@1 99.9760%
06/02 02:26:58 PM | Valid: [19/200] Step 000/078 Loss 1.054 Prec@(1,5) (76.6%, 91.4%)
06/02 02:27:00 PM | Valid: [19/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.1%)
06/02 02:27:00 PM | Valid: [19/200] Final Prec@1 70.8100%
06/02 02:27:01 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:27:01 PM | Train: [20/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:27:11 PM | Train: [20/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:27:22 PM | Train: [20/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:27:32 PM | Train: [20/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:27:40 PM | Train: [20/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:27:41 PM | Train: [20/200] Final Prec@1 99.9720%
06/02 02:27:41 PM | Valid: [20/200] Step 000/078 Loss 1.077 Prec@(1,5) (76.6%, 91.4%)
06/02 02:27:43 PM | Valid: [20/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.9%, 90.3%)
06/02 02:27:43 PM | Valid: [20/200] Final Prec@1 70.9200%
06/02 02:27:44 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:27:44 PM | Train: [21/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:27:54 PM | Train: [21/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:28:05 PM | Train: [21/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:28:15 PM | Train: [21/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:28:25 PM | Train: [21/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:28:25 PM | Train: [21/200] Final Prec@1 99.9680%
06/02 02:28:25 PM | Valid: [21/200] Step 000/078 Loss 1.055 Prec@(1,5) (75.0%, 91.4%)
06/02 02:28:28 PM | Valid: [21/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.2%)
06/02 02:28:28 PM | Valid: [21/200] Final Prec@1 70.8000%
06/02 02:28:28 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:28:29 PM | Train: [22/200] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/02 02:28:40 PM | Train: [22/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:28:50 PM | Train: [22/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:29:00 PM | Train: [22/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:29:10 PM | Train: [22/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:29:10 PM | Train: [22/200] Final Prec@1 99.9700%
06/02 02:29:10 PM | Valid: [22/200] Step 000/078 Loss 1.084 Prec@(1,5) (75.0%, 92.2%)
06/02 02:29:12 PM | Valid: [22/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.7%, 90.3%)
06/02 02:29:12 PM | Valid: [22/200] Final Prec@1 70.6600%
06/02 02:29:13 PM | Current best Prec@1 = 70.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:29:13 PM | Train: [23/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:29:24 PM | Train: [23/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:29:34 PM | Train: [23/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:29:44 PM | Train: [23/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:29:54 PM | Train: [23/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:29:54 PM | Train: [23/200] Final Prec@1 99.9680%
06/02 02:29:54 PM | Valid: [23/200] Step 000/078 Loss 1.083 Prec@(1,5) (75.8%, 91.4%)
06/02 02:29:57 PM | Valid: [23/200] Step 078/078 Loss 1.269 Prec@(1,5) (71.0%, 90.1%)
06/02 02:29:57 PM | Valid: [23/200] Final Prec@1 71.0400%
06/02 02:29:57 PM | Current best Prec@1 = 71.0400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:29:58 PM | Train: [24/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:30:08 PM | Train: [24/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:30:19 PM | Train: [24/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:30:29 PM | Train: [24/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:30:39 PM | Train: [24/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:30:39 PM | Train: [24/200] Final Prec@1 99.9720%
06/02 02:30:39 PM | Valid: [24/200] Step 000/078 Loss 1.084 Prec@(1,5) (75.0%, 91.4%)
06/02 02:30:41 PM | Valid: [24/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.7%, 90.2%)
06/02 02:30:41 PM | Valid: [24/200] Final Prec@1 70.6700%
06/02 02:30:42 PM | Current best Prec@1 = 71.0400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:30:42 PM | Train: [25/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:30:53 PM | Train: [25/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 02:31:03 PM | Train: [25/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:31:14 PM | Train: [25/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:31:23 PM | Train: [25/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:31:23 PM | Train: [25/200] Final Prec@1 99.9660%
06/02 02:31:23 PM | Valid: [25/200] Step 000/078 Loss 1.083 Prec@(1,5) (75.8%, 92.2%)
06/02 02:31:25 PM | Valid: [25/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.8%, 90.2%)
06/02 02:31:25 PM | Valid: [25/200] Final Prec@1 70.7800%
06/02 02:31:26 PM | Current best Prec@1 = 71.0400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:31:27 PM | Train: [26/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:31:37 PM | Train: [26/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:31:47 PM | Train: [26/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:31:57 PM | Train: [26/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:32:07 PM | Train: [26/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:32:07 PM | Train: [26/200] Final Prec@1 99.9700%
06/02 02:32:07 PM | Valid: [26/200] Step 000/078 Loss 1.073 Prec@(1,5) (75.8%, 92.2%)
06/02 02:32:09 PM | Valid: [26/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.7%, 90.2%)
06/02 02:32:09 PM | Valid: [26/200] Final Prec@1 70.6700%
06/02 02:32:10 PM | Current best Prec@1 = 71.0400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:32:10 PM | Train: [27/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:32:21 PM | Train: [27/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:32:31 PM | Train: [27/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:32:42 PM | Train: [27/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:32:51 PM | Train: [27/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:32:51 PM | Train: [27/200] Final Prec@1 99.9820%
06/02 02:32:51 PM | Valid: [27/200] Step 000/078 Loss 1.091 Prec@(1,5) (76.6%, 92.2%)
06/02 02:32:54 PM | Valid: [27/200] Step 078/078 Loss 1.270 Prec@(1,5) (70.7%, 90.3%)
06/02 02:32:54 PM | Valid: [27/200] Final Prec@1 70.7400%
06/02 02:32:54 PM | Current best Prec@1 = 71.0400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:32:55 PM | Train: [28/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:33:05 PM | Train: [28/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:33:15 PM | Train: [28/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:33:26 PM | Train: [28/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:33:35 PM | Train: [28/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:33:35 PM | Train: [28/200] Final Prec@1 99.9720%
06/02 02:33:35 PM | Valid: [28/200] Step 000/078 Loss 1.075 Prec@(1,5) (75.8%, 91.4%)
06/02 02:33:38 PM | Valid: [28/200] Step 078/078 Loss 1.267 Prec@(1,5) (71.0%, 90.3%)
06/02 02:33:38 PM | Valid: [28/200] Final Prec@1 70.9500%
06/02 02:33:38 PM | Current best Prec@1 = 71.0400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:33:39 PM | Train: [29/200] Step 000/390 Loss 0.012 Prec@(1,5) (99.2%, 100.0%)
06/02 02:33:49 PM | Train: [29/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:34:00 PM | Train: [29/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:34:10 PM | Train: [29/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:34:20 PM | Train: [29/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:34:20 PM | Train: [29/200] Final Prec@1 99.9660%
06/02 02:34:20 PM | Valid: [29/200] Step 000/078 Loss 1.077 Prec@(1,5) (75.0%, 92.2%)
06/02 02:34:22 PM | Valid: [29/200] Step 078/078 Loss 1.269 Prec@(1,5) (71.0%, 90.2%)
06/02 02:34:22 PM | Valid: [29/200] Final Prec@1 71.0500%
06/02 02:34:23 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:34:23 PM | Train: [30/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:34:33 PM | Train: [30/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:34:44 PM | Train: [30/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:34:54 PM | Train: [30/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:35:03 PM | Train: [30/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:35:03 PM | Train: [30/200] Final Prec@1 99.9700%
06/02 02:35:04 PM | Valid: [30/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.8%, 92.2%)
06/02 02:35:06 PM | Valid: [30/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.3%)
06/02 02:35:06 PM | Valid: [30/200] Final Prec@1 70.7600%
06/02 02:35:07 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:35:07 PM | Train: [31/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:35:17 PM | Train: [31/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:35:28 PM | Train: [31/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:35:39 PM | Train: [31/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:35:48 PM | Train: [31/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:35:48 PM | Train: [31/200] Final Prec@1 99.9680%
06/02 02:35:48 PM | Valid: [31/200] Step 000/078 Loss 1.071 Prec@(1,5) (75.8%, 91.4%)
06/02 02:35:51 PM | Valid: [31/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.7%, 90.2%)
06/02 02:35:51 PM | Valid: [31/200] Final Prec@1 70.7300%
06/02 02:35:51 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:35:52 PM | Train: [32/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:36:02 PM | Train: [32/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:36:12 PM | Train: [32/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:36:23 PM | Train: [32/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:36:32 PM | Train: [32/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:36:32 PM | Train: [32/200] Final Prec@1 99.9740%
06/02 02:36:32 PM | Valid: [32/200] Step 000/078 Loss 1.072 Prec@(1,5) (76.6%, 91.4%)
06/02 02:36:35 PM | Valid: [32/200] Step 078/078 Loss 1.268 Prec@(1,5) (71.0%, 90.3%)
06/02 02:36:35 PM | Valid: [32/200] Final Prec@1 70.9600%
06/02 02:36:35 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:36:36 PM | Train: [33/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:36:46 PM | Train: [33/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:36:56 PM | Train: [33/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:37:07 PM | Train: [33/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:37:16 PM | Train: [33/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:37:16 PM | Train: [33/200] Final Prec@1 99.9720%
06/02 02:37:17 PM | Valid: [33/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.0%, 91.4%)
06/02 02:37:19 PM | Valid: [33/200] Step 078/078 Loss 1.266 Prec@(1,5) (71.0%, 90.2%)
06/02 02:37:19 PM | Valid: [33/200] Final Prec@1 71.0500%
06/02 02:37:20 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:37:20 PM | Train: [34/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:37:30 PM | Train: [34/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:37:41 PM | Train: [34/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:37:51 PM | Train: [34/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:38:00 PM | Train: [34/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:38:00 PM | Train: [34/200] Final Prec@1 99.9800%
06/02 02:38:01 PM | Valid: [34/200] Step 000/078 Loss 1.068 Prec@(1,5) (76.6%, 93.0%)
06/02 02:38:03 PM | Valid: [34/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.9%, 90.2%)
06/02 02:38:03 PM | Valid: [34/200] Final Prec@1 70.9100%
06/02 02:38:04 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:38:04 PM | Train: [35/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:38:15 PM | Train: [35/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:38:25 PM | Train: [35/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:38:36 PM | Train: [35/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:38:45 PM | Train: [35/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:38:45 PM | Train: [35/200] Final Prec@1 99.9680%
06/02 02:38:46 PM | Valid: [35/200] Step 000/078 Loss 1.082 Prec@(1,5) (75.8%, 92.2%)
06/02 02:38:48 PM | Valid: [35/200] Step 078/078 Loss 1.268 Prec@(1,5) (71.0%, 90.3%)
06/02 02:38:48 PM | Valid: [35/200] Final Prec@1 71.0000%
06/02 02:38:48 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:38:49 PM | Train: [36/200] Step 000/390 Loss 0.011 Prec@(1,5) (99.2%, 100.0%)
06/02 02:39:00 PM | Train: [36/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:39:10 PM | Train: [36/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:39:21 PM | Train: [36/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:39:30 PM | Train: [36/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:39:30 PM | Train: [36/200] Final Prec@1 99.9740%
06/02 02:39:30 PM | Valid: [36/200] Step 000/078 Loss 1.059 Prec@(1,5) (75.8%, 91.4%)
06/02 02:39:33 PM | Valid: [36/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.9%, 90.2%)
06/02 02:39:33 PM | Valid: [36/200] Final Prec@1 70.9100%
06/02 02:39:33 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:39:34 PM | Train: [37/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 02:39:44 PM | Train: [37/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:39:54 PM | Train: [37/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:40:05 PM | Train: [37/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:40:15 PM | Train: [37/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:40:15 PM | Train: [37/200] Final Prec@1 99.9720%
06/02 02:40:15 PM | Valid: [37/200] Step 000/078 Loss 1.071 Prec@(1,5) (75.8%, 92.2%)
06/02 02:40:17 PM | Valid: [37/200] Step 078/078 Loss 1.267 Prec@(1,5) (71.0%, 90.3%)
06/02 02:40:17 PM | Valid: [37/200] Final Prec@1 70.9900%
06/02 02:40:18 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:40:18 PM | Train: [38/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 02:40:29 PM | Train: [38/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:40:39 PM | Train: [38/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:40:50 PM | Train: [38/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:41:00 PM | Train: [38/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:41:00 PM | Train: [38/200] Final Prec@1 99.9720%
06/02 02:41:00 PM | Valid: [38/200] Step 000/078 Loss 1.073 Prec@(1,5) (75.0%, 91.4%)
06/02 02:41:02 PM | Valid: [38/200] Step 078/078 Loss 1.268 Prec@(1,5) (71.0%, 90.4%)
06/02 02:41:02 PM | Valid: [38/200] Final Prec@1 71.0000%
06/02 02:41:03 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:41:03 PM | Train: [39/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:41:14 PM | Train: [39/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:41:24 PM | Train: [39/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:41:35 PM | Train: [39/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:41:45 PM | Train: [39/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:41:45 PM | Train: [39/200] Final Prec@1 99.9680%
06/02 02:41:45 PM | Valid: [39/200] Step 000/078 Loss 1.065 Prec@(1,5) (75.8%, 91.4%)
06/02 02:41:47 PM | Valid: [39/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.9%, 90.2%)
06/02 02:41:47 PM | Valid: [39/200] Final Prec@1 70.8600%
06/02 02:41:48 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:41:48 PM | Train: [40/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:41:59 PM | Train: [40/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:42:10 PM | Train: [40/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:42:20 PM | Train: [40/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:42:29 PM | Train: [40/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:42:30 PM | Train: [40/200] Final Prec@1 99.9700%
06/02 02:42:30 PM | Valid: [40/200] Step 000/078 Loss 1.077 Prec@(1,5) (76.6%, 91.4%)
06/02 02:42:32 PM | Valid: [40/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.9%, 90.3%)
06/02 02:42:32 PM | Valid: [40/200] Final Prec@1 70.8800%
06/02 02:42:33 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:42:33 PM | Train: [41/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:42:43 PM | Train: [41/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:42:53 PM | Train: [41/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:43:04 PM | Train: [41/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:43:13 PM | Train: [41/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:43:13 PM | Train: [41/200] Final Prec@1 99.9780%
06/02 02:43:13 PM | Valid: [41/200] Step 000/078 Loss 1.062 Prec@(1,5) (76.6%, 91.4%)
06/02 02:43:15 PM | Valid: [41/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.9%, 90.1%)
06/02 02:43:16 PM | Valid: [41/200] Final Prec@1 70.9000%
06/02 02:43:16 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:43:16 PM | Train: [42/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:43:27 PM | Train: [42/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 02:43:37 PM | Train: [42/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:43:48 PM | Train: [42/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:43:57 PM | Train: [42/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:43:57 PM | Train: [42/200] Final Prec@1 99.9540%
06/02 02:43:57 PM | Valid: [42/200] Step 000/078 Loss 1.068 Prec@(1,5) (75.8%, 91.4%)
06/02 02:44:00 PM | Valid: [42/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.7%, 90.2%)
06/02 02:44:00 PM | Valid: [42/200] Final Prec@1 70.7400%
06/02 02:44:00 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:44:01 PM | Train: [43/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:44:11 PM | Train: [43/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:44:21 PM | Train: [43/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:44:32 PM | Train: [43/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:44:41 PM | Train: [43/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:44:41 PM | Train: [43/200] Final Prec@1 99.9700%
06/02 02:44:42 PM | Valid: [43/200] Step 000/078 Loss 1.074 Prec@(1,5) (76.6%, 92.2%)
06/02 02:44:44 PM | Valid: [43/200] Step 078/078 Loss 1.269 Prec@(1,5) (71.0%, 90.2%)
06/02 02:44:44 PM | Valid: [43/200] Final Prec@1 70.9800%
06/02 02:44:45 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:44:45 PM | Train: [44/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:44:55 PM | Train: [44/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:45:05 PM | Train: [44/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:45:16 PM | Train: [44/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:45:25 PM | Train: [44/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:45:25 PM | Train: [44/200] Final Prec@1 99.9680%
06/02 02:45:26 PM | Valid: [44/200] Step 000/078 Loss 1.060 Prec@(1,5) (76.6%, 91.4%)
06/02 02:45:28 PM | Valid: [44/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.0%, 90.3%)
06/02 02:45:28 PM | Valid: [44/200] Final Prec@1 70.9600%
06/02 02:45:29 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:45:29 PM | Train: [45/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:45:39 PM | Train: [45/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:45:50 PM | Train: [45/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:46:00 PM | Train: [45/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:46:10 PM | Train: [45/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:46:10 PM | Train: [45/200] Final Prec@1 99.9780%
06/02 02:46:10 PM | Valid: [45/200] Step 000/078 Loss 1.064 Prec@(1,5) (75.8%, 91.4%)
06/02 02:46:12 PM | Valid: [45/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.9%, 90.2%)
06/02 02:46:13 PM | Valid: [45/200] Final Prec@1 70.9100%
06/02 02:46:13 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:46:13 PM | Train: [46/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:46:24 PM | Train: [46/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:46:35 PM | Train: [46/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:46:45 PM | Train: [46/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:46:54 PM | Train: [46/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:46:54 PM | Train: [46/200] Final Prec@1 99.9700%
06/02 02:46:54 PM | Valid: [46/200] Step 000/078 Loss 1.062 Prec@(1,5) (75.8%, 91.4%)
06/02 02:46:57 PM | Valid: [46/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 02:46:57 PM | Valid: [46/200] Final Prec@1 70.9100%
06/02 02:46:57 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:46:58 PM | Train: [47/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:47:08 PM | Train: [47/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:47:19 PM | Train: [47/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:47:29 PM | Train: [47/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:47:39 PM | Train: [47/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:47:39 PM | Train: [47/200] Final Prec@1 99.9680%
06/02 02:47:39 PM | Valid: [47/200] Step 000/078 Loss 1.067 Prec@(1,5) (75.8%, 91.4%)
06/02 02:47:41 PM | Valid: [47/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.7%, 90.2%)
06/02 02:47:41 PM | Valid: [47/200] Final Prec@1 70.7000%
06/02 02:47:42 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:47:42 PM | Train: [48/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:47:53 PM | Train: [48/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:48:03 PM | Train: [48/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:48:14 PM | Train: [48/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:48:23 PM | Train: [48/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:48:23 PM | Train: [48/200] Final Prec@1 99.9760%
06/02 02:48:23 PM | Valid: [48/200] Step 000/078 Loss 1.071 Prec@(1,5) (75.8%, 92.2%)
06/02 02:48:26 PM | Valid: [48/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.2%)
06/02 02:48:26 PM | Valid: [48/200] Final Prec@1 70.7600%
06/02 02:48:26 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:48:27 PM | Train: [49/200] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/02 02:48:37 PM | Train: [49/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:48:48 PM | Train: [49/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:48:58 PM | Train: [49/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:49:07 PM | Train: [49/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:49:07 PM | Train: [49/200] Final Prec@1 99.9720%
06/02 02:49:08 PM | Valid: [49/200] Step 000/078 Loss 1.055 Prec@(1,5) (76.6%, 92.2%)
06/02 02:49:10 PM | Valid: [49/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.6%, 90.3%)
06/02 02:49:10 PM | Valid: [49/200] Final Prec@1 70.5800%
06/02 02:49:11 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:49:11 PM | Train: [50/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:49:21 PM | Train: [50/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:49:32 PM | Train: [50/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:49:42 PM | Train: [50/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:49:52 PM | Train: [50/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:49:52 PM | Train: [50/200] Final Prec@1 99.9800%
06/02 02:49:52 PM | Valid: [50/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.0%, 92.2%)
06/02 02:49:54 PM | Valid: [50/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.3%)
06/02 02:49:54 PM | Valid: [50/200] Final Prec@1 70.9800%
06/02 02:49:55 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:49:55 PM | Train: [51/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:50:06 PM | Train: [51/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:50:16 PM | Train: [51/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:50:27 PM | Train: [51/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:50:37 PM | Train: [51/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:50:37 PM | Train: [51/200] Final Prec@1 99.9680%
06/02 02:50:37 PM | Valid: [51/200] Step 000/078 Loss 1.066 Prec@(1,5) (75.8%, 92.2%)
06/02 02:50:39 PM | Valid: [51/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.9%, 90.3%)
06/02 02:50:39 PM | Valid: [51/200] Final Prec@1 70.8600%
06/02 02:50:40 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:50:40 PM | Train: [52/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:50:51 PM | Train: [52/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 02:51:01 PM | Train: [52/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:51:12 PM | Train: [52/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:51:21 PM | Train: [52/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:51:21 PM | Train: [52/200] Final Prec@1 99.9660%
06/02 02:51:21 PM | Valid: [52/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.8%, 92.2%)
06/02 02:51:24 PM | Valid: [52/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.8%, 90.2%)
06/02 02:51:24 PM | Valid: [52/200] Final Prec@1 70.7700%
06/02 02:51:25 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:51:25 PM | Train: [53/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:51:35 PM | Train: [53/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 02:51:46 PM | Train: [53/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:51:56 PM | Train: [53/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:52:05 PM | Train: [53/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:52:05 PM | Train: [53/200] Final Prec@1 99.9680%
06/02 02:52:06 PM | Valid: [53/200] Step 000/078 Loss 1.078 Prec@(1,5) (75.8%, 91.4%)
06/02 02:52:08 PM | Valid: [53/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.9%, 90.3%)
06/02 02:52:08 PM | Valid: [53/200] Final Prec@1 70.8600%
06/02 02:52:09 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:52:09 PM | Train: [54/200] Step 000/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
06/02 02:52:19 PM | Train: [54/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:52:30 PM | Train: [54/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:52:40 PM | Train: [54/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:52:49 PM | Train: [54/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:52:49 PM | Train: [54/200] Final Prec@1 99.9680%
06/02 02:52:49 PM | Valid: [54/200] Step 000/078 Loss 1.067 Prec@(1,5) (75.8%, 91.4%)
06/02 02:52:51 PM | Valid: [54/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.9%, 90.2%)
06/02 02:52:51 PM | Valid: [54/200] Final Prec@1 70.9000%
06/02 02:52:52 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:52:52 PM | Train: [55/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:53:03 PM | Train: [55/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:53:13 PM | Train: [55/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:53:24 PM | Train: [55/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:53:33 PM | Train: [55/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:53:33 PM | Train: [55/200] Final Prec@1 99.9760%
06/02 02:53:33 PM | Valid: [55/200] Step 000/078 Loss 1.063 Prec@(1,5) (75.0%, 91.4%)
06/02 02:53:36 PM | Valid: [55/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 02:53:36 PM | Valid: [55/200] Final Prec@1 70.8900%
06/02 02:53:36 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:53:37 PM | Train: [56/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:53:47 PM | Train: [56/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:53:57 PM | Train: [56/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:54:08 PM | Train: [56/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:54:17 PM | Train: [56/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:54:17 PM | Train: [56/200] Final Prec@1 99.9740%
06/02 02:54:17 PM | Valid: [56/200] Step 000/078 Loss 1.074 Prec@(1,5) (75.0%, 92.2%)
06/02 02:54:20 PM | Valid: [56/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.7%, 90.3%)
06/02 02:54:20 PM | Valid: [56/200] Final Prec@1 70.6900%
06/02 02:54:20 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:54:21 PM | Train: [57/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:54:31 PM | Train: [57/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:54:42 PM | Train: [57/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:54:52 PM | Train: [57/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:55:02 PM | Train: [57/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:55:02 PM | Train: [57/200] Final Prec@1 99.9720%
06/02 02:55:02 PM | Valid: [57/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.8%, 91.4%)
06/02 02:55:04 PM | Valid: [57/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.9%, 90.2%)
06/02 02:55:04 PM | Valid: [57/200] Final Prec@1 70.8600%
06/02 02:55:05 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:55:05 PM | Train: [58/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:55:16 PM | Train: [58/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:55:26 PM | Train: [58/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:55:37 PM | Train: [58/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:55:46 PM | Train: [58/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:55:46 PM | Train: [58/200] Final Prec@1 99.9660%
06/02 02:55:46 PM | Valid: [58/200] Step 000/078 Loss 1.082 Prec@(1,5) (75.8%, 91.4%)
06/02 02:55:49 PM | Valid: [58/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.1%)
06/02 02:55:49 PM | Valid: [58/200] Final Prec@1 70.7500%
06/02 02:55:49 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:55:50 PM | Train: [59/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:56:00 PM | Train: [59/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:56:11 PM | Train: [59/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:56:21 PM | Train: [59/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:56:31 PM | Train: [59/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:56:31 PM | Train: [59/200] Final Prec@1 99.9680%
06/02 02:56:31 PM | Valid: [59/200] Step 000/078 Loss 1.067 Prec@(1,5) (75.0%, 91.4%)
06/02 02:56:34 PM | Valid: [59/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.7%, 90.2%)
06/02 02:56:34 PM | Valid: [59/200] Final Prec@1 70.7200%
06/02 02:56:34 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:56:35 PM | Train: [60/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 02:56:45 PM | Train: [60/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:56:55 PM | Train: [60/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:57:06 PM | Train: [60/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:57:16 PM | Train: [60/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:57:16 PM | Train: [60/200] Final Prec@1 99.9800%
06/02 02:57:16 PM | Valid: [60/200] Step 000/078 Loss 1.081 Prec@(1,5) (75.8%, 90.6%)
06/02 02:57:18 PM | Valid: [60/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.7%, 90.1%)
06/02 02:57:18 PM | Valid: [60/200] Final Prec@1 70.7300%
06/02 02:57:19 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:57:19 PM | Train: [61/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:57:30 PM | Train: [61/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:57:40 PM | Train: [61/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:57:50 PM | Train: [61/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:58:00 PM | Train: [61/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:58:00 PM | Train: [61/200] Final Prec@1 99.9700%
06/02 02:58:00 PM | Valid: [61/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.0%, 91.4%)
06/02 02:58:03 PM | Valid: [61/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.2%)
06/02 02:58:03 PM | Valid: [61/200] Final Prec@1 70.9600%
06/02 02:58:03 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:58:04 PM | Train: [62/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:58:14 PM | Train: [62/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:58:25 PM | Train: [62/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:58:35 PM | Train: [62/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:58:44 PM | Train: [62/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:58:44 PM | Train: [62/200] Final Prec@1 99.9700%
06/02 02:58:45 PM | Valid: [62/200] Step 000/078 Loss 1.079 Prec@(1,5) (74.2%, 92.2%)
06/02 02:58:47 PM | Valid: [62/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.9%, 90.2%)
06/02 02:58:47 PM | Valid: [62/200] Final Prec@1 70.8500%
06/02 02:58:48 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:58:48 PM | Train: [63/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:58:59 PM | Train: [63/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:59:09 PM | Train: [63/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:59:20 PM | Train: [63/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:59:29 PM | Train: [63/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:59:29 PM | Train: [63/200] Final Prec@1 99.9700%
06/02 02:59:30 PM | Valid: [63/200] Step 000/078 Loss 1.074 Prec@(1,5) (75.0%, 91.4%)
06/02 02:59:32 PM | Valid: [63/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.3%)
06/02 02:59:32 PM | Valid: [63/200] Final Prec@1 70.8200%
06/02 02:59:33 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 02:59:33 PM | Train: [64/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 02:59:43 PM | Train: [64/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 02:59:53 PM | Train: [64/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:00:04 PM | Train: [64/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:00:13 PM | Train: [64/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:00:13 PM | Train: [64/200] Final Prec@1 99.9700%
06/02 03:00:13 PM | Valid: [64/200] Step 000/078 Loss 1.067 Prec@(1,5) (76.6%, 91.4%)
06/02 03:00:16 PM | Valid: [64/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.8%, 90.2%)
06/02 03:00:16 PM | Valid: [64/200] Final Prec@1 70.7600%
06/02 03:00:16 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:00:17 PM | Train: [65/200] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/02 03:00:27 PM | Train: [65/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:00:37 PM | Train: [65/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:00:48 PM | Train: [65/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:00:57 PM | Train: [65/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:00:57 PM | Train: [65/200] Final Prec@1 99.9700%
06/02 03:00:57 PM | Valid: [65/200] Step 000/078 Loss 1.088 Prec@(1,5) (75.8%, 91.4%)
06/02 03:01:00 PM | Valid: [65/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.4%)
06/02 03:01:00 PM | Valid: [65/200] Final Prec@1 70.8000%
06/02 03:01:01 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:01:01 PM | Train: [66/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:01:11 PM | Train: [66/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:01:22 PM | Train: [66/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:01:33 PM | Train: [66/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:01:42 PM | Train: [66/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:01:42 PM | Train: [66/200] Final Prec@1 99.9580%
06/02 03:01:42 PM | Valid: [66/200] Step 000/078 Loss 1.071 Prec@(1,5) (76.6%, 91.4%)
06/02 03:01:45 PM | Valid: [66/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.2%)
06/02 03:01:45 PM | Valid: [66/200] Final Prec@1 70.7900%
06/02 03:01:45 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:01:46 PM | Train: [67/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:01:56 PM | Train: [67/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:02:07 PM | Train: [67/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:02:17 PM | Train: [67/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:02:26 PM | Train: [67/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:02:26 PM | Train: [67/200] Final Prec@1 99.9720%
06/02 03:02:26 PM | Valid: [67/200] Step 000/078 Loss 1.072 Prec@(1,5) (74.2%, 91.4%)
06/02 03:02:29 PM | Valid: [67/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.8%, 90.3%)
06/02 03:02:29 PM | Valid: [67/200] Final Prec@1 70.8000%
06/02 03:02:29 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:02:30 PM | Train: [68/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:02:40 PM | Train: [68/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:02:51 PM | Train: [68/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:03:01 PM | Train: [68/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:03:10 PM | Train: [68/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:03:11 PM | Train: [68/200] Final Prec@1 99.9660%
06/02 03:03:11 PM | Valid: [68/200] Step 000/078 Loss 1.079 Prec@(1,5) (75.8%, 92.2%)
06/02 03:03:13 PM | Valid: [68/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.7%, 90.2%)
06/02 03:03:13 PM | Valid: [68/200] Final Prec@1 70.7400%
06/02 03:03:14 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:03:14 PM | Train: [69/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 03:03:25 PM | Train: [69/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:03:35 PM | Train: [69/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:03:46 PM | Train: [69/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:03:56 PM | Train: [69/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:03:56 PM | Train: [69/200] Final Prec@1 99.9740%
06/02 03:03:56 PM | Valid: [69/200] Step 000/078 Loss 1.084 Prec@(1,5) (75.0%, 91.4%)
06/02 03:03:59 PM | Valid: [69/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.7%, 90.3%)
06/02 03:03:59 PM | Valid: [69/200] Final Prec@1 70.6500%
06/02 03:03:59 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:04:00 PM | Train: [70/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:04:10 PM | Train: [70/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:04:20 PM | Train: [70/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:04:31 PM | Train: [70/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:04:40 PM | Train: [70/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:04:40 PM | Train: [70/200] Final Prec@1 99.9720%
06/02 03:04:41 PM | Valid: [70/200] Step 000/078 Loss 1.088 Prec@(1,5) (74.2%, 92.2%)
06/02 03:04:43 PM | Valid: [70/200] Step 078/078 Loss 1.268 Prec@(1,5) (71.0%, 90.3%)
06/02 03:04:43 PM | Valid: [70/200] Final Prec@1 71.0000%
06/02 03:04:44 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:04:44 PM | Train: [71/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:04:55 PM | Train: [71/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 03:05:05 PM | Train: [71/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 03:05:16 PM | Train: [71/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:05:25 PM | Train: [71/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:05:25 PM | Train: [71/200] Final Prec@1 99.9660%
06/02 03:05:25 PM | Valid: [71/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.8%, 91.4%)
06/02 03:05:28 PM | Valid: [71/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.8%, 90.3%)
06/02 03:05:28 PM | Valid: [71/200] Final Prec@1 70.7600%
06/02 03:05:28 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:05:29 PM | Train: [72/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:05:39 PM | Train: [72/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:05:50 PM | Train: [72/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:06:00 PM | Train: [72/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:06:10 PM | Train: [72/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:06:10 PM | Train: [72/200] Final Prec@1 99.9740%
06/02 03:06:10 PM | Valid: [72/200] Step 000/078 Loss 1.074 Prec@(1,5) (75.0%, 92.2%)
06/02 03:06:12 PM | Valid: [72/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.9%, 90.3%)
06/02 03:06:12 PM | Valid: [72/200] Final Prec@1 70.9000%
06/02 03:06:13 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:06:13 PM | Train: [73/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 03:06:24 PM | Train: [73/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:06:35 PM | Train: [73/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:06:46 PM | Train: [73/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:06:55 PM | Train: [73/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:06:55 PM | Train: [73/200] Final Prec@1 99.9800%
06/02 03:06:55 PM | Valid: [73/200] Step 000/078 Loss 1.083 Prec@(1,5) (75.8%, 91.4%)
06/02 03:06:58 PM | Valid: [73/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.7%, 90.2%)
06/02 03:06:58 PM | Valid: [73/200] Final Prec@1 70.7000%
06/02 03:06:58 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:06:59 PM | Train: [74/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:07:09 PM | Train: [74/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:07:19 PM | Train: [74/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:07:30 PM | Train: [74/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:07:39 PM | Train: [74/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:07:39 PM | Train: [74/200] Final Prec@1 99.9680%
06/02 03:07:40 PM | Valid: [74/200] Step 000/078 Loss 1.076 Prec@(1,5) (75.8%, 91.4%)
06/02 03:07:42 PM | Valid: [74/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.8%, 90.2%)
06/02 03:07:42 PM | Valid: [74/200] Final Prec@1 70.8000%
06/02 03:07:42 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:07:43 PM | Train: [75/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:07:53 PM | Train: [75/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:08:03 PM | Train: [75/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:08:14 PM | Train: [75/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:08:23 PM | Train: [75/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:08:23 PM | Train: [75/200] Final Prec@1 99.9660%
06/02 03:08:24 PM | Valid: [75/200] Step 000/078 Loss 1.085 Prec@(1,5) (75.0%, 92.2%)
06/02 03:08:26 PM | Valid: [75/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.8%, 90.3%)
06/02 03:08:26 PM | Valid: [75/200] Final Prec@1 70.8400%
06/02 03:08:27 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:08:27 PM | Train: [76/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:08:37 PM | Train: [76/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:08:47 PM | Train: [76/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:08:58 PM | Train: [76/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:09:07 PM | Train: [76/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:09:07 PM | Train: [76/200] Final Prec@1 99.9760%
06/02 03:09:07 PM | Valid: [76/200] Step 000/078 Loss 1.100 Prec@(1,5) (75.0%, 91.4%)
06/02 03:09:10 PM | Valid: [76/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.8%, 90.3%)
06/02 03:09:10 PM | Valid: [76/200] Final Prec@1 70.8400%
06/02 03:09:10 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:09:11 PM | Train: [77/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:09:21 PM | Train: [77/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:09:31 PM | Train: [77/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:09:42 PM | Train: [77/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:09:51 PM | Train: [77/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:09:51 PM | Train: [77/200] Final Prec@1 99.9780%
06/02 03:09:51 PM | Valid: [77/200] Step 000/078 Loss 1.082 Prec@(1,5) (75.0%, 91.4%)
06/02 03:09:53 PM | Valid: [77/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.6%, 90.3%)
06/02 03:09:53 PM | Valid: [77/200] Final Prec@1 70.6300%
06/02 03:09:54 PM | Current best Prec@1 = 71.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:09:54 PM | Train: [78/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:10:05 PM | Train: [78/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:10:15 PM | Train: [78/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:10:25 PM | Train: [78/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:10:35 PM | Train: [78/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:10:35 PM | Train: [78/200] Final Prec@1 99.9800%
06/02 03:10:35 PM | Valid: [78/200] Step 000/078 Loss 1.091 Prec@(1,5) (75.0%, 91.4%)
06/02 03:10:37 PM | Valid: [78/200] Step 078/078 Loss 1.266 Prec@(1,5) (71.1%, 90.3%)
06/02 03:10:37 PM | Valid: [78/200] Final Prec@1 71.0700%
06/02 03:10:38 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:10:38 PM | Train: [79/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:10:48 PM | Train: [79/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:10:59 PM | Train: [79/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:11:09 PM | Train: [79/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:11:18 PM | Train: [79/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:11:18 PM | Train: [79/200] Final Prec@1 99.9660%
06/02 03:11:19 PM | Valid: [79/200] Step 000/078 Loss 1.068 Prec@(1,5) (75.8%, 92.2%)
06/02 03:11:21 PM | Valid: [79/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.7%, 90.3%)
06/02 03:11:21 PM | Valid: [79/200] Final Prec@1 70.7200%
06/02 03:11:22 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:11:22 PM | Train: [80/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:11:32 PM | Train: [80/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:11:43 PM | Train: [80/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:11:53 PM | Train: [80/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:12:02 PM | Train: [80/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:12:02 PM | Train: [80/200] Final Prec@1 99.9640%
06/02 03:12:02 PM | Valid: [80/200] Step 000/078 Loss 1.092 Prec@(1,5) (75.8%, 91.4%)
06/02 03:12:05 PM | Valid: [80/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.3%)
06/02 03:12:05 PM | Valid: [80/200] Final Prec@1 70.8600%
06/02 03:12:05 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:12:06 PM | Train: [81/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:12:16 PM | Train: [81/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:12:26 PM | Train: [81/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:12:36 PM | Train: [81/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:12:45 PM | Train: [81/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:12:45 PM | Train: [81/200] Final Prec@1 99.9680%
06/02 03:12:45 PM | Valid: [81/200] Step 000/078 Loss 1.094 Prec@(1,5) (75.8%, 92.2%)
06/02 03:12:48 PM | Valid: [81/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.9%, 90.3%)
06/02 03:12:48 PM | Valid: [81/200] Final Prec@1 70.8600%
06/02 03:12:48 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:12:49 PM | Train: [82/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:12:59 PM | Train: [82/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:13:09 PM | Train: [82/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:13:19 PM | Train: [82/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:13:28 PM | Train: [82/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:13:28 PM | Train: [82/200] Final Prec@1 99.9720%
06/02 03:13:29 PM | Valid: [82/200] Step 000/078 Loss 1.076 Prec@(1,5) (75.8%, 92.2%)
06/02 03:13:31 PM | Valid: [82/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.2%)
06/02 03:13:31 PM | Valid: [82/200] Final Prec@1 70.7700%
06/02 03:13:32 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:13:32 PM | Train: [83/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:13:43 PM | Train: [83/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 03:13:53 PM | Train: [83/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:14:03 PM | Train: [83/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:14:13 PM | Train: [83/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:14:13 PM | Train: [83/200] Final Prec@1 99.9680%
06/02 03:14:13 PM | Valid: [83/200] Step 000/078 Loss 1.059 Prec@(1,5) (76.6%, 91.4%)
06/02 03:14:16 PM | Valid: [83/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.8%, 90.2%)
06/02 03:14:16 PM | Valid: [83/200] Final Prec@1 70.8100%
06/02 03:14:16 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:14:17 PM | Train: [84/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:14:27 PM | Train: [84/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:14:37 PM | Train: [84/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:14:48 PM | Train: [84/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:14:57 PM | Train: [84/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:14:57 PM | Train: [84/200] Final Prec@1 99.9780%
06/02 03:14:57 PM | Valid: [84/200] Step 000/078 Loss 1.091 Prec@(1,5) (74.2%, 92.2%)
06/02 03:14:59 PM | Valid: [84/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.9%, 90.3%)
06/02 03:14:59 PM | Valid: [84/200] Final Prec@1 70.8800%
06/02 03:15:00 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:15:00 PM | Train: [85/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:15:11 PM | Train: [85/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:15:21 PM | Train: [85/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:15:31 PM | Train: [85/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:15:40 PM | Train: [85/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:15:40 PM | Train: [85/200] Final Prec@1 99.9880%
06/02 03:15:41 PM | Valid: [85/200] Step 000/078 Loss 1.074 Prec@(1,5) (75.8%, 91.4%)
06/02 03:15:43 PM | Valid: [85/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.3%)
06/02 03:15:43 PM | Valid: [85/200] Final Prec@1 70.7800%
06/02 03:15:44 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:15:44 PM | Train: [86/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:15:54 PM | Train: [86/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:16:04 PM | Train: [86/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:16:15 PM | Train: [86/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:16:24 PM | Train: [86/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:16:24 PM | Train: [86/200] Final Prec@1 99.9720%
06/02 03:16:24 PM | Valid: [86/200] Step 000/078 Loss 1.055 Prec@(1,5) (75.0%, 91.4%)
06/02 03:16:26 PM | Valid: [86/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.3%)
06/02 03:16:26 PM | Valid: [86/200] Final Prec@1 70.9100%
06/02 03:16:27 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:16:27 PM | Train: [87/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 03:16:38 PM | Train: [87/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:16:48 PM | Train: [87/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:16:58 PM | Train: [87/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:17:08 PM | Train: [87/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:17:08 PM | Train: [87/200] Final Prec@1 99.9820%
06/02 03:17:08 PM | Valid: [87/200] Step 000/078 Loss 1.093 Prec@(1,5) (75.8%, 92.2%)
06/02 03:17:10 PM | Valid: [87/200] Step 078/078 Loss 1.266 Prec@(1,5) (71.0%, 90.4%)
06/02 03:17:10 PM | Valid: [87/200] Final Prec@1 70.9700%
06/02 03:17:11 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:17:11 PM | Train: [88/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 03:17:22 PM | Train: [88/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:17:32 PM | Train: [88/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:17:43 PM | Train: [88/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:17:52 PM | Train: [88/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:17:52 PM | Train: [88/200] Final Prec@1 99.9660%
06/02 03:17:52 PM | Valid: [88/200] Step 000/078 Loss 1.080 Prec@(1,5) (74.2%, 92.2%)
06/02 03:17:55 PM | Valid: [88/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.7%, 90.4%)
06/02 03:17:55 PM | Valid: [88/200] Final Prec@1 70.7400%
06/02 03:17:55 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:17:56 PM | Train: [89/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:18:06 PM | Train: [89/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:18:16 PM | Train: [89/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:18:27 PM | Train: [89/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:18:36 PM | Train: [89/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:18:36 PM | Train: [89/200] Final Prec@1 99.9760%
06/02 03:18:36 PM | Valid: [89/200] Step 000/078 Loss 1.072 Prec@(1,5) (75.8%, 92.2%)
06/02 03:18:38 PM | Valid: [89/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.7%, 90.3%)
06/02 03:18:39 PM | Valid: [89/200] Final Prec@1 70.7200%
06/02 03:18:39 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:18:39 PM | Train: [90/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:18:50 PM | Train: [90/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:19:00 PM | Train: [90/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:19:10 PM | Train: [90/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:19:20 PM | Train: [90/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:19:20 PM | Train: [90/200] Final Prec@1 99.9820%
06/02 03:19:20 PM | Valid: [90/200] Step 000/078 Loss 1.082 Prec@(1,5) (74.2%, 92.2%)
06/02 03:19:22 PM | Valid: [90/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 03:19:22 PM | Valid: [90/200] Final Prec@1 70.8900%
06/02 03:19:23 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:19:23 PM | Train: [91/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:19:33 PM | Train: [91/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:19:44 PM | Train: [91/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:19:54 PM | Train: [91/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:20:03 PM | Train: [91/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:20:04 PM | Train: [91/200] Final Prec@1 99.9700%
06/02 03:20:04 PM | Valid: [91/200] Step 000/078 Loss 1.080 Prec@(1,5) (75.0%, 92.2%)
06/02 03:20:06 PM | Valid: [91/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.3%)
06/02 03:20:06 PM | Valid: [91/200] Final Prec@1 70.8900%
06/02 03:20:07 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:20:07 PM | Train: [92/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:20:17 PM | Train: [92/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:20:28 PM | Train: [92/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:20:38 PM | Train: [92/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:20:47 PM | Train: [92/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:20:47 PM | Train: [92/200] Final Prec@1 99.9720%
06/02 03:20:48 PM | Valid: [92/200] Step 000/078 Loss 1.090 Prec@(1,5) (75.8%, 91.4%)
06/02 03:20:50 PM | Valid: [92/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.2%)
06/02 03:20:50 PM | Valid: [92/200] Final Prec@1 70.8100%
06/02 03:20:50 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:20:51 PM | Train: [93/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:21:01 PM | Train: [93/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:21:12 PM | Train: [93/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:21:22 PM | Train: [93/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:21:31 PM | Train: [93/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:21:31 PM | Train: [93/200] Final Prec@1 99.9720%
06/02 03:21:31 PM | Valid: [93/200] Step 000/078 Loss 1.093 Prec@(1,5) (75.8%, 92.2%)
06/02 03:21:34 PM | Valid: [93/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.3%)
06/02 03:21:34 PM | Valid: [93/200] Final Prec@1 70.8400%
06/02 03:21:34 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:21:35 PM | Train: [94/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:21:45 PM | Train: [94/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:21:55 PM | Train: [94/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:22:05 PM | Train: [94/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:22:14 PM | Train: [94/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:22:15 PM | Train: [94/200] Final Prec@1 99.9760%
06/02 03:22:15 PM | Valid: [94/200] Step 000/078 Loss 1.072 Prec@(1,5) (75.8%, 91.4%)
06/02 03:22:17 PM | Valid: [94/200] Step 078/078 Loss 1.260 Prec@(1,5) (71.0%, 90.3%)
06/02 03:22:17 PM | Valid: [94/200] Final Prec@1 70.9500%
06/02 03:22:18 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:22:18 PM | Train: [95/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:22:28 PM | Train: [95/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:22:39 PM | Train: [95/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:22:49 PM | Train: [95/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:22:58 PM | Train: [95/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:22:58 PM | Train: [95/200] Final Prec@1 99.9740%
06/02 03:22:58 PM | Valid: [95/200] Step 000/078 Loss 1.056 Prec@(1,5) (75.8%, 91.4%)
06/02 03:23:01 PM | Valid: [95/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 03:23:01 PM | Valid: [95/200] Final Prec@1 70.9300%
06/02 03:23:01 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:23:02 PM | Train: [96/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 03:23:12 PM | Train: [96/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:23:22 PM | Train: [96/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:23:33 PM | Train: [96/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:23:42 PM | Train: [96/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:23:42 PM | Train: [96/200] Final Prec@1 99.9700%
06/02 03:23:42 PM | Valid: [96/200] Step 000/078 Loss 1.065 Prec@(1,5) (76.6%, 92.2%)
06/02 03:23:44 PM | Valid: [96/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.3%)
06/02 03:23:44 PM | Valid: [96/200] Final Prec@1 70.9600%
06/02 03:23:45 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:23:45 PM | Train: [97/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:23:55 PM | Train: [97/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:24:06 PM | Train: [97/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:24:16 PM | Train: [97/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:24:25 PM | Train: [97/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:24:25 PM | Train: [97/200] Final Prec@1 99.9720%
06/02 03:24:26 PM | Valid: [97/200] Step 000/078 Loss 1.060 Prec@(1,5) (75.8%, 91.4%)
06/02 03:24:28 PM | Valid: [97/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.2%)
06/02 03:24:28 PM | Valid: [97/200] Final Prec@1 70.9800%
06/02 03:24:29 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:24:29 PM | Train: [98/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:24:39 PM | Train: [98/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:24:50 PM | Train: [98/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:25:00 PM | Train: [98/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:25:10 PM | Train: [98/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:25:10 PM | Train: [98/200] Final Prec@1 99.9740%
06/02 03:25:10 PM | Valid: [98/200] Step 000/078 Loss 1.076 Prec@(1,5) (75.8%, 91.4%)
06/02 03:25:12 PM | Valid: [98/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.9%, 90.2%)
06/02 03:25:12 PM | Valid: [98/200] Final Prec@1 70.9200%
06/02 03:25:13 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:25:13 PM | Train: [99/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:25:24 PM | Train: [99/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:25:34 PM | Train: [99/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:25:45 PM | Train: [99/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:25:54 PM | Train: [99/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:25:54 PM | Train: [99/200] Final Prec@1 99.9760%
06/02 03:25:54 PM | Valid: [99/200] Step 000/078 Loss 1.061 Prec@(1,5) (75.8%, 92.2%)
06/02 03:25:57 PM | Valid: [99/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.3%)
06/02 03:25:57 PM | Valid: [99/200] Final Prec@1 70.7800%
06/02 03:25:57 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:25:58 PM | Train: [100/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:26:08 PM | Train: [100/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:26:19 PM | Train: [100/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:26:30 PM | Train: [100/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:26:39 PM | Train: [100/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:26:39 PM | Train: [100/200] Final Prec@1 99.9700%
06/02 03:26:39 PM | Valid: [100/200] Step 000/078 Loss 1.081 Prec@(1,5) (75.8%, 91.4%)
06/02 03:26:42 PM | Valid: [100/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.9%, 90.2%)
06/02 03:26:42 PM | Valid: [100/200] Final Prec@1 70.8800%
06/02 03:26:42 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:26:43 PM | Train: [101/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:26:53 PM | Train: [101/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:27:03 PM | Train: [101/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:27:13 PM | Train: [101/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:27:22 PM | Train: [101/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:27:22 PM | Train: [101/200] Final Prec@1 99.9740%
06/02 03:27:23 PM | Valid: [101/200] Step 000/078 Loss 1.053 Prec@(1,5) (75.8%, 92.2%)
06/02 03:27:25 PM | Valid: [101/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.9%, 90.2%)
06/02 03:27:25 PM | Valid: [101/200] Final Prec@1 70.9000%
06/02 03:27:26 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:27:26 PM | Train: [102/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:27:36 PM | Train: [102/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:27:46 PM | Train: [102/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:27:57 PM | Train: [102/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:28:06 PM | Train: [102/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:28:06 PM | Train: [102/200] Final Prec@1 99.9700%
06/02 03:28:06 PM | Valid: [102/200] Step 000/078 Loss 1.077 Prec@(1,5) (76.6%, 91.4%)
06/02 03:28:09 PM | Valid: [102/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.0%, 90.2%)
06/02 03:28:09 PM | Valid: [102/200] Final Prec@1 70.9800%
06/02 03:28:09 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:28:10 PM | Train: [103/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:28:20 PM | Train: [103/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:28:30 PM | Train: [103/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:28:41 PM | Train: [103/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:28:50 PM | Train: [103/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:28:50 PM | Train: [103/200] Final Prec@1 99.9740%
06/02 03:28:51 PM | Valid: [103/200] Step 000/078 Loss 1.073 Prec@(1,5) (75.8%, 92.2%)
06/02 03:28:53 PM | Valid: [103/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.9%, 90.2%)
06/02 03:28:53 PM | Valid: [103/200] Final Prec@1 70.8600%
06/02 03:28:54 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:28:54 PM | Train: [104/200] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 03:29:04 PM | Train: [104/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:29:14 PM | Train: [104/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:29:24 PM | Train: [104/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:29:34 PM | Train: [104/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:29:34 PM | Train: [104/200] Final Prec@1 99.9560%
06/02 03:29:34 PM | Valid: [104/200] Step 000/078 Loss 1.053 Prec@(1,5) (76.6%, 91.4%)
06/02 03:29:37 PM | Valid: [104/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.9%, 90.2%)
06/02 03:29:37 PM | Valid: [104/200] Final Prec@1 70.9300%
06/02 03:29:37 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:29:38 PM | Train: [105/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:29:48 PM | Train: [105/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:29:58 PM | Train: [105/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:30:09 PM | Train: [105/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:30:17 PM | Train: [105/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:30:17 PM | Train: [105/200] Final Prec@1 99.9720%
06/02 03:30:17 PM | Valid: [105/200] Step 000/078 Loss 1.092 Prec@(1,5) (75.8%, 92.2%)
06/02 03:30:20 PM | Valid: [105/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 03:30:20 PM | Valid: [105/200] Final Prec@1 70.8600%
06/02 03:30:20 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:30:21 PM | Train: [106/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:30:31 PM | Train: [106/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:30:41 PM | Train: [106/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:30:51 PM | Train: [106/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:31:00 PM | Train: [106/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:31:00 PM | Train: [106/200] Final Prec@1 99.9760%
06/02 03:31:01 PM | Valid: [106/200] Step 000/078 Loss 1.067 Prec@(1,5) (75.0%, 91.4%)
06/02 03:31:03 PM | Valid: [106/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.8%, 90.2%)
06/02 03:31:03 PM | Valid: [106/200] Final Prec@1 70.8100%
06/02 03:31:04 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:31:04 PM | Train: [107/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:31:14 PM | Train: [107/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:31:24 PM | Train: [107/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:31:34 PM | Train: [107/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:31:44 PM | Train: [107/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:31:44 PM | Train: [107/200] Final Prec@1 99.9660%
06/02 03:31:44 PM | Valid: [107/200] Step 000/078 Loss 1.071 Prec@(1,5) (76.6%, 91.4%)
06/02 03:31:47 PM | Valid: [107/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.3%)
06/02 03:31:47 PM | Valid: [107/200] Final Prec@1 70.9200%
06/02 03:31:47 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:31:48 PM | Train: [108/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:31:58 PM | Train: [108/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:32:08 PM | Train: [108/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:32:19 PM | Train: [108/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:32:28 PM | Train: [108/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:32:28 PM | Train: [108/200] Final Prec@1 99.9700%
06/02 03:32:28 PM | Valid: [108/200] Step 000/078 Loss 1.098 Prec@(1,5) (75.8%, 92.2%)
06/02 03:32:31 PM | Valid: [108/200] Step 078/078 Loss 1.265 Prec@(1,5) (71.0%, 90.3%)
06/02 03:32:31 PM | Valid: [108/200] Final Prec@1 70.9500%
06/02 03:32:31 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:32:32 PM | Train: [109/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:32:42 PM | Train: [109/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 03:32:52 PM | Train: [109/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:33:02 PM | Train: [109/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:33:12 PM | Train: [109/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:33:12 PM | Train: [109/200] Final Prec@1 99.9660%
06/02 03:33:12 PM | Valid: [109/200] Step 000/078 Loss 1.047 Prec@(1,5) (75.8%, 91.4%)
06/02 03:33:14 PM | Valid: [109/200] Step 078/078 Loss 1.265 Prec@(1,5) (71.0%, 90.2%)
06/02 03:33:14 PM | Valid: [109/200] Final Prec@1 70.9800%
06/02 03:33:15 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:33:15 PM | Train: [110/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:33:26 PM | Train: [110/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:33:36 PM | Train: [110/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:33:46 PM | Train: [110/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:33:56 PM | Train: [110/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:33:56 PM | Train: [110/200] Final Prec@1 99.9720%
06/02 03:33:56 PM | Valid: [110/200] Step 000/078 Loss 1.066 Prec@(1,5) (75.0%, 91.4%)
06/02 03:33:58 PM | Valid: [110/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.2%)
06/02 03:33:58 PM | Valid: [110/200] Final Prec@1 70.9500%
06/02 03:33:59 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:33:59 PM | Train: [111/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:34:10 PM | Train: [111/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:34:20 PM | Train: [111/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:34:31 PM | Train: [111/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:34:40 PM | Train: [111/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:34:40 PM | Train: [111/200] Final Prec@1 99.9640%
06/02 03:34:40 PM | Valid: [111/200] Step 000/078 Loss 1.098 Prec@(1,5) (75.0%, 91.4%)
06/02 03:34:43 PM | Valid: [111/200] Step 078/078 Loss 1.270 Prec@(1,5) (70.7%, 90.0%)
06/02 03:34:43 PM | Valid: [111/200] Final Prec@1 70.6700%
06/02 03:34:43 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:34:44 PM | Train: [112/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:34:54 PM | Train: [112/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:35:04 PM | Train: [112/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:35:14 PM | Train: [112/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:35:24 PM | Train: [112/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:35:24 PM | Train: [112/200] Final Prec@1 99.9700%
06/02 03:35:24 PM | Valid: [112/200] Step 000/078 Loss 1.085 Prec@(1,5) (75.8%, 91.4%)
06/02 03:35:27 PM | Valid: [112/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.1%)
06/02 03:35:27 PM | Valid: [112/200] Final Prec@1 70.7600%
06/02 03:35:27 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:35:28 PM | Train: [113/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:35:38 PM | Train: [113/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:35:48 PM | Train: [113/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:35:58 PM | Train: [113/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:36:06 PM | Train: [113/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:36:06 PM | Train: [113/200] Final Prec@1 99.9700%
06/02 03:36:07 PM | Valid: [113/200] Step 000/078 Loss 1.087 Prec@(1,5) (75.0%, 91.4%)
06/02 03:36:09 PM | Valid: [113/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.3%)
06/02 03:36:09 PM | Valid: [113/200] Final Prec@1 70.8800%
06/02 03:36:10 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:36:10 PM | Train: [114/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:36:20 PM | Train: [114/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:36:30 PM | Train: [114/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:36:41 PM | Train: [114/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:36:50 PM | Train: [114/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:36:50 PM | Train: [114/200] Final Prec@1 99.9760%
06/02 03:36:50 PM | Valid: [114/200] Step 000/078 Loss 1.083 Prec@(1,5) (75.8%, 92.2%)
06/02 03:36:53 PM | Valid: [114/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.9%, 90.2%)
06/02 03:36:53 PM | Valid: [114/200] Final Prec@1 70.8900%
06/02 03:36:54 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:36:54 PM | Train: [115/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:37:04 PM | Train: [115/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:37:14 PM | Train: [115/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:37:25 PM | Train: [115/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:37:33 PM | Train: [115/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:37:33 PM | Train: [115/200] Final Prec@1 99.9720%
06/02 03:37:33 PM | Valid: [115/200] Step 000/078 Loss 1.089 Prec@(1,5) (76.6%, 91.4%)
06/02 03:37:36 PM | Valid: [115/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 03:37:36 PM | Valid: [115/200] Final Prec@1 70.9400%
06/02 03:37:36 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:37:37 PM | Train: [116/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:37:47 PM | Train: [116/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:37:58 PM | Train: [116/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:38:08 PM | Train: [116/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:38:17 PM | Train: [116/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:38:18 PM | Train: [116/200] Final Prec@1 99.9580%
06/02 03:38:18 PM | Valid: [116/200] Step 000/078 Loss 1.083 Prec@(1,5) (75.0%, 91.4%)
06/02 03:38:20 PM | Valid: [116/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.8%, 90.3%)
06/02 03:38:20 PM | Valid: [116/200] Final Prec@1 70.7900%
06/02 03:38:21 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:38:21 PM | Train: [117/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:38:31 PM | Train: [117/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:38:41 PM | Train: [117/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:38:51 PM | Train: [117/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:39:01 PM | Train: [117/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:39:01 PM | Train: [117/200] Final Prec@1 99.9760%
06/02 03:39:01 PM | Valid: [117/200] Step 000/078 Loss 1.072 Prec@(1,5) (75.8%, 92.2%)
06/02 03:39:03 PM | Valid: [117/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.4%)
06/02 03:39:04 PM | Valid: [117/200] Final Prec@1 70.8700%
06/02 03:39:04 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:39:05 PM | Train: [118/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:39:15 PM | Train: [118/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 03:39:25 PM | Train: [118/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:39:35 PM | Train: [118/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:39:44 PM | Train: [118/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:39:44 PM | Train: [118/200] Final Prec@1 99.9660%
06/02 03:39:44 PM | Valid: [118/200] Step 000/078 Loss 1.084 Prec@(1,5) (75.8%, 91.4%)
06/02 03:39:47 PM | Valid: [118/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.2%)
06/02 03:39:47 PM | Valid: [118/200] Final Prec@1 71.0200%
06/02 03:39:47 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:39:48 PM | Train: [119/200] Step 000/390 Loss 0.009 Prec@(1,5) (99.2%, 100.0%)
06/02 03:39:57 PM | Train: [119/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:40:07 PM | Train: [119/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:40:17 PM | Train: [119/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:40:26 PM | Train: [119/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:40:26 PM | Train: [119/200] Final Prec@1 99.9740%
06/02 03:40:27 PM | Valid: [119/200] Step 000/078 Loss 1.083 Prec@(1,5) (75.8%, 91.4%)
06/02 03:40:29 PM | Valid: [119/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.8%, 90.3%)
06/02 03:40:29 PM | Valid: [119/200] Final Prec@1 70.8400%
06/02 03:40:30 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:40:30 PM | Train: [120/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:40:40 PM | Train: [120/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:40:50 PM | Train: [120/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:41:01 PM | Train: [120/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:41:10 PM | Train: [120/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:41:10 PM | Train: [120/200] Final Prec@1 99.9680%
06/02 03:41:11 PM | Valid: [120/200] Step 000/078 Loss 1.082 Prec@(1,5) (76.6%, 92.2%)
06/02 03:41:13 PM | Valid: [120/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.7%, 90.3%)
06/02 03:41:13 PM | Valid: [120/200] Final Prec@1 70.7000%
06/02 03:41:14 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:41:14 PM | Train: [121/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:41:24 PM | Train: [121/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:41:35 PM | Train: [121/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:41:45 PM | Train: [121/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:41:55 PM | Train: [121/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:41:55 PM | Train: [121/200] Final Prec@1 99.9620%
06/02 03:41:55 PM | Valid: [121/200] Step 000/078 Loss 1.063 Prec@(1,5) (76.6%, 91.4%)
06/02 03:41:58 PM | Valid: [121/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.8%, 90.2%)
06/02 03:41:58 PM | Valid: [121/200] Final Prec@1 70.7500%
06/02 03:41:58 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:41:59 PM | Train: [122/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:42:09 PM | Train: [122/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:42:19 PM | Train: [122/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:42:29 PM | Train: [122/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:42:39 PM | Train: [122/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:42:39 PM | Train: [122/200] Final Prec@1 99.9740%
06/02 03:42:39 PM | Valid: [122/200] Step 000/078 Loss 1.081 Prec@(1,5) (75.8%, 91.4%)
06/02 03:42:41 PM | Valid: [122/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.3%)
06/02 03:42:41 PM | Valid: [122/200] Final Prec@1 70.8500%
06/02 03:42:42 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:42:42 PM | Train: [123/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 03:42:52 PM | Train: [123/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:43:02 PM | Train: [123/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:43:13 PM | Train: [123/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:43:22 PM | Train: [123/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:43:22 PM | Train: [123/200] Final Prec@1 99.9700%
06/02 03:43:22 PM | Valid: [123/200] Step 000/078 Loss 1.080 Prec@(1,5) (76.6%, 91.4%)
06/02 03:43:25 PM | Valid: [123/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.3%)
06/02 03:43:25 PM | Valid: [123/200] Final Prec@1 70.8400%
06/02 03:43:25 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:43:26 PM | Train: [124/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:43:36 PM | Train: [124/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:43:46 PM | Train: [124/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:43:57 PM | Train: [124/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:44:06 PM | Train: [124/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:44:06 PM | Train: [124/200] Final Prec@1 99.9780%
06/02 03:44:06 PM | Valid: [124/200] Step 000/078 Loss 1.074 Prec@(1,5) (75.8%, 91.4%)
06/02 03:44:09 PM | Valid: [124/200] Step 078/078 Loss 1.261 Prec@(1,5) (71.0%, 90.3%)
06/02 03:44:09 PM | Valid: [124/200] Final Prec@1 70.9600%
06/02 03:44:09 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:44:10 PM | Train: [125/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:44:20 PM | Train: [125/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:44:31 PM | Train: [125/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:44:41 PM | Train: [125/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:44:51 PM | Train: [125/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:44:51 PM | Train: [125/200] Final Prec@1 99.9760%
06/02 03:44:51 PM | Valid: [125/200] Step 000/078 Loss 1.068 Prec@(1,5) (75.8%, 92.2%)
06/02 03:44:53 PM | Valid: [125/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.9%, 90.2%)
06/02 03:44:53 PM | Valid: [125/200] Final Prec@1 70.8900%
06/02 03:44:54 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:44:54 PM | Train: [126/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:45:05 PM | Train: [126/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:45:14 PM | Train: [126/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:45:25 PM | Train: [126/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:45:34 PM | Train: [126/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:45:34 PM | Train: [126/200] Final Prec@1 99.9720%
06/02 03:45:34 PM | Valid: [126/200] Step 000/078 Loss 1.066 Prec@(1,5) (75.8%, 91.4%)
06/02 03:45:37 PM | Valid: [126/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.2%)
06/02 03:45:37 PM | Valid: [126/200] Final Prec@1 70.7900%
06/02 03:45:37 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:45:38 PM | Train: [127/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:45:48 PM | Train: [127/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:45:58 PM | Train: [127/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:46:08 PM | Train: [127/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:46:17 PM | Train: [127/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:46:17 PM | Train: [127/200] Final Prec@1 99.9780%
06/02 03:46:17 PM | Valid: [127/200] Step 000/078 Loss 1.085 Prec@(1,5) (75.0%, 91.4%)
06/02 03:46:20 PM | Valid: [127/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.9%, 90.2%)
06/02 03:46:20 PM | Valid: [127/200] Final Prec@1 70.9100%
06/02 03:46:20 PM | Current best Prec@1 = 71.0700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:46:21 PM | Train: [128/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:46:31 PM | Train: [128/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:46:41 PM | Train: [128/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:46:52 PM | Train: [128/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:47:01 PM | Train: [128/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:47:01 PM | Train: [128/200] Final Prec@1 99.9720%
06/02 03:47:01 PM | Valid: [128/200] Step 000/078 Loss 1.066 Prec@(1,5) (76.6%, 91.4%)
06/02 03:47:04 PM | Valid: [128/200] Step 078/078 Loss 1.261 Prec@(1,5) (71.1%, 90.2%)
06/02 03:47:04 PM | Valid: [128/200] Final Prec@1 71.1000%
06/02 03:47:04 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:47:05 PM | Train: [129/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:47:15 PM | Train: [129/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:47:26 PM | Train: [129/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:47:36 PM | Train: [129/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:47:45 PM | Train: [129/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:47:45 PM | Train: [129/200] Final Prec@1 99.9760%
06/02 03:47:45 PM | Valid: [129/200] Step 000/078 Loss 1.084 Prec@(1,5) (75.8%, 92.2%)
06/02 03:47:48 PM | Valid: [129/200] Step 078/078 Loss 1.266 Prec@(1,5) (71.0%, 90.2%)
06/02 03:47:48 PM | Valid: [129/200] Final Prec@1 70.9800%
06/02 03:47:48 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:47:49 PM | Train: [130/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:47:59 PM | Train: [130/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:48:09 PM | Train: [130/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:48:19 PM | Train: [130/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:48:28 PM | Train: [130/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:48:28 PM | Train: [130/200] Final Prec@1 99.9780%
06/02 03:48:28 PM | Valid: [130/200] Step 000/078 Loss 1.067 Prec@(1,5) (75.0%, 91.4%)
06/02 03:48:30 PM | Valid: [130/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.2%)
06/02 03:48:30 PM | Valid: [130/200] Final Prec@1 70.8200%
06/02 03:48:31 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:48:31 PM | Train: [131/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:48:41 PM | Train: [131/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:48:52 PM | Train: [131/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:49:02 PM | Train: [131/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:49:11 PM | Train: [131/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:49:11 PM | Train: [131/200] Final Prec@1 99.9700%
06/02 03:49:11 PM | Valid: [131/200] Step 000/078 Loss 1.075 Prec@(1,5) (75.8%, 91.4%)
06/02 03:49:14 PM | Valid: [131/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.2%)
06/02 03:49:14 PM | Valid: [131/200] Final Prec@1 70.9300%
06/02 03:49:14 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:49:15 PM | Train: [132/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:49:25 PM | Train: [132/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:49:36 PM | Train: [132/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:49:46 PM | Train: [132/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:49:55 PM | Train: [132/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:49:56 PM | Train: [132/200] Final Prec@1 99.9700%
06/02 03:49:56 PM | Valid: [132/200] Step 000/078 Loss 1.074 Prec@(1,5) (75.8%, 92.2%)
06/02 03:49:58 PM | Valid: [132/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.2%)
06/02 03:49:58 PM | Valid: [132/200] Final Prec@1 70.8300%
06/02 03:49:59 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:49:59 PM | Train: [133/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 03:50:10 PM | Train: [133/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:50:20 PM | Train: [133/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:50:30 PM | Train: [133/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:50:40 PM | Train: [133/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:50:40 PM | Train: [133/200] Final Prec@1 99.9720%
06/02 03:50:40 PM | Valid: [133/200] Step 000/078 Loss 1.067 Prec@(1,5) (76.6%, 91.4%)
06/02 03:50:42 PM | Valid: [133/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.9%, 90.2%)
06/02 03:50:42 PM | Valid: [133/200] Final Prec@1 70.9200%
06/02 03:50:43 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:50:43 PM | Train: [134/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:50:54 PM | Train: [134/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:51:04 PM | Train: [134/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:51:14 PM | Train: [134/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:51:23 PM | Train: [134/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:51:23 PM | Train: [134/200] Final Prec@1 99.9740%
06/02 03:51:23 PM | Valid: [134/200] Step 000/078 Loss 1.099 Prec@(1,5) (75.8%, 92.2%)
06/02 03:51:26 PM | Valid: [134/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.2%)
06/02 03:51:26 PM | Valid: [134/200] Final Prec@1 70.8100%
06/02 03:51:26 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:51:27 PM | Train: [135/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:51:37 PM | Train: [135/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:51:47 PM | Train: [135/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:51:57 PM | Train: [135/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:52:07 PM | Train: [135/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:52:07 PM | Train: [135/200] Final Prec@1 99.9760%
06/02 03:52:07 PM | Valid: [135/200] Step 000/078 Loss 1.064 Prec@(1,5) (75.8%, 91.4%)
06/02 03:52:09 PM | Valid: [135/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.1%)
06/02 03:52:10 PM | Valid: [135/200] Final Prec@1 70.7700%
06/02 03:52:10 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:52:11 PM | Train: [136/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:52:20 PM | Train: [136/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:52:30 PM | Train: [136/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:52:40 PM | Train: [136/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:52:49 PM | Train: [136/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:52:50 PM | Train: [136/200] Final Prec@1 99.9780%
06/02 03:52:50 PM | Valid: [136/200] Step 000/078 Loss 1.070 Prec@(1,5) (75.8%, 91.4%)
06/02 03:52:52 PM | Valid: [136/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.8%, 90.2%)
06/02 03:52:52 PM | Valid: [136/200] Final Prec@1 70.8200%
06/02 03:52:53 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:52:53 PM | Train: [137/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 03:53:03 PM | Train: [137/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:53:13 PM | Train: [137/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:53:24 PM | Train: [137/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:53:33 PM | Train: [137/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:53:33 PM | Train: [137/200] Final Prec@1 99.9760%
06/02 03:53:34 PM | Valid: [137/200] Step 000/078 Loss 1.083 Prec@(1,5) (76.6%, 91.4%)
06/02 03:53:36 PM | Valid: [137/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.2%)
06/02 03:53:36 PM | Valid: [137/200] Final Prec@1 70.7500%
06/02 03:53:37 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:53:37 PM | Train: [138/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:53:47 PM | Train: [138/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:53:57 PM | Train: [138/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:54:08 PM | Train: [138/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:54:16 PM | Train: [138/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:54:16 PM | Train: [138/200] Final Prec@1 99.9720%
06/02 03:54:17 PM | Valid: [138/200] Step 000/078 Loss 1.069 Prec@(1,5) (75.8%, 91.4%)
06/02 03:54:19 PM | Valid: [138/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.9%, 90.3%)
06/02 03:54:19 PM | Valid: [138/200] Final Prec@1 70.8900%
06/02 03:54:20 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:54:20 PM | Train: [139/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:54:30 PM | Train: [139/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:54:40 PM | Train: [139/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:54:51 PM | Train: [139/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:55:00 PM | Train: [139/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:55:00 PM | Train: [139/200] Final Prec@1 99.9780%
06/02 03:55:00 PM | Valid: [139/200] Step 000/078 Loss 1.073 Prec@(1,5) (76.6%, 91.4%)
06/02 03:55:03 PM | Valid: [139/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 03:55:03 PM | Valid: [139/200] Final Prec@1 70.8800%
06/02 03:55:03 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:55:04 PM | Train: [140/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:55:14 PM | Train: [140/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:55:23 PM | Train: [140/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:55:33 PM | Train: [140/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:55:42 PM | Train: [140/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:55:43 PM | Train: [140/200] Final Prec@1 99.9720%
06/02 03:55:43 PM | Valid: [140/200] Step 000/078 Loss 1.076 Prec@(1,5) (76.6%, 91.4%)
06/02 03:55:45 PM | Valid: [140/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.3%)
06/02 03:55:45 PM | Valid: [140/200] Final Prec@1 70.7700%
06/02 03:55:46 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:55:46 PM | Train: [141/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 03:55:57 PM | Train: [141/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:56:07 PM | Train: [141/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:56:18 PM | Train: [141/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:56:27 PM | Train: [141/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:56:27 PM | Train: [141/200] Final Prec@1 99.9640%
06/02 03:56:27 PM | Valid: [141/200] Step 000/078 Loss 1.080 Prec@(1,5) (75.8%, 91.4%)
06/02 03:56:30 PM | Valid: [141/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.2%)
06/02 03:56:30 PM | Valid: [141/200] Final Prec@1 70.8000%
06/02 03:56:30 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:56:31 PM | Train: [142/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 03:56:41 PM | Train: [142/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:56:52 PM | Train: [142/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:57:01 PM | Train: [142/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:57:10 PM | Train: [142/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:57:10 PM | Train: [142/200] Final Prec@1 99.9720%
06/02 03:57:11 PM | Valid: [142/200] Step 000/078 Loss 1.068 Prec@(1,5) (76.6%, 91.4%)
06/02 03:57:13 PM | Valid: [142/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.0%, 90.2%)
06/02 03:57:13 PM | Valid: [142/200] Final Prec@1 71.0000%
06/02 03:57:14 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:57:14 PM | Train: [143/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:57:25 PM | Train: [143/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:57:35 PM | Train: [143/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:57:46 PM | Train: [143/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:57:55 PM | Train: [143/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:57:55 PM | Train: [143/200] Final Prec@1 99.9700%
06/02 03:57:56 PM | Valid: [143/200] Step 000/078 Loss 1.075 Prec@(1,5) (75.8%, 91.4%)
06/02 03:57:58 PM | Valid: [143/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.8%, 90.4%)
06/02 03:57:58 PM | Valid: [143/200] Final Prec@1 70.7800%
06/02 03:57:59 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:57:59 PM | Train: [144/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:58:09 PM | Train: [144/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:58:19 PM | Train: [144/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:58:30 PM | Train: [144/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:58:39 PM | Train: [144/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:58:39 PM | Train: [144/200] Final Prec@1 99.9680%
06/02 03:58:39 PM | Valid: [144/200] Step 000/078 Loss 1.093 Prec@(1,5) (75.0%, 92.2%)
06/02 03:58:42 PM | Valid: [144/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.2%)
06/02 03:58:42 PM | Valid: [144/200] Final Prec@1 70.7900%
06/02 03:58:42 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:58:43 PM | Train: [145/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:58:53 PM | Train: [145/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:59:03 PM | Train: [145/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:59:14 PM | Train: [145/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:59:23 PM | Train: [145/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:59:23 PM | Train: [145/200] Final Prec@1 99.9780%
06/02 03:59:23 PM | Valid: [145/200] Step 000/078 Loss 1.094 Prec@(1,5) (75.8%, 91.4%)
06/02 03:59:25 PM | Valid: [145/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.0%, 90.3%)
06/02 03:59:25 PM | Valid: [145/200] Final Prec@1 70.9700%
06/02 03:59:26 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 03:59:26 PM | Train: [146/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 03:59:36 PM | Train: [146/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:59:46 PM | Train: [146/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 03:59:57 PM | Train: [146/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:00:06 PM | Train: [146/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:00:06 PM | Train: [146/200] Final Prec@1 99.9740%
06/02 04:00:07 PM | Valid: [146/200] Step 000/078 Loss 1.090 Prec@(1,5) (77.3%, 91.4%)
06/02 04:00:09 PM | Valid: [146/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.2%)
06/02 04:00:09 PM | Valid: [146/200] Final Prec@1 70.7700%
06/02 04:00:10 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:00:10 PM | Train: [147/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:00:20 PM | Train: [147/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 04:00:31 PM | Train: [147/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:00:41 PM | Train: [147/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:00:50 PM | Train: [147/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:00:50 PM | Train: [147/200] Final Prec@1 99.9660%
06/02 04:00:50 PM | Valid: [147/200] Step 000/078 Loss 1.069 Prec@(1,5) (77.3%, 91.4%)
06/02 04:00:53 PM | Valid: [147/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.8%, 90.2%)
06/02 04:00:53 PM | Valid: [147/200] Final Prec@1 70.8100%
06/02 04:00:54 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:00:54 PM | Train: [148/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 04:01:04 PM | Train: [148/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 04:01:14 PM | Train: [148/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:01:23 PM | Train: [148/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:01:31 PM | Train: [148/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:01:32 PM | Train: [148/200] Final Prec@1 99.9700%
06/02 04:01:32 PM | Valid: [148/200] Step 000/078 Loss 1.097 Prec@(1,5) (75.0%, 91.4%)
06/02 04:01:34 PM | Valid: [148/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.0%, 90.4%)
06/02 04:01:34 PM | Valid: [148/200] Final Prec@1 71.0100%
06/02 04:01:35 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:01:35 PM | Train: [149/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:01:45 PM | Train: [149/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:01:55 PM | Train: [149/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:02:06 PM | Train: [149/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:02:15 PM | Train: [149/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:02:15 PM | Train: [149/200] Final Prec@1 99.9720%
06/02 04:02:15 PM | Valid: [149/200] Step 000/078 Loss 1.080 Prec@(1,5) (76.6%, 91.4%)
06/02 04:02:18 PM | Valid: [149/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.8%, 90.2%)
06/02 04:02:18 PM | Valid: [149/200] Final Prec@1 70.8100%
06/02 04:02:18 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:02:19 PM | Train: [150/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:02:29 PM | Train: [150/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:02:39 PM | Train: [150/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:02:49 PM | Train: [150/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:02:58 PM | Train: [150/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:02:58 PM | Train: [150/200] Final Prec@1 99.9740%
06/02 04:02:59 PM | Valid: [150/200] Step 000/078 Loss 1.080 Prec@(1,5) (75.8%, 92.2%)
06/02 04:03:01 PM | Valid: [150/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.7%, 90.3%)
06/02 04:03:01 PM | Valid: [150/200] Final Prec@1 70.7200%
06/02 04:03:02 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:03:02 PM | Train: [151/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:03:12 PM | Train: [151/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:03:23 PM | Train: [151/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:03:33 PM | Train: [151/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:03:42 PM | Train: [151/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:03:42 PM | Train: [151/200] Final Prec@1 99.9620%
06/02 04:03:43 PM | Valid: [151/200] Step 000/078 Loss 1.068 Prec@(1,5) (75.8%, 91.4%)
06/02 04:03:45 PM | Valid: [151/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.3%)
06/02 04:03:45 PM | Valid: [151/200] Final Prec@1 70.8700%
06/02 04:03:46 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:03:46 PM | Train: [152/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:03:57 PM | Train: [152/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:04:07 PM | Train: [152/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:04:17 PM | Train: [152/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:04:27 PM | Train: [152/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:04:27 PM | Train: [152/200] Final Prec@1 99.9700%
06/02 04:04:27 PM | Valid: [152/200] Step 000/078 Loss 1.061 Prec@(1,5) (75.8%, 91.4%)
06/02 04:04:29 PM | Valid: [152/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.8%, 90.1%)
06/02 04:04:29 PM | Valid: [152/200] Final Prec@1 70.8100%
06/02 04:04:30 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:04:30 PM | Train: [153/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:04:41 PM | Train: [153/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:04:51 PM | Train: [153/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:05:01 PM | Train: [153/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:05:10 PM | Train: [153/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:05:10 PM | Train: [153/200] Final Prec@1 99.9720%
06/02 04:05:11 PM | Valid: [153/200] Step 000/078 Loss 1.064 Prec@(1,5) (76.6%, 91.4%)
06/02 04:05:13 PM | Valid: [153/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.0%, 90.2%)
06/02 04:05:13 PM | Valid: [153/200] Final Prec@1 71.0400%
06/02 04:05:14 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:05:14 PM | Train: [154/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:05:24 PM | Train: [154/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:05:34 PM | Train: [154/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:05:44 PM | Train: [154/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:05:54 PM | Train: [154/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:05:54 PM | Train: [154/200] Final Prec@1 99.9740%
06/02 04:05:54 PM | Valid: [154/200] Step 000/078 Loss 1.069 Prec@(1,5) (76.6%, 91.4%)
06/02 04:05:56 PM | Valid: [154/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.3%)
06/02 04:05:56 PM | Valid: [154/200] Final Prec@1 70.9100%
06/02 04:05:57 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:05:57 PM | Train: [155/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:06:07 PM | Train: [155/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:06:18 PM | Train: [155/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:06:28 PM | Train: [155/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:06:37 PM | Train: [155/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:06:37 PM | Train: [155/200] Final Prec@1 99.9700%
06/02 04:06:38 PM | Valid: [155/200] Step 000/078 Loss 1.085 Prec@(1,5) (76.6%, 92.2%)
06/02 04:06:40 PM | Valid: [155/200] Step 078/078 Loss 1.269 Prec@(1,5) (70.9%, 90.2%)
06/02 04:06:40 PM | Valid: [155/200] Final Prec@1 70.9100%
06/02 04:06:41 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:06:41 PM | Train: [156/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:06:51 PM | Train: [156/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:07:02 PM | Train: [156/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:07:12 PM | Train: [156/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:07:22 PM | Train: [156/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:07:22 PM | Train: [156/200] Final Prec@1 99.9720%
06/02 04:07:22 PM | Valid: [156/200] Step 000/078 Loss 1.071 Prec@(1,5) (76.6%, 91.4%)
06/02 04:07:24 PM | Valid: [156/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.2%)
06/02 04:07:24 PM | Valid: [156/200] Final Prec@1 70.9200%
06/02 04:07:25 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:07:25 PM | Train: [157/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:07:36 PM | Train: [157/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:07:46 PM | Train: [157/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:07:56 PM | Train: [157/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:08:06 PM | Train: [157/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:08:06 PM | Train: [157/200] Final Prec@1 99.9720%
06/02 04:08:06 PM | Valid: [157/200] Step 000/078 Loss 1.079 Prec@(1,5) (76.6%, 91.4%)
06/02 04:08:08 PM | Valid: [157/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.9%, 90.1%)
06/02 04:08:08 PM | Valid: [157/200] Final Prec@1 70.8800%
06/02 04:08:09 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:08:09 PM | Train: [158/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:08:20 PM | Train: [158/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:08:30 PM | Train: [158/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:08:41 PM | Train: [158/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:08:50 PM | Train: [158/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:08:50 PM | Train: [158/200] Final Prec@1 99.9720%
06/02 04:08:50 PM | Valid: [158/200] Step 000/078 Loss 1.080 Prec@(1,5) (75.8%, 92.2%)
06/02 04:08:52 PM | Valid: [158/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.3%)
06/02 04:08:52 PM | Valid: [158/200] Final Prec@1 70.9400%
06/02 04:08:53 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:08:53 PM | Train: [159/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:09:04 PM | Train: [159/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:09:14 PM | Train: [159/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:09:24 PM | Train: [159/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:09:33 PM | Train: [159/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:09:33 PM | Train: [159/200] Final Prec@1 99.9660%
06/02 04:09:34 PM | Valid: [159/200] Step 000/078 Loss 1.095 Prec@(1,5) (75.8%, 92.2%)
06/02 04:09:36 PM | Valid: [159/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.8%, 90.3%)
06/02 04:09:36 PM | Valid: [159/200] Final Prec@1 70.8400%
06/02 04:09:36 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:09:37 PM | Train: [160/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 04:09:47 PM | Train: [160/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 04:09:58 PM | Train: [160/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:10:08 PM | Train: [160/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:10:17 PM | Train: [160/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:10:17 PM | Train: [160/200] Final Prec@1 99.9660%
06/02 04:10:18 PM | Valid: [160/200] Step 000/078 Loss 1.074 Prec@(1,5) (75.8%, 91.4%)
06/02 04:10:20 PM | Valid: [160/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.9%, 90.3%)
06/02 04:10:20 PM | Valid: [160/200] Final Prec@1 70.8800%
06/02 04:10:21 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:10:21 PM | Train: [161/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:10:32 PM | Train: [161/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:10:42 PM | Train: [161/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:10:53 PM | Train: [161/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:11:02 PM | Train: [161/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:11:02 PM | Train: [161/200] Final Prec@1 99.9760%
06/02 04:11:02 PM | Valid: [161/200] Step 000/078 Loss 1.077 Prec@(1,5) (75.8%, 91.4%)
06/02 04:11:04 PM | Valid: [161/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.3%)
06/02 04:11:04 PM | Valid: [161/200] Final Prec@1 70.7700%
06/02 04:11:05 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:11:05 PM | Train: [162/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:11:14 PM | Train: [162/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:11:24 PM | Train: [162/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:11:34 PM | Train: [162/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:11:43 PM | Train: [162/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:11:43 PM | Train: [162/200] Final Prec@1 99.9740%
06/02 04:11:44 PM | Valid: [162/200] Step 000/078 Loss 1.077 Prec@(1,5) (76.6%, 91.4%)
06/02 04:11:46 PM | Valid: [162/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.6%, 90.3%)
06/02 04:11:46 PM | Valid: [162/200] Final Prec@1 70.6400%
06/02 04:11:47 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:11:47 PM | Train: [163/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:11:57 PM | Train: [163/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:12:08 PM | Train: [163/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:12:18 PM | Train: [163/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:12:27 PM | Train: [163/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:12:27 PM | Train: [163/200] Final Prec@1 99.9720%
06/02 04:12:27 PM | Valid: [163/200] Step 000/078 Loss 1.065 Prec@(1,5) (76.6%, 91.4%)
06/02 04:12:29 PM | Valid: [163/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.8%, 90.3%)
06/02 04:12:29 PM | Valid: [163/200] Final Prec@1 70.7700%
06/02 04:12:30 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:12:30 PM | Train: [164/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:12:41 PM | Train: [164/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:12:51 PM | Train: [164/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:13:01 PM | Train: [164/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:13:11 PM | Train: [164/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:13:11 PM | Train: [164/200] Final Prec@1 99.9800%
06/02 04:13:11 PM | Valid: [164/200] Step 000/078 Loss 1.091 Prec@(1,5) (75.8%, 91.4%)
06/02 04:13:13 PM | Valid: [164/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.9%, 90.2%)
06/02 04:13:13 PM | Valid: [164/200] Final Prec@1 70.8700%
06/02 04:13:14 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:13:14 PM | Train: [165/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:13:25 PM | Train: [165/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:13:36 PM | Train: [165/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:13:46 PM | Train: [165/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:13:55 PM | Train: [165/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:13:55 PM | Train: [165/200] Final Prec@1 99.9740%
06/02 04:13:56 PM | Valid: [165/200] Step 000/078 Loss 1.094 Prec@(1,5) (76.6%, 92.2%)
06/02 04:13:58 PM | Valid: [165/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.3%)
06/02 04:13:58 PM | Valid: [165/200] Final Prec@1 70.8200%
06/02 04:13:59 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:13:59 PM | Train: [166/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:14:10 PM | Train: [166/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:14:20 PM | Train: [166/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:14:30 PM | Train: [166/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:14:40 PM | Train: [166/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:14:40 PM | Train: [166/200] Final Prec@1 99.9700%
06/02 04:14:40 PM | Valid: [166/200] Step 000/078 Loss 1.090 Prec@(1,5) (75.8%, 92.2%)
06/02 04:14:43 PM | Valid: [166/200] Step 078/078 Loss 1.268 Prec@(1,5) (70.8%, 90.2%)
06/02 04:14:43 PM | Valid: [166/200] Final Prec@1 70.8100%
06/02 04:14:43 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:14:44 PM | Train: [167/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:14:54 PM | Train: [167/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:15:05 PM | Train: [167/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:15:15 PM | Train: [167/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:15:25 PM | Train: [167/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:15:25 PM | Train: [167/200] Final Prec@1 99.9600%
06/02 04:15:25 PM | Valid: [167/200] Step 000/078 Loss 1.064 Prec@(1,5) (76.6%, 92.2%)
06/02 04:15:27 PM | Valid: [167/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.3%)
06/02 04:15:27 PM | Valid: [167/200] Final Prec@1 70.8300%
06/02 04:15:28 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:15:28 PM | Train: [168/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:15:39 PM | Train: [168/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:15:49 PM | Train: [168/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:16:00 PM | Train: [168/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:16:09 PM | Train: [168/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:16:09 PM | Train: [168/200] Final Prec@1 99.9680%
06/02 04:16:10 PM | Valid: [168/200] Step 000/078 Loss 1.079 Prec@(1,5) (76.6%, 92.2%)
06/02 04:16:12 PM | Valid: [168/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.9%, 90.3%)
06/02 04:16:12 PM | Valid: [168/200] Final Prec@1 70.9000%
06/02 04:16:13 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:16:13 PM | Train: [169/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:16:24 PM | Train: [169/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:16:34 PM | Train: [169/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:16:44 PM | Train: [169/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:16:54 PM | Train: [169/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:16:54 PM | Train: [169/200] Final Prec@1 99.9760%
06/02 04:16:54 PM | Valid: [169/200] Step 000/078 Loss 1.084 Prec@(1,5) (76.6%, 92.2%)
06/02 04:16:56 PM | Valid: [169/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.3%)
06/02 04:16:56 PM | Valid: [169/200] Final Prec@1 70.8000%
06/02 04:16:57 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:16:57 PM | Train: [170/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:17:08 PM | Train: [170/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 04:17:18 PM | Train: [170/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:17:28 PM | Train: [170/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:17:36 PM | Train: [170/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:17:37 PM | Train: [170/200] Final Prec@1 99.9680%
06/02 04:17:37 PM | Valid: [170/200] Step 000/078 Loss 1.101 Prec@(1,5) (75.8%, 91.4%)
06/02 04:17:39 PM | Valid: [170/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.7%, 90.3%)
06/02 04:17:39 PM | Valid: [170/200] Final Prec@1 70.6700%
06/02 04:17:40 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:17:40 PM | Train: [171/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:17:51 PM | Train: [171/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:18:01 PM | Train: [171/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:18:11 PM | Train: [171/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:18:21 PM | Train: [171/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:18:21 PM | Train: [171/200] Final Prec@1 99.9800%
06/02 04:18:21 PM | Valid: [171/200] Step 000/078 Loss 1.077 Prec@(1,5) (76.6%, 91.4%)
06/02 04:18:24 PM | Valid: [171/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.3%)
06/02 04:18:24 PM | Valid: [171/200] Final Prec@1 70.8500%
06/02 04:18:24 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:18:25 PM | Train: [172/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:18:35 PM | Train: [172/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:18:45 PM | Train: [172/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:18:55 PM | Train: [172/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:19:04 PM | Train: [172/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:19:04 PM | Train: [172/200] Final Prec@1 99.9720%
06/02 04:19:05 PM | Valid: [172/200] Step 000/078 Loss 1.085 Prec@(1,5) (75.0%, 92.2%)
06/02 04:19:07 PM | Valid: [172/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.3%)
06/02 04:19:07 PM | Valid: [172/200] Final Prec@1 70.9000%
06/02 04:19:08 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:19:08 PM | Train: [173/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:19:18 PM | Train: [173/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:19:28 PM | Train: [173/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:19:39 PM | Train: [173/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:19:49 PM | Train: [173/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:19:49 PM | Train: [173/200] Final Prec@1 99.9640%
06/02 04:19:49 PM | Valid: [173/200] Step 000/078 Loss 1.071 Prec@(1,5) (76.6%, 91.4%)
06/02 04:19:51 PM | Valid: [173/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.7%, 90.2%)
06/02 04:19:51 PM | Valid: [173/200] Final Prec@1 70.7200%
06/02 04:19:52 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:19:52 PM | Train: [174/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 04:20:03 PM | Train: [174/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:20:13 PM | Train: [174/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 04:20:24 PM | Train: [174/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:20:33 PM | Train: [174/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:20:33 PM | Train: [174/200] Final Prec@1 99.9600%
06/02 04:20:34 PM | Valid: [174/200] Step 000/078 Loss 1.072 Prec@(1,5) (75.8%, 91.4%)
06/02 04:20:36 PM | Valid: [174/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.7%, 90.2%)
06/02 04:20:36 PM | Valid: [174/200] Final Prec@1 70.6700%
06/02 04:20:37 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:20:37 PM | Train: [175/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 04:20:47 PM | Train: [175/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:20:58 PM | Train: [175/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:21:08 PM | Train: [175/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:21:18 PM | Train: [175/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:21:18 PM | Train: [175/200] Final Prec@1 99.9700%
06/02 04:21:18 PM | Valid: [175/200] Step 000/078 Loss 1.083 Prec@(1,5) (76.6%, 91.4%)
06/02 04:21:20 PM | Valid: [175/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.3%)
06/02 04:21:20 PM | Valid: [175/200] Final Prec@1 70.9800%
06/02 04:21:21 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:21:21 PM | Train: [176/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:21:32 PM | Train: [176/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:21:42 PM | Train: [176/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:21:53 PM | Train: [176/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:22:02 PM | Train: [176/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:22:02 PM | Train: [176/200] Final Prec@1 99.9760%
06/02 04:22:02 PM | Valid: [176/200] Step 000/078 Loss 1.085 Prec@(1,5) (76.6%, 92.2%)
06/02 04:22:05 PM | Valid: [176/200] Step 078/078 Loss 1.267 Prec@(1,5) (71.0%, 90.3%)
06/02 04:22:05 PM | Valid: [176/200] Final Prec@1 70.9500%
06/02 04:22:05 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:22:06 PM | Train: [177/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:22:16 PM | Train: [177/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:22:26 PM | Train: [177/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:22:36 PM | Train: [177/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:22:45 PM | Train: [177/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:22:45 PM | Train: [177/200] Final Prec@1 99.9800%
06/02 04:22:46 PM | Valid: [177/200] Step 000/078 Loss 1.097 Prec@(1,5) (76.6%, 91.4%)
06/02 04:22:48 PM | Valid: [177/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.2%)
06/02 04:22:48 PM | Valid: [177/200] Final Prec@1 70.9800%
06/02 04:22:49 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:22:49 PM | Train: [178/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:22:59 PM | Train: [178/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 04:23:09 PM | Train: [178/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 04:23:20 PM | Train: [178/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:23:29 PM | Train: [178/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:23:29 PM | Train: [178/200] Final Prec@1 99.9560%
06/02 04:23:29 PM | Valid: [178/200] Step 000/078 Loss 1.076 Prec@(1,5) (76.6%, 91.4%)
06/02 04:23:31 PM | Valid: [178/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.0%, 90.2%)
06/02 04:23:31 PM | Valid: [178/200] Final Prec@1 70.9900%
06/02 04:23:32 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:23:32 PM | Train: [179/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:23:43 PM | Train: [179/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:23:53 PM | Train: [179/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:24:03 PM | Train: [179/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:24:12 PM | Train: [179/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:24:12 PM | Train: [179/200] Final Prec@1 99.9740%
06/02 04:24:13 PM | Valid: [179/200] Step 000/078 Loss 1.079 Prec@(1,5) (75.8%, 92.2%)
06/02 04:24:15 PM | Valid: [179/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 04:24:15 PM | Valid: [179/200] Final Prec@1 70.9200%
06/02 04:24:16 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:24:16 PM | Train: [180/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:24:27 PM | Train: [180/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:24:37 PM | Train: [180/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:24:47 PM | Train: [180/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:24:57 PM | Train: [180/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:24:57 PM | Train: [180/200] Final Prec@1 99.9760%
06/02 04:24:57 PM | Valid: [180/200] Step 000/078 Loss 1.089 Prec@(1,5) (76.6%, 91.4%)
06/02 04:25:00 PM | Valid: [180/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.3%)
06/02 04:25:00 PM | Valid: [180/200] Final Prec@1 70.8500%
06/02 04:25:00 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:25:01 PM | Train: [181/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:25:11 PM | Train: [181/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:25:22 PM | Train: [181/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:25:32 PM | Train: [181/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:25:42 PM | Train: [181/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:25:42 PM | Train: [181/200] Final Prec@1 99.9780%
06/02 04:25:42 PM | Valid: [181/200] Step 000/078 Loss 1.093 Prec@(1,5) (75.8%, 91.4%)
06/02 04:25:44 PM | Valid: [181/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.9%, 90.3%)
06/02 04:25:44 PM | Valid: [181/200] Final Prec@1 70.8600%
06/02 04:25:45 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:25:45 PM | Train: [182/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:25:56 PM | Train: [182/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:26:06 PM | Train: [182/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:26:17 PM | Train: [182/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:26:26 PM | Train: [182/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:26:26 PM | Train: [182/200] Final Prec@1 99.9820%
06/02 04:26:27 PM | Valid: [182/200] Step 000/078 Loss 1.105 Prec@(1,5) (75.0%, 91.4%)
06/02 04:26:29 PM | Valid: [182/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.2%)
06/02 04:26:29 PM | Valid: [182/200] Final Prec@1 70.7500%
06/02 04:26:30 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:26:30 PM | Train: [183/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:26:40 PM | Train: [183/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:26:51 PM | Train: [183/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:27:01 PM | Train: [183/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:27:10 PM | Train: [183/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:27:10 PM | Train: [183/200] Final Prec@1 99.9740%
06/02 04:27:10 PM | Valid: [183/200] Step 000/078 Loss 1.088 Prec@(1,5) (76.6%, 91.4%)
06/02 04:27:13 PM | Valid: [183/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.0%, 90.2%)
06/02 04:27:13 PM | Valid: [183/200] Final Prec@1 70.9600%
06/02 04:27:14 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:27:14 PM | Train: [184/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:27:24 PM | Train: [184/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:27:34 PM | Train: [184/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:27:44 PM | Train: [184/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:27:53 PM | Train: [184/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:27:53 PM | Train: [184/200] Final Prec@1 99.9780%
06/02 04:27:54 PM | Valid: [184/200] Step 000/078 Loss 1.072 Prec@(1,5) (76.6%, 91.4%)
06/02 04:27:56 PM | Valid: [184/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.2%)
06/02 04:27:56 PM | Valid: [184/200] Final Prec@1 70.9400%
06/02 04:27:57 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:27:57 PM | Train: [185/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:28:07 PM | Train: [185/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:28:18 PM | Train: [185/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:28:28 PM | Train: [185/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:28:37 PM | Train: [185/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:28:37 PM | Train: [185/200] Final Prec@1 99.9760%
06/02 04:28:37 PM | Valid: [185/200] Step 000/078 Loss 1.078 Prec@(1,5) (76.6%, 92.2%)
06/02 04:28:40 PM | Valid: [185/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.9%, 90.2%)
06/02 04:28:40 PM | Valid: [185/200] Final Prec@1 70.9400%
06/02 04:28:40 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:28:41 PM | Train: [186/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:28:51 PM | Train: [186/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:29:02 PM | Train: [186/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:29:12 PM | Train: [186/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:29:22 PM | Train: [186/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:29:22 PM | Train: [186/200] Final Prec@1 99.9700%
06/02 04:29:22 PM | Valid: [186/200] Step 000/078 Loss 1.082 Prec@(1,5) (76.6%, 92.2%)
06/02 04:29:25 PM | Valid: [186/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.2%)
06/02 04:29:25 PM | Valid: [186/200] Final Prec@1 70.9400%
06/02 04:29:25 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:29:26 PM | Train: [187/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:29:36 PM | Train: [187/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:29:46 PM | Train: [187/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:29:57 PM | Train: [187/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:30:06 PM | Train: [187/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:30:06 PM | Train: [187/200] Final Prec@1 99.9820%
06/02 04:30:06 PM | Valid: [187/200] Step 000/078 Loss 1.091 Prec@(1,5) (76.6%, 92.2%)
06/02 04:30:09 PM | Valid: [187/200] Step 078/078 Loss 1.263 Prec@(1,5) (71.1%, 90.3%)
06/02 04:30:09 PM | Valid: [187/200] Final Prec@1 71.0900%
06/02 04:30:09 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:30:10 PM | Train: [188/200] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 04:30:20 PM | Train: [188/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:30:30 PM | Train: [188/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:30:41 PM | Train: [188/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:30:50 PM | Train: [188/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:30:50 PM | Train: [188/200] Final Prec@1 99.9700%
06/02 04:30:51 PM | Valid: [188/200] Step 000/078 Loss 1.090 Prec@(1,5) (75.8%, 91.4%)
06/02 04:30:53 PM | Valid: [188/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.8%, 90.1%)
06/02 04:30:53 PM | Valid: [188/200] Final Prec@1 70.8300%
06/02 04:30:54 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:30:54 PM | Train: [189/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:31:05 PM | Train: [189/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:31:15 PM | Train: [189/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:31:25 PM | Train: [189/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:31:34 PM | Train: [189/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:31:34 PM | Train: [189/200] Final Prec@1 99.9720%
06/02 04:31:34 PM | Valid: [189/200] Step 000/078 Loss 1.086 Prec@(1,5) (75.8%, 91.4%)
06/02 04:31:37 PM | Valid: [189/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.8%, 90.2%)
06/02 04:31:37 PM | Valid: [189/200] Final Prec@1 70.8200%
06/02 04:31:37 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:31:38 PM | Train: [190/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:31:48 PM | Train: [190/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:31:58 PM | Train: [190/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:32:09 PM | Train: [190/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:32:19 PM | Train: [190/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:32:19 PM | Train: [190/200] Final Prec@1 99.9780%
06/02 04:32:19 PM | Valid: [190/200] Step 000/078 Loss 1.080 Prec@(1,5) (76.6%, 92.2%)
06/02 04:32:21 PM | Valid: [190/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.2%)
06/02 04:32:21 PM | Valid: [190/200] Final Prec@1 70.9500%
06/02 04:32:22 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:32:22 PM | Train: [191/200] Step 000/390 Loss 0.009 Prec@(1,5) (99.2%, 100.0%)
06/02 04:32:33 PM | Train: [191/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:32:43 PM | Train: [191/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:32:54 PM | Train: [191/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:33:03 PM | Train: [191/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:33:03 PM | Train: [191/200] Final Prec@1 99.9780%
06/02 04:33:03 PM | Valid: [191/200] Step 000/078 Loss 1.082 Prec@(1,5) (75.0%, 92.2%)
06/02 04:33:06 PM | Valid: [191/200] Step 078/078 Loss 1.262 Prec@(1,5) (71.0%, 90.3%)
06/02 04:33:06 PM | Valid: [191/200] Final Prec@1 70.9600%
06/02 04:33:06 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:33:07 PM | Train: [192/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:33:17 PM | Train: [192/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:33:28 PM | Train: [192/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:33:38 PM | Train: [192/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:33:48 PM | Train: [192/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:33:48 PM | Train: [192/200] Final Prec@1 99.9720%
06/02 04:33:48 PM | Valid: [192/200] Step 000/078 Loss 1.071 Prec@(1,5) (76.6%, 92.2%)
06/02 04:33:50 PM | Valid: [192/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.4%)
06/02 04:33:50 PM | Valid: [192/200] Final Prec@1 70.8700%
06/02 04:33:51 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:33:51 PM | Train: [193/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:34:02 PM | Train: [193/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:34:12 PM | Train: [193/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:34:22 PM | Train: [193/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:34:32 PM | Train: [193/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:34:32 PM | Train: [193/200] Final Prec@1 99.9780%
06/02 04:34:32 PM | Valid: [193/200] Step 000/078 Loss 1.075 Prec@(1,5) (75.0%, 91.4%)
06/02 04:34:34 PM | Valid: [193/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.3%)
06/02 04:34:34 PM | Valid: [193/200] Final Prec@1 70.8700%
06/02 04:34:35 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:34:35 PM | Train: [194/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:34:46 PM | Train: [194/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:34:56 PM | Train: [194/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:35:06 PM | Train: [194/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:35:15 PM | Train: [194/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:35:15 PM | Train: [194/200] Final Prec@1 99.9720%
06/02 04:35:15 PM | Valid: [194/200] Step 000/078 Loss 1.069 Prec@(1,5) (75.8%, 91.4%)
06/02 04:35:18 PM | Valid: [194/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.4%)
06/02 04:35:18 PM | Valid: [194/200] Final Prec@1 70.8900%
06/02 04:35:18 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:35:19 PM | Train: [195/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 04:35:29 PM | Train: [195/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:35:40 PM | Train: [195/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:35:50 PM | Train: [195/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:35:59 PM | Train: [195/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:36:00 PM | Train: [195/200] Final Prec@1 99.9700%
06/02 04:36:00 PM | Valid: [195/200] Step 000/078 Loss 1.069 Prec@(1,5) (76.6%, 91.4%)
06/02 04:36:02 PM | Valid: [195/200] Step 078/078 Loss 1.265 Prec@(1,5) (71.0%, 90.4%)
06/02 04:36:02 PM | Valid: [195/200] Final Prec@1 70.9800%
06/02 04:36:03 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:36:03 PM | Train: [196/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:36:14 PM | Train: [196/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:36:24 PM | Train: [196/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:36:34 PM | Train: [196/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:36:44 PM | Train: [196/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:36:44 PM | Train: [196/200] Final Prec@1 99.9820%
06/02 04:36:44 PM | Valid: [196/200] Step 000/078 Loss 1.087 Prec@(1,5) (75.8%, 91.4%)
06/02 04:36:46 PM | Valid: [196/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.8%, 90.2%)
06/02 04:36:46 PM | Valid: [196/200] Final Prec@1 70.8300%
06/02 04:36:47 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:36:47 PM | Train: [197/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 04:36:58 PM | Train: [197/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:37:08 PM | Train: [197/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:37:19 PM | Train: [197/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:37:28 PM | Train: [197/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:37:28 PM | Train: [197/200] Final Prec@1 99.9720%
06/02 04:37:28 PM | Valid: [197/200] Step 000/078 Loss 1.077 Prec@(1,5) (76.6%, 91.4%)
06/02 04:37:31 PM | Valid: [197/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.2%)
06/02 04:37:31 PM | Valid: [197/200] Final Prec@1 70.8600%
06/02 04:37:31 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:37:32 PM | Train: [198/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:37:42 PM | Train: [198/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:37:53 PM | Train: [198/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:38:03 PM | Train: [198/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:38:12 PM | Train: [198/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:38:12 PM | Train: [198/200] Final Prec@1 99.9720%
06/02 04:38:13 PM | Valid: [198/200] Step 000/078 Loss 1.084 Prec@(1,5) (75.0%, 92.2%)
06/02 04:38:15 PM | Valid: [198/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.8%, 90.2%)
06/02 04:38:15 PM | Valid: [198/200] Final Prec@1 70.8400%
06/02 04:38:16 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:38:16 PM | Train: [199/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:38:26 PM | Train: [199/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:38:37 PM | Train: [199/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:38:47 PM | Train: [199/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:38:56 PM | Train: [199/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:38:56 PM | Train: [199/200] Final Prec@1 99.9680%
06/02 04:38:56 PM | Valid: [199/200] Step 000/078 Loss 1.090 Prec@(1,5) (75.8%, 91.4%)
06/02 04:38:59 PM | Valid: [199/200] Step 078/078 Loss 1.264 Prec@(1,5) (71.0%, 90.2%)
06/02 04:38:59 PM | Valid: [199/200] Final Prec@1 70.9800%
06/02 04:38:59 PM | Current best Prec@1 = 71.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9120.0, 0.13916015625]
['model.relu.alpha_mask_1_0', 16384, 3957.0, 0.24151611328125]
['model.relu.alpha_mask_2_0', 16384, 3130.0, 0.1910400390625]
['model.relu.alpha_mask_3_0', 16384, 3913.0, 0.23883056640625]
['model.relu.alpha_mask_4_0', 16384, 3192.0, 0.19482421875]
['model.relu.alpha_mask_5_0', 8192, 3171.0, 0.3870849609375]
['model.relu.alpha_mask_6_0', 8192, 2650.0, 0.323486328125]
['model.relu.alpha_mask_7_0', 8192, 2783.0, 0.3397216796875]
['model.relu.alpha_mask_8_0', 8192, 2585.0, 0.3155517578125]
['model.relu.alpha_mask_9_0', 4096, 2055.0, 0.501708984375]
['model.relu.alpha_mask_10_0', 4096, 1915.0, 0.467529296875]
['model.relu.alpha_mask_11_0', 4096, 2171.0, 0.530029296875]
['model.relu.alpha_mask_12_0', 4096, 1912.0, 0.466796875]
['model.relu.alpha_mask_13_0', 2048, 1830.0, 0.8935546875]
['model.relu.alpha_mask_14_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_15_0', 2048, 1619.0, 0.79052734375]
['model.relu.alpha_mask_16_0', 2048, 1802.0, 0.8798828125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 49805.0, 0.264335300611413]
########## End ###########
06/02 04:39:00 PM | Train: [200/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 04:39:10 PM | Train: [200/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:39:20 PM | Train: [200/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:39:31 PM | Train: [200/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:39:40 PM | Train: [200/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 04:39:40 PM | Train: [200/200] Final Prec@1 99.9660%
06/02 04:39:40 PM | Valid: [200/200] Step 000/078 Loss 1.081 Prec@(1,5) (75.8%, 91.4%)
06/02 04:39:43 PM | Valid: [200/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.9%, 90.3%)
06/02 04:39:43 PM | Valid: [200/200] Final Prec@1 70.8900%
06/02 04:39:43 PM | Current best Prec@1 = 71.1000%
06/02 04:39:43 PM | Final best validation Prec@1 = 71.1000%
[HAMI-core Msg(543964:127731242920832:multiprocess_memory_limit.c:498)]: Calling exit handler 543964
