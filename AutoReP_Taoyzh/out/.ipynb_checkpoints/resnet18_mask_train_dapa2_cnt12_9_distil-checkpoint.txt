[HAMI-core Msg(465233:124733484747648:libvgpu.c:837)]: Initializing.....
06/01 02:05:59 AM | 
06/01 02:05:59 AM | Parameters:
06/01 02:05:59 AM | NUM_MASK=1
06/01 02:05:59 AM | RELU_COUNT=12.9
06/01 02:05:59 AM | ACT_TYPE=ReLU_masked_dapa_relay
06/01 02:05:59 AM | ALPHA_LR=0.0002
06/01 02:05:59 AM | ALPHA_WEIGHT_DECAY=0.001
06/01 02:05:59 AM | ARCH=resnet18
06/01 02:05:59 AM | BATCH_SIZE=128
06/01 02:05:59 AM | CHECKPOINT_PATH=None
06/01 02:05:59 AM | CLIP_X2=1.0
06/01 02:05:59 AM | CLIP_X2_BOOL=True
06/01 02:05:59 AM | DATA_PATH=./data/
06/01 02:05:59 AM | DATASET=cifar100
06/01 02:05:59 AM | DEGREE=2
06/01 02:05:59 AM | DISTIL=True
06/01 02:05:59 AM | DROPOUT=0
06/01 02:05:59 AM | ENABLE_GRAD_NORM=False
06/01 02:05:59 AM | ENABLE_LOOKAHEAD=True
06/01 02:05:59 AM | EPOCHS=200
06/01 02:05:59 AM | EVALUATE=None
06/01 02:05:59 AM | EXT=baseline
06/01 02:05:59 AM | FREEZEACT=False
06/01 02:05:59 AM | GPUS=[0]
06/01 02:05:59 AM | LAMDA=240.0
06/01 02:05:59 AM | MASK_DROPOUT=0
06/01 02:05:59 AM | MASK_EPOCHS=80
06/01 02:05:59 AM | NUM_CLASSES=100
06/01 02:05:59 AM | OPTIM=cosine
06/01 02:05:59 AM | PATH=train_cifar_dapa2_distil_relay/resnet18_resnet18_cifar100_relay_0.003/cosine_ReLUs12.9wm_lr0.001mep80_baseline
06/01 02:05:59 AM | PLOT_PATH=train_cifar_dapa2_distil_relay/resnet18_resnet18_cifar100_relay_0.003/cosine_ReLUs12.9wm_lr0.001mep80_baseline/plots
06/01 02:05:59 AM | PRECISION=full
06/01 02:05:59 AM | PRETRAINED=False
06/01 02:05:59 AM | PRETRAINED_PATH=./train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar
06/01 02:05:59 AM | PRINT_FREQ=100
06/01 02:05:59 AM | SCALE_X1=1.0
06/01 02:05:59 AM | SCALE_X2=2.0
06/01 02:05:59 AM | SEED=2
06/01 02:05:59 AM | START_EPOCH=0
06/01 02:05:59 AM | TEACHER_ARCH=resnet18
06/01 02:05:59 AM | TEACHER_PATH=./train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar
06/01 02:05:59 AM | THRESHOLD=0.003
06/01 02:05:59 AM | VAR_MIN=0.5
06/01 02:05:59 AM | W_DECAY_EPOCH=20
06/01 02:05:59 AM | W_GRAD_CLIP=5.0
06/01 02:05:59 AM | W_LR=0.0001
06/01 02:05:59 AM | W_LR_MIN=1e-05
06/01 02:05:59 AM | W_MASK_LR=0.001
06/01 02:05:59 AM | W_MOMENTUM=0.9
06/01 02:05:59 AM | W_WEIGHT_DECAY=0.0005
06/01 02:05:59 AM | WORKERS=4
06/01 02:05:59 AM | X_SIZE=[1, 3, 32, 32]
06/01 02:05:59 AM | 
06/01 02:05:59 AM | Logger is set - training start
[HAMI-core Msg(465233:124733484747648:libvgpu.c:856)]: Initialized
==> Load pretrained
=> loading checkpoint './train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar'
/mnt/ann25-22336216/AutoReP/models_util/model_util.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_path, map_location = "cpu")
=> loading checkpoint './train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar'
Files already downloaded and verified
Files already downloaded and verified
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 65536.0, 1.0]
['model.relu.alpha_mask_1_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_2_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_3_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_4_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_5_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_6_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_7_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_8_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_9_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_10_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_11_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_12_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_13_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_14_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_15_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_16_0', 2048, 2048.0, 1.0]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 188416.0, 1.0]
########## End ###########
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16601579520 total=17059545088 limit=4194304000 usage=415246848
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16366698496 total=17059545088 limit=4194304000 usage=650127872
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16106651648 total=17059545088 limit=4194304000 usage=910174720
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16085680128 total=17059545088 limit=4194304000 usage=931146240
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16066805760 total=17059545088 limit=4194304000 usage=950020608
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16045834240 total=17059545088 limit=4194304000 usage=970992128
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16039542784 total=17059545088 limit=4194304000 usage=977283584
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16037445632 total=17059545088 limit=4194304000 usage=979380736
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15760621568 total=17059545088 limit=4194304000 usage=1252285440
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15703998464 total=17059545088 limit=4194304000 usage=1308908544
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15703998464 total=17059545088 limit=4194304000 usage=1308908544
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15708192768 total=17059545088 limit=4194304000 usage=1304714240
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15651569664 total=17059545088 limit=4194304000 usage=1361337344
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15636889600 total=17059545088 limit=4194304000 usage=1376017408
06/01 02:06:04 AM | Train: [ 1/80] Step 000/390 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)
06/01 02:06:04 AM | layerwise density: [65536.0, 16384.0, 16384.0, 16384.0, 16384.0, 8192.0, 8192.0, 8192.0, 8192.0, 4096.0, 4096.0, 4096.0, 4096.0, 2048.0, 2048.0, 2048.0, 2048.0]
layerwise density percentage: ['1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000']
Global density: 1.0
06/01 02:06:15 AM | Train: [ 1/80] Step 100/390 Loss 0.024 Prec@(1,5) (99.9%, 100.0%)
06/01 02:06:15 AM | layerwise density: [62126.0, 15548.0, 15555.0, 15539.0, 15548.0, 7759.0, 7767.0, 7748.0, 7760.0, 3881.0, 3893.0, 3904.0, 3901.0, 1975.0, 1970.0, 1954.0, 1934.0]
layerwise density percentage: ['0.948', '0.949', '0.949', '0.948', '0.949', '0.947', '0.948', '0.946', '0.947', '0.948', '0.950', '0.953', '0.952', '0.964', '0.962', '0.954', '0.944']
Global density: 0.9487623572349548
06/01 02:06:26 AM | Train: [ 1/80] Step 200/390 Loss 0.032 Prec@(1,5) (99.8%, 100.0%)
06/01 02:06:26 AM | layerwise density: [58887.0, 14811.0, 14758.0, 14797.0, 14780.0, 7359.0, 7304.0, 7347.0, 7409.0, 3708.0, 3700.0, 3726.0, 3705.0, 1904.0, 1892.0, 1875.0, 1854.0]
layerwise density percentage: ['0.899', '0.904', '0.901', '0.903', '0.902', '0.898', '0.892', '0.897', '0.904', '0.905', '0.903', '0.910', '0.905', '0.930', '0.924', '0.916', '0.905']
Global density: 0.9012823104858398
06/01 02:06:37 AM | Train: [ 1/80] Step 300/390 Loss 0.043 Prec@(1,5) (99.7%, 100.0%)
06/01 02:06:37 AM | layerwise density: [55630.0, 13997.0, 13960.0, 13989.0, 14019.0, 6923.0, 6924.0, 6960.0, 7003.0, 3523.0, 3517.0, 3532.0, 3527.0, 1818.0, 1817.0, 1772.0, 1778.0]
layerwise density percentage: ['0.849', '0.854', '0.852', '0.854', '0.856', '0.845', '0.845', '0.850', '0.855', '0.860', '0.859', '0.862', '0.861', '0.888', '0.887', '0.865', '0.868']
Global density: 0.8528416156768799
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15464923136 total=17059545088 limit=4194304000 usage=1547983872
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16190537728 total=17059545088 limit=4194304000 usage=822369280
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16085680128 total=17059545088 limit=4194304000 usage=927226880
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16064708608 total=17059545088 limit=4194304000 usage=948198400
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16064708608 total=17059545088 limit=4194304000 usage=948198400
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16022765568 total=17059545088 limit=4194304000 usage=990141440
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16022765568 total=17059545088 limit=4194304000 usage=990141440
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16005988352 total=17059545088 limit=4194304000 usage=1006918656
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15989211136 total=17059545088 limit=4194304000 usage=1023695872
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15999696896 total=17059545088 limit=4194304000 usage=1013210112
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15999696896 total=17059545088 limit=4194304000 usage=1013210112
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15821438976 total=17059545088 limit=4194304000 usage=1191468032
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15792078848 total=17059545088 limit=4194304000 usage=1220828160
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15792078848 total=17059545088 limit=4194304000 usage=1220828160
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15794176000 total=17059545088 limit=4194304000 usage=1218731008
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15827730432 total=17059545088 limit=4194304000 usage=1185176576
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15827730432 total=17059545088 limit=4194304000 usage=1185176576
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(465233:124728373151296:memory.c:512)]: orig free=15857090560 total=17059545088 limit=4194304000 usage=1155816448
06/01 02:06:48 AM | Train: [ 1/80] Step 390/390 Loss 0.058 Prec@(1,5) (99.4%, 100.0%)
06/01 02:06:48 AM | layerwise density: [52854.0, 13287.0, 13275.0, 13327.0, 13249.0, 6583.0, 6594.0, 6585.0, 6648.0, 3356.0, 3353.0, 3358.0, 3345.0, 1748.0, 1752.0, 1701.0, 1672.0]
layerwise density percentage: ['0.806', '0.811', '0.810', '0.813', '0.809', '0.804', '0.805', '0.804', '0.812', '0.819', '0.819', '0.820', '0.817', '0.854', '0.855', '0.831', '0.816']
Global density: 0.8103717565536499
06/01 02:06:48 AM | Train: [ 1/200] Final Prec@1 99.4400%
06/01 02:06:48 AM | Valid: [ 1/200] Step 000/078 Loss 1.133 Prec@(1,5) (72.7%, 91.4%)
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=15781593088 total=17059545088 limit=4194304000 usage=1231313920
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16305881088 total=17059545088 limit=4194304000 usage=707025920
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(465233:124733484747648:memory.c:512)]: orig free=16282812416 total=17059545088 limit=4194304000 usage=730094592
06/01 02:06:51 AM | Valid: [ 1/200] Step 078/078 Loss 1.328 Prec@(1,5) (68.7%, 89.0%)
06/01 02:06:51 AM | Valid: [ 1/200] Final Prec@1 68.6900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 52854.0, 0.806488037109375]
['model.relu.alpha_mask_1_0', 16384, 13287.0, 0.81097412109375]
['model.relu.alpha_mask_2_0', 16384, 13275.0, 0.81024169921875]
['model.relu.alpha_mask_3_0', 16384, 13327.0, 0.81341552734375]
['model.relu.alpha_mask_4_0', 16384, 13249.0, 0.80865478515625]
['model.relu.alpha_mask_5_0', 8192, 6583.0, 0.8035888671875]
['model.relu.alpha_mask_6_0', 8192, 6594.0, 0.804931640625]
['model.relu.alpha_mask_7_0', 8192, 6585.0, 0.8038330078125]
['model.relu.alpha_mask_8_0', 8192, 6648.0, 0.8115234375]
['model.relu.alpha_mask_9_0', 4096, 3356.0, 0.8193359375]
['model.relu.alpha_mask_10_0', 4096, 3353.0, 0.818603515625]
['model.relu.alpha_mask_11_0', 4096, 3358.0, 0.81982421875]
['model.relu.alpha_mask_12_0', 4096, 3345.0, 0.816650390625]
['model.relu.alpha_mask_13_0', 2048, 1748.0, 0.853515625]
['model.relu.alpha_mask_14_0', 2048, 1752.0, 0.85546875]
['model.relu.alpha_mask_15_0', 2048, 1701.0, 0.83056640625]
['model.relu.alpha_mask_16_0', 2048, 1672.0, 0.81640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 152687.0, 0.8103717306385869]
########## End ###########
06/01 02:06:52 AM | Train: [ 2/80] Step 000/390 Loss 0.071 Prec@(1,5) (99.2%, 100.0%)
06/01 02:06:52 AM | layerwise density: [52854.0, 13287.0, 13275.0, 13327.0, 13249.0, 6583.0, 6594.0, 6585.0, 6648.0, 3356.0, 3353.0, 3358.0, 3345.0, 1748.0, 1752.0, 1701.0, 1672.0]
layerwise density percentage: ['0.806', '0.811', '0.810', '0.813', '0.809', '0.804', '0.805', '0.804', '0.812', '0.819', '0.819', '0.820', '0.817', '0.854', '0.855', '0.831', '0.816']
Global density: 0.8103717565536499
06/01 02:07:03 AM | Train: [ 2/80] Step 100/390 Loss 0.112 Prec@(1,5) (98.2%, 100.0%)
06/01 02:07:03 AM | layerwise density: [49714.0, 12542.0, 12532.0, 12543.0, 12446.0, 6221.0, 6196.0, 6193.0, 6268.0, 3157.0, 3192.0, 3148.0, 3146.0, 1682.0, 1680.0, 1607.0, 1574.0]
layerwise density percentage: ['0.759', '0.766', '0.765', '0.766', '0.760', '0.759', '0.756', '0.756', '0.765', '0.771', '0.779', '0.769', '0.768', '0.821', '0.820', '0.785', '0.769']
Global density: 0.763422429561615
06/01 02:07:14 AM | Train: [ 2/80] Step 200/390 Loss 0.131 Prec@(1,5) (97.8%, 100.0%)
06/01 02:07:14 AM | layerwise density: [46766.0, 11769.0, 11800.0, 11831.0, 11705.0, 5859.0, 5824.0, 5855.0, 5911.0, 2984.0, 3011.0, 2984.0, 2961.0, 1609.0, 1605.0, 1528.0, 1491.0]
layerwise density percentage: ['0.714', '0.718', '0.720', '0.722', '0.714', '0.715', '0.711', '0.715', '0.722', '0.729', '0.735', '0.729', '0.723', '0.786', '0.784', '0.746', '0.728']
Global density: 0.7191162109375
06/01 02:07:24 AM | Train: [ 2/80] Step 300/390 Loss 0.157 Prec@(1,5) (97.1%, 100.0%)
06/01 02:07:24 AM | layerwise density: [43771.0, 11043.0, 11056.0, 11050.0, 10973.0, 5524.0, 5483.0, 5460.0, 5527.0, 2808.0, 2827.0, 2823.0, 2765.0, 1547.0, 1527.0, 1444.0, 1399.0]
layerwise density percentage: ['0.668', '0.674', '0.675', '0.674', '0.670', '0.674', '0.669', '0.667', '0.675', '0.686', '0.690', '0.689', '0.675', '0.755', '0.746', '0.705', '0.683']
Global density: 0.67418372631073
06/01 02:07:34 AM | Train: [ 2/80] Step 390/390 Loss 0.181 Prec@(1,5) (96.4%, 100.0%)
06/01 02:07:34 AM | layerwise density: [41124.0, 10381.0, 10387.0, 10350.0, 10264.0, 5194.0, 5155.0, 5102.0, 5225.0, 2673.0, 2661.0, 2665.0, 2623.0, 1493.0, 1483.0, 1376.0, 1314.0]
layerwise density percentage: ['0.628', '0.634', '0.634', '0.632', '0.626', '0.634', '0.629', '0.623', '0.638', '0.653', '0.650', '0.651', '0.640', '0.729', '0.724', '0.672', '0.642']
Global density: 0.6340757012367249
06/01 02:07:34 AM | Train: [ 2/200] Final Prec@1 96.4000%
06/01 02:07:35 AM | Valid: [ 2/200] Step 000/078 Loss 1.109 Prec@(1,5) (76.6%, 90.6%)
06/01 02:07:37 AM | Valid: [ 2/200] Step 078/078 Loss 1.383 Prec@(1,5) (67.3%, 88.2%)
06/01 02:07:37 AM | Valid: [ 2/200] Final Prec@1 67.3000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 41123.0, 0.6274871826171875]
['model.relu.alpha_mask_1_0', 16384, 10380.0, 0.633544921875]
['model.relu.alpha_mask_2_0', 16384, 10384.0, 0.6337890625]
['model.relu.alpha_mask_3_0', 16384, 10349.0, 0.63165283203125]
['model.relu.alpha_mask_4_0', 16384, 10264.0, 0.62646484375]
['model.relu.alpha_mask_5_0', 8192, 5194.0, 0.634033203125]
['model.relu.alpha_mask_6_0', 8192, 5155.0, 0.6292724609375]
['model.relu.alpha_mask_7_0', 8192, 5102.0, 0.622802734375]
['model.relu.alpha_mask_8_0', 8192, 5222.0, 0.637451171875]
['model.relu.alpha_mask_9_0', 4096, 2673.0, 0.652587890625]
['model.relu.alpha_mask_10_0', 4096, 2661.0, 0.649658203125]
['model.relu.alpha_mask_11_0', 4096, 2665.0, 0.650634765625]
['model.relu.alpha_mask_12_0', 4096, 2623.0, 0.640380859375]
['model.relu.alpha_mask_13_0', 2048, 1493.0, 0.72900390625]
['model.relu.alpha_mask_14_0', 2048, 1483.0, 0.72412109375]
['model.relu.alpha_mask_15_0', 2048, 1376.0, 0.671875]
['model.relu.alpha_mask_16_0', 2048, 1313.0, 0.64111328125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119460.0, 0.6340225883152174]
########## End ###########
06/01 02:07:38 AM | Train: [ 3/80] Step 000/390 Loss 0.269 Prec@(1,5) (94.5%, 100.0%)
06/01 02:07:38 AM | layerwise density: [41123.0, 10380.0, 10384.0, 10349.0, 10264.0, 5194.0, 5155.0, 5102.0, 5222.0, 2673.0, 2661.0, 2665.0, 2623.0, 1493.0, 1483.0, 1376.0, 1313.0]
layerwise density percentage: ['0.627', '0.634', '0.634', '0.632', '0.626', '0.634', '0.629', '0.623', '0.637', '0.653', '0.650', '0.651', '0.640', '0.729', '0.724', '0.672', '0.641']
Global density: 0.63402259349823
06/01 02:07:48 AM | Train: [ 3/80] Step 100/390 Loss 0.218 Prec@(1,5) (95.5%, 99.9%)
06/01 02:07:48 AM | layerwise density: [38195.0, 9626.0, 9640.0, 9641.0, 9552.0, 4831.0, 4787.0, 4735.0, 4871.0, 2494.0, 2476.0, 2490.0, 2447.0, 1431.0, 1420.0, 1312.0, 1244.0]
layerwise density percentage: ['0.583', '0.588', '0.588', '0.588', '0.583', '0.590', '0.584', '0.578', '0.595', '0.609', '0.604', '0.608', '0.597', '0.699', '0.693', '0.641', '0.607']
Global density: 0.5901409983634949
06/01 02:07:59 AM | Train: [ 3/80] Step 200/390 Loss 0.252 Prec@(1,5) (94.4%, 99.8%)
06/01 02:07:59 AM | layerwise density: [35414.0, 8923.0, 8960.0, 8933.0, 8801.0, 4487.0, 4453.0, 4366.0, 4502.0, 2319.0, 2307.0, 2327.0, 2316.0, 1377.0, 1356.0, 1235.0, 1162.0]
layerwise density percentage: ['0.540', '0.545', '0.547', '0.545', '0.537', '0.548', '0.544', '0.533', '0.550', '0.566', '0.563', '0.568', '0.565', '0.672', '0.662', '0.603', '0.567']
Global density: 0.5479258894920349
06/01 02:08:10 AM | Train: [ 3/80] Step 300/390 Loss 0.285 Prec@(1,5) (93.3%, 99.7%)
06/01 02:08:10 AM | layerwise density: [32656.0, 8189.0, 8254.0, 8230.0, 8178.0, 4189.0, 4110.0, 4054.0, 4171.0, 2156.0, 2151.0, 2147.0, 2154.0, 1316.0, 1307.0, 1162.0, 1080.0]
layerwise density percentage: ['0.498', '0.500', '0.504', '0.502', '0.499', '0.511', '0.502', '0.495', '0.509', '0.526', '0.525', '0.524', '0.526', '0.643', '0.638', '0.567', '0.527']
Global density: 0.5068784356117249
06/01 02:08:20 AM | Train: [ 3/80] Step 390/390 Loss 0.317 Prec@(1,5) (92.2%, 99.7%)
06/01 02:08:20 AM | layerwise density: [30126.0, 7568.0, 7654.0, 7636.0, 7547.0, 3875.0, 3843.0, 3750.0, 3821.0, 2011.0, 1998.0, 2008.0, 2007.0, 1279.0, 1257.0, 1091.0, 992.0]
layerwise density percentage: ['0.460', '0.462', '0.467', '0.466', '0.461', '0.473', '0.469', '0.458', '0.466', '0.491', '0.488', '0.490', '0.490', '0.625', '0.614', '0.533', '0.484']
Global density: 0.46950897574424744
06/01 02:08:20 AM | Train: [ 3/200] Final Prec@1 92.2300%
06/01 02:08:21 AM | Valid: [ 3/200] Step 000/078 Loss 1.267 Prec@(1,5) (71.9%, 90.6%)
06/01 02:08:23 AM | Valid: [ 3/200] Step 078/078 Loss 1.489 Prec@(1,5) (64.2%, 87.0%)
06/01 02:08:23 AM | Valid: [ 3/200] Final Prec@1 64.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 30072.0, 0.4588623046875]
['model.relu.alpha_mask_1_0', 16384, 7556.0, 0.461181640625]
['model.relu.alpha_mask_2_0', 16384, 7643.0, 0.46649169921875]
['model.relu.alpha_mask_3_0', 16384, 7630.0, 0.4656982421875]
['model.relu.alpha_mask_4_0', 16384, 7529.0, 0.45953369140625]
['model.relu.alpha_mask_5_0', 8192, 3869.0, 0.4722900390625]
['model.relu.alpha_mask_6_0', 8192, 3832.0, 0.4677734375]
['model.relu.alpha_mask_7_0', 8192, 3748.0, 0.45751953125]
['model.relu.alpha_mask_8_0', 8192, 3818.0, 0.466064453125]
['model.relu.alpha_mask_9_0', 4096, 2006.0, 0.48974609375]
['model.relu.alpha_mask_10_0', 4096, 1997.0, 0.487548828125]
['model.relu.alpha_mask_11_0', 4096, 2005.0, 0.489501953125]
['model.relu.alpha_mask_12_0', 4096, 2007.0, 0.489990234375]
['model.relu.alpha_mask_13_0', 2048, 1278.0, 0.6240234375]
['model.relu.alpha_mask_14_0', 2048, 1256.0, 0.61328125]
['model.relu.alpha_mask_15_0', 2048, 1087.0, 0.53076171875]
['model.relu.alpha_mask_16_0', 2048, 990.0, 0.4833984375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 88323.0, 0.4687659222146739]
########## End ###########
06/01 02:08:24 AM | Train: [ 4/80] Step 000/390 Loss 0.391 Prec@(1,5) (89.8%, 99.2%)
06/01 02:08:24 AM | layerwise density: [30072.0, 7556.0, 7643.0, 7630.0, 7529.0, 3869.0, 3832.0, 3748.0, 3818.0, 2006.0, 1997.0, 2005.0, 2007.0, 1278.0, 1256.0, 1087.0, 990.0]
layerwise density percentage: ['0.459', '0.461', '0.466', '0.466', '0.460', '0.472', '0.468', '0.458', '0.466', '0.490', '0.488', '0.490', '0.490', '0.624', '0.613', '0.531', '0.483']
Global density: 0.4687659442424774
06/01 02:08:35 AM | Train: [ 4/80] Step 100/390 Loss 0.359 Prec@(1,5) (91.0%, 99.7%)
06/01 02:08:35 AM | layerwise density: [27430.0, 6854.0, 6958.0, 6946.0, 6855.0, 3541.0, 3507.0, 3450.0, 3476.0, 1832.0, 1844.0, 1831.0, 1865.0, 1211.0, 1209.0, 1029.0, 915.0]
layerwise density percentage: ['0.419', '0.418', '0.425', '0.424', '0.418', '0.432', '0.428', '0.421', '0.424', '0.447', '0.450', '0.447', '0.455', '0.591', '0.590', '0.502', '0.447']
Global density: 0.4285888671875
06/01 02:08:45 AM | Train: [ 4/80] Step 200/390 Loss 0.400 Prec@(1,5) (89.7%, 99.5%)
06/01 02:08:45 AM | layerwise density: [24613.0, 6127.0, 6214.0, 6248.0, 6194.0, 3200.0, 3197.0, 3137.0, 3173.0, 1678.0, 1696.0, 1677.0, 1692.0, 1162.0, 1170.0, 970.0, 836.0]
layerwise density percentage: ['0.376', '0.374', '0.379', '0.381', '0.378', '0.391', '0.390', '0.383', '0.387', '0.410', '0.414', '0.409', '0.413', '0.567', '0.571', '0.474', '0.408']
Global density: 0.38735565543174744
06/01 02:08:56 AM | Train: [ 4/80] Step 300/390 Loss 0.445 Prec@(1,5) (88.3%, 99.3%)
06/01 02:08:56 AM | layerwise density: [22009.0, 5490.0, 5572.0, 5566.0, 5572.0, 2891.0, 2897.0, 2851.0, 2840.0, 1515.0, 1546.0, 1520.0, 1527.0, 1124.0, 1127.0, 904.0, 758.0]
layerwise density percentage: ['0.336', '0.335', '0.340', '0.340', '0.340', '0.353', '0.354', '0.348', '0.347', '0.370', '0.377', '0.371', '0.373', '0.549', '0.550', '0.441', '0.370']
Global density: 0.34874427318573
06/01 02:09:06 AM | Train: [ 4/80] Step 390/390 Loss 0.480 Prec@(1,5) (87.2%, 99.1%)
06/01 02:09:06 AM | layerwise density: [19647.0, 4871.0, 4971.0, 4917.0, 4974.0, 2578.0, 2619.0, 2544.0, 2568.0, 1354.0, 1435.0, 1394.0, 1390.0, 1086.0, 1099.0, 860.0, 667.0]
layerwise density percentage: ['0.300', '0.297', '0.303', '0.300', '0.304', '0.315', '0.320', '0.311', '0.313', '0.331', '0.350', '0.340', '0.339', '0.530', '0.537', '0.420', '0.326']
Global density: 0.31299889087677
06/01 02:09:06 AM | Train: [ 4/200] Final Prec@1 87.2140%
06/01 02:09:06 AM | Valid: [ 4/200] Step 000/078 Loss 1.286 Prec@(1,5) (68.0%, 90.6%)
06/01 02:09:09 AM | Valid: [ 4/200] Step 078/078 Loss 1.577 Prec@(1,5) (62.5%, 86.1%)
06/01 02:09:09 AM | Valid: [ 4/200] Final Prec@1 62.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 19590.0, 0.298919677734375]
['model.relu.alpha_mask_1_0', 16384, 4858.0, 0.2965087890625]
['model.relu.alpha_mask_2_0', 16384, 4955.0, 0.30242919921875]
['model.relu.alpha_mask_3_0', 16384, 4905.0, 0.29937744140625]
['model.relu.alpha_mask_4_0', 16384, 4962.0, 0.3028564453125]
['model.relu.alpha_mask_5_0', 8192, 2565.0, 0.3131103515625]
['model.relu.alpha_mask_6_0', 8192, 2615.0, 0.3192138671875]
['model.relu.alpha_mask_7_0', 8192, 2538.0, 0.309814453125]
['model.relu.alpha_mask_8_0', 8192, 2562.0, 0.312744140625]
['model.relu.alpha_mask_9_0', 4096, 1350.0, 0.32958984375]
['model.relu.alpha_mask_10_0', 4096, 1432.0, 0.349609375]
['model.relu.alpha_mask_11_0', 4096, 1392.0, 0.33984375]
['model.relu.alpha_mask_12_0', 4096, 1384.0, 0.337890625]
['model.relu.alpha_mask_13_0', 2048, 1086.0, 0.5302734375]
['model.relu.alpha_mask_14_0', 2048, 1100.0, 0.537109375]
['model.relu.alpha_mask_15_0', 2048, 858.0, 0.4189453125]
['model.relu.alpha_mask_16_0', 2048, 663.0, 0.32373046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 58815.0, 0.31215501868206524]
########## End ###########
06/01 02:09:10 AM | Train: [ 5/80] Step 000/390 Loss 0.442 Prec@(1,5) (85.9%, 100.0%)
06/01 02:09:10 AM | layerwise density: [19590.0, 4858.0, 4955.0, 4905.0, 4962.0, 2565.0, 2615.0, 2538.0, 2562.0, 1350.0, 1432.0, 1392.0, 1384.0, 1086.0, 1100.0, 858.0, 663.0]
layerwise density percentage: ['0.299', '0.297', '0.302', '0.299', '0.303', '0.313', '0.319', '0.310', '0.313', '0.330', '0.350', '0.340', '0.338', '0.530', '0.537', '0.419', '0.324']
Global density: 0.3121550381183624
06/01 02:09:21 AM | Train: [ 5/80] Step 100/390 Loss 0.535 Prec@(1,5) (85.7%, 98.8%)
06/01 02:09:21 AM | layerwise density: [17005.0, 4205.0, 4304.0, 4215.0, 4309.0, 2269.0, 2290.0, 2237.0, 2216.0, 1214.0, 1302.0, 1240.0, 1252.0, 1033.0, 1064.0, 807.0, 570.0]
layerwise density percentage: ['0.259', '0.257', '0.263', '0.257', '0.263', '0.277', '0.280', '0.273', '0.271', '0.296', '0.318', '0.303', '0.306', '0.504', '0.520', '0.394', '0.278']
Global density: 0.2735011875629425
06/01 02:09:32 AM | Train: [ 5/80] Step 200/390 Loss 0.589 Prec@(1,5) (84.1%, 98.4%)
06/01 02:09:32 AM | layerwise density: [14218.0, 3523.0, 3633.0, 3567.0, 3571.0, 1928.0, 1956.0, 1895.0, 1896.0, 1084.0, 1137.0, 1108.0, 1103.0, 987.0, 1024.0, 742.0, 475.0]
layerwise density percentage: ['0.217', '0.215', '0.222', '0.218', '0.218', '0.235', '0.239', '0.231', '0.231', '0.265', '0.278', '0.271', '0.269', '0.482', '0.500', '0.362', '0.232']
Global density: 0.23271378874778748
06/01 02:09:42 AM | Train: [ 5/80] Step 300/390 Loss 0.640 Prec@(1,5) (82.5%, 98.0%)
06/01 02:09:42 AM | layerwise density: [11608.0, 2861.0, 2997.0, 2937.0, 2907.0, 1651.0, 1645.0, 1596.0, 1580.0, 938.0, 977.0, 966.0, 972.0, 950.0, 991.0, 692.0, 400.0]
layerwise density percentage: ['0.177', '0.175', '0.183', '0.179', '0.177', '0.202', '0.201', '0.195', '0.193', '0.229', '0.239', '0.236', '0.237', '0.464', '0.484', '0.338', '0.195']
Global density: 0.19461192190647125
06/01 02:09:52 AM | Train: [ 5/80] Step 390/390 Loss 0.694 Prec@(1,5) (81.1%, 97.6%)
06/01 02:09:52 AM | layerwise density: [9291.0, 2243.0, 2401.0, 2366.0, 2320.0, 1391.0, 1380.0, 1295.0, 1309.0, 800.0, 867.0, 838.0, 842.0, 902.0, 963.0, 641.0, 327.0]
layerwise density percentage: ['0.142', '0.137', '0.147', '0.144', '0.142', '0.170', '0.168', '0.158', '0.160', '0.195', '0.212', '0.205', '0.206', '0.440', '0.470', '0.313', '0.160']
Global density: 0.16015625
06/01 02:09:52 AM | Train: [ 5/200] Final Prec@1 81.0720%
06/01 02:09:52 AM | Valid: [ 5/200] Step 000/078 Loss 1.625 Prec@(1,5) (65.6%, 82.8%)
06/01 02:09:55 AM | Valid: [ 5/200] Step 078/078 Loss 1.790 Prec@(1,5) (56.8%, 83.2%)
06/01 02:09:55 AM | Valid: [ 5/200] Final Prec@1 56.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 9238.0, 0.140960693359375]
['model.relu.alpha_mask_1_0', 16384, 2230.0, 0.1361083984375]
['model.relu.alpha_mask_2_0', 16384, 2389.0, 0.14581298828125]
['model.relu.alpha_mask_3_0', 16384, 2349.0, 0.14337158203125]
['model.relu.alpha_mask_4_0', 16384, 2312.0, 0.14111328125]
['model.relu.alpha_mask_5_0', 8192, 1384.0, 0.1689453125]
['model.relu.alpha_mask_6_0', 8192, 1375.0, 0.1678466796875]
['model.relu.alpha_mask_7_0', 8192, 1294.0, 0.157958984375]
['model.relu.alpha_mask_8_0', 8192, 1301.0, 0.1588134765625]
['model.relu.alpha_mask_9_0', 4096, 797.0, 0.194580078125]
['model.relu.alpha_mask_10_0', 4096, 866.0, 0.21142578125]
['model.relu.alpha_mask_11_0', 4096, 835.0, 0.203857421875]
['model.relu.alpha_mask_12_0', 4096, 841.0, 0.205322265625]
['model.relu.alpha_mask_13_0', 2048, 899.0, 0.43896484375]
['model.relu.alpha_mask_14_0', 2048, 963.0, 0.47021484375]
['model.relu.alpha_mask_15_0', 2048, 641.0, 0.31298828125]
['model.relu.alpha_mask_16_0', 2048, 322.0, 0.1572265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 30036.0, 0.15941321331521738]
########## End ###########
06/01 02:09:56 AM | Train: [ 6/80] Step 000/390 Loss 0.701 Prec@(1,5) (83.6%, 98.4%)
06/01 02:09:56 AM | layerwise density: [9238.0, 2230.0, 2389.0, 2349.0, 2312.0, 1384.0, 1375.0, 1294.0, 1301.0, 797.0, 866.0, 835.0, 841.0, 899.0, 963.0, 641.0, 322.0]
layerwise density percentage: ['0.141', '0.136', '0.146', '0.143', '0.141', '0.169', '0.168', '0.158', '0.159', '0.195', '0.211', '0.204', '0.205', '0.439', '0.470', '0.313', '0.157']
Global density: 0.15941321849822998
06/01 02:10:06 AM | Train: [ 6/80] Step 100/390 Loss 0.794 Prec@(1,5) (78.1%, 96.8%)
06/01 02:10:06 AM | layerwise density: [6680.0, 1604.0, 1739.0, 1684.0, 1652.0, 1077.0, 1080.0, 966.0, 1024.0, 669.0, 722.0, 688.0, 685.0, 847.0, 929.0, 588.0, 240.0]
layerwise density percentage: ['0.102', '0.098', '0.106', '0.103', '0.101', '0.131', '0.132', '0.118', '0.125', '0.163', '0.176', '0.168', '0.167', '0.414', '0.454', '0.287', '0.117']
Global density: 0.1214015781879425
06/01 02:10:17 AM | Train: [ 6/80] Step 200/390 Loss 0.870 Prec@(1,5) (76.2%, 96.0%)
06/01 02:10:17 AM | layerwise density: [4100.0, 981.0, 1094.0, 1052.0, 1008.0, 780.0, 811.0, 686.0, 724.0, 523.0, 627.0, 542.0, 537.0, 815.0, 890.0, 552.0, 166.0]
layerwise density percentage: ['0.063', '0.060', '0.067', '0.064', '0.062', '0.095', '0.099', '0.084', '0.088', '0.128', '0.153', '0.132', '0.131', '0.398', '0.435', '0.270', '0.081']
Global density: 0.08432404696941376
06/01 02:10:28 AM | Train: [ 6/80] Step 300/390 Loss 0.923 Prec@(1,5) (74.7%, 95.4%)
06/01 02:10:28 AM | layerwise density: [2900.0, 669.0, 788.0, 773.0, 732.0, 647.0, 673.0, 534.0, 590.0, 448.0, 561.0, 467.0, 473.0, 839.0, 924.0, 551.0, 114.0]
layerwise density percentage: ['0.044', '0.041', '0.048', '0.047', '0.045', '0.079', '0.082', '0.065', '0.072', '0.109', '0.137', '0.114', '0.115', '0.410', '0.451', '0.269', '0.056']
Global density: 0.06731382012367249
06/01 02:10:39 AM | Train: [ 6/80] Step 390/390 Loss 0.930 Prec@(1,5) (74.5%, 95.3%)
06/01 02:10:39 AM | layerwise density: [2859.0, 672.0, 782.0, 769.0, 728.0, 663.0, 679.0, 541.0, 600.0, 465.0, 578.0, 489.0, 487.0, 890.0, 982.0, 600.0, 115.0]
layerwise density percentage: ['0.044', '0.041', '0.048', '0.047', '0.044', '0.081', '0.083', '0.066', '0.073', '0.114', '0.141', '0.119', '0.119', '0.435', '0.479', '0.293', '0.056']
Global density: 0.06846021860837936
06/01 02:10:39 AM | Train: [ 6/200] Final Prec@1 74.5020%
06/01 02:10:39 AM | Valid: [ 6/200] Step 000/078 Loss 1.375 Prec@(1,5) (70.3%, 88.3%)
06/01 02:10:41 AM | Valid: [ 6/200] Step 078/078 Loss 1.572 Prec@(1,5) (62.3%, 86.2%)
06/01 02:10:41 AM | Valid: [ 6/200] Final Prec@1 62.2500%
06/01 02:10:42 AM | Current mask training best Prec@1 = 62.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 2859.0, 0.0436248779296875]
['model.relu.alpha_mask_1_0', 16384, 672.0, 0.041015625]
['model.relu.alpha_mask_2_0', 16384, 782.0, 0.0477294921875]
['model.relu.alpha_mask_3_0', 16384, 769.0, 0.04693603515625]
['model.relu.alpha_mask_4_0', 16384, 728.0, 0.04443359375]
['model.relu.alpha_mask_5_0', 8192, 663.0, 0.0809326171875]
['model.relu.alpha_mask_6_0', 8192, 679.0, 0.0828857421875]
['model.relu.alpha_mask_7_0', 8192, 541.0, 0.0660400390625]
['model.relu.alpha_mask_8_0', 8192, 600.0, 0.0732421875]
['model.relu.alpha_mask_9_0', 4096, 465.0, 0.113525390625]
['model.relu.alpha_mask_10_0', 4096, 578.0, 0.14111328125]
['model.relu.alpha_mask_11_0', 4096, 489.0, 0.119384765625]
['model.relu.alpha_mask_12_0', 4096, 487.0, 0.118896484375]
['model.relu.alpha_mask_13_0', 2048, 890.0, 0.4345703125]
['model.relu.alpha_mask_14_0', 2048, 982.0, 0.4794921875]
['model.relu.alpha_mask_15_0', 2048, 600.0, 0.29296875]
['model.relu.alpha_mask_16_0', 2048, 115.0, 0.05615234375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 12899.0, 0.06846021569293478]
########## End ###########
06/01 02:10:43 AM | Train: [ 7/80] Step 000/390 Loss 0.660 Prec@(1,5) (84.4%, 96.9%)
06/01 02:10:43 AM | layerwise density: [2859.0, 672.0, 782.0, 769.0, 728.0, 663.0, 679.0, 541.0, 600.0, 465.0, 578.0, 489.0, 487.0, 890.0, 982.0, 600.0, 115.0]
layerwise density percentage: ['0.044', '0.041', '0.048', '0.047', '0.044', '0.081', '0.083', '0.066', '0.073', '0.114', '0.141', '0.119', '0.119', '0.435', '0.479', '0.293', '0.056']
Global density: 0.06846021860837936
06/01 02:10:53 AM | Train: [ 7/80] Step 100/390 Loss 0.702 Prec@(1,5) (80.2%, 97.6%)
06/01 02:10:53 AM | layerwise density: [2726.0, 660.0, 764.0, 757.0, 703.0, 681.0, 688.0, 544.0, 607.0, 475.0, 596.0, 506.0, 501.0, 913.0, 1016.0, 634.0, 117.0]
layerwise density percentage: ['0.042', '0.040', '0.047', '0.046', '0.043', '0.083', '0.084', '0.066', '0.074', '0.116', '0.146', '0.124', '0.122', '0.446', '0.496', '0.310', '0.057']
Global density: 0.06840183585882187
06/01 02:11:04 AM | Train: [ 7/80] Step 200/390 Loss 0.690 Prec@(1,5) (80.6%, 97.7%)
06/01 02:11:04 AM | layerwise density: [2557.0, 646.0, 743.0, 731.0, 678.0, 690.0, 690.0, 548.0, 609.0, 486.0, 616.0, 520.0, 520.0, 948.0, 1044.0, 664.0, 118.0]
layerwise density percentage: ['0.039', '0.039', '0.045', '0.045', '0.041', '0.084', '0.084', '0.067', '0.074', '0.119', '0.150', '0.127', '0.127', '0.463', '0.510', '0.324', '0.058']
Global density: 0.06797724217176437
06/01 02:11:15 AM | Train: [ 7/80] Step 300/390 Loss 0.682 Prec@(1,5) (80.9%, 97.7%)
06/01 02:11:15 AM | layerwise density: [2445.0, 645.0, 729.0, 720.0, 673.0, 711.0, 701.0, 556.0, 615.0, 504.0, 638.0, 542.0, 545.0, 965.0, 1080.0, 692.0, 120.0]
layerwise density percentage: ['0.037', '0.039', '0.044', '0.044', '0.041', '0.087', '0.086', '0.068', '0.075', '0.123', '0.156', '0.132', '0.133', '0.471', '0.527', '0.338', '0.059']
Global density: 0.0683646872639656
