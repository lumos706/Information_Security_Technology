[HAMI-core Msg(622451:133346881969024:libvgpu.c:837)]: Initializing.....
06/02 06:14:00 PM | 
06/02 06:14:00 PM | Parameters:
06/02 06:14:00 PM | NUM_MASK=1
06/02 06:14:00 PM | RELU_COUNT=120.0
06/02 06:14:00 PM | ACT_TYPE=ReLU_masked_dapa_relay
06/02 06:14:00 PM | ALPHA_LR=0.0002
06/02 06:14:00 PM | ALPHA_WEIGHT_DECAY=0.001
06/02 06:14:00 PM | ARCH=resnet18
06/02 06:14:00 PM | BATCH_SIZE=128
06/02 06:14:00 PM | CHECKPOINT_PATH=None
06/02 06:14:00 PM | CLIP_X2=1.0
06/02 06:14:00 PM | CLIP_X2_BOOL=True
06/02 06:14:00 PM | DATA_PATH=./data/
06/02 06:14:00 PM | DATASET=cifar100
06/02 06:14:00 PM | DEGREE=2
06/02 06:14:00 PM | DISTIL=True
06/02 06:14:00 PM | DROPOUT=0
06/02 06:14:00 PM | ENABLE_GRAD_NORM=False
06/02 06:14:00 PM | ENABLE_LOOKAHEAD=True
06/02 06:14:00 PM | EPOCHS=200
06/02 06:14:00 PM | EVALUATE=None
06/02 06:14:00 PM | EXT=baseline
06/02 06:14:00 PM | FREEZEACT=False
06/02 06:14:00 PM | GPUS=[0]
06/02 06:14:00 PM | LAMDA=240.0
06/02 06:14:00 PM | MASK_DROPOUT=0
06/02 06:14:00 PM | MASK_EPOCHS=80
06/02 06:14:00 PM | NUM_CLASSES=100
06/02 06:14:00 PM | OPTIM=cosine
06/02 06:14:00 PM | PATH=train_cifar_dapa2_distil_relay/resnet18_resnet18_cifar100_relay_0.003/cosine_ReLUs120.0wm_lr0.001mep80_baseline
06/02 06:14:00 PM | PLOT_PATH=train_cifar_dapa2_distil_relay/resnet18_resnet18_cifar100_relay_0.003/cosine_ReLUs120.0wm_lr0.001mep80_baseline/plots
06/02 06:14:00 PM | PRECISION=full
06/02 06:14:00 PM | PRETRAINED=False
06/02 06:14:00 PM | PRETRAINED_PATH=./train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar
06/02 06:14:00 PM | PRINT_FREQ=100
06/02 06:14:00 PM | SCALE_X1=1.0
06/02 06:14:00 PM | SCALE_X2=2.0
06/02 06:14:00 PM | SEED=2
06/02 06:14:00 PM | START_EPOCH=0
06/02 06:14:00 PM | TEACHER_ARCH=resnet18
06/02 06:14:00 PM | TEACHER_PATH=./train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar
06/02 06:14:00 PM | THRESHOLD=0.003
06/02 06:14:00 PM | VAR_MIN=0.5
06/02 06:14:00 PM | W_DECAY_EPOCH=20
06/02 06:14:00 PM | W_GRAD_CLIP=5.0
06/02 06:14:00 PM | W_LR=0.0001
06/02 06:14:00 PM | W_LR_MIN=1e-05
06/02 06:14:00 PM | W_MASK_LR=0.001
06/02 06:14:00 PM | W_MOMENTUM=0.9
06/02 06:14:00 PM | W_WEIGHT_DECAY=0.0005
06/02 06:14:00 PM | WORKERS=4
06/02 06:14:00 PM | X_SIZE=[1, 3, 32, 32]
06/02 06:14:00 PM | 
06/02 06:14:00 PM | Logger is set - training start
[HAMI-core Msg(622451:133346881969024:libvgpu.c:856)]: Initialized
==> Load pretrained
=> loading checkpoint './train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar'
/mnt/ann25-22336216/AutoReP/models_util/model_util.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_path, map_location = "cpu")
=> loading checkpoint './train_cifar/resnet18__cifar100/cosine_baseline_ReLUs0lr0.1ep400_baseline/best.pth.tar'
Files already downloaded and verified
Files already downloaded and verified
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 65536.0, 1.0]
['model.relu.alpha_mask_1_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_2_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_3_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_4_0', 16384, 16384.0, 1.0]
['model.relu.alpha_mask_5_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_6_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_7_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_8_0', 8192, 8192.0, 1.0]
['model.relu.alpha_mask_9_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_10_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_11_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_12_0', 4096, 4096.0, 1.0]
['model.relu.alpha_mask_13_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_14_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_15_0', 2048, 2048.0, 1.0]
['model.relu.alpha_mask_16_0', 2048, 2048.0, 1.0]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 188416.0, 1.0]
########## End ###########
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16601579520 total=17059545088 limit=4194304000 usage=415246848
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16366698496 total=17059545088 limit=4194304000 usage=650127872
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16148594688 total=17059545088 limit=4194304000 usage=868231680
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16106651648 total=17059545088 limit=4194304000 usage=910174720
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16085680128 total=17059545088 limit=4194304000 usage=931146240
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16066805760 total=17059545088 limit=4194304000 usage=950020608
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16045834240 total=17059545088 limit=4194304000 usage=970992128
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16039542784 total=17059545088 limit=4194304000 usage=977283584
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16037445632 total=17059545088 limit=4194304000 usage=979380736
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15760621568 total=17059545088 limit=4194304000 usage=1252285440
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15703998464 total=17059545088 limit=4194304000 usage=1308908544
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15703998464 total=17059545088 limit=4194304000 usage=1308908544
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15708192768 total=17059545088 limit=4194304000 usage=1304714240
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15651569664 total=17059545088 limit=4194304000 usage=1361337344
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15710289920 total=17059545088 limit=4194304000 usage=1302617088
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15636889600 total=17059545088 limit=4194304000 usage=1376017408
06/02 06:14:05 PM | Train: [ 1/80] Step 000/390 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)
06/02 06:14:05 PM | layerwise density: [65536.0, 16384.0, 16384.0, 16384.0, 16384.0, 8192.0, 8192.0, 8192.0, 8192.0, 4096.0, 4096.0, 4096.0, 4096.0, 2048.0, 2048.0, 2048.0, 2048.0]
layerwise density percentage: ['1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000', '1.000']
Global density: 1.0
06/02 06:14:16 PM | Train: [ 1/80] Step 100/390 Loss 0.024 Prec@(1,5) (99.9%, 100.0%)
06/02 06:14:16 PM | layerwise density: [62127.0, 15548.0, 15555.0, 15539.0, 15548.0, 7760.0, 7767.0, 7748.0, 7761.0, 3883.0, 3893.0, 3905.0, 3901.0, 1974.0, 1971.0, 1954.0, 1934.0]
layerwise density percentage: ['0.948', '0.949', '0.949', '0.948', '0.949', '0.947', '0.948', '0.946', '0.947', '0.948', '0.950', '0.953', '0.952', '0.964', '0.962', '0.954', '0.944']
Global density: 0.9487941861152649
06/02 06:14:27 PM | Train: [ 1/80] Step 200/390 Loss 0.032 Prec@(1,5) (99.8%, 100.0%)
06/02 06:14:27 PM | layerwise density: [58887.0, 14811.0, 14758.0, 14797.0, 14781.0, 7359.0, 7305.0, 7346.0, 7408.0, 3708.0, 3700.0, 3727.0, 3705.0, 1904.0, 1893.0, 1877.0, 1854.0]
layerwise density percentage: ['0.899', '0.904', '0.901', '0.903', '0.902', '0.898', '0.892', '0.897', '0.904', '0.905', '0.903', '0.910', '0.905', '0.930', '0.924', '0.917', '0.905']
Global density: 0.9013035297393799
06/02 06:14:38 PM | Train: [ 1/80] Step 300/390 Loss 0.044 Prec@(1,5) (99.7%, 100.0%)
06/02 06:14:38 PM | layerwise density: [55632.0, 13997.0, 13960.0, 13989.0, 14020.0, 6923.0, 6925.0, 6961.0, 7002.0, 3523.0, 3518.0, 3532.0, 3527.0, 1819.0, 1817.0, 1772.0, 1778.0]
layerwise density percentage: ['0.849', '0.854', '0.852', '0.854', '0.856', '0.845', '0.845', '0.850', '0.855', '0.860', '0.859', '0.862', '0.861', '0.888', '0.887', '0.865', '0.868']
Global density: 0.8528734445571899
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=15464923136 total=17059545088 limit=4194304000 usage=1547983872
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16190537728 total=17059545088 limit=4194304000 usage=822369280
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16085680128 total=17059545088 limit=4194304000 usage=927226880
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16064708608 total=17059545088 limit=4194304000 usage=948198400
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16064708608 total=17059545088 limit=4194304000 usage=948198400
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16022765568 total=17059545088 limit=4194304000 usage=990141440
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16022765568 total=17059545088 limit=4194304000 usage=990141440
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16005988352 total=17059545088 limit=4194304000 usage=1006918656
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=15989211136 total=17059545088 limit=4194304000 usage=1023695872
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=15999696896 total=17059545088 limit=4194304000 usage=1013210112
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=15999696896 total=17059545088 limit=4194304000 usage=1013210112
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15821438976 total=17059545088 limit=4194304000 usage=1191468032
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15792078848 total=17059545088 limit=4194304000 usage=1220828160
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15792078848 total=17059545088 limit=4194304000 usage=1220828160
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15794176000 total=17059545088 limit=4194304000 usage=1218731008
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15827730432 total=17059545088 limit=4194304000 usage=1185176576
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15827730432 total=17059545088 limit=4194304000 usage=1185176576
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15844507648 total=17059545088 limit=4194304000 usage=1168399360
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15861284864 total=17059545088 limit=4194304000 usage=1151622144
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15882256384 total=17059545088 limit=4194304000 usage=1130650624
[HAMI-core Msg(622451:133341768582720:memory.c:512)]: orig free=15857090560 total=17059545088 limit=4194304000 usage=1155816448
06/02 06:14:49 PM | Train: [ 1/80] Step 390/390 Loss 0.058 Prec@(1,5) (99.4%, 100.0%)
06/02 06:14:49 PM | layerwise density: [52855.0, 13289.0, 13276.0, 13327.0, 13249.0, 6583.0, 6593.0, 6586.0, 6651.0, 3354.0, 3354.0, 3356.0, 3345.0, 1750.0, 1752.0, 1702.0, 1672.0]
layerwise density percentage: ['0.807', '0.811', '0.810', '0.813', '0.809', '0.804', '0.805', '0.804', '0.812', '0.819', '0.819', '0.819', '0.817', '0.854', '0.855', '0.831', '0.816']
Global density: 0.810408890247345
06/02 06:14:49 PM | Train: [ 1/200] Final Prec@1 99.4140%
06/02 06:14:49 PM | Valid: [ 1/200] Step 000/078 Loss 1.103 Prec@(1,5) (73.4%, 91.4%)
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=15781593088 total=17059545088 limit=4194304000 usage=1231313920
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16305881088 total=17059545088 limit=4194304000 usage=707025920
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16307978240 total=17059545088 limit=4194304000 usage=704928768
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16310075392 total=17059545088 limit=4194304000 usage=702831616
[HAMI-core Msg(622451:133346881969024:memory.c:512)]: orig free=16282812416 total=17059545088 limit=4194304000 usage=730094592
06/02 06:14:52 PM | Valid: [ 1/200] Step 078/078 Loss 1.324 Prec@(1,5) (68.7%, 89.1%)
06/02 06:14:52 PM | Valid: [ 1/200] Final Prec@1 68.6900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 52855.0, 0.8065032958984375]
['model.relu.alpha_mask_1_0', 16384, 13289.0, 0.81109619140625]
['model.relu.alpha_mask_2_0', 16384, 13276.0, 0.810302734375]
['model.relu.alpha_mask_3_0', 16384, 13327.0, 0.81341552734375]
['model.relu.alpha_mask_4_0', 16384, 13249.0, 0.80865478515625]
['model.relu.alpha_mask_5_0', 8192, 6583.0, 0.8035888671875]
['model.relu.alpha_mask_6_0', 8192, 6593.0, 0.8048095703125]
['model.relu.alpha_mask_7_0', 8192, 6586.0, 0.803955078125]
['model.relu.alpha_mask_8_0', 8192, 6651.0, 0.8118896484375]
['model.relu.alpha_mask_9_0', 4096, 3354.0, 0.81884765625]
['model.relu.alpha_mask_10_0', 4096, 3354.0, 0.81884765625]
['model.relu.alpha_mask_11_0', 4096, 3356.0, 0.8193359375]
['model.relu.alpha_mask_12_0', 4096, 3345.0, 0.816650390625]
['model.relu.alpha_mask_13_0', 2048, 1750.0, 0.8544921875]
['model.relu.alpha_mask_14_0', 2048, 1752.0, 0.85546875]
['model.relu.alpha_mask_15_0', 2048, 1702.0, 0.8310546875]
['model.relu.alpha_mask_16_0', 2048, 1672.0, 0.81640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 152694.0, 0.810408882472826]
########## End ###########
06/02 06:14:53 PM | Train: [ 2/80] Step 000/390 Loss 0.077 Prec@(1,5) (99.2%, 100.0%)
06/02 06:14:53 PM | layerwise density: [52855.0, 13289.0, 13276.0, 13327.0, 13249.0, 6583.0, 6593.0, 6586.0, 6651.0, 3354.0, 3354.0, 3356.0, 3345.0, 1750.0, 1752.0, 1702.0, 1672.0]
layerwise density percentage: ['0.807', '0.811', '0.810', '0.813', '0.809', '0.804', '0.805', '0.804', '0.812', '0.819', '0.819', '0.819', '0.817', '0.854', '0.855', '0.831', '0.816']
Global density: 0.810408890247345
06/02 06:15:04 PM | Train: [ 2/80] Step 100/390 Loss 0.111 Prec@(1,5) (98.4%, 100.0%)
06/02 06:15:04 PM | layerwise density: [49713.0, 12539.0, 12533.0, 12544.0, 12445.0, 6220.0, 6200.0, 6193.0, 6267.0, 3157.0, 3193.0, 3150.0, 3148.0, 1682.0, 1679.0, 1607.0, 1574.0]
layerwise density percentage: ['0.759', '0.765', '0.765', '0.766', '0.760', '0.759', '0.757', '0.756', '0.765', '0.771', '0.780', '0.769', '0.769', '0.821', '0.820', '0.785', '0.769']
Global density: 0.76343834400177
06/02 06:15:14 PM | Train: [ 2/80] Step 200/390 Loss 0.131 Prec@(1,5) (97.9%, 100.0%)
06/02 06:15:14 PM | layerwise density: [46770.0, 11768.0, 11803.0, 11828.0, 11708.0, 5859.0, 5823.0, 5856.0, 5910.0, 2984.0, 3012.0, 2984.0, 2957.0, 1609.0, 1603.0, 1529.0, 1491.0]
layerwise density percentage: ['0.714', '0.718', '0.720', '0.722', '0.715', '0.715', '0.711', '0.715', '0.721', '0.729', '0.735', '0.729', '0.722', '0.786', '0.783', '0.747', '0.728']
Global density: 0.719121515750885
06/02 06:15:25 PM | Train: [ 2/80] Step 300/390 Loss 0.158 Prec@(1,5) (97.2%, 100.0%)
06/02 06:15:25 PM | layerwise density: [43761.0, 11041.0, 11054.0, 11051.0, 10977.0, 5523.0, 5485.0, 5461.0, 5529.0, 2810.0, 2825.0, 2822.0, 2765.0, 1547.0, 1526.0, 1445.0, 1399.0]
layerwise density percentage: ['0.668', '0.674', '0.675', '0.674', '0.670', '0.674', '0.670', '0.667', '0.675', '0.686', '0.690', '0.689', '0.675', '0.755', '0.745', '0.706', '0.683']
Global density: 0.6741518974304199
06/02 06:15:35 PM | Train: [ 2/80] Step 390/390 Loss 0.181 Prec@(1,5) (96.4%, 100.0%)
06/02 06:15:35 PM | layerwise density: [41221.0, 10407.0, 10408.0, 10376.0, 10288.0, 5205.0, 5167.0, 5111.0, 5241.0, 2679.0, 2662.0, 2669.0, 2630.0, 1500.0, 1484.0, 1377.0, 1320.0]
layerwise density percentage: ['0.629', '0.635', '0.635', '0.633', '0.628', '0.635', '0.631', '0.624', '0.640', '0.654', '0.650', '0.652', '0.642', '0.732', '0.725', '0.672', '0.645']
Global density: 0.6355352401733398
06/02 06:15:35 PM | Train: [ 2/200] Final Prec@1 96.4460%
06/02 06:15:35 PM | Valid: [ 2/200] Step 000/078 Loss 1.140 Prec@(1,5) (75.0%, 91.4%)
06/02 06:15:37 PM | Valid: [ 2/200] Step 078/078 Loss 1.381 Prec@(1,5) (67.2%, 88.3%)
06/02 06:15:38 PM | Valid: [ 2/200] Final Prec@1 67.2200%
06/02 06:15:38 PM | Current mask training best Prec@1 = 67.2200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 41221.0, 0.6289825439453125]
['model.relu.alpha_mask_1_0', 16384, 10407.0, 0.63519287109375]
['model.relu.alpha_mask_2_0', 16384, 10408.0, 0.63525390625]
['model.relu.alpha_mask_3_0', 16384, 10376.0, 0.63330078125]
['model.relu.alpha_mask_4_0', 16384, 10288.0, 0.6279296875]
['model.relu.alpha_mask_5_0', 8192, 5205.0, 0.6353759765625]
['model.relu.alpha_mask_6_0', 8192, 5167.0, 0.6307373046875]
['model.relu.alpha_mask_7_0', 8192, 5111.0, 0.6239013671875]
['model.relu.alpha_mask_8_0', 8192, 5241.0, 0.6397705078125]
['model.relu.alpha_mask_9_0', 4096, 2679.0, 0.654052734375]
['model.relu.alpha_mask_10_0', 4096, 2662.0, 0.64990234375]
['model.relu.alpha_mask_11_0', 4096, 2669.0, 0.651611328125]
['model.relu.alpha_mask_12_0', 4096, 2630.0, 0.64208984375]
['model.relu.alpha_mask_13_0', 2048, 1500.0, 0.732421875]
['model.relu.alpha_mask_14_0', 2048, 1484.0, 0.724609375]
['model.relu.alpha_mask_15_0', 2048, 1377.0, 0.67236328125]
['model.relu.alpha_mask_16_0', 2048, 1320.0, 0.64453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119745.0, 0.6355351987092391]
########## End ###########
06/02 06:15:39 PM | Train: [ 3/80] Step 000/390 Loss 0.251 Prec@(1,5) (96.1%, 100.0%)
06/02 06:15:39 PM | layerwise density: [41221.0, 10407.0, 10408.0, 10376.0, 10288.0, 5205.0, 5167.0, 5111.0, 5241.0, 2679.0, 2662.0, 2669.0, 2630.0, 1500.0, 1484.0, 1377.0, 1320.0]
layerwise density percentage: ['0.629', '0.635', '0.635', '0.633', '0.628', '0.635', '0.631', '0.624', '0.640', '0.654', '0.650', '0.652', '0.642', '0.732', '0.725', '0.672', '0.645']
Global density: 0.6355352401733398
06/02 06:15:50 PM | Train: [ 3/80] Step 100/390 Loss 0.162 Prec@(1,5) (97.1%, 99.9%)
06/02 06:15:50 PM | layerwise density: [41153.0, 10393.0, 10389.0, 10359.0, 10271.0, 5197.0, 5155.0, 5103.0, 5228.0, 2676.0, 2659.0, 2666.0, 2626.0, 1553.0, 1544.0, 1423.0, 1323.0]
layerwise density percentage: ['0.628', '0.634', '0.634', '0.632', '0.627', '0.634', '0.629', '0.623', '0.638', '0.653', '0.649', '0.651', '0.641', '0.758', '0.754', '0.695', '0.646']
Global density: 0.635391891002655
06/02 06:16:01 PM | Train: [ 3/80] Step 200/390 Loss 0.152 Prec@(1,5) (97.2%, 99.9%)
06/02 06:16:01 PM | layerwise density: [41139.0, 10393.0, 10388.0, 10353.0, 10263.0, 5194.0, 5151.0, 5100.0, 5226.0, 2676.0, 2664.0, 2672.0, 2629.0, 1591.0, 1586.0, 1448.0, 1335.0]
layerwise density percentage: ['0.628', '0.634', '0.634', '0.632', '0.626', '0.634', '0.629', '0.623', '0.638', '0.653', '0.650', '0.652', '0.642', '0.777', '0.774', '0.707', '0.652']
Global density: 0.635869562625885
06/02 06:16:11 PM | Train: [ 3/80] Step 300/390 Loss 0.141 Prec@(1,5) (97.3%, 100.0%)
06/02 06:16:11 PM | layerwise density: [41130.0, 10390.0, 10387.0, 10354.0, 10261.0, 5197.0, 5154.0, 5099.0, 5228.0, 2682.0, 2669.0, 2676.0, 2639.0, 1621.0, 1639.0, 1464.0, 1343.0]
layerwise density percentage: ['0.628', '0.634', '0.634', '0.632', '0.626', '0.634', '0.629', '0.622', '0.638', '0.655', '0.652', '0.653', '0.644', '0.792', '0.800', '0.715', '0.656']
Global density: 0.6365330219268799
06/02 06:16:21 PM | Train: [ 3/80] Step 390/390 Loss 0.133 Prec@(1,5) (97.5%, 100.0%)
06/02 06:16:21 PM | layerwise density: [41099.0, 10381.0, 10380.0, 10347.0, 10258.0, 5199.0, 5148.0, 5100.0, 5222.0, 2687.0, 2674.0, 2684.0, 2642.0, 1644.0, 1670.0, 1481.0, 1350.0]
layerwise density percentage: ['0.627', '0.634', '0.634', '0.632', '0.626', '0.635', '0.628', '0.623', '0.637', '0.656', '0.653', '0.655', '0.645', '0.803', '0.815', '0.723', '0.659']
Global density: 0.63670814037323
06/02 06:16:21 PM | Train: [ 3/200] Final Prec@1 97.4980%
06/02 06:16:21 PM | Valid: [ 3/200] Step 000/078 Loss 1.184 Prec@(1,5) (71.1%, 93.0%)
06/02 06:16:23 PM | Valid: [ 3/200] Step 078/078 Loss 1.409 Prec@(1,5) (67.3%, 87.9%)
06/02 06:16:23 PM | Valid: [ 3/200] Final Prec@1 67.3500%
06/02 06:16:24 PM | Current mask training best Prec@1 = 67.3500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 41097.0, 0.6270904541015625]
['model.relu.alpha_mask_1_0', 16384, 10381.0, 0.63360595703125]
['model.relu.alpha_mask_2_0', 16384, 10380.0, 0.633544921875]
['model.relu.alpha_mask_3_0', 16384, 10347.0, 0.63153076171875]
['model.relu.alpha_mask_4_0', 16384, 10258.0, 0.6260986328125]
['model.relu.alpha_mask_5_0', 8192, 5199.0, 0.6346435546875]
['model.relu.alpha_mask_6_0', 8192, 5148.0, 0.62841796875]
['model.relu.alpha_mask_7_0', 8192, 5100.0, 0.62255859375]
['model.relu.alpha_mask_8_0', 8192, 5222.0, 0.637451171875]
['model.relu.alpha_mask_9_0', 4096, 2686.0, 0.65576171875]
['model.relu.alpha_mask_10_0', 4096, 2674.0, 0.65283203125]
['model.relu.alpha_mask_11_0', 4096, 2684.0, 0.6552734375]
['model.relu.alpha_mask_12_0', 4096, 2642.0, 0.64501953125]
['model.relu.alpha_mask_13_0', 2048, 1643.0, 0.80224609375]
['model.relu.alpha_mask_14_0', 2048, 1670.0, 0.8154296875]
['model.relu.alpha_mask_15_0', 2048, 1481.0, 0.72314453125]
['model.relu.alpha_mask_16_0', 2048, 1350.0, 0.6591796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119962.0, 0.6366869055706522]
########## End ###########
06/02 06:16:25 PM | Train: [ 4/80] Step 000/390 Loss 0.097 Prec@(1,5) (97.7%, 100.0%)
06/02 06:16:25 PM | layerwise density: [41097.0, 10381.0, 10380.0, 10347.0, 10258.0, 5199.0, 5148.0, 5100.0, 5222.0, 2686.0, 2674.0, 2684.0, 2642.0, 1643.0, 1670.0, 1481.0, 1350.0]
layerwise density percentage: ['0.627', '0.634', '0.634', '0.632', '0.626', '0.635', '0.628', '0.623', '0.637', '0.656', '0.653', '0.655', '0.645', '0.802', '0.815', '0.723', '0.659']
Global density: 0.6366869211196899
06/02 06:16:36 PM | Train: [ 4/80] Step 100/390 Loss 0.072 Prec@(1,5) (98.8%, 100.0%)
06/02 06:16:36 PM | layerwise density: [41064.0, 10377.0, 10373.0, 10335.0, 10255.0, 5201.0, 5149.0, 5100.0, 5218.0, 2688.0, 2683.0, 2686.0, 2646.0, 1653.0, 1695.0, 1499.0, 1357.0]
layerwise density percentage: ['0.627', '0.633', '0.633', '0.631', '0.626', '0.635', '0.629', '0.623', '0.637', '0.656', '0.655', '0.656', '0.646', '0.807', '0.828', '0.732', '0.663']
Global density: 0.6367771625518799
06/02 06:16:47 PM | Train: [ 4/80] Step 200/390 Loss 0.070 Prec@(1,5) (98.9%, 100.0%)
06/02 06:16:47 PM | layerwise density: [41033.0, 10366.0, 10365.0, 10328.0, 10241.0, 5202.0, 5145.0, 5101.0, 5213.0, 2690.0, 2685.0, 2689.0, 2647.0, 1658.0, 1706.0, 1511.0, 1361.0]
layerwise density percentage: ['0.626', '0.633', '0.633', '0.630', '0.625', '0.635', '0.628', '0.623', '0.636', '0.657', '0.656', '0.656', '0.646', '0.810', '0.833', '0.738', '0.665']
Global density: 0.63657546043396
06/02 06:16:58 PM | Train: [ 4/80] Step 300/390 Loss 0.069 Prec@(1,5) (98.9%, 100.0%)
06/02 06:16:58 PM | layerwise density: [41017.0, 10360.0, 10360.0, 10325.0, 10237.0, 5203.0, 5144.0, 5102.0, 5211.0, 2693.0, 2688.0, 2694.0, 2654.0, 1678.0, 1738.0, 1524.0, 1365.0]
layerwise density percentage: ['0.626', '0.632', '0.632', '0.630', '0.625', '0.635', '0.628', '0.623', '0.636', '0.657', '0.656', '0.658', '0.648', '0.819', '0.849', '0.744', '0.667']
Global density: 0.63685142993927
06/02 06:17:07 PM | Train: [ 4/80] Step 390/390 Loss 0.067 Prec@(1,5) (98.9%, 100.0%)
06/02 06:17:07 PM | layerwise density: [40967.0, 10350.0, 10352.0, 10320.0, 10223.0, 5202.0, 5138.0, 5105.0, 5211.0, 2694.0, 2697.0, 2699.0, 2650.0, 1678.0, 1740.0, 1538.0, 1373.0]
layerwise density percentage: ['0.625', '0.632', '0.632', '0.630', '0.624', '0.635', '0.627', '0.623', '0.636', '0.658', '0.658', '0.659', '0.647', '0.819', '0.850', '0.751', '0.670']
Global density: 0.6365542411804199
06/02 06:17:07 PM | Train: [ 4/200] Final Prec@1 98.8980%
06/02 06:17:08 PM | Valid: [ 4/200] Step 000/078 Loss 1.178 Prec@(1,5) (72.7%, 90.6%)
06/02 06:17:10 PM | Valid: [ 4/200] Step 078/078 Loss 1.386 Prec@(1,5) (68.1%, 88.3%)
06/02 06:17:10 PM | Valid: [ 4/200] Final Prec@1 68.1200%
06/02 06:17:11 PM | Current mask training best Prec@1 = 68.1200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 40967.0, 0.6251068115234375]
['model.relu.alpha_mask_1_0', 16384, 10350.0, 0.6317138671875]
['model.relu.alpha_mask_2_0', 16384, 10352.0, 0.6318359375]
['model.relu.alpha_mask_3_0', 16384, 10320.0, 0.6298828125]
['model.relu.alpha_mask_4_0', 16384, 10223.0, 0.62396240234375]
['model.relu.alpha_mask_5_0', 8192, 5202.0, 0.635009765625]
['model.relu.alpha_mask_6_0', 8192, 5138.0, 0.627197265625]
['model.relu.alpha_mask_7_0', 8192, 5105.0, 0.6231689453125]
['model.relu.alpha_mask_8_0', 8192, 5211.0, 0.6361083984375]
['model.relu.alpha_mask_9_0', 4096, 2694.0, 0.65771484375]
['model.relu.alpha_mask_10_0', 4096, 2698.0, 0.65869140625]
['model.relu.alpha_mask_11_0', 4096, 2699.0, 0.658935546875]
['model.relu.alpha_mask_12_0', 4096, 2650.0, 0.64697265625]
['model.relu.alpha_mask_13_0', 2048, 1678.0, 0.8193359375]
['model.relu.alpha_mask_14_0', 2048, 1740.0, 0.849609375]
['model.relu.alpha_mask_15_0', 2048, 1538.0, 0.7509765625]
['model.relu.alpha_mask_16_0', 2048, 1373.0, 0.67041015625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119938.0, 0.6365595278532609]
########## End ###########
06/02 06:17:12 PM | Train: [ 5/80] Step 000/390 Loss 0.031 Prec@(1,5) (100.0%, 100.0%)
06/02 06:17:12 PM | layerwise density: [40967.0, 10350.0, 10352.0, 10320.0, 10223.0, 5202.0, 5138.0, 5105.0, 5211.0, 2694.0, 2698.0, 2699.0, 2650.0, 1678.0, 1740.0, 1538.0, 1373.0]
layerwise density percentage: ['0.625', '0.632', '0.632', '0.630', '0.624', '0.635', '0.627', '0.623', '0.636', '0.658', '0.659', '0.659', '0.647', '0.819', '0.850', '0.751', '0.670']
Global density: 0.6365595459938049
06/02 06:17:22 PM | Train: [ 5/80] Step 100/390 Loss 0.049 Prec@(1,5) (99.3%, 100.0%)
06/02 06:17:22 PM | layerwise density: [40958.0, 10345.0, 10348.0, 10320.0, 10223.0, 5197.0, 5134.0, 5107.0, 5206.0, 2701.0, 2708.0, 2708.0, 2656.0, 1692.0, 1761.0, 1546.0, 1383.0]
layerwise density percentage: ['0.625', '0.631', '0.632', '0.630', '0.624', '0.634', '0.627', '0.623', '0.635', '0.659', '0.661', '0.661', '0.648', '0.826', '0.860', '0.755', '0.675']
Global density: 0.63685142993927
06/02 06:17:33 PM | Train: [ 5/80] Step 200/390 Loss 0.049 Prec@(1,5) (99.3%, 100.0%)
06/02 06:17:33 PM | layerwise density: [40916.0, 10339.0, 10333.0, 10304.0, 10218.0, 5194.0, 5128.0, 5107.0, 5200.0, 2702.0, 2703.0, 2713.0, 2660.0, 1704.0, 1770.0, 1562.0, 1389.0]
layerwise density percentage: ['0.624', '0.631', '0.631', '0.629', '0.624', '0.634', '0.626', '0.623', '0.635', '0.660', '0.660', '0.662', '0.649', '0.832', '0.864', '0.763', '0.678']
Global density: 0.636580765247345
06/02 06:17:44 PM | Train: [ 5/80] Step 300/390 Loss 0.048 Prec@(1,5) (99.3%, 100.0%)
06/02 06:17:44 PM | layerwise density: [40874.0, 10327.0, 10321.0, 10295.0, 10209.0, 5196.0, 5129.0, 5109.0, 5203.0, 2702.0, 2707.0, 2720.0, 2661.0, 1706.0, 1776.0, 1573.0, 1401.0]
layerwise density percentage: ['0.624', '0.630', '0.630', '0.628', '0.623', '0.634', '0.626', '0.624', '0.635', '0.660', '0.661', '0.664', '0.650', '0.833', '0.867', '0.768', '0.684']
Global density: 0.6364056468009949
06/02 06:17:53 PM | Train: [ 5/80] Step 390/390 Loss 0.049 Prec@(1,5) (99.3%, 100.0%)
06/02 06:17:53 PM | layerwise density: [40861.0, 10320.0, 10312.0, 10290.0, 10201.0, 5193.0, 5133.0, 5106.0, 5203.0, 2708.0, 2704.0, 2723.0, 2657.0, 1723.0, 1787.0, 1583.0, 1414.0]
layerwise density percentage: ['0.623', '0.630', '0.629', '0.628', '0.623', '0.634', '0.627', '0.623', '0.635', '0.661', '0.660', '0.665', '0.649', '0.841', '0.873', '0.773', '0.690']
Global density: 0.63645339012146
06/02 06:17:53 PM | Train: [ 5/200] Final Prec@1 99.2840%
06/02 06:17:54 PM | Valid: [ 5/200] Step 000/078 Loss 1.183 Prec@(1,5) (69.5%, 90.6%)
06/02 06:17:56 PM | Valid: [ 5/200] Step 078/078 Loss 1.395 Prec@(1,5) (67.7%, 88.3%)
06/02 06:17:56 PM | Valid: [ 5/200] Final Prec@1 67.7000%
06/02 06:17:56 PM | Current mask training best Prec@1 = 68.1200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 40861.0, 0.6234893798828125]
['model.relu.alpha_mask_1_0', 16384, 10320.0, 0.6298828125]
['model.relu.alpha_mask_2_0', 16384, 10312.0, 0.62939453125]
['model.relu.alpha_mask_3_0', 16384, 10290.0, 0.6280517578125]
['model.relu.alpha_mask_4_0', 16384, 10201.0, 0.62261962890625]
['model.relu.alpha_mask_5_0', 8192, 5193.0, 0.6339111328125]
['model.relu.alpha_mask_6_0', 8192, 5133.0, 0.6265869140625]
['model.relu.alpha_mask_7_0', 8192, 5106.0, 0.623291015625]
['model.relu.alpha_mask_8_0', 8192, 5203.0, 0.6351318359375]
['model.relu.alpha_mask_9_0', 4096, 2708.0, 0.6611328125]
['model.relu.alpha_mask_10_0', 4096, 2705.0, 0.660400390625]
['model.relu.alpha_mask_11_0', 4096, 2724.0, 0.6650390625]
['model.relu.alpha_mask_12_0', 4096, 2657.0, 0.648681640625]
['model.relu.alpha_mask_13_0', 2048, 1724.0, 0.841796875]
['model.relu.alpha_mask_14_0', 2048, 1787.0, 0.87255859375]
['model.relu.alpha_mask_15_0', 2048, 1583.0, 0.77294921875]
['model.relu.alpha_mask_16_0', 2048, 1414.0, 0.6904296875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119921.0, 0.6364693019701086]
########## End ###########
06/02 06:17:57 PM | Train: [ 6/80] Step 000/390 Loss 0.030 Prec@(1,5) (100.0%, 100.0%)
06/02 06:17:57 PM | layerwise density: [40861.0, 10320.0, 10312.0, 10290.0, 10201.0, 5193.0, 5133.0, 5106.0, 5203.0, 2708.0, 2705.0, 2724.0, 2657.0, 1724.0, 1787.0, 1583.0, 1414.0]
layerwise density percentage: ['0.623', '0.630', '0.629', '0.628', '0.623', '0.634', '0.627', '0.623', '0.635', '0.661', '0.660', '0.665', '0.649', '0.842', '0.873', '0.773', '0.690']
Global density: 0.636469304561615
06/02 06:18:07 PM | Train: [ 6/80] Step 100/390 Loss 0.042 Prec@(1,5) (99.5%, 100.0%)
06/02 06:18:07 PM | layerwise density: [40852.0, 10318.0, 10310.0, 10288.0, 10196.0, 5197.0, 5140.0, 5106.0, 5204.0, 2712.0, 2708.0, 2728.0, 2661.0, 1727.0, 1796.0, 1595.0, 1422.0]
layerwise density percentage: ['0.623', '0.630', '0.629', '0.628', '0.622', '0.634', '0.627', '0.623', '0.635', '0.662', '0.661', '0.666', '0.650', '0.843', '0.877', '0.779', '0.694']
Global density: 0.6366763114929199
06/02 06:18:18 PM | Train: [ 6/80] Step 200/390 Loss 0.042 Prec@(1,5) (99.5%, 100.0%)
06/02 06:18:18 PM | layerwise density: [40804.0, 10307.0, 10298.0, 10278.0, 10184.0, 5197.0, 5139.0, 5103.0, 5203.0, 2709.0, 2713.0, 2736.0, 2672.0, 1735.0, 1813.0, 1602.0, 1425.0]
layerwise density percentage: ['0.623', '0.629', '0.629', '0.627', '0.622', '0.634', '0.627', '0.623', '0.635', '0.661', '0.662', '0.668', '0.652', '0.847', '0.885', '0.782', '0.696']
Global density: 0.63645339012146
06/02 06:18:29 PM | Train: [ 6/80] Step 300/390 Loss 0.042 Prec@(1,5) (99.4%, 100.0%)
06/02 06:18:29 PM | layerwise density: [40790.0, 10305.0, 10292.0, 10271.0, 10181.0, 5203.0, 5139.0, 5103.0, 5207.0, 2716.0, 2722.0, 2743.0, 2673.0, 1734.0, 1825.0, 1611.0, 1437.0]
layerwise density percentage: ['0.622', '0.629', '0.628', '0.627', '0.621', '0.635', '0.627', '0.623', '0.636', '0.663', '0.665', '0.670', '0.653', '0.847', '0.891', '0.787', '0.702']
Global density: 0.6366338729858398
06/02 06:18:38 PM | Train: [ 6/80] Step 390/390 Loss 0.042 Prec@(1,5) (99.4%, 100.0%)
06/02 06:18:38 PM | layerwise density: [40734.0, 10298.0, 10290.0, 10260.0, 10171.0, 5203.0, 5141.0, 5101.0, 5209.0, 2719.0, 2721.0, 2748.0, 2669.0, 1751.0, 1834.0, 1615.0, 1443.0]
layerwise density percentage: ['0.622', '0.629', '0.628', '0.626', '0.621', '0.635', '0.628', '0.623', '0.636', '0.664', '0.664', '0.671', '0.652', '0.855', '0.896', '0.789', '0.705']
Global density: 0.6363950371742249
06/02 06:18:38 PM | Train: [ 6/200] Final Prec@1 99.4200%
06/02 06:18:38 PM | Valid: [ 6/200] Step 000/078 Loss 1.034 Prec@(1,5) (74.2%, 93.8%)
06/02 06:18:40 PM | Valid: [ 6/200] Step 078/078 Loss 1.340 Prec@(1,5) (69.1%, 88.7%)
06/02 06:18:41 PM | Valid: [ 6/200] Final Prec@1 69.1300%
06/02 06:18:41 PM | Current mask training best Prec@1 = 69.1300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 40734.0, 0.621551513671875]
['model.relu.alpha_mask_1_0', 16384, 10298.0, 0.6285400390625]
['model.relu.alpha_mask_2_0', 16384, 10290.0, 0.6280517578125]
['model.relu.alpha_mask_3_0', 16384, 10260.0, 0.626220703125]
['model.relu.alpha_mask_4_0', 16384, 10171.0, 0.62078857421875]
['model.relu.alpha_mask_5_0', 8192, 5203.0, 0.6351318359375]
['model.relu.alpha_mask_6_0', 8192, 5141.0, 0.6275634765625]
['model.relu.alpha_mask_7_0', 8192, 5101.0, 0.6226806640625]
['model.relu.alpha_mask_8_0', 8192, 5209.0, 0.6358642578125]
['model.relu.alpha_mask_9_0', 4096, 2719.0, 0.663818359375]
['model.relu.alpha_mask_10_0', 4096, 2721.0, 0.664306640625]
['model.relu.alpha_mask_11_0', 4096, 2748.0, 0.6708984375]
['model.relu.alpha_mask_12_0', 4096, 2669.0, 0.651611328125]
['model.relu.alpha_mask_13_0', 2048, 1751.0, 0.85498046875]
['model.relu.alpha_mask_14_0', 2048, 1834.0, 0.8955078125]
['model.relu.alpha_mask_15_0', 2048, 1615.0, 0.78857421875]
['model.relu.alpha_mask_16_0', 2048, 1443.0, 0.70458984375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119907.0, 0.6363949983016305]
########## End ###########
06/02 06:18:42 PM | Train: [ 7/80] Step 000/390 Loss 0.031 Prec@(1,5) (100.0%, 100.0%)
06/02 06:18:42 PM | layerwise density: [40734.0, 10298.0, 10290.0, 10260.0, 10171.0, 5203.0, 5141.0, 5101.0, 5209.0, 2719.0, 2721.0, 2748.0, 2669.0, 1751.0, 1834.0, 1615.0, 1443.0]
layerwise density percentage: ['0.622', '0.629', '0.628', '0.626', '0.621', '0.635', '0.628', '0.623', '0.636', '0.664', '0.664', '0.671', '0.652', '0.855', '0.896', '0.789', '0.705']
Global density: 0.6363950371742249
06/02 06:18:53 PM | Train: [ 7/80] Step 100/390 Loss 0.034 Prec@(1,5) (99.6%, 100.0%)
06/02 06:18:53 PM | layerwise density: [40711.0, 10295.0, 10289.0, 10258.0, 10169.0, 5209.0, 5144.0, 5103.0, 5209.0, 2718.0, 2731.0, 2749.0, 2673.0, 1757.0, 1840.0, 1630.0, 1447.0]
layerwise density percentage: ['0.621', '0.628', '0.628', '0.626', '0.621', '0.636', '0.628', '0.623', '0.636', '0.664', '0.667', '0.671', '0.653', '0.858', '0.898', '0.796', '0.707']
Global density: 0.6365277171134949
06/02 06:19:03 PM | Train: [ 7/80] Step 200/390 Loss 0.034 Prec@(1,5) (99.6%, 100.0%)
06/02 06:19:03 PM | layerwise density: [40707.0, 10293.0, 10288.0, 10257.0, 10171.0, 5212.0, 5146.0, 5110.0, 5206.0, 2725.0, 2724.0, 2757.0, 2682.0, 1766.0, 1863.0, 1635.0, 1450.0]
layerwise density percentage: ['0.621', '0.628', '0.628', '0.626', '0.621', '0.636', '0.628', '0.624', '0.635', '0.665', '0.665', '0.673', '0.655', '0.862', '0.910', '0.798', '0.708']
Global density: 0.636846125125885
06/02 06:19:14 PM | Train: [ 7/80] Step 300/390 Loss 0.035 Prec@(1,5) (99.5%, 100.0%)
06/02 06:19:14 PM | layerwise density: [40617.0, 10278.0, 10268.0, 10239.0, 10154.0, 5213.0, 5133.0, 5101.0, 5198.0, 2728.0, 2723.0, 2758.0, 2685.0, 1771.0, 1868.0, 1637.0, 1460.0]
layerwise density percentage: ['0.620', '0.627', '0.627', '0.625', '0.620', '0.636', '0.627', '0.623', '0.635', '0.666', '0.665', '0.673', '0.656', '0.865', '0.912', '0.799', '0.713']
Global density: 0.635991632938385
06/02 06:19:23 PM | Train: [ 7/80] Step 390/390 Loss 0.036 Prec@(1,5) (99.5%, 100.0%)
06/02 06:19:23 PM | layerwise density: [40599.0, 10271.0, 10264.0, 10238.0, 10151.0, 5214.0, 5134.0, 5106.0, 5200.0, 2732.0, 2719.0, 2765.0, 2700.0, 1781.0, 1874.0, 1645.0, 1463.0]
layerwise density percentage: ['0.619', '0.627', '0.626', '0.625', '0.620', '0.636', '0.627', '0.623', '0.635', '0.667', '0.664', '0.675', '0.659', '0.870', '0.915', '0.803', '0.714']
Global density: 0.636124312877655
06/02 06:19:23 PM | Train: [ 7/200] Final Prec@1 99.5080%
06/02 06:19:24 PM | Valid: [ 7/200] Step 000/078 Loss 1.084 Prec@(1,5) (74.2%, 92.2%)
06/02 06:19:26 PM | Valid: [ 7/200] Step 078/078 Loss 1.341 Prec@(1,5) (68.8%, 89.0%)
06/02 06:19:26 PM | Valid: [ 7/200] Final Prec@1 68.7500%
06/02 06:19:26 PM | Current mask training best Prec@1 = 69.1300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 40599.0, 0.6194915771484375]
['model.relu.alpha_mask_1_0', 16384, 10271.0, 0.62689208984375]
['model.relu.alpha_mask_2_0', 16384, 10264.0, 0.62646484375]
['model.relu.alpha_mask_3_0', 16384, 10238.0, 0.6248779296875]
['model.relu.alpha_mask_4_0', 16384, 10151.0, 0.61956787109375]
['model.relu.alpha_mask_5_0', 8192, 5214.0, 0.636474609375]
['model.relu.alpha_mask_6_0', 8192, 5134.0, 0.626708984375]
['model.relu.alpha_mask_7_0', 8192, 5106.0, 0.623291015625]
['model.relu.alpha_mask_8_0', 8192, 5200.0, 0.634765625]
['model.relu.alpha_mask_9_0', 4096, 2732.0, 0.6669921875]
['model.relu.alpha_mask_10_0', 4096, 2719.0, 0.663818359375]
['model.relu.alpha_mask_11_0', 4096, 2765.0, 0.675048828125]
['model.relu.alpha_mask_12_0', 4096, 2700.0, 0.6591796875]
['model.relu.alpha_mask_13_0', 2048, 1780.0, 0.869140625]
['model.relu.alpha_mask_14_0', 2048, 1874.0, 0.9150390625]
['model.relu.alpha_mask_15_0', 2048, 1645.0, 0.80322265625]
['model.relu.alpha_mask_16_0', 2048, 1463.0, 0.71435546875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119855.0, 0.6361190132472826]
########## End ###########
06/02 06:19:27 PM | Train: [ 8/80] Step 000/390 Loss 0.018 Prec@(1,5) (100.0%, 100.0%)
06/02 06:19:27 PM | layerwise density: [40599.0, 10271.0, 10264.0, 10238.0, 10151.0, 5214.0, 5134.0, 5106.0, 5200.0, 2732.0, 2719.0, 2765.0, 2700.0, 1780.0, 1874.0, 1645.0, 1463.0]
layerwise density percentage: ['0.619', '0.627', '0.626', '0.625', '0.620', '0.636', '0.627', '0.623', '0.635', '0.667', '0.664', '0.675', '0.659', '0.869', '0.915', '0.803', '0.714']
Global density: 0.63611900806427
06/02 06:19:37 PM | Train: [ 8/80] Step 100/390 Loss 0.032 Prec@(1,5) (99.6%, 100.0%)
06/02 06:19:37 PM | layerwise density: [40591.0, 10271.0, 10265.0, 10237.0, 10153.0, 5216.0, 5145.0, 5111.0, 5206.0, 2737.0, 2730.0, 2764.0, 2704.0, 1786.0, 1889.0, 1653.0, 1473.0]
layerwise density percentage: ['0.619', '0.627', '0.627', '0.625', '0.620', '0.637', '0.628', '0.624', '0.635', '0.668', '0.667', '0.675', '0.660', '0.872', '0.922', '0.807', '0.719']
Global density: 0.6365224123001099
06/02 06:19:48 PM | Train: [ 8/80] Step 200/390 Loss 0.030 Prec@(1,5) (99.7%, 100.0%)
06/02 06:19:48 PM | layerwise density: [40526.0, 10263.0, 10251.0, 10230.0, 10137.0, 5216.0, 5144.0, 5115.0, 5197.0, 2740.0, 2738.0, 2772.0, 2698.0, 1780.0, 1889.0, 1657.0, 1478.0]
layerwise density percentage: ['0.618', '0.626', '0.626', '0.624', '0.619', '0.637', '0.628', '0.624', '0.634', '0.669', '0.668', '0.677', '0.659', '0.869', '0.922', '0.809', '0.722']
Global density: 0.635991632938385
06/02 06:19:59 PM | Train: [ 8/80] Step 300/390 Loss 0.031 Prec@(1,5) (99.6%, 100.0%)
06/02 06:19:59 PM | layerwise density: [40508.0, 10256.0, 10240.0, 10228.0, 10135.0, 5215.0, 5142.0, 5108.0, 5191.0, 2748.0, 2745.0, 2779.0, 2698.0, 1794.0, 1897.0, 1665.0, 1489.0]
layerwise density percentage: ['0.618', '0.626', '0.625', '0.624', '0.619', '0.637', '0.628', '0.624', '0.634', '0.671', '0.670', '0.678', '0.659', '0.876', '0.926', '0.813', '0.727']
Global density: 0.6360288262367249
06/02 06:20:08 PM | Train: [ 8/80] Step 390/390 Loss 0.031 Prec@(1,5) (99.6%, 100.0%)
06/02 06:20:08 PM | layerwise density: [40500.0, 10253.0, 10243.0, 10228.0, 10128.0, 5225.0, 5149.0, 5110.0, 5190.0, 2755.0, 2750.0, 2787.0, 2705.0, 1805.0, 1904.0, 1674.0, 1495.0]
layerwise density percentage: ['0.618', '0.626', '0.625', '0.624', '0.618', '0.638', '0.629', '0.624', '0.634', '0.673', '0.671', '0.680', '0.660', '0.881', '0.930', '0.817', '0.730']
Global density: 0.63636314868927
06/02 06:20:08 PM | Train: [ 8/200] Final Prec@1 99.6340%
06/02 06:20:08 PM | Valid: [ 8/200] Step 000/078 Loss 1.154 Prec@(1,5) (75.8%, 93.8%)
06/02 06:20:11 PM | Valid: [ 8/200] Step 078/078 Loss 1.329 Prec@(1,5) (69.2%, 89.3%)
06/02 06:20:11 PM | Valid: [ 8/200] Final Prec@1 69.1800%
06/02 06:20:11 PM | Current mask training best Prec@1 = 69.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 40500.0, 0.61798095703125]
['model.relu.alpha_mask_1_0', 16384, 10253.0, 0.62579345703125]
['model.relu.alpha_mask_2_0', 16384, 10243.0, 0.62518310546875]
['model.relu.alpha_mask_3_0', 16384, 10228.0, 0.624267578125]
['model.relu.alpha_mask_4_0', 16384, 10128.0, 0.6181640625]
['model.relu.alpha_mask_5_0', 8192, 5224.0, 0.6376953125]
['model.relu.alpha_mask_6_0', 8192, 5149.0, 0.6285400390625]
['model.relu.alpha_mask_7_0', 8192, 5110.0, 0.623779296875]
['model.relu.alpha_mask_8_0', 8192, 5190.0, 0.633544921875]
['model.relu.alpha_mask_9_0', 4096, 2755.0, 0.672607421875]
['model.relu.alpha_mask_10_0', 4096, 2750.0, 0.67138671875]
['model.relu.alpha_mask_11_0', 4096, 2787.0, 0.680419921875]
['model.relu.alpha_mask_12_0', 4096, 2704.0, 0.66015625]
['model.relu.alpha_mask_13_0', 2048, 1804.0, 0.880859375]
['model.relu.alpha_mask_14_0', 2048, 1904.0, 0.9296875]
['model.relu.alpha_mask_15_0', 2048, 1674.0, 0.8173828125]
['model.relu.alpha_mask_16_0', 2048, 1496.0, 0.73046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119899.0, 0.6363525390625]
########## End ###########
06/02 06:20:12 PM | Train: [ 9/80] Step 000/390 Loss 0.015 Prec@(1,5) (100.0%, 100.0%)
06/02 06:20:12 PM | layerwise density: [40500.0, 10253.0, 10243.0, 10228.0, 10128.0, 5224.0, 5149.0, 5110.0, 5190.0, 2755.0, 2750.0, 2787.0, 2704.0, 1804.0, 1904.0, 1674.0, 1496.0]
layerwise density percentage: ['0.618', '0.626', '0.625', '0.624', '0.618', '0.638', '0.629', '0.624', '0.634', '0.673', '0.671', '0.680', '0.660', '0.881', '0.930', '0.817', '0.730']
Global density: 0.6363525390625
06/02 06:20:23 PM | Train: [ 9/80] Step 100/390 Loss 0.027 Prec@(1,5) (99.7%, 100.0%)
06/02 06:20:23 PM | layerwise density: [40493.0, 10256.0, 10239.0, 10224.0, 10132.0, 5231.0, 5152.0, 5119.0, 5196.0, 2764.0, 2754.0, 2796.0, 2707.0, 1798.0, 1915.0, 1683.0, 1502.0]
layerwise density percentage: ['0.618', '0.626', '0.625', '0.624', '0.618', '0.639', '0.629', '0.625', '0.634', '0.675', '0.672', '0.683', '0.661', '0.878', '0.935', '0.822', '0.733']
Global density: 0.6366816163063049
06/02 06:20:33 PM | Train: [ 9/80] Step 200/390 Loss 0.028 Prec@(1,5) (99.7%, 100.0%)
06/02 06:20:33 PM | layerwise density: [40400.0, 10236.0, 10218.0, 10212.0, 10117.0, 5232.0, 5148.0, 5116.0, 5184.0, 2755.0, 2749.0, 2799.0, 2706.0, 1800.0, 1918.0, 1687.0, 1512.0]
layerwise density percentage: ['0.616', '0.625', '0.624', '0.623', '0.617', '0.639', '0.628', '0.625', '0.633', '0.673', '0.671', '0.683', '0.661', '0.879', '0.937', '0.824', '0.738']
Global density: 0.635768711566925
06/02 06:20:44 PM | Train: [ 9/80] Step 300/390 Loss 0.028 Prec@(1,5) (99.7%, 100.0%)
06/02 06:20:44 PM | layerwise density: [40382.0, 10225.0, 10209.0, 10207.0, 10109.0, 5228.0, 5140.0, 5123.0, 5185.0, 2757.0, 2753.0, 2804.0, 2707.0, 1811.0, 1926.0, 1695.0, 1517.0]
layerwise density percentage: ['0.616', '0.624', '0.623', '0.623', '0.617', '0.638', '0.627', '0.625', '0.633', '0.673', '0.672', '0.685', '0.661', '0.884', '0.940', '0.828', '0.741']
Global density: 0.6357103586196899
06/02 06:20:53 PM | Train: [ 9/80] Step 390/390 Loss 0.027 Prec@(1,5) (99.7%, 100.0%)
06/02 06:20:53 PM | layerwise density: [40365.0, 10223.0, 10208.0, 10208.0, 10109.0, 5236.0, 5147.0, 5132.0, 5187.0, 2762.0, 2770.0, 2811.0, 2721.0, 1819.0, 1932.0, 1705.0, 1527.0]
layerwise density percentage: ['0.616', '0.624', '0.623', '0.623', '0.617', '0.639', '0.628', '0.626', '0.633', '0.674', '0.676', '0.686', '0.664', '0.888', '0.943', '0.833', '0.746']
Global density: 0.6361562013626099
06/02 06:20:53 PM | Train: [ 9/200] Final Prec@1 99.7040%
06/02 06:20:54 PM | Valid: [ 9/200] Step 000/078 Loss 1.204 Prec@(1,5) (73.4%, 93.0%)
06/02 06:20:56 PM | Valid: [ 9/200] Step 078/078 Loss 1.335 Prec@(1,5) (69.4%, 88.7%)
06/02 06:20:56 PM | Valid: [ 9/200] Final Prec@1 69.4200%
06/02 06:20:57 PM | Current mask training best Prec@1 = 69.4200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 40364.0, 0.61590576171875]
['model.relu.alpha_mask_1_0', 16384, 10223.0, 0.62396240234375]
['model.relu.alpha_mask_2_0', 16384, 10208.0, 0.623046875]
['model.relu.alpha_mask_3_0', 16384, 10207.0, 0.62298583984375]
['model.relu.alpha_mask_4_0', 16384, 10109.0, 0.61700439453125]
['model.relu.alpha_mask_5_0', 8192, 5236.0, 0.63916015625]
['model.relu.alpha_mask_6_0', 8192, 5148.0, 0.62841796875]
['model.relu.alpha_mask_7_0', 8192, 5132.0, 0.62646484375]
['model.relu.alpha_mask_8_0', 8192, 5186.0, 0.633056640625]
['model.relu.alpha_mask_9_0', 4096, 2762.0, 0.67431640625]
['model.relu.alpha_mask_10_0', 4096, 2770.0, 0.67626953125]
['model.relu.alpha_mask_11_0', 4096, 2812.0, 0.6865234375]
['model.relu.alpha_mask_12_0', 4096, 2720.0, 0.6640625]
['model.relu.alpha_mask_13_0', 2048, 1820.0, 0.888671875]
['model.relu.alpha_mask_14_0', 2048, 1932.0, 0.943359375]
['model.relu.alpha_mask_15_0', 2048, 1705.0, 0.83251953125]
['model.relu.alpha_mask_16_0', 2048, 1528.0, 0.74609375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119862.0, 0.6361561650815217]
########## End ###########
06/02 06:20:58 PM | Train: [10/80] Step 000/390 Loss 0.027 Prec@(1,5) (100.0%, 100.0%)
06/02 06:20:58 PM | layerwise density: [40364.0, 10223.0, 10208.0, 10207.0, 10109.0, 5236.0, 5148.0, 5132.0, 5186.0, 2762.0, 2770.0, 2812.0, 2720.0, 1820.0, 1932.0, 1705.0, 1528.0]
layerwise density percentage: ['0.616', '0.624', '0.623', '0.623', '0.617', '0.639', '0.628', '0.626', '0.633', '0.674', '0.676', '0.687', '0.664', '0.889', '0.943', '0.833', '0.746']
Global density: 0.6361562013626099
06/02 06:21:08 PM | Train: [10/80] Step 100/390 Loss 0.026 Prec@(1,5) (99.7%, 100.0%)
06/02 06:21:08 PM | layerwise density: [40352.0, 10221.0, 10212.0, 10205.0, 10108.0, 5246.0, 5160.0, 5138.0, 5195.0, 2772.0, 2769.0, 2820.0, 2727.0, 1826.0, 1939.0, 1710.0, 1533.0]
layerwise density percentage: ['0.616', '0.624', '0.623', '0.623', '0.617', '0.640', '0.630', '0.627', '0.634', '0.677', '0.676', '0.688', '0.666', '0.892', '0.947', '0.835', '0.749']
Global density: 0.6365330219268799
06/02 06:21:18 PM | Train: [10/80] Step 200/390 Loss 0.025 Prec@(1,5) (99.7%, 100.0%)
06/02 06:21:18 PM | layerwise density: [40247.0, 10197.0, 10184.0, 10192.0, 10073.0, 5243.0, 5155.0, 5137.0, 5189.0, 2768.0, 2763.0, 2822.0, 2717.0, 1827.0, 1940.0, 1717.0, 1538.0]
layerwise density percentage: ['0.614', '0.622', '0.622', '0.622', '0.615', '0.640', '0.629', '0.627', '0.633', '0.676', '0.675', '0.689', '0.663', '0.892', '0.947', '0.838', '0.751']
Global density: 0.6353441476821899
06/02 06:21:29 PM | Train: [10/80] Step 300/390 Loss 0.026 Prec@(1,5) (99.7%, 100.0%)
06/02 06:21:29 PM | layerwise density: [40224.0, 10187.0, 10178.0, 10186.0, 10063.0, 5240.0, 5153.0, 5133.0, 5189.0, 2770.0, 2763.0, 2819.0, 2722.0, 1826.0, 1953.0, 1721.0, 1547.0]
layerwise density percentage: ['0.614', '0.622', '0.621', '0.622', '0.614', '0.640', '0.629', '0.627', '0.633', '0.676', '0.675', '0.688', '0.665', '0.892', '0.954', '0.840', '0.755']
Global density: 0.635158360004425
06/02 06:21:38 PM | Train: [10/80] Step 390/390 Loss 0.026 Prec@(1,5) (99.7%, 100.0%)
06/02 06:21:38 PM | layerwise density: [40211.0, 10187.0, 10182.0, 10183.0, 10061.0, 5254.0, 5160.0, 5141.0, 5193.0, 2780.0, 2782.0, 2832.0, 2720.0, 1839.0, 1952.0, 1737.0, 1556.0]
layerwise density percentage: ['0.614', '0.622', '0.621', '0.622', '0.614', '0.641', '0.630', '0.628', '0.634', '0.679', '0.679', '0.691', '0.664', '0.898', '0.953', '0.848', '0.760']
Global density: 0.6356679201126099
06/02 06:21:38 PM | Train: [10/200] Final Prec@1 99.7080%
06/02 06:21:39 PM | Valid: [10/200] Step 000/078 Loss 1.219 Prec@(1,5) (73.4%, 91.4%)
06/02 06:21:41 PM | Valid: [10/200] Step 078/078 Loss 1.357 Prec@(1,5) (68.5%, 88.7%)
06/02 06:21:41 PM | Valid: [10/200] Final Prec@1 68.4900%
06/02 06:21:41 PM | Current mask training best Prec@1 = 69.4200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 40211.0, 0.6135711669921875]
['model.relu.alpha_mask_1_0', 16384, 10187.0, 0.62176513671875]
['model.relu.alpha_mask_2_0', 16384, 10182.0, 0.6214599609375]
['model.relu.alpha_mask_3_0', 16384, 10183.0, 0.62152099609375]
['model.relu.alpha_mask_4_0', 16384, 10061.0, 0.61407470703125]
['model.relu.alpha_mask_5_0', 8192, 5253.0, 0.6412353515625]
['model.relu.alpha_mask_6_0', 8192, 5160.0, 0.6298828125]
['model.relu.alpha_mask_7_0', 8192, 5141.0, 0.6275634765625]
['model.relu.alpha_mask_8_0', 8192, 5193.0, 0.6339111328125]
['model.relu.alpha_mask_9_0', 4096, 2780.0, 0.6787109375]
['model.relu.alpha_mask_10_0', 4096, 2782.0, 0.67919921875]
['model.relu.alpha_mask_11_0', 4096, 2832.0, 0.69140625]
['model.relu.alpha_mask_12_0', 4096, 2720.0, 0.6640625]
['model.relu.alpha_mask_13_0', 2048, 1839.0, 0.89794921875]
['model.relu.alpha_mask_14_0', 2048, 1952.0, 0.953125]
['model.relu.alpha_mask_15_0', 2048, 1737.0, 0.84814453125]
['model.relu.alpha_mask_16_0', 2048, 1556.0, 0.759765625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119769.0, 0.6356625764266305]
########## End ###########
06/02 06:21:42 PM | Train: [11/80] Step 000/390 Loss 0.013 Prec@(1,5) (100.0%, 100.0%)
06/02 06:21:42 PM | layerwise density: [40211.0, 10187.0, 10182.0, 10183.0, 10061.0, 5253.0, 5160.0, 5141.0, 5193.0, 2780.0, 2782.0, 2832.0, 2720.0, 1839.0, 1952.0, 1737.0, 1556.0]
layerwise density percentage: ['0.614', '0.622', '0.621', '0.622', '0.614', '0.641', '0.630', '0.628', '0.634', '0.679', '0.679', '0.691', '0.664', '0.898', '0.953', '0.848', '0.760']
Global density: 0.6356626152992249
06/02 06:21:52 PM | Train: [11/80] Step 100/390 Loss 0.025 Prec@(1,5) (99.7%, 100.0%)
06/02 06:21:52 PM | layerwise density: [40204.0, 10188.0, 10184.0, 10180.0, 10062.0, 5270.0, 5171.0, 5139.0, 5194.0, 2783.0, 2792.0, 2839.0, 2738.0, 1847.0, 1964.0, 1744.0, 1566.0]
layerwise density percentage: ['0.613', '0.622', '0.622', '0.621', '0.614', '0.643', '0.631', '0.627', '0.634', '0.679', '0.682', '0.693', '0.668', '0.902', '0.959', '0.852', '0.765']
Global density: 0.6361721158027649
06/02 06:22:03 PM | Train: [11/80] Step 200/390 Loss 0.024 Prec@(1,5) (99.7%, 100.0%)
06/02 06:22:03 PM | layerwise density: [40198.0, 10197.0, 10183.0, 10185.0, 10070.0, 5275.0, 5177.0, 5148.0, 5203.0, 2795.0, 2795.0, 2854.0, 2741.0, 1851.0, 1970.0, 1748.0, 1578.0]
layerwise density percentage: ['0.613', '0.622', '0.622', '0.622', '0.615', '0.644', '0.632', '0.628', '0.635', '0.682', '0.682', '0.697', '0.669', '0.904', '0.962', '0.854', '0.771']
Global density: 0.63671875
06/02 06:22:13 PM | Train: [11/80] Step 300/390 Loss 0.024 Prec@(1,5) (99.7%, 100.0%)
06/02 06:22:13 PM | layerwise density: [40051.0, 10162.0, 10152.0, 10148.0, 10047.0, 5251.0, 5148.0, 5135.0, 5174.0, 2790.0, 2780.0, 2857.0, 2731.0, 1856.0, 1963.0, 1747.0, 1580.0]
layerwise density percentage: ['0.611', '0.620', '0.620', '0.619', '0.613', '0.641', '0.628', '0.627', '0.632', '0.681', '0.679', '0.698', '0.667', '0.906', '0.958', '0.853', '0.771']
Global density: 0.634617030620575
06/02 06:22:23 PM | Train: [11/80] Step 390/390 Loss 0.024 Prec@(1,5) (99.7%, 100.0%)
06/02 06:22:23 PM | layerwise density: [40016.0, 10163.0, 10153.0, 10141.0, 10041.0, 5249.0, 5161.0, 5126.0, 5177.0, 2795.0, 2784.0, 2867.0, 2735.0, 1859.0, 1976.0, 1756.0, 1586.0]
layerwise density percentage: ['0.611', '0.620', '0.620', '0.619', '0.613', '0.641', '0.630', '0.626', '0.632', '0.682', '0.680', '0.700', '0.668', '0.908', '0.965', '0.857', '0.774']
Global density: 0.6346860527992249
06/02 06:22:23 PM | Train: [11/200] Final Prec@1 99.7340%
06/02 06:22:23 PM | Valid: [11/200] Step 000/078 Loss 1.126 Prec@(1,5) (70.3%, 93.8%)
06/02 06:22:26 PM | Valid: [11/200] Step 078/078 Loss 1.338 Prec@(1,5) (68.6%, 89.3%)
06/02 06:22:26 PM | Valid: [11/200] Final Prec@1 68.6000%
06/02 06:22:26 PM | Current mask training best Prec@1 = 69.4200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 40014.0, 0.610565185546875]
['model.relu.alpha_mask_1_0', 16384, 10163.0, 0.62030029296875]
['model.relu.alpha_mask_2_0', 16384, 10153.0, 0.61968994140625]
['model.relu.alpha_mask_3_0', 16384, 10142.0, 0.6190185546875]
['model.relu.alpha_mask_4_0', 16384, 10041.0, 0.61285400390625]
['model.relu.alpha_mask_5_0', 8192, 5249.0, 0.6407470703125]
['model.relu.alpha_mask_6_0', 8192, 5162.0, 0.630126953125]
['model.relu.alpha_mask_7_0', 8192, 5126.0, 0.625732421875]
['model.relu.alpha_mask_8_0', 8192, 5176.0, 0.6318359375]
['model.relu.alpha_mask_9_0', 4096, 2795.0, 0.682373046875]
['model.relu.alpha_mask_10_0', 4096, 2784.0, 0.6796875]
['model.relu.alpha_mask_11_0', 4096, 2867.0, 0.699951171875]
['model.relu.alpha_mask_12_0', 4096, 2737.0, 0.668212890625]
['model.relu.alpha_mask_13_0', 2048, 1859.0, 0.90771484375]
['model.relu.alpha_mask_14_0', 2048, 1977.0, 0.96533203125]
['model.relu.alpha_mask_15_0', 2048, 1756.0, 0.857421875]
['model.relu.alpha_mask_16_0', 2048, 1586.0, 0.7744140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119587.0, 0.6346966287364131]
########## End ###########
06/02 06:22:27 PM | Train: [12/80] Step 000/390 Loss 0.015 Prec@(1,5) (100.0%, 100.0%)
06/02 06:22:27 PM | layerwise density: [40014.0, 10163.0, 10153.0, 10142.0, 10041.0, 5249.0, 5162.0, 5126.0, 5176.0, 2795.0, 2784.0, 2867.0, 2737.0, 1859.0, 1977.0, 1756.0, 1586.0]
layerwise density percentage: ['0.611', '0.620', '0.620', '0.619', '0.613', '0.641', '0.630', '0.626', '0.632', '0.682', '0.680', '0.700', '0.668', '0.908', '0.965', '0.857', '0.774']
Global density: 0.6346966624259949
06/02 06:22:37 PM | Train: [12/80] Step 100/390 Loss 0.022 Prec@(1,5) (99.8%, 100.0%)
06/02 06:22:37 PM | layerwise density: [40001.0, 10164.0, 10152.0, 10144.0, 10039.0, 5248.0, 5171.0, 5144.0, 5190.0, 2808.0, 2790.0, 2873.0, 2746.0, 1866.0, 1994.0, 1763.0, 1598.0]
layerwise density percentage: ['0.610', '0.620', '0.620', '0.619', '0.613', '0.641', '0.631', '0.628', '0.634', '0.686', '0.681', '0.701', '0.670', '0.911', '0.974', '0.861', '0.780']
Global density: 0.635248601436615
06/02 06:22:48 PM | Train: [12/80] Step 200/390 Loss 0.021 Prec@(1,5) (99.8%, 100.0%)
06/02 06:22:48 PM | layerwise density: [39989.0, 10164.0, 10146.0, 10153.0, 10039.0, 5279.0, 5180.0, 5157.0, 5201.0, 2811.0, 2809.0, 2878.0, 2751.0, 1871.0, 1987.0, 1767.0, 1610.0]
layerwise density percentage: ['0.610', '0.620', '0.619', '0.620', '0.613', '0.644', '0.632', '0.630', '0.635', '0.686', '0.686', '0.703', '0.672', '0.914', '0.970', '0.863', '0.786']
Global density: 0.6357846856117249
06/02 06:22:58 PM | Train: [12/80] Step 300/390 Loss 0.022 Prec@(1,5) (99.8%, 100.0%)
06/02 06:22:58 PM | layerwise density: [39983.0, 10178.0, 10150.0, 10158.0, 10048.0, 5297.0, 5185.0, 5166.0, 5202.0, 2824.0, 2829.0, 2892.0, 2753.0, 1874.0, 1989.0, 1777.0, 1621.0]
layerwise density percentage: ['0.610', '0.621', '0.620', '0.620', '0.613', '0.647', '0.633', '0.631', '0.635', '0.689', '0.691', '0.706', '0.672', '0.915', '0.971', '0.868', '0.792']
Global density: 0.63649582862854
06/02 06:23:08 PM | Train: [12/80] Step 390/390 Loss 0.022 Prec@(1,5) (99.8%, 100.0%)
06/02 06:23:08 PM | layerwise density: [39835.0, 10141.0, 10118.0, 10133.0, 10016.0, 5264.0, 5156.0, 5145.0, 5187.0, 2808.0, 2807.0, 2893.0, 2762.0, 1871.0, 1993.0, 1773.0, 1630.0]
layerwise density percentage: ['0.608', '0.619', '0.618', '0.618', '0.611', '0.643', '0.629', '0.628', '0.633', '0.686', '0.685', '0.706', '0.674', '0.914', '0.973', '0.866', '0.796']
Global density: 0.634404718875885
06/02 06:23:08 PM | Train: [12/200] Final Prec@1 99.7640%
06/02 06:23:08 PM | Valid: [12/200] Step 000/078 Loss 1.112 Prec@(1,5) (72.7%, 93.0%)
06/02 06:23:10 PM | Valid: [12/200] Step 078/078 Loss 1.338 Prec@(1,5) (69.1%, 89.1%)
06/02 06:23:10 PM | Valid: [12/200] Final Prec@1 69.1100%
06/02 06:23:10 PM | Current mask training best Prec@1 = 69.4200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 39835.0, 0.6078338623046875]
['model.relu.alpha_mask_1_0', 16384, 10141.0, 0.61895751953125]
['model.relu.alpha_mask_2_0', 16384, 10118.0, 0.6175537109375]
['model.relu.alpha_mask_3_0', 16384, 10133.0, 0.61846923828125]
['model.relu.alpha_mask_4_0', 16384, 10016.0, 0.611328125]
['model.relu.alpha_mask_5_0', 8192, 5264.0, 0.642578125]
['model.relu.alpha_mask_6_0', 8192, 5156.0, 0.62939453125]
['model.relu.alpha_mask_7_0', 8192, 5145.0, 0.6280517578125]
['model.relu.alpha_mask_8_0', 8192, 5187.0, 0.6331787109375]
['model.relu.alpha_mask_9_0', 4096, 2808.0, 0.685546875]
['model.relu.alpha_mask_10_0', 4096, 2807.0, 0.685302734375]
['model.relu.alpha_mask_11_0', 4096, 2893.0, 0.706298828125]
['model.relu.alpha_mask_12_0', 4096, 2762.0, 0.67431640625]
['model.relu.alpha_mask_13_0', 2048, 1871.0, 0.91357421875]
['model.relu.alpha_mask_14_0', 2048, 1993.0, 0.97314453125]
['model.relu.alpha_mask_15_0', 2048, 1773.0, 0.86572265625]
['model.relu.alpha_mask_16_0', 2048, 1630.0, 0.7958984375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119532.0, 0.6344047214673914]
########## End ###########
06/02 06:23:11 PM | Train: [13/80] Step 000/390 Loss 0.019 Prec@(1,5) (100.0%, 100.0%)
06/02 06:23:11 PM | layerwise density: [39835.0, 10141.0, 10118.0, 10133.0, 10016.0, 5264.0, 5156.0, 5145.0, 5187.0, 2808.0, 2807.0, 2893.0, 2762.0, 1871.0, 1993.0, 1773.0, 1630.0]
layerwise density percentage: ['0.608', '0.619', '0.618', '0.618', '0.611', '0.643', '0.629', '0.628', '0.633', '0.686', '0.685', '0.706', '0.674', '0.914', '0.973', '0.866', '0.796']
Global density: 0.634404718875885
06/02 06:23:22 PM | Train: [13/80] Step 100/390 Loss 0.019 Prec@(1,5) (99.8%, 100.0%)
06/02 06:23:22 PM | layerwise density: [39801.0, 10128.0, 10098.0, 10120.0, 10009.0, 5249.0, 5147.0, 5138.0, 5174.0, 2816.0, 2806.0, 2900.0, 2755.0, 1869.0, 1999.0, 1784.0, 1636.0]
layerwise density percentage: ['0.607', '0.618', '0.616', '0.618', '0.611', '0.641', '0.628', '0.627', '0.632', '0.688', '0.685', '0.708', '0.673', '0.913', '0.976', '0.871', '0.799']
Global density: 0.6338580846786499
06/02 06:23:33 PM | Train: [13/80] Step 200/390 Loss 0.019 Prec@(1,5) (99.8%, 100.0%)
06/02 06:23:33 PM | layerwise density: [39776.0, 10127.0, 10101.0, 10119.0, 10012.0, 5266.0, 5152.0, 5131.0, 5186.0, 2828.0, 2821.0, 2898.0, 2773.0, 1878.0, 2005.0, 1791.0, 1642.0]
layerwise density percentage: ['0.607', '0.618', '0.617', '0.618', '0.611', '0.643', '0.629', '0.626', '0.633', '0.690', '0.689', '0.708', '0.677', '0.917', '0.979', '0.875', '0.802']
Global density: 0.63426673412323
06/02 06:23:43 PM | Train: [13/80] Step 300/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/02 06:23:43 PM | layerwise density: [39765.0, 10129.0, 10098.0, 10124.0, 10016.0, 5280.0, 5180.0, 5149.0, 5205.0, 2849.0, 2839.0, 2903.0, 2769.0, 1890.0, 2014.0, 1796.0, 1647.0]
layerwise density percentage: ['0.607', '0.618', '0.616', '0.618', '0.611', '0.645', '0.632', '0.629', '0.635', '0.696', '0.693', '0.709', '0.676', '0.923', '0.983', '0.877', '0.804']
Global density: 0.6350469589233398
06/02 06:23:53 PM | Train: [13/80] Step 390/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/02 06:23:53 PM | layerwise density: [39753.0, 10135.0, 10101.0, 10128.0, 10023.0, 5297.0, 5173.0, 5166.0, 5205.0, 2861.0, 2834.0, 2908.0, 2775.0, 1896.0, 2009.0, 1807.0, 1652.0]
layerwise density percentage: ['0.607', '0.619', '0.617', '0.618', '0.612', '0.647', '0.631', '0.631', '0.635', '0.698', '0.692', '0.710', '0.677', '0.926', '0.981', '0.882', '0.807']
Global density: 0.6354184746742249
06/02 06:23:53 PM | Train: [13/200] Final Prec@1 99.7820%
06/02 06:23:53 PM | Valid: [13/200] Step 000/078 Loss 1.221 Prec@(1,5) (69.5%, 92.2%)
06/02 06:23:55 PM | Valid: [13/200] Step 078/078 Loss 1.325 Prec@(1,5) (69.4%, 89.5%)
06/02 06:23:55 PM | Valid: [13/200] Final Prec@1 69.4400%
06/02 06:23:56 PM | Current mask training best Prec@1 = 69.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 39753.0, 0.6065826416015625]
['model.relu.alpha_mask_1_0', 16384, 10135.0, 0.61859130859375]
['model.relu.alpha_mask_2_0', 16384, 10101.0, 0.61651611328125]
['model.relu.alpha_mask_3_0', 16384, 10128.0, 0.6181640625]
['model.relu.alpha_mask_4_0', 16384, 10023.0, 0.61175537109375]
['model.relu.alpha_mask_5_0', 8192, 5297.0, 0.6466064453125]
['model.relu.alpha_mask_6_0', 8192, 5173.0, 0.6314697265625]
['model.relu.alpha_mask_7_0', 8192, 5166.0, 0.630615234375]
['model.relu.alpha_mask_8_0', 8192, 5205.0, 0.6353759765625]
['model.relu.alpha_mask_9_0', 4096, 2861.0, 0.698486328125]
['model.relu.alpha_mask_10_0', 4096, 2834.0, 0.69189453125]
['model.relu.alpha_mask_11_0', 4096, 2908.0, 0.7099609375]
['model.relu.alpha_mask_12_0', 4096, 2775.0, 0.677490234375]
['model.relu.alpha_mask_13_0', 2048, 1896.0, 0.92578125]
['model.relu.alpha_mask_14_0', 2048, 2009.0, 0.98095703125]
['model.relu.alpha_mask_15_0', 2048, 1807.0, 0.88232421875]
['model.relu.alpha_mask_16_0', 2048, 1652.0, 0.806640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119723.0, 0.6354184358016305]
########## End ###########
06/02 06:23:57 PM | Train: [14/80] Step 000/390 Loss 0.014 Prec@(1,5) (100.0%, 100.0%)
06/02 06:23:57 PM | layerwise density: [39753.0, 10135.0, 10101.0, 10128.0, 10023.0, 5297.0, 5173.0, 5166.0, 5205.0, 2861.0, 2834.0, 2908.0, 2775.0, 1896.0, 2009.0, 1807.0, 1652.0]
layerwise density percentage: ['0.607', '0.619', '0.617', '0.618', '0.612', '0.647', '0.631', '0.631', '0.635', '0.698', '0.692', '0.710', '0.677', '0.926', '0.981', '0.882', '0.807']
Global density: 0.6354184746742249
06/02 06:24:08 PM | Train: [14/80] Step 100/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/02 06:24:08 PM | layerwise density: [39744.0, 10145.0, 10109.0, 10134.0, 10023.0, 5315.0, 5180.0, 5182.0, 5214.0, 2864.0, 2841.0, 2914.0, 2773.0, 1900.0, 2007.0, 1816.0, 1654.0]
layerwise density percentage: ['0.606', '0.619', '0.617', '0.619', '0.612', '0.649', '0.632', '0.633', '0.636', '0.699', '0.694', '0.711', '0.677', '0.928', '0.980', '0.887', '0.808']
Global density: 0.6359067559242249
06/02 06:24:18 PM | Train: [14/80] Step 200/390 Loss 0.019 Prec@(1,5) (99.9%, 100.0%)
06/02 06:24:18 PM | layerwise density: [39739.0, 10153.0, 10113.0, 10146.0, 10036.0, 5332.0, 5195.0, 5189.0, 5231.0, 2860.0, 2852.0, 2923.0, 2786.0, 1906.0, 2017.0, 1818.0, 1658.0]
layerwise density percentage: ['0.606', '0.620', '0.617', '0.619', '0.613', '0.651', '0.634', '0.633', '0.639', '0.698', '0.696', '0.714', '0.680', '0.931', '0.985', '0.888', '0.810']
Global density: 0.6366444826126099
06/02 06:24:29 PM | Train: [14/80] Step 300/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/02 06:24:29 PM | layerwise density: [39560.0, 10098.0, 10074.0, 10093.0, 9983.0, 5293.0, 5155.0, 5161.0, 5178.0, 2840.0, 2823.0, 2922.0, 2765.0, 1904.0, 2007.0, 1809.0, 1664.0]
layerwise density percentage: ['0.604', '0.616', '0.615', '0.616', '0.609', '0.646', '0.629', '0.630', '0.632', '0.693', '0.689', '0.713', '0.675', '0.930', '0.980', '0.883', '0.812']
Global density: 0.633327305316925
06/02 06:24:38 PM | Train: [14/80] Step 390/390 Loss 0.020 Prec@(1,5) (99.8%, 100.0%)
06/02 06:24:38 PM | layerwise density: [39527.0, 10083.0, 10057.0, 10076.0, 9962.0, 5285.0, 5152.0, 5148.0, 5174.0, 2846.0, 2824.0, 2917.0, 2772.0, 1907.0, 2016.0, 1821.0, 1671.0]
layerwise density percentage: ['0.603', '0.615', '0.614', '0.615', '0.608', '0.645', '0.629', '0.628', '0.632', '0.695', '0.689', '0.712', '0.677', '0.931', '0.984', '0.889', '0.816']
Global density: 0.6328443288803101
06/02 06:24:39 PM | Train: [14/200] Final Prec@1 99.8100%
06/02 06:24:39 PM | Valid: [14/200] Step 000/078 Loss 1.116 Prec@(1,5) (72.7%, 90.6%)
06/02 06:24:41 PM | Valid: [14/200] Step 078/078 Loss 1.332 Prec@(1,5) (69.3%, 89.3%)
06/02 06:24:41 PM | Valid: [14/200] Final Prec@1 69.3500%
06/02 06:24:41 PM | Current mask training best Prec@1 = 69.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 39527.0, 0.6031341552734375]
['model.relu.alpha_mask_1_0', 16384, 10083.0, 0.61541748046875]
['model.relu.alpha_mask_2_0', 16384, 10057.0, 0.61383056640625]
['model.relu.alpha_mask_3_0', 16384, 10076.0, 0.614990234375]
['model.relu.alpha_mask_4_0', 16384, 9962.0, 0.6080322265625]
['model.relu.alpha_mask_5_0', 8192, 5284.0, 0.64501953125]
['model.relu.alpha_mask_6_0', 8192, 5152.0, 0.62890625]
['model.relu.alpha_mask_7_0', 8192, 5148.0, 0.62841796875]
['model.relu.alpha_mask_8_0', 8192, 5174.0, 0.631591796875]
['model.relu.alpha_mask_9_0', 4096, 2846.0, 0.69482421875]
['model.relu.alpha_mask_10_0', 4096, 2825.0, 0.689697265625]
['model.relu.alpha_mask_11_0', 4096, 2917.0, 0.712158203125]
['model.relu.alpha_mask_12_0', 4096, 2770.0, 0.67626953125]
['model.relu.alpha_mask_13_0', 2048, 1907.0, 0.93115234375]
['model.relu.alpha_mask_14_0', 2048, 2015.0, 0.98388671875]
['model.relu.alpha_mask_15_0', 2048, 1821.0, 0.88916015625]
['model.relu.alpha_mask_16_0', 2048, 1671.0, 0.81591796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119235.0, 0.632828422214674]
########## End ###########
06/02 06:24:42 PM | Train: [15/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/02 06:24:42 PM | layerwise density: [39527.0, 10083.0, 10057.0, 10076.0, 9962.0, 5284.0, 5152.0, 5148.0, 5174.0, 2846.0, 2825.0, 2917.0, 2770.0, 1907.0, 2015.0, 1821.0, 1671.0]
layerwise density percentage: ['0.603', '0.615', '0.614', '0.615', '0.608', '0.645', '0.629', '0.628', '0.632', '0.695', '0.690', '0.712', '0.676', '0.931', '0.984', '0.889', '0.816']
Global density: 0.632828414440155
06/02 06:24:52 PM | Train: [15/80] Step 100/390 Loss 0.016 Prec@(1,5) (99.9%, 100.0%)
06/02 06:24:52 PM | layerwise density: [39515.0, 10084.0, 10053.0, 10078.0, 9958.0, 5311.0, 5160.0, 5153.0, 5173.0, 2857.0, 2856.0, 2928.0, 2782.0, 1907.0, 2025.0, 1830.0, 1682.0]
layerwise density percentage: ['0.603', '0.615', '0.614', '0.615', '0.608', '0.648', '0.630', '0.629', '0.631', '0.698', '0.697', '0.715', '0.679', '0.931', '0.989', '0.894', '0.821']
Global density: 0.633449375629425
06/02 06:25:03 PM | Train: [15/80] Step 200/390 Loss 0.017 Prec@(1,5) (99.9%, 100.0%)
06/02 06:25:03 PM | layerwise density: [39497.0, 10088.0, 10063.0, 10090.0, 9961.0, 5325.0, 5165.0, 5166.0, 5188.0, 2864.0, 2850.0, 2934.0, 2793.0, 1914.0, 2024.0, 1838.0, 1693.0]
layerwise density percentage: ['0.603', '0.616', '0.614', '0.616', '0.608', '0.650', '0.630', '0.631', '0.633', '0.699', '0.696', '0.716', '0.682', '0.935', '0.988', '0.897', '0.827']
Global density: 0.6339854598045349
06/02 06:25:14 PM | Train: [15/80] Step 300/390 Loss 0.017 Prec@(1,5) (99.9%, 100.0%)
06/02 06:25:14 PM | layerwise density: [39489.0, 10093.0, 10069.0, 10103.0, 9968.0, 5348.0, 5179.0, 5185.0, 5206.0, 2871.0, 2853.0, 2945.0, 2802.0, 1917.0, 2020.0, 1846.0, 1703.0]
layerwise density percentage: ['0.603', '0.616', '0.615', '0.617', '0.608', '0.653', '0.632', '0.633', '0.635', '0.701', '0.697', '0.719', '0.684', '0.936', '0.986', '0.901', '0.832']
Global density: 0.634749710559845
06/02 06:25:23 PM | Train: [15/80] Step 390/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/02 06:25:23 PM | layerwise density: [39483.0, 10102.0, 10075.0, 10104.0, 9966.0, 5368.0, 5185.0, 5197.0, 5214.0, 2893.0, 2856.0, 2957.0, 2813.0, 1925.0, 2024.0, 1853.0, 1711.0]
layerwise density percentage: ['0.602', '0.617', '0.615', '0.617', '0.608', '0.655', '0.633', '0.634', '0.636', '0.706', '0.697', '0.722', '0.687', '0.940', '0.988', '0.905', '0.835']
Global density: 0.6354343891143799
06/02 06:25:23 PM | Train: [15/200] Final Prec@1 99.8460%
06/02 06:25:24 PM | Valid: [15/200] Step 000/078 Loss 1.087 Prec@(1,5) (73.4%, 93.0%)
06/02 06:25:26 PM | Valid: [15/200] Step 078/078 Loss 1.340 Prec@(1,5) (69.2%, 89.2%)
06/02 06:25:26 PM | Valid: [15/200] Final Prec@1 69.1700%
06/02 06:25:26 PM | Current mask training best Prec@1 = 69.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 39483.0, 0.6024627685546875]
['model.relu.alpha_mask_1_0', 16384, 10103.0, 0.61663818359375]
['model.relu.alpha_mask_2_0', 16384, 10075.0, 0.61492919921875]
['model.relu.alpha_mask_3_0', 16384, 10104.0, 0.61669921875]
['model.relu.alpha_mask_4_0', 16384, 9965.0, 0.60821533203125]
['model.relu.alpha_mask_5_0', 8192, 5370.0, 0.655517578125]
['model.relu.alpha_mask_6_0', 8192, 5185.0, 0.6329345703125]
['model.relu.alpha_mask_7_0', 8192, 5197.0, 0.6343994140625]
['model.relu.alpha_mask_8_0', 8192, 5214.0, 0.636474609375]
['model.relu.alpha_mask_9_0', 4096, 2894.0, 0.70654296875]
['model.relu.alpha_mask_10_0', 4096, 2854.0, 0.69677734375]
['model.relu.alpha_mask_11_0', 4096, 2957.0, 0.721923828125]
['model.relu.alpha_mask_12_0', 4096, 2813.0, 0.686767578125]
['model.relu.alpha_mask_13_0', 2048, 1924.0, 0.939453125]
['model.relu.alpha_mask_14_0', 2048, 2025.0, 0.98876953125]
['model.relu.alpha_mask_15_0', 2048, 1854.0, 0.9052734375]
['model.relu.alpha_mask_16_0', 2048, 1711.0, 0.83544921875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119728.0, 0.6354449728260869]
########## End ###########
06/02 06:25:27 PM | Train: [16/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:25:27 PM | layerwise density: [39483.0, 10103.0, 10075.0, 10104.0, 9965.0, 5370.0, 5185.0, 5197.0, 5214.0, 2894.0, 2854.0, 2957.0, 2813.0, 1924.0, 2025.0, 1854.0, 1711.0]
layerwise density percentage: ['0.602', '0.617', '0.615', '0.617', '0.608', '0.656', '0.633', '0.634', '0.636', '0.707', '0.697', '0.722', '0.687', '0.939', '0.989', '0.905', '0.835']
Global density: 0.6354449987411499
06/02 06:25:38 PM | Train: [16/80] Step 100/390 Loss 0.016 Prec@(1,5) (99.9%, 100.0%)
06/02 06:25:38 PM | layerwise density: [39477.0, 10113.0, 10077.0, 10113.0, 9966.0, 5381.0, 5196.0, 5210.0, 5218.0, 2904.0, 2856.0, 2966.0, 2821.0, 1929.0, 2028.0, 1850.0, 1718.0]
layerwise density percentage: ['0.602', '0.617', '0.615', '0.617', '0.608', '0.657', '0.634', '0.636', '0.637', '0.709', '0.697', '0.724', '0.689', '0.942', '0.990', '0.903', '0.839']
Global density: 0.6359491944313049
06/02 06:25:48 PM | Train: [16/80] Step 200/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 06:25:48 PM | layerwise density: [39468.0, 10126.0, 10081.0, 10125.0, 9984.0, 5376.0, 5193.0, 5226.0, 5217.0, 2909.0, 2867.0, 2980.0, 2813.0, 1934.0, 2031.0, 1863.0, 1722.0]
layerwise density percentage: ['0.602', '0.618', '0.615', '0.618', '0.609', '0.656', '0.634', '0.638', '0.637', '0.710', '0.700', '0.728', '0.687', '0.944', '0.992', '0.910', '0.841']
Global density: 0.6364374756813049
06/02 06:25:59 PM | Train: [16/80] Step 300/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/02 06:25:59 PM | layerwise density: [39280.0, 10065.0, 10007.0, 10082.0, 9921.0, 5328.0, 5155.0, 5193.0, 5188.0, 2885.0, 2847.0, 2970.0, 2787.0, 1917.0, 2017.0, 1842.0, 1730.0]
layerwise density percentage: ['0.599', '0.614', '0.611', '0.615', '0.606', '0.650', '0.629', '0.634', '0.633', '0.704', '0.695', '0.725', '0.680', '0.936', '0.985', '0.899', '0.845']
Global density: 0.632716953754425
06/02 06:26:08 PM | Train: [16/80] Step 390/390 Loss 0.017 Prec@(1,5) (99.8%, 100.0%)
06/02 06:26:08 PM | layerwise density: [39237.0, 10036.0, 9977.0, 10062.0, 9895.0, 5310.0, 5146.0, 5181.0, 5166.0, 2875.0, 2833.0, 2962.0, 2770.0, 1926.0, 2025.0, 1858.0, 1732.0]
layerwise density percentage: ['0.599', '0.613', '0.609', '0.614', '0.604', '0.648', '0.628', '0.632', '0.631', '0.702', '0.692', '0.723', '0.676', '0.940', '0.989', '0.907', '0.846']
Global density: 0.6315334439277649
06/02 06:26:08 PM | Train: [16/200] Final Prec@1 99.8040%
06/02 06:26:08 PM | Valid: [16/200] Step 000/078 Loss 1.124 Prec@(1,5) (72.7%, 91.4%)
06/02 06:26:11 PM | Valid: [16/200] Step 078/078 Loss 1.316 Prec@(1,5) (69.3%, 89.3%)
06/02 06:26:11 PM | Valid: [16/200] Final Prec@1 69.3200%
06/02 06:26:11 PM | Current mask training best Prec@1 = 69.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 39237.0, 0.5987091064453125]
['model.relu.alpha_mask_1_0', 16384, 10035.0, 0.61248779296875]
['model.relu.alpha_mask_2_0', 16384, 9977.0, 0.60894775390625]
['model.relu.alpha_mask_3_0', 16384, 10062.0, 0.6141357421875]
['model.relu.alpha_mask_4_0', 16384, 9897.0, 0.60406494140625]
['model.relu.alpha_mask_5_0', 8192, 5309.0, 0.6480712890625]
['model.relu.alpha_mask_6_0', 8192, 5144.0, 0.6279296875]
['model.relu.alpha_mask_7_0', 8192, 5179.0, 0.6322021484375]
['model.relu.alpha_mask_8_0', 8192, 5167.0, 0.6307373046875]
['model.relu.alpha_mask_9_0', 4096, 2876.0, 0.7021484375]
['model.relu.alpha_mask_10_0', 4096, 2834.0, 0.69189453125]
['model.relu.alpha_mask_11_0', 4096, 2962.0, 0.72314453125]
['model.relu.alpha_mask_12_0', 4096, 2770.0, 0.67626953125]
['model.relu.alpha_mask_13_0', 2048, 1927.0, 0.94091796875]
['model.relu.alpha_mask_14_0', 2048, 2025.0, 0.98876953125]
['model.relu.alpha_mask_15_0', 2048, 1858.0, 0.9072265625]
['model.relu.alpha_mask_16_0', 2048, 1732.0, 0.845703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118991.0, 0.6315334154211957]
########## End ###########
06/02 06:26:12 PM | Train: [17/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/02 06:26:12 PM | layerwise density: [39237.0, 10035.0, 9977.0, 10062.0, 9897.0, 5309.0, 5144.0, 5179.0, 5167.0, 2876.0, 2834.0, 2962.0, 2770.0, 1927.0, 2025.0, 1858.0, 1732.0]
layerwise density percentage: ['0.599', '0.612', '0.609', '0.614', '0.604', '0.648', '0.628', '0.632', '0.631', '0.702', '0.692', '0.723', '0.676', '0.941', '0.989', '0.907', '0.846']
Global density: 0.6315334439277649
06/02 06:26:22 PM | Train: [17/80] Step 100/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 06:26:22 PM | layerwise density: [39210.0, 10037.0, 9973.0, 10046.0, 9891.0, 5337.0, 5159.0, 5194.0, 5173.0, 2895.0, 2853.0, 2974.0, 2788.0, 1934.0, 2029.0, 1872.0, 1738.0]
layerwise density percentage: ['0.598', '0.613', '0.609', '0.613', '0.604', '0.651', '0.630', '0.634', '0.631', '0.707', '0.697', '0.726', '0.681', '0.944', '0.991', '0.914', '0.849']
Global density: 0.6321278810501099
06/02 06:26:32 PM | Train: [17/80] Step 200/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 06:26:32 PM | layerwise density: [39191.0, 10041.0, 9990.0, 10048.0, 9887.0, 5360.0, 5174.0, 5213.0, 5190.0, 2907.0, 2859.0, 2988.0, 2809.0, 1935.0, 2037.0, 1884.0, 1748.0]
layerwise density percentage: ['0.598', '0.613', '0.610', '0.613', '0.603', '0.654', '0.632', '0.636', '0.634', '0.710', '0.698', '0.729', '0.686', '0.945', '0.995', '0.920', '0.854']
Global density: 0.6329663991928101
06/02 06:26:43 PM | Train: [17/80] Step 300/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 06:26:43 PM | layerwise density: [39187.0, 10049.0, 10002.0, 10047.0, 9902.0, 5358.0, 5177.0, 5225.0, 5202.0, 2918.0, 2856.0, 2993.0, 2818.0, 1945.0, 2027.0, 1881.0, 1752.0]
layerwise density percentage: ['0.598', '0.613', '0.610', '0.613', '0.604', '0.654', '0.632', '0.638', '0.635', '0.712', '0.697', '0.731', '0.688', '0.950', '0.990', '0.918', '0.855']
Global density: 0.6333804130554199
06/02 06:26:53 PM | Train: [17/80] Step 390/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 06:26:53 PM | layerwise density: [39180.0, 10063.0, 10010.0, 10061.0, 9896.0, 5375.0, 5193.0, 5230.0, 5208.0, 2918.0, 2862.0, 3005.0, 2840.0, 1948.0, 2039.0, 1889.0, 1760.0]
layerwise density percentage: ['0.598', '0.614', '0.611', '0.614', '0.604', '0.656', '0.634', '0.638', '0.636', '0.712', '0.699', '0.734', '0.693', '0.951', '0.996', '0.922', '0.859']
Global density: 0.6341128349304199
06/02 06:26:53 PM | Train: [17/200] Final Prec@1 99.8260%
06/02 06:26:53 PM | Valid: [17/200] Step 000/078 Loss 1.119 Prec@(1,5) (73.4%, 94.5%)
06/02 06:26:55 PM | Valid: [17/200] Step 078/078 Loss 1.345 Prec@(1,5) (69.1%, 89.3%)
06/02 06:26:55 PM | Valid: [17/200] Final Prec@1 69.0600%
06/02 06:26:55 PM | Current mask training best Prec@1 = 69.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 39179.0, 0.5978240966796875]
['model.relu.alpha_mask_1_0', 16384, 10064.0, 0.6142578125]
['model.relu.alpha_mask_2_0', 16384, 10010.0, 0.6109619140625]
['model.relu.alpha_mask_3_0', 16384, 10061.0, 0.61407470703125]
['model.relu.alpha_mask_4_0', 16384, 9895.0, 0.60394287109375]
['model.relu.alpha_mask_5_0', 8192, 5375.0, 0.6561279296875]
['model.relu.alpha_mask_6_0', 8192, 5193.0, 0.6339111328125]
['model.relu.alpha_mask_7_0', 8192, 5230.0, 0.638427734375]
['model.relu.alpha_mask_8_0', 8192, 5207.0, 0.6356201171875]
['model.relu.alpha_mask_9_0', 4096, 2920.0, 0.712890625]
['model.relu.alpha_mask_10_0', 4096, 2865.0, 0.699462890625]
['model.relu.alpha_mask_11_0', 4096, 3005.0, 0.733642578125]
['model.relu.alpha_mask_12_0', 4096, 2840.0, 0.693359375]
['model.relu.alpha_mask_13_0', 2048, 1948.0, 0.951171875]
['model.relu.alpha_mask_14_0', 2048, 2039.0, 0.99560546875]
['model.relu.alpha_mask_15_0', 2048, 1889.0, 0.92236328125]
['model.relu.alpha_mask_16_0', 2048, 1760.0, 0.859375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119480.0, 0.6341287364130435]
########## End ###########
06/02 06:26:56 PM | Train: [18/80] Step 000/390 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)
06/02 06:26:56 PM | layerwise density: [39179.0, 10064.0, 10010.0, 10061.0, 9895.0, 5375.0, 5193.0, 5230.0, 5207.0, 2920.0, 2865.0, 3005.0, 2840.0, 1948.0, 2039.0, 1889.0, 1760.0]
layerwise density percentage: ['0.598', '0.614', '0.611', '0.614', '0.604', '0.656', '0.634', '0.638', '0.636', '0.713', '0.699', '0.734', '0.693', '0.951', '0.996', '0.922', '0.859']
Global density: 0.634128749370575
06/02 06:27:07 PM | Train: [18/80] Step 100/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 06:27:07 PM | layerwise density: [39160.0, 10065.0, 10016.0, 10080.0, 9915.0, 5397.0, 5197.0, 5242.0, 5222.0, 2920.0, 2878.0, 3011.0, 2840.0, 1954.0, 2035.0, 1893.0, 1765.0]
layerwise density percentage: ['0.598', '0.614', '0.611', '0.615', '0.605', '0.659', '0.634', '0.640', '0.637', '0.713', '0.703', '0.735', '0.693', '0.954', '0.994', '0.924', '0.862']
Global density: 0.6347125768661499
06/02 06:27:17 PM | Train: [18/80] Step 200/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 06:27:17 PM | layerwise density: [39152.0, 10074.0, 10015.0, 10094.0, 9920.0, 5411.0, 5192.0, 5261.0, 5236.0, 2927.0, 2876.0, 3018.0, 2857.0, 1962.0, 2029.0, 1887.0, 1769.0]
layerwise density percentage: ['0.597', '0.615', '0.611', '0.616', '0.605', '0.661', '0.634', '0.642', '0.639', '0.715', '0.702', '0.737', '0.698', '0.958', '0.991', '0.921', '0.864']
Global density: 0.6351902484893799
06/02 06:27:28 PM | Train: [18/80] Step 300/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 06:27:28 PM | layerwise density: [39154.0, 10085.0, 10012.0, 10106.0, 9925.0, 5425.0, 5211.0, 5285.0, 5253.0, 2943.0, 2897.0, 3023.0, 2857.0, 1955.0, 2033.0, 1900.0, 1773.0]
layerwise density percentage: ['0.597', '0.616', '0.611', '0.617', '0.606', '0.662', '0.636', '0.645', '0.641', '0.719', '0.707', '0.738', '0.698', '0.955', '0.993', '0.928', '0.866']
Global density: 0.6360235214233398
06/02 06:27:37 PM | Train: [18/80] Step 390/390 Loss 0.016 Prec@(1,5) (99.8%, 100.0%)
06/02 06:27:37 PM | layerwise density: [39143.0, 10101.0, 10026.0, 10115.0, 9943.0, 5420.0, 5239.0, 5295.0, 5263.0, 2946.0, 2905.0, 3036.0, 2856.0, 1964.0, 2034.0, 1898.0, 1776.0]
layerwise density percentage: ['0.597', '0.617', '0.612', '0.617', '0.607', '0.662', '0.640', '0.646', '0.642', '0.719', '0.709', '0.741', '0.697', '0.959', '0.993', '0.927', '0.867']
Global density: 0.6366763114929199
06/02 06:27:37 PM | Train: [18/200] Final Prec@1 99.8400%
06/02 06:27:37 PM | Valid: [18/200] Step 000/078 Loss 1.166 Prec@(1,5) (69.5%, 93.0%)
06/02 06:27:39 PM | Valid: [18/200] Step 078/078 Loss 1.317 Prec@(1,5) (69.6%, 89.5%)
06/02 06:27:39 PM | Valid: [18/200] Final Prec@1 69.5600%
06/02 06:27:40 PM | Current mask training best Prec@1 = 69.5600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 39143.0, 0.5972747802734375]
['model.relu.alpha_mask_1_0', 16384, 10101.0, 0.61651611328125]
['model.relu.alpha_mask_2_0', 16384, 10026.0, 0.6119384765625]
['model.relu.alpha_mask_3_0', 16384, 10115.0, 0.61737060546875]
['model.relu.alpha_mask_4_0', 16384, 9943.0, 0.60687255859375]
['model.relu.alpha_mask_5_0', 8192, 5420.0, 0.66162109375]
['model.relu.alpha_mask_6_0', 8192, 5239.0, 0.6395263671875]
['model.relu.alpha_mask_7_0', 8192, 5295.0, 0.6463623046875]
['model.relu.alpha_mask_8_0', 8192, 5263.0, 0.6424560546875]
['model.relu.alpha_mask_9_0', 4096, 2946.0, 0.71923828125]
['model.relu.alpha_mask_10_0', 4096, 2905.0, 0.709228515625]
['model.relu.alpha_mask_11_0', 4096, 3036.0, 0.7412109375]
['model.relu.alpha_mask_12_0', 4096, 2856.0, 0.697265625]
['model.relu.alpha_mask_13_0', 2048, 1964.0, 0.958984375]
['model.relu.alpha_mask_14_0', 2048, 2034.0, 0.9931640625]
['model.relu.alpha_mask_15_0', 2048, 1898.0, 0.9267578125]
['model.relu.alpha_mask_16_0', 2048, 1776.0, 0.8671875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119960.0, 0.6366762907608695]
########## End ###########
06/02 06:27:41 PM | Train: [19/80] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
06/02 06:27:41 PM | layerwise density: [39143.0, 10101.0, 10026.0, 10115.0, 9943.0, 5420.0, 5239.0, 5295.0, 5263.0, 2946.0, 2905.0, 3036.0, 2856.0, 1964.0, 2034.0, 1898.0, 1776.0]
layerwise density percentage: ['0.597', '0.617', '0.612', '0.617', '0.607', '0.662', '0.640', '0.646', '0.642', '0.719', '0.709', '0.741', '0.697', '0.959', '0.993', '0.927', '0.867']
Global density: 0.6366763114929199
06/02 06:27:51 PM | Train: [19/80] Step 100/390 Loss 0.014 Prec@(1,5) (99.9%, 100.0%)
06/02 06:27:51 PM | layerwise density: [38857.0, 9987.0, 9938.0, 10016.0, 9836.0, 5343.0, 5166.0, 5238.0, 5186.0, 2914.0, 2853.0, 3026.0, 2813.0, 1956.0, 2026.0, 1877.0, 1778.0]
layerwise density percentage: ['0.593', '0.610', '0.607', '0.611', '0.600', '0.652', '0.631', '0.639', '0.633', '0.711', '0.697', '0.739', '0.687', '0.955', '0.989', '0.917', '0.868']
Global density: 0.6305727958679199
06/02 06:28:02 PM | Train: [19/80] Step 200/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 06:28:02 PM | layerwise density: [38801.0, 9959.0, 9907.0, 9996.0, 9800.0, 5346.0, 5154.0, 5224.0, 5175.0, 2918.0, 2872.0, 3022.0, 2823.0, 1962.0, 2035.0, 1889.0, 1781.0]
layerwise density percentage: ['0.592', '0.608', '0.605', '0.610', '0.598', '0.653', '0.629', '0.638', '0.632', '0.712', '0.701', '0.738', '0.689', '0.958', '0.994', '0.922', '0.870']
Global density: 0.6297979354858398
06/02 06:28:12 PM | Train: [19/80] Step 300/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 06:28:12 PM | layerwise density: [38784.0, 9965.0, 9912.0, 9994.0, 9807.0, 5374.0, 5171.0, 5225.0, 5187.0, 2944.0, 2887.0, 3032.0, 2840.0, 1970.0, 2038.0, 1910.0, 1785.0]
layerwise density percentage: ['0.592', '0.608', '0.605', '0.610', '0.599', '0.656', '0.631', '0.638', '0.633', '0.719', '0.705', '0.740', '0.693', '0.962', '0.995', '0.933', '0.872']
Global density: 0.6306524276733398
06/02 06:28:21 PM | Train: [19/80] Step 390/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 06:28:21 PM | layerwise density: [38780.0, 9983.0, 9919.0, 9997.0, 9816.0, 5398.0, 5197.0, 5248.0, 5198.0, 2954.0, 2883.0, 3041.0, 2848.0, 1968.0, 2033.0, 1913.0, 1788.0]
layerwise density percentage: ['0.592', '0.609', '0.605', '0.610', '0.599', '0.659', '0.634', '0.641', '0.635', '0.721', '0.704', '0.742', '0.695', '0.961', '0.993', '0.934', '0.873']
Global density: 0.6313901543617249
06/02 06:28:21 PM | Train: [19/200] Final Prec@1 99.8460%
06/02 06:28:21 PM | Valid: [19/200] Step 000/078 Loss 1.116 Prec@(1,5) (75.8%, 90.6%)
06/02 06:28:23 PM | Valid: [19/200] Step 078/078 Loss 1.317 Prec@(1,5) (69.7%, 89.3%)
06/02 06:28:23 PM | Valid: [19/200] Final Prec@1 69.6500%
06/02 06:28:24 PM | Current mask training best Prec@1 = 69.6500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 38780.0, 0.59173583984375]
['model.relu.alpha_mask_1_0', 16384, 9983.0, 0.60931396484375]
['model.relu.alpha_mask_2_0', 16384, 9919.0, 0.60540771484375]
['model.relu.alpha_mask_3_0', 16384, 9997.0, 0.61016845703125]
['model.relu.alpha_mask_4_0', 16384, 9816.0, 0.59912109375]
['model.relu.alpha_mask_5_0', 8192, 5398.0, 0.658935546875]
['model.relu.alpha_mask_6_0', 8192, 5197.0, 0.6343994140625]
['model.relu.alpha_mask_7_0', 8192, 5248.0, 0.640625]
['model.relu.alpha_mask_8_0', 8192, 5198.0, 0.634521484375]
['model.relu.alpha_mask_9_0', 4096, 2954.0, 0.72119140625]
['model.relu.alpha_mask_10_0', 4096, 2883.0, 0.703857421875]
['model.relu.alpha_mask_11_0', 4096, 3041.0, 0.742431640625]
['model.relu.alpha_mask_12_0', 4096, 2848.0, 0.6953125]
['model.relu.alpha_mask_13_0', 2048, 1968.0, 0.9609375]
['model.relu.alpha_mask_14_0', 2048, 2033.0, 0.99267578125]
['model.relu.alpha_mask_15_0', 2048, 1913.0, 0.93408203125]
['model.relu.alpha_mask_16_0', 2048, 1788.0, 0.873046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118964.0, 0.6313901154891305]
########## End ###########
06/02 06:28:25 PM | Train: [20/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/02 06:28:25 PM | layerwise density: [38780.0, 9983.0, 9919.0, 9997.0, 9816.0, 5398.0, 5197.0, 5248.0, 5198.0, 2954.0, 2883.0, 3041.0, 2848.0, 1968.0, 2033.0, 1913.0, 1788.0]
layerwise density percentage: ['0.592', '0.609', '0.605', '0.610', '0.599', '0.659', '0.634', '0.641', '0.635', '0.721', '0.704', '0.742', '0.695', '0.961', '0.993', '0.934', '0.873']
Global density: 0.6313901543617249
06/02 06:28:36 PM | Train: [20/80] Step 100/390 Loss 0.015 Prec@(1,5) (99.8%, 100.0%)
06/02 06:28:36 PM | layerwise density: [38768.0, 9997.0, 9923.0, 10020.0, 9838.0, 5422.0, 5200.0, 5257.0, 5222.0, 2963.0, 2903.0, 3049.0, 2855.0, 1971.0, 2040.0, 1912.0, 1792.0]
layerwise density percentage: ['0.592', '0.610', '0.606', '0.612', '0.600', '0.662', '0.635', '0.642', '0.637', '0.723', '0.709', '0.744', '0.697', '0.962', '0.996', '0.934', '0.875']
Global density: 0.6322817802429199
06/02 06:28:46 PM | Train: [20/80] Step 200/390 Loss 0.014 Prec@(1,5) (99.9%, 100.0%)
06/02 06:28:46 PM | layerwise density: [38763.0, 10013.0, 9931.0, 10032.0, 9855.0, 5440.0, 5212.0, 5277.0, 5220.0, 2976.0, 2906.0, 3060.0, 2862.0, 1966.0, 2044.0, 1914.0, 1799.0]
layerwise density percentage: ['0.591', '0.611', '0.606', '0.612', '0.602', '0.664', '0.636', '0.644', '0.637', '0.727', '0.709', '0.747', '0.699', '0.960', '0.998', '0.935', '0.878']
Global density: 0.6330142021179199
06/02 06:28:57 PM | Train: [20/80] Step 300/390 Loss 0.014 Prec@(1,5) (99.9%, 100.0%)
06/02 06:28:57 PM | layerwise density: [38759.0, 10020.0, 9930.0, 10039.0, 9859.0, 5444.0, 5224.0, 5295.0, 5243.0, 2976.0, 2913.0, 3069.0, 2888.0, 1975.0, 2039.0, 1915.0, 1808.0]
layerwise density percentage: ['0.591', '0.612', '0.606', '0.613', '0.602', '0.665', '0.638', '0.646', '0.640', '0.727', '0.711', '0.749', '0.705', '0.964', '0.996', '0.935', '0.883']
Global density: 0.633682906627655
06/02 06:29:06 PM | Train: [20/80] Step 390/390 Loss 0.014 Prec@(1,5) (99.9%, 100.0%)
06/02 06:29:06 PM | layerwise density: [38756.0, 10030.0, 9950.0, 10044.0, 9865.0, 5464.0, 5231.0, 5300.0, 5255.0, 2978.0, 2921.0, 3078.0, 2896.0, 1973.0, 2039.0, 1919.0, 1811.0]
layerwise density percentage: ['0.591', '0.612', '0.607', '0.613', '0.602', '0.667', '0.639', '0.647', '0.641', '0.727', '0.713', '0.751', '0.707', '0.963', '0.996', '0.937', '0.884']
Global density: 0.63428795337677
06/02 06:29:06 PM | Train: [20/200] Final Prec@1 99.8500%
06/02 06:29:07 PM | Valid: [20/200] Step 000/078 Loss 1.140 Prec@(1,5) (71.9%, 91.4%)
06/02 06:29:09 PM | Valid: [20/200] Step 078/078 Loss 1.316 Prec@(1,5) (69.5%, 89.4%)
06/02 06:29:09 PM | Valid: [20/200] Final Prec@1 69.4900%
06/02 06:29:09 PM | Current mask training best Prec@1 = 69.6500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 38756.0, 0.59136962890625]
['model.relu.alpha_mask_1_0', 16384, 10030.0, 0.6121826171875]
['model.relu.alpha_mask_2_0', 16384, 9950.0, 0.6072998046875]
['model.relu.alpha_mask_3_0', 16384, 10044.0, 0.613037109375]
['model.relu.alpha_mask_4_0', 16384, 9866.0, 0.6021728515625]
['model.relu.alpha_mask_5_0', 8192, 5464.0, 0.6669921875]
['model.relu.alpha_mask_6_0', 8192, 5232.0, 0.638671875]
['model.relu.alpha_mask_7_0', 8192, 5300.0, 0.64697265625]
['model.relu.alpha_mask_8_0', 8192, 5255.0, 0.6414794921875]
['model.relu.alpha_mask_9_0', 4096, 2978.0, 0.72705078125]
['model.relu.alpha_mask_10_0', 4096, 2922.0, 0.71337890625]
['model.relu.alpha_mask_11_0', 4096, 3078.0, 0.75146484375]
['model.relu.alpha_mask_12_0', 4096, 2898.0, 0.70751953125]
['model.relu.alpha_mask_13_0', 2048, 1973.0, 0.96337890625]
['model.relu.alpha_mask_14_0', 2048, 2039.0, 0.99560546875]
['model.relu.alpha_mask_15_0', 2048, 1919.0, 0.93701171875]
['model.relu.alpha_mask_16_0', 2048, 1811.0, 0.88427734375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119515.0, 0.6343144955842391]
########## End ###########
06/02 06:29:10 PM | Train: [21/80] Step 000/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
06/02 06:29:10 PM | layerwise density: [38756.0, 10030.0, 9950.0, 10044.0, 9866.0, 5464.0, 5232.0, 5300.0, 5255.0, 2978.0, 2922.0, 3078.0, 2898.0, 1973.0, 2039.0, 1919.0, 1811.0]
layerwise density percentage: ['0.591', '0.612', '0.607', '0.613', '0.602', '0.667', '0.639', '0.647', '0.641', '0.727', '0.713', '0.751', '0.708', '0.963', '0.996', '0.937', '0.884']
Global density: 0.6343145370483398
06/02 06:29:20 PM | Train: [21/80] Step 100/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 06:29:20 PM | layerwise density: [38753.0, 10050.0, 9955.0, 10046.0, 9880.0, 5480.0, 5239.0, 5310.0, 5269.0, 2997.0, 2936.0, 3085.0, 2899.0, 1980.0, 2038.0, 1922.0, 1821.0]
layerwise density percentage: ['0.591', '0.613', '0.608', '0.613', '0.603', '0.669', '0.640', '0.648', '0.643', '0.732', '0.717', '0.753', '0.708', '0.967', '0.995', '0.938', '0.889']
Global density: 0.6350840926170349
06/02 06:29:31 PM | Train: [21/80] Step 200/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 06:29:31 PM | layerwise density: [38747.0, 10063.0, 9979.0, 10051.0, 9879.0, 5494.0, 5252.0, 5315.0, 5266.0, 3006.0, 2928.0, 3087.0, 2897.0, 1981.0, 2043.0, 1919.0, 1824.0]
layerwise density percentage: ['0.591', '0.614', '0.609', '0.613', '0.603', '0.671', '0.641', '0.649', '0.643', '0.734', '0.715', '0.754', '0.707', '0.967', '0.998', '0.937', '0.891']
Global density: 0.6354609131813049
06/02 06:29:41 PM | Train: [21/80] Step 300/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 06:29:41 PM | layerwise density: [38742.0, 10078.0, 9988.0, 10053.0, 9882.0, 5497.0, 5259.0, 5325.0, 5272.0, 3005.0, 2933.0, 3090.0, 2887.0, 1989.0, 2045.0, 1926.0, 1828.0]
layerwise density percentage: ['0.591', '0.615', '0.610', '0.614', '0.603', '0.671', '0.642', '0.650', '0.644', '0.734', '0.716', '0.754', '0.705', '0.971', '0.999', '0.940', '0.893']
Global density: 0.6358218193054199
06/02 06:29:51 PM | Train: [21/80] Step 390/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 06:29:51 PM | layerwise density: [38749.0, 10090.0, 9994.0, 10064.0, 9895.0, 5511.0, 5257.0, 5339.0, 5291.0, 3021.0, 2949.0, 3097.0, 2904.0, 1990.0, 2041.0, 1926.0, 1833.0]
layerwise density percentage: ['0.591', '0.616', '0.610', '0.614', '0.604', '0.673', '0.642', '0.652', '0.646', '0.738', '0.720', '0.756', '0.709', '0.972', '0.997', '0.940', '0.895']
Global density: 0.6366285085678101
06/02 06:29:51 PM | Train: [21/200] Final Prec@1 99.8820%
06/02 06:29:51 PM | Valid: [21/200] Step 000/078 Loss 1.025 Prec@(1,5) (72.7%, 94.5%)
06/02 06:29:53 PM | Valid: [21/200] Step 078/078 Loss 1.295 Prec@(1,5) (70.1%, 89.8%)
06/02 06:29:53 PM | Valid: [21/200] Final Prec@1 70.1000%
06/02 06:29:54 PM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 38749.0, 0.5912628173828125]
['model.relu.alpha_mask_1_0', 16384, 10090.0, 0.6158447265625]
['model.relu.alpha_mask_2_0', 16384, 9993.0, 0.60992431640625]
['model.relu.alpha_mask_3_0', 16384, 10064.0, 0.6142578125]
['model.relu.alpha_mask_4_0', 16384, 9895.0, 0.60394287109375]
['model.relu.alpha_mask_5_0', 8192, 5511.0, 0.6727294921875]
['model.relu.alpha_mask_6_0', 8192, 5256.0, 0.6416015625]
['model.relu.alpha_mask_7_0', 8192, 5340.0, 0.65185546875]
['model.relu.alpha_mask_8_0', 8192, 5291.0, 0.6458740234375]
['model.relu.alpha_mask_9_0', 4096, 3020.0, 0.7373046875]
['model.relu.alpha_mask_10_0', 4096, 2948.0, 0.7197265625]
['model.relu.alpha_mask_11_0', 4096, 3098.0, 0.75634765625]
['model.relu.alpha_mask_12_0', 4096, 2902.0, 0.70849609375]
['model.relu.alpha_mask_13_0', 2048, 1990.0, 0.9716796875]
['model.relu.alpha_mask_14_0', 2048, 2041.0, 0.99658203125]
['model.relu.alpha_mask_15_0', 2048, 1927.0, 0.94091796875]
['model.relu.alpha_mask_16_0', 2048, 1833.0, 0.89501953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119948.0, 0.636612601902174]
########## End ###########
06/02 06:29:55 PM | Train: [22/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/02 06:29:55 PM | layerwise density: [38749.0, 10090.0, 9993.0, 10064.0, 9895.0, 5511.0, 5256.0, 5340.0, 5291.0, 3020.0, 2948.0, 3098.0, 2902.0, 1990.0, 2041.0, 1927.0, 1833.0]
layerwise density percentage: ['0.591', '0.616', '0.610', '0.614', '0.604', '0.673', '0.642', '0.652', '0.646', '0.737', '0.720', '0.756', '0.708', '0.972', '0.997', '0.941', '0.895']
Global density: 0.636612594127655
06/02 06:30:05 PM | Train: [22/80] Step 100/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
06/02 06:30:05 PM | layerwise density: [38745.0, 10091.0, 9988.0, 10072.0, 9886.0, 5519.0, 5251.0, 5354.0, 5298.0, 3039.0, 2958.0, 3106.0, 2893.0, 1983.0, 2044.0, 1928.0, 1837.0]
layerwise density percentage: ['0.591', '0.616', '0.610', '0.615', '0.603', '0.674', '0.641', '0.654', '0.647', '0.742', '0.722', '0.758', '0.706', '0.968', '0.998', '0.941', '0.897']
Global density: 0.636846125125885
06/02 06:30:16 PM | Train: [22/80] Step 200/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/02 06:30:16 PM | layerwise density: [38340.0, 9930.0, 9828.0, 9952.0, 9736.0, 5388.0, 5161.0, 5249.0, 5173.0, 2974.0, 2906.0, 3081.0, 2844.0, 1977.0, 2039.0, 1908.0, 1836.0]
layerwise density percentage: ['0.585', '0.606', '0.600', '0.607', '0.594', '0.658', '0.630', '0.641', '0.631', '0.726', '0.709', '0.752', '0.694', '0.965', '0.996', '0.932', '0.896']
Global density: 0.6279827952384949
06/02 06:30:26 PM | Train: [22/80] Step 300/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/02 06:30:26 PM | layerwise density: [38304.0, 9924.0, 9822.0, 9944.0, 9727.0, 5396.0, 5168.0, 5257.0, 5171.0, 2974.0, 2914.0, 3086.0, 2854.0, 1986.0, 2035.0, 1932.0, 1839.0]
layerwise density percentage: ['0.584', '0.606', '0.599', '0.607', '0.594', '0.659', '0.631', '0.642', '0.631', '0.726', '0.711', '0.753', '0.697', '0.970', '0.994', '0.943', '0.898']
Global density: 0.62804114818573
06/02 06:30:36 PM | Train: [22/80] Step 390/390 Loss 0.013 Prec@(1,5) (99.9%, 100.0%)
06/02 06:30:36 PM | layerwise density: [38291.0, 9926.0, 9822.0, 9949.0, 9727.0, 5417.0, 5188.0, 5263.0, 5201.0, 2991.0, 2920.0, 3092.0, 2877.0, 1981.0, 2043.0, 1938.0, 1844.0]
layerwise density percentage: ['0.584', '0.606', '0.599', '0.607', '0.594', '0.661', '0.633', '0.642', '0.635', '0.730', '0.713', '0.755', '0.702', '0.967', '0.998', '0.946', '0.900']
Global density: 0.628768265247345
06/02 06:30:36 PM | Train: [22/200] Final Prec@1 99.8640%
06/02 06:30:36 PM | Valid: [22/200] Step 000/078 Loss 1.100 Prec@(1,5) (74.2%, 92.2%)
06/02 06:30:38 PM | Valid: [22/200] Step 078/078 Loss 1.334 Prec@(1,5) (69.1%, 89.5%)
06/02 06:30:38 PM | Valid: [22/200] Final Prec@1 69.1000%
06/02 06:30:38 PM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 38291.0, 0.5842742919921875]
['model.relu.alpha_mask_1_0', 16384, 9925.0, 0.60577392578125]
['model.relu.alpha_mask_2_0', 16384, 9822.0, 0.5994873046875]
['model.relu.alpha_mask_3_0', 16384, 9949.0, 0.60723876953125]
['model.relu.alpha_mask_4_0', 16384, 9728.0, 0.59375]
['model.relu.alpha_mask_5_0', 8192, 5416.0, 0.6611328125]
['model.relu.alpha_mask_6_0', 8192, 5187.0, 0.6331787109375]
['model.relu.alpha_mask_7_0', 8192, 5265.0, 0.6427001953125]
['model.relu.alpha_mask_8_0', 8192, 5202.0, 0.635009765625]
['model.relu.alpha_mask_9_0', 4096, 2992.0, 0.73046875]
['model.relu.alpha_mask_10_0', 4096, 2919.0, 0.712646484375]
['model.relu.alpha_mask_11_0', 4096, 3092.0, 0.7548828125]
['model.relu.alpha_mask_12_0', 4096, 2876.0, 0.7021484375]
['model.relu.alpha_mask_13_0', 2048, 1981.0, 0.96728515625]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1939.0, 0.94677734375]
['model.relu.alpha_mask_16_0', 2048, 1844.0, 0.900390625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118471.0, 0.6287735648777174]
########## End ###########
06/02 06:30:39 PM | Train: [23/80] Step 000/390 Loss 0.013 Prec@(1,5) (100.0%, 100.0%)
06/02 06:30:39 PM | layerwise density: [38291.0, 9925.0, 9822.0, 9949.0, 9728.0, 5416.0, 5187.0, 5265.0, 5202.0, 2992.0, 2919.0, 3092.0, 2876.0, 1981.0, 2043.0, 1939.0, 1844.0]
layerwise density percentage: ['0.584', '0.606', '0.599', '0.607', '0.594', '0.661', '0.633', '0.643', '0.635', '0.730', '0.713', '0.755', '0.702', '0.967', '0.998', '0.947', '0.900']
Global density: 0.62877357006073
06/02 06:30:50 PM | Train: [23/80] Step 100/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 06:30:50 PM | layerwise density: [38280.0, 9935.0, 9838.0, 9958.0, 9724.0, 5425.0, 5207.0, 5274.0, 5217.0, 3000.0, 2933.0, 3103.0, 2876.0, 1986.0, 2043.0, 1932.0, 1850.0]
layerwise density percentage: ['0.584', '0.606', '0.600', '0.608', '0.594', '0.662', '0.636', '0.644', '0.637', '0.732', '0.716', '0.758', '0.702', '0.970', '0.998', '0.943', '0.903']
Global density: 0.6293573975563049
06/02 06:31:01 PM | Train: [23/80] Step 200/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 06:31:01 PM | layerwise density: [38274.0, 9952.0, 9830.0, 9966.0, 9738.0, 5445.0, 5234.0, 5293.0, 5233.0, 3008.0, 2932.0, 3111.0, 2886.0, 1992.0, 2039.0, 1942.0, 1852.0]
layerwise density percentage: ['0.584', '0.607', '0.600', '0.608', '0.594', '0.665', '0.639', '0.646', '0.639', '0.734', '0.716', '0.760', '0.705', '0.973', '0.996', '0.948', '0.904']
Global density: 0.630132257938385
06/02 06:31:12 PM | Train: [23/80] Step 300/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 06:31:12 PM | layerwise density: [38276.0, 9970.0, 9840.0, 9986.0, 9752.0, 5461.0, 5238.0, 5305.0, 5239.0, 3010.0, 2949.0, 3122.0, 2898.0, 1990.0, 2042.0, 1945.0, 1856.0]
layerwise density percentage: ['0.584', '0.609', '0.601', '0.609', '0.595', '0.667', '0.639', '0.648', '0.640', '0.735', '0.720', '0.762', '0.708', '0.972', '0.997', '0.950', '0.906']
Global density: 0.6309390068054199
06/02 06:31:21 PM | Train: [23/80] Step 390/390 Loss 0.012 Prec@(1,5) (99.9%, 100.0%)
06/02 06:31:21 PM | layerwise density: [38263.0, 9978.0, 9844.0, 10003.0, 9766.0, 5484.0, 5244.0, 5314.0, 5248.0, 3026.0, 2953.0, 3132.0, 2916.0, 1995.0, 2043.0, 1945.0, 1858.0]
layerwise density percentage: ['0.584', '0.609', '0.601', '0.611', '0.596', '0.669', '0.640', '0.649', '0.641', '0.739', '0.721', '0.765', '0.712', '0.974', '0.998', '0.950', '0.907']
Global density: 0.6316449046134949
06/02 06:31:21 PM | Train: [23/200] Final Prec@1 99.8720%
06/02 06:31:21 PM | Valid: [23/200] Step 000/078 Loss 1.102 Prec@(1,5) (70.3%, 93.0%)
06/02 06:31:23 PM | Valid: [23/200] Step 078/078 Loss 1.329 Prec@(1,5) (69.2%, 89.4%)
06/02 06:31:23 PM | Valid: [23/200] Final Prec@1 69.1500%
06/02 06:31:23 PM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 38264.0, 0.5838623046875]
['model.relu.alpha_mask_1_0', 16384, 9978.0, 0.6090087890625]
['model.relu.alpha_mask_2_0', 16384, 9843.0, 0.60076904296875]
['model.relu.alpha_mask_3_0', 16384, 10002.0, 0.6104736328125]
['model.relu.alpha_mask_4_0', 16384, 9768.0, 0.59619140625]
['model.relu.alpha_mask_5_0', 8192, 5485.0, 0.6695556640625]
['model.relu.alpha_mask_6_0', 8192, 5244.0, 0.64013671875]
['model.relu.alpha_mask_7_0', 8192, 5315.0, 0.6488037109375]
['model.relu.alpha_mask_8_0', 8192, 5250.0, 0.640869140625]
['model.relu.alpha_mask_9_0', 4096, 3025.0, 0.738525390625]
['model.relu.alpha_mask_10_0', 4096, 2954.0, 0.72119140625]
['model.relu.alpha_mask_11_0', 4096, 3132.0, 0.7646484375]
['model.relu.alpha_mask_12_0', 4096, 2918.0, 0.71240234375]
['model.relu.alpha_mask_13_0', 2048, 1995.0, 0.97412109375]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1944.0, 0.94921875]
['model.relu.alpha_mask_16_0', 2048, 1859.0, 0.90771484375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119019.0, 0.6316820227581522]
########## End ###########
06/02 06:31:24 PM | Train: [24/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 06:31:24 PM | layerwise density: [38264.0, 9978.0, 9843.0, 10002.0, 9768.0, 5485.0, 5244.0, 5315.0, 5250.0, 3025.0, 2954.0, 3132.0, 2918.0, 1995.0, 2043.0, 1944.0, 1859.0]
layerwise density percentage: ['0.584', '0.609', '0.601', '0.610', '0.596', '0.670', '0.640', '0.649', '0.641', '0.739', '0.721', '0.765', '0.712', '0.974', '0.998', '0.949', '0.908']
Global density: 0.6316820383071899
06/02 06:31:35 PM | Train: [24/80] Step 100/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 06:31:35 PM | layerwise density: [38250.0, 9974.0, 9853.0, 10007.0, 9770.0, 5504.0, 5245.0, 5326.0, 5280.0, 3037.0, 2954.0, 3137.0, 2916.0, 2000.0, 2039.0, 1942.0, 1862.0]
layerwise density percentage: ['0.584', '0.609', '0.601', '0.611', '0.596', '0.672', '0.640', '0.650', '0.645', '0.741', '0.721', '0.766', '0.712', '0.977', '0.996', '0.948', '0.909']
Global density: 0.63209068775177
06/02 06:31:46 PM | Train: [24/80] Step 200/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 06:31:46 PM | layerwise density: [38248.0, 9979.0, 9881.0, 10018.0, 9770.0, 5511.0, 5263.0, 5328.0, 5287.0, 3039.0, 2960.0, 3135.0, 2920.0, 2002.0, 2041.0, 1947.0, 1864.0]
layerwise density percentage: ['0.584', '0.609', '0.603', '0.611', '0.596', '0.673', '0.642', '0.650', '0.645', '0.742', '0.723', '0.765', '0.713', '0.978', '0.997', '0.951', '0.910']
Global density: 0.6326055526733398
06/02 06:31:57 PM | Train: [24/80] Step 300/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 06:31:57 PM | layerwise density: [38249.0, 9986.0, 9885.0, 10035.0, 9776.0, 5514.0, 5284.0, 5350.0, 5297.0, 3041.0, 2969.0, 3148.0, 2922.0, 1995.0, 2042.0, 1943.0, 1868.0]
layerwise density percentage: ['0.584', '0.609', '0.603', '0.612', '0.597', '0.673', '0.645', '0.653', '0.647', '0.742', '0.725', '0.769', '0.713', '0.974', '0.997', '0.949', '0.912']
Global density: 0.633194625377655
06/02 06:32:06 PM | Train: [24/80] Step 390/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 06:32:06 PM | layerwise density: [38261.0, 9994.0, 9891.0, 10026.0, 9776.0, 5528.0, 5302.0, 5365.0, 5298.0, 3060.0, 2970.0, 3157.0, 2929.0, 1998.0, 2039.0, 1951.0, 1870.0]
layerwise density percentage: ['0.584', '0.610', '0.604', '0.612', '0.597', '0.675', '0.647', '0.655', '0.647', '0.747', '0.725', '0.771', '0.715', '0.976', '0.996', '0.953', '0.913']
Global density: 0.633783757686615
06/02 06:32:06 PM | Train: [24/200] Final Prec@1 99.9020%
06/02 06:32:06 PM | Valid: [24/200] Step 000/078 Loss 1.140 Prec@(1,5) (75.8%, 91.4%)
06/02 06:32:08 PM | Valid: [24/200] Step 078/078 Loss 1.301 Prec@(1,5) (69.6%, 89.9%)
06/02 06:32:08 PM | Valid: [24/200] Final Prec@1 69.5800%
06/02 06:32:08 PM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 38261.0, 0.5838165283203125]
['model.relu.alpha_mask_1_0', 16384, 9994.0, 0.6099853515625]
['model.relu.alpha_mask_2_0', 16384, 9891.0, 0.60369873046875]
['model.relu.alpha_mask_3_0', 16384, 10026.0, 0.6119384765625]
['model.relu.alpha_mask_4_0', 16384, 9776.0, 0.5966796875]
['model.relu.alpha_mask_5_0', 8192, 5528.0, 0.6748046875]
['model.relu.alpha_mask_6_0', 8192, 5302.0, 0.647216796875]
['model.relu.alpha_mask_7_0', 8192, 5365.0, 0.6549072265625]
['model.relu.alpha_mask_8_0', 8192, 5298.0, 0.646728515625]
['model.relu.alpha_mask_9_0', 4096, 3060.0, 0.7470703125]
['model.relu.alpha_mask_10_0', 4096, 2970.0, 0.72509765625]
['model.relu.alpha_mask_11_0', 4096, 3157.0, 0.770751953125]
['model.relu.alpha_mask_12_0', 4096, 2929.0, 0.715087890625]
['model.relu.alpha_mask_13_0', 2048, 1998.0, 0.9755859375]
['model.relu.alpha_mask_14_0', 2048, 2039.0, 0.99560546875]
['model.relu.alpha_mask_15_0', 2048, 1951.0, 0.95263671875]
['model.relu.alpha_mask_16_0', 2048, 1870.0, 0.9130859375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119415.0, 0.6337837550951086]
########## End ###########
06/02 06:32:10 PM | Train: [25/80] Step 000/390 Loss 0.019 Prec@(1,5) (99.2%, 100.0%)
06/02 06:32:10 PM | layerwise density: [38261.0, 9994.0, 9891.0, 10026.0, 9776.0, 5528.0, 5302.0, 5365.0, 5298.0, 3060.0, 2970.0, 3157.0, 2929.0, 1998.0, 2039.0, 1951.0, 1870.0]
layerwise density percentage: ['0.584', '0.610', '0.604', '0.612', '0.597', '0.675', '0.647', '0.655', '0.647', '0.747', '0.725', '0.771', '0.715', '0.976', '0.996', '0.953', '0.913']
Global density: 0.633783757686615
06/02 06:32:20 PM | Train: [25/80] Step 100/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 06:32:20 PM | layerwise density: [38260.0, 10006.0, 9893.0, 10044.0, 9790.0, 5528.0, 5294.0, 5386.0, 5306.0, 3052.0, 2994.0, 3161.0, 2937.0, 1995.0, 2042.0, 1944.0, 1874.0]
layerwise density percentage: ['0.584', '0.611', '0.604', '0.613', '0.598', '0.675', '0.646', '0.657', '0.648', '0.745', '0.731', '0.772', '0.717', '0.974', '0.997', '0.949', '0.915']
Global density: 0.63426673412323
06/02 06:32:30 PM | Train: [25/80] Step 200/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 06:32:30 PM | layerwise density: [38250.0, 10026.0, 9893.0, 10057.0, 9811.0, 5557.0, 5294.0, 5384.0, 5317.0, 3047.0, 2981.0, 3166.0, 2941.0, 1999.0, 2040.0, 1952.0, 1874.0]
layerwise density percentage: ['0.584', '0.612', '0.604', '0.614', '0.599', '0.678', '0.646', '0.657', '0.649', '0.744', '0.728', '0.773', '0.718', '0.976', '0.996', '0.953', '0.915']
Global density: 0.6347072720527649
06/02 06:32:41 PM | Train: [25/80] Step 300/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 06:32:41 PM | layerwise density: [38231.0, 10043.0, 9907.0, 10063.0, 9822.0, 5576.0, 5301.0, 5386.0, 5313.0, 3067.0, 2982.0, 3168.0, 2947.0, 1994.0, 2038.0, 1963.0, 1881.0]
layerwise density percentage: ['0.583', '0.613', '0.605', '0.614', '0.599', '0.681', '0.647', '0.657', '0.649', '0.749', '0.728', '0.773', '0.719', '0.974', '0.995', '0.958', '0.918']
Global density: 0.6352008581161499
06/02 06:32:51 PM | Train: [25/80] Step 390/390 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)
06/02 06:32:51 PM | layerwise density: [38251.0, 10048.0, 9919.0, 10070.0, 9831.0, 5597.0, 5303.0, 5401.0, 5329.0, 3075.0, 2983.0, 3178.0, 2939.0, 1997.0, 2044.0, 1950.0, 1883.0]
layerwise density percentage: ['0.584', '0.613', '0.605', '0.615', '0.600', '0.683', '0.647', '0.659', '0.651', '0.751', '0.728', '0.776', '0.718', '0.975', '0.998', '0.952', '0.919']
Global density: 0.6358165144920349
06/02 06:32:51 PM | Train: [25/200] Final Prec@1 99.8900%
06/02 06:32:51 PM | Valid: [25/200] Step 000/078 Loss 1.108 Prec@(1,5) (75.0%, 92.2%)
06/02 06:32:53 PM | Valid: [25/200] Step 078/078 Loss 1.303 Prec@(1,5) (70.0%, 89.6%)
06/02 06:32:53 PM | Valid: [25/200] Final Prec@1 70.0000%
06/02 06:32:53 PM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 38251.0, 0.5836639404296875]
['model.relu.alpha_mask_1_0', 16384, 10049.0, 0.61334228515625]
['model.relu.alpha_mask_2_0', 16384, 9919.0, 0.60540771484375]
['model.relu.alpha_mask_3_0', 16384, 10070.0, 0.6146240234375]
['model.relu.alpha_mask_4_0', 16384, 9831.0, 0.60003662109375]
['model.relu.alpha_mask_5_0', 8192, 5596.0, 0.68310546875]
['model.relu.alpha_mask_6_0', 8192, 5302.0, 0.647216796875]
['model.relu.alpha_mask_7_0', 8192, 5401.0, 0.6593017578125]
['model.relu.alpha_mask_8_0', 8192, 5329.0, 0.6505126953125]
['model.relu.alpha_mask_9_0', 4096, 3075.0, 0.750732421875]
['model.relu.alpha_mask_10_0', 4096, 2983.0, 0.728271484375]
['model.relu.alpha_mask_11_0', 4096, 3178.0, 0.77587890625]
['model.relu.alpha_mask_12_0', 4096, 2939.0, 0.717529296875]
['model.relu.alpha_mask_13_0', 2048, 1997.0, 0.97509765625]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1950.0, 0.9521484375]
['model.relu.alpha_mask_16_0', 2048, 1883.0, 0.91943359375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119797.0, 0.6358111837635869]
########## End ###########
06/02 06:32:54 PM | Train: [26/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:32:54 PM | layerwise density: [38251.0, 10049.0, 9919.0, 10070.0, 9831.0, 5596.0, 5302.0, 5401.0, 5329.0, 3075.0, 2983.0, 3178.0, 2939.0, 1997.0, 2044.0, 1950.0, 1883.0]
layerwise density percentage: ['0.584', '0.613', '0.605', '0.615', '0.600', '0.683', '0.647', '0.659', '0.651', '0.751', '0.728', '0.776', '0.718', '0.975', '0.998', '0.952', '0.919']
Global density: 0.6358112096786499
06/02 06:33:05 PM | Train: [26/80] Step 100/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:33:05 PM | layerwise density: [38239.0, 10047.0, 9918.0, 10082.0, 9829.0, 5584.0, 5307.0, 5411.0, 5330.0, 3088.0, 2993.0, 3189.0, 2943.0, 2006.0, 2043.0, 1945.0, 1884.0]
layerwise density percentage: ['0.583', '0.613', '0.605', '0.615', '0.600', '0.682', '0.648', '0.661', '0.651', '0.754', '0.731', '0.779', '0.719', '0.979', '0.998', '0.950', '0.920']
Global density: 0.6360288262367249
06/02 06:33:15 PM | Train: [26/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:33:15 PM | layerwise density: [38234.0, 10052.0, 9909.0, 10095.0, 9853.0, 5587.0, 5316.0, 5423.0, 5345.0, 3093.0, 3002.0, 3193.0, 2942.0, 2010.0, 2040.0, 1950.0, 1887.0]
layerwise density percentage: ['0.583', '0.614', '0.605', '0.616', '0.601', '0.682', '0.649', '0.662', '0.652', '0.755', '0.733', '0.780', '0.718', '0.981', '0.996', '0.952', '0.921']
Global density: 0.6365224123001099
06/02 06:33:26 PM | Train: [26/80] Step 300/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 06:33:26 PM | layerwise density: [37869.0, 9915.0, 9761.0, 9939.0, 9689.0, 5519.0, 5239.0, 5341.0, 5263.0, 3042.0, 2964.0, 3180.0, 2919.0, 1999.0, 2038.0, 1923.0, 1885.0]
layerwise density percentage: ['0.578', '0.605', '0.596', '0.607', '0.591', '0.674', '0.640', '0.652', '0.642', '0.743', '0.724', '0.776', '0.713', '0.976', '0.995', '0.939', '0.920']
Global density: 0.6288478970527649
06/02 06:33:35 PM | Train: [26/80] Step 390/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 06:33:35 PM | layerwise density: [37784.0, 9879.0, 9731.0, 9888.0, 9632.0, 5467.0, 5204.0, 5320.0, 5222.0, 3018.0, 2948.0, 3165.0, 2903.0, 2000.0, 2046.0, 1926.0, 1883.0]
layerwise density percentage: ['0.577', '0.603', '0.594', '0.604', '0.588', '0.667', '0.635', '0.649', '0.637', '0.737', '0.720', '0.773', '0.709', '0.977', '0.999', '0.940', '0.919']
Global density: 0.626358687877655
06/02 06:33:35 PM | Train: [26/200] Final Prec@1 99.8980%
06/02 06:33:36 PM | Valid: [26/200] Step 000/078 Loss 1.142 Prec@(1,5) (75.0%, 93.0%)
06/02 06:33:38 PM | Valid: [26/200] Step 078/078 Loss 1.316 Prec@(1,5) (70.0%, 89.8%)
06/02 06:33:38 PM | Valid: [26/200] Final Prec@1 69.9800%
06/02 06:33:38 PM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37784.0, 0.5765380859375]
['model.relu.alpha_mask_1_0', 16384, 9879.0, 0.60296630859375]
['model.relu.alpha_mask_2_0', 16384, 9731.0, 0.59393310546875]
['model.relu.alpha_mask_3_0', 16384, 9887.0, 0.60345458984375]
['model.relu.alpha_mask_4_0', 16384, 9631.0, 0.58782958984375]
['model.relu.alpha_mask_5_0', 8192, 5467.0, 0.6673583984375]
['model.relu.alpha_mask_6_0', 8192, 5204.0, 0.63525390625]
['model.relu.alpha_mask_7_0', 8192, 5320.0, 0.6494140625]
['model.relu.alpha_mask_8_0', 8192, 5222.0, 0.637451171875]
['model.relu.alpha_mask_9_0', 4096, 3018.0, 0.73681640625]
['model.relu.alpha_mask_10_0', 4096, 2948.0, 0.7197265625]
['model.relu.alpha_mask_11_0', 4096, 3165.0, 0.772705078125]
['model.relu.alpha_mask_12_0', 4096, 2904.0, 0.708984375]
['model.relu.alpha_mask_13_0', 2048, 2000.0, 0.9765625]
['model.relu.alpha_mask_14_0', 2048, 2046.0, 0.9990234375]
['model.relu.alpha_mask_15_0', 2048, 1925.0, 0.93994140625]
['model.relu.alpha_mask_16_0', 2048, 1883.0, 0.91943359375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118014.0, 0.6263480808423914]
########## End ###########
06/02 06:33:39 PM | Train: [27/80] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
06/02 06:33:39 PM | layerwise density: [37784.0, 9879.0, 9731.0, 9887.0, 9631.0, 5467.0, 5204.0, 5320.0, 5222.0, 3018.0, 2948.0, 3165.0, 2904.0, 2000.0, 2046.0, 1925.0, 1883.0]
layerwise density percentage: ['0.577', '0.603', '0.594', '0.603', '0.588', '0.667', '0.635', '0.649', '0.637', '0.737', '0.720', '0.773', '0.709', '0.977', '0.999', '0.940', '0.919']
Global density: 0.626348078250885
06/02 06:33:49 PM | Train: [27/80] Step 100/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:33:49 PM | layerwise density: [37766.0, 9861.0, 9728.0, 9883.0, 9624.0, 5477.0, 5205.0, 5317.0, 5210.0, 3021.0, 2952.0, 3163.0, 2894.0, 1998.0, 2043.0, 1941.0, 1886.0]
layerwise density percentage: ['0.576', '0.602', '0.594', '0.603', '0.587', '0.669', '0.635', '0.649', '0.636', '0.738', '0.721', '0.772', '0.707', '0.976', '0.998', '0.948', '0.921']
Global density: 0.62610924243927
06/02 06:33:59 PM | Train: [27/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:33:59 PM | layerwise density: [37755.0, 9864.0, 9725.0, 9893.0, 9633.0, 5497.0, 5221.0, 5331.0, 5230.0, 3044.0, 2978.0, 3172.0, 2906.0, 2003.0, 2040.0, 1952.0, 1889.0]
layerwise density percentage: ['0.576', '0.602', '0.594', '0.604', '0.588', '0.671', '0.637', '0.651', '0.638', '0.743', '0.727', '0.774', '0.709', '0.978', '0.996', '0.953', '0.922']
Global density: 0.626979649066925
06/02 06:34:10 PM | Train: [27/80] Step 300/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 06:34:10 PM | layerwise density: [37746.0, 9886.0, 9730.0, 9897.0, 9650.0, 5519.0, 5240.0, 5351.0, 5258.0, 3057.0, 2988.0, 3185.0, 2923.0, 2010.0, 2039.0, 1955.0, 1891.0]
layerwise density percentage: ['0.576', '0.603', '0.594', '0.604', '0.589', '0.674', '0.640', '0.653', '0.642', '0.746', '0.729', '0.778', '0.714', '0.981', '0.996', '0.955', '0.923']
Global density: 0.6279987096786499
06/02 06:34:19 PM | Train: [27/80] Step 390/390 Loss 0.010 Prec@(1,5) (99.9%, 100.0%)
06/02 06:34:19 PM | layerwise density: [37745.0, 9907.0, 9738.0, 9912.0, 9659.0, 5545.0, 5252.0, 5378.0, 5277.0, 3085.0, 2997.0, 3192.0, 2945.0, 2007.0, 2039.0, 1958.0, 1894.0]
layerwise density percentage: ['0.576', '0.605', '0.594', '0.605', '0.590', '0.677', '0.641', '0.656', '0.644', '0.753', '0.732', '0.779', '0.719', '0.980', '0.996', '0.956', '0.925']
Global density: 0.6290867328643799
06/02 06:34:19 PM | Train: [27/200] Final Prec@1 99.9020%
06/02 06:34:19 PM | Valid: [27/200] Step 000/078 Loss 1.160 Prec@(1,5) (75.0%, 91.4%)
06/02 06:34:22 PM | Valid: [27/200] Step 078/078 Loss 1.308 Prec@(1,5) (70.0%, 89.8%)
06/02 06:34:22 PM | Valid: [27/200] Final Prec@1 70.0300%
06/02 06:34:22 PM | Current mask training best Prec@1 = 70.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37745.0, 0.5759429931640625]
['model.relu.alpha_mask_1_0', 16384, 9906.0, 0.6046142578125]
['model.relu.alpha_mask_2_0', 16384, 9739.0, 0.59442138671875]
['model.relu.alpha_mask_3_0', 16384, 9912.0, 0.60498046875]
['model.relu.alpha_mask_4_0', 16384, 9659.0, 0.58953857421875]
['model.relu.alpha_mask_5_0', 8192, 5545.0, 0.6768798828125]
['model.relu.alpha_mask_6_0', 8192, 5251.0, 0.6409912109375]
['model.relu.alpha_mask_7_0', 8192, 5378.0, 0.656494140625]
['model.relu.alpha_mask_8_0', 8192, 5277.0, 0.6441650390625]
['model.relu.alpha_mask_9_0', 4096, 3085.0, 0.753173828125]
['model.relu.alpha_mask_10_0', 4096, 2998.0, 0.73193359375]
['model.relu.alpha_mask_11_0', 4096, 3192.0, 0.779296875]
['model.relu.alpha_mask_12_0', 4096, 2947.0, 0.719482421875]
['model.relu.alpha_mask_13_0', 2048, 2006.0, 0.9794921875]
['model.relu.alpha_mask_14_0', 2048, 2039.0, 0.99560546875]
['model.relu.alpha_mask_15_0', 2048, 1958.0, 0.9560546875]
['model.relu.alpha_mask_16_0', 2048, 1894.0, 0.9248046875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118531.0, 0.6290920091711957]
########## End ###########
06/02 06:34:23 PM | Train: [28/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:34:23 PM | layerwise density: [37745.0, 9906.0, 9739.0, 9912.0, 9659.0, 5545.0, 5251.0, 5378.0, 5277.0, 3085.0, 2998.0, 3192.0, 2947.0, 2006.0, 2039.0, 1958.0, 1894.0]
layerwise density percentage: ['0.576', '0.605', '0.594', '0.605', '0.590', '0.677', '0.641', '0.656', '0.644', '0.753', '0.732', '0.779', '0.719', '0.979', '0.996', '0.956', '0.925']
Global density: 0.6290920376777649
06/02 06:34:32 PM | Train: [28/80] Step 100/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:34:32 PM | layerwise density: [37745.0, 9919.0, 9749.0, 9924.0, 9673.0, 5559.0, 5270.0, 5394.0, 5282.0, 3099.0, 3008.0, 3194.0, 2943.0, 2007.0, 2039.0, 1970.0, 1898.0]
layerwise density percentage: ['0.576', '0.605', '0.595', '0.606', '0.590', '0.679', '0.643', '0.658', '0.645', '0.757', '0.734', '0.780', '0.719', '0.980', '0.996', '0.962', '0.927']
Global density: 0.6298456788063049
06/02 06:34:42 PM | Train: [28/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:34:42 PM | layerwise density: [37745.0, 9932.0, 9774.0, 9933.0, 9688.0, 5586.0, 5281.0, 5406.0, 5293.0, 3100.0, 3000.0, 3206.0, 2940.0, 2011.0, 2045.0, 1966.0, 1902.0]
layerwise density percentage: ['0.576', '0.606', '0.597', '0.606', '0.591', '0.682', '0.645', '0.660', '0.646', '0.757', '0.732', '0.783', '0.718', '0.982', '0.999', '0.960', '0.929']
Global density: 0.6305621862411499
06/02 06:34:52 PM | Train: [28/80] Step 300/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:34:52 PM | layerwise density: [37756.0, 9944.0, 9771.0, 9955.0, 9698.0, 5583.0, 5277.0, 5419.0, 5297.0, 3099.0, 2995.0, 3218.0, 2947.0, 2013.0, 2042.0, 1963.0, 1905.0]
layerwise density percentage: ['0.576', '0.607', '0.596', '0.608', '0.592', '0.682', '0.644', '0.661', '0.647', '0.757', '0.731', '0.786', '0.719', '0.983', '0.997', '0.958', '0.930']
Global density: 0.630954921245575
06/02 06:35:01 PM | Train: [28/80] Step 390/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:35:01 PM | layerwise density: [37756.0, 9942.0, 9781.0, 9960.0, 9694.0, 5569.0, 5285.0, 5433.0, 5301.0, 3090.0, 2996.0, 3221.0, 2957.0, 2013.0, 2039.0, 1964.0, 1905.0]
layerwise density percentage: ['0.576', '0.607', '0.597', '0.608', '0.592', '0.680', '0.645', '0.663', '0.647', '0.754', '0.731', '0.786', '0.722', '0.983', '0.996', '0.959', '0.930']
Global density: 0.63108229637146
06/02 06:35:01 PM | Train: [28/200] Final Prec@1 99.9000%
06/02 06:35:01 PM | Valid: [28/200] Step 000/078 Loss 1.239 Prec@(1,5) (72.7%, 90.6%)
06/02 06:35:03 PM | Valid: [28/200] Step 078/078 Loss 1.299 Prec@(1,5) (70.2%, 89.6%)
06/02 06:35:04 PM | Valid: [28/200] Final Prec@1 70.1500%
06/02 06:35:04 PM | Current mask training best Prec@1 = 70.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37756.0, 0.57611083984375]
['model.relu.alpha_mask_1_0', 16384, 9943.0, 0.60687255859375]
['model.relu.alpha_mask_2_0', 16384, 9784.0, 0.59716796875]
['model.relu.alpha_mask_3_0', 16384, 9960.0, 0.60791015625]
['model.relu.alpha_mask_4_0', 16384, 9696.0, 0.591796875]
['model.relu.alpha_mask_5_0', 8192, 5569.0, 0.6798095703125]
['model.relu.alpha_mask_6_0', 8192, 5285.0, 0.6451416015625]
['model.relu.alpha_mask_7_0', 8192, 5432.0, 0.6630859375]
['model.relu.alpha_mask_8_0', 8192, 5302.0, 0.647216796875]
['model.relu.alpha_mask_9_0', 4096, 3091.0, 0.754638671875]
['model.relu.alpha_mask_10_0', 4096, 2995.0, 0.731201171875]
['model.relu.alpha_mask_11_0', 4096, 3221.0, 0.786376953125]
['model.relu.alpha_mask_12_0', 4096, 2958.0, 0.72216796875]
['model.relu.alpha_mask_13_0', 2048, 2012.0, 0.982421875]
['model.relu.alpha_mask_14_0', 2048, 2039.0, 0.99560546875]
['model.relu.alpha_mask_15_0', 2048, 1964.0, 0.958984375]
['model.relu.alpha_mask_16_0', 2048, 1905.0, 0.93017578125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118912.0, 0.6311141304347826]
########## End ###########
06/02 06:35:05 PM | Train: [29/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:35:05 PM | layerwise density: [37756.0, 9943.0, 9784.0, 9960.0, 9696.0, 5569.0, 5285.0, 5432.0, 5302.0, 3091.0, 2995.0, 3221.0, 2958.0, 2012.0, 2039.0, 1964.0, 1905.0]
layerwise density percentage: ['0.576', '0.607', '0.597', '0.608', '0.592', '0.680', '0.645', '0.663', '0.647', '0.755', '0.731', '0.786', '0.722', '0.982', '0.996', '0.959', '0.930']
Global density: 0.63111412525177
06/02 06:35:15 PM | Train: [29/80] Step 100/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:35:15 PM | layerwise density: [37770.0, 9944.0, 9803.0, 9968.0, 9702.0, 5584.0, 5295.0, 5439.0, 5319.0, 3099.0, 2991.0, 3233.0, 2977.0, 2014.0, 2045.0, 1952.0, 1908.0]
layerwise density percentage: ['0.576', '0.607', '0.598', '0.608', '0.592', '0.682', '0.646', '0.664', '0.649', '0.757', '0.730', '0.789', '0.727', '0.983', '0.999', '0.953', '0.932']
Global density: 0.631809413433075
06/02 06:35:25 PM | Train: [29/80] Step 200/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:35:25 PM | layerwise density: [37759.0, 9952.0, 9799.0, 9978.0, 9716.0, 5613.0, 5302.0, 5447.0, 5340.0, 3109.0, 2995.0, 3242.0, 2977.0, 2015.0, 2047.0, 1953.0, 1909.0]
layerwise density percentage: ['0.576', '0.607', '0.598', '0.609', '0.593', '0.685', '0.647', '0.665', '0.652', '0.759', '0.731', '0.792', '0.727', '0.984', '1.000', '0.954', '0.932']
Global density: 0.6323932409286499
06/02 06:35:36 PM | Train: [29/80] Step 300/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:35:36 PM | layerwise density: [37761.0, 9967.0, 9792.0, 9987.0, 9718.0, 5634.0, 5317.0, 5462.0, 5336.0, 3122.0, 3005.0, 3245.0, 2979.0, 2019.0, 2042.0, 1959.0, 1911.0]
layerwise density percentage: ['0.576', '0.608', '0.598', '0.610', '0.593', '0.688', '0.649', '0.667', '0.651', '0.762', '0.734', '0.792', '0.727', '0.986', '0.997', '0.957', '0.933']
Global density: 0.632939875125885
06/02 06:35:45 PM | Train: [29/80] Step 390/390 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)
06/02 06:35:45 PM | layerwise density: [37759.0, 9967.0, 9800.0, 10000.0, 9722.0, 5651.0, 5324.0, 5468.0, 5344.0, 3122.0, 3013.0, 3247.0, 2984.0, 2014.0, 2041.0, 1962.0, 1912.0]
layerwise density percentage: ['0.576', '0.608', '0.598', '0.610', '0.593', '0.690', '0.650', '0.667', '0.652', '0.762', '0.736', '0.793', '0.729', '0.983', '0.997', '0.958', '0.934']
Global density: 0.6333326101303101
06/02 06:35:46 PM | Train: [29/200] Final Prec@1 99.9140%
06/02 06:35:46 PM | Valid: [29/200] Step 000/078 Loss 1.239 Prec@(1,5) (70.3%, 89.8%)
06/02 06:35:48 PM | Valid: [29/200] Step 078/078 Loss 1.306 Prec@(1,5) (70.0%, 89.6%)
06/02 06:35:48 PM | Valid: [29/200] Final Prec@1 69.9800%
06/02 06:35:48 PM | Current mask training best Prec@1 = 70.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37759.0, 0.5761566162109375]
['model.relu.alpha_mask_1_0', 16384, 9968.0, 0.6083984375]
['model.relu.alpha_mask_2_0', 16384, 9800.0, 0.59814453125]
['model.relu.alpha_mask_3_0', 16384, 10002.0, 0.6104736328125]
['model.relu.alpha_mask_4_0', 16384, 9721.0, 0.59332275390625]
['model.relu.alpha_mask_5_0', 8192, 5649.0, 0.6895751953125]
['model.relu.alpha_mask_6_0', 8192, 5325.0, 0.6500244140625]
['model.relu.alpha_mask_7_0', 8192, 5468.0, 0.66748046875]
['model.relu.alpha_mask_8_0', 8192, 5344.0, 0.65234375]
['model.relu.alpha_mask_9_0', 4096, 3124.0, 0.7626953125]
['model.relu.alpha_mask_10_0', 4096, 3012.0, 0.7353515625]
['model.relu.alpha_mask_11_0', 4096, 3247.0, 0.792724609375]
['model.relu.alpha_mask_12_0', 4096, 2986.0, 0.72900390625]
['model.relu.alpha_mask_13_0', 2048, 2014.0, 0.9833984375]
['model.relu.alpha_mask_14_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_15_0', 2048, 1963.0, 0.95849609375]
['model.relu.alpha_mask_16_0', 2048, 1912.0, 0.93359375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119334.0, 0.6333538552989131]
########## End ###########
06/02 06:35:49 PM | Train: [30/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:35:49 PM | layerwise density: [37759.0, 9968.0, 9800.0, 10002.0, 9721.0, 5649.0, 5325.0, 5468.0, 5344.0, 3124.0, 3012.0, 3247.0, 2986.0, 2014.0, 2040.0, 1963.0, 1912.0]
layerwise density percentage: ['0.576', '0.608', '0.598', '0.610', '0.593', '0.690', '0.650', '0.667', '0.652', '0.763', '0.735', '0.793', '0.729', '0.983', '0.996', '0.958', '0.934']
Global density: 0.6333538889884949
06/02 06:35:59 PM | Train: [30/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:35:59 PM | layerwise density: [37749.0, 9971.0, 9806.0, 10001.0, 9733.0, 5678.0, 5319.0, 5478.0, 5350.0, 3123.0, 3015.0, 3252.0, 2984.0, 2011.0, 2046.0, 1960.0, 1916.0]
layerwise density percentage: ['0.576', '0.609', '0.599', '0.610', '0.594', '0.693', '0.649', '0.669', '0.653', '0.762', '0.736', '0.794', '0.729', '0.982', '0.999', '0.957', '0.936']
Global density: 0.633661687374115
06/02 06:36:10 PM | Train: [30/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:36:10 PM | layerwise density: [37756.0, 9981.0, 9813.0, 10013.0, 9750.0, 5685.0, 5330.0, 5476.0, 5359.0, 3134.0, 3023.0, 3253.0, 2993.0, 2007.0, 2045.0, 1964.0, 1919.0]
layerwise density percentage: ['0.576', '0.609', '0.599', '0.611', '0.595', '0.694', '0.651', '0.668', '0.654', '0.765', '0.738', '0.794', '0.731', '0.980', '0.999', '0.959', '0.937']
Global density: 0.6342402100563049
06/02 06:36:20 PM | Train: [30/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:36:20 PM | layerwise density: [37765.0, 10004.0, 9826.0, 10036.0, 9744.0, 5678.0, 5343.0, 5481.0, 5356.0, 3155.0, 3032.0, 3256.0, 2998.0, 2021.0, 2046.0, 1971.0, 1920.0]
layerwise density percentage: ['0.576', '0.611', '0.600', '0.613', '0.595', '0.693', '0.652', '0.669', '0.654', '0.770', '0.740', '0.795', '0.732', '0.987', '0.999', '0.962', '0.938']
Global density: 0.6349354982376099
06/02 06:36:30 PM | Train: [30/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:36:30 PM | layerwise density: [37768.0, 10017.0, 9818.0, 10047.0, 9746.0, 5679.0, 5355.0, 5486.0, 5367.0, 3161.0, 3034.0, 3261.0, 2985.0, 2018.0, 2043.0, 1967.0, 1922.0]
layerwise density percentage: ['0.576', '0.611', '0.599', '0.613', '0.595', '0.693', '0.654', '0.670', '0.655', '0.772', '0.741', '0.796', '0.729', '0.985', '0.998', '0.960', '0.938']
Global density: 0.635158360004425
06/02 06:36:30 PM | Train: [30/200] Final Prec@1 99.9300%
06/02 06:36:30 PM | Valid: [30/200] Step 000/078 Loss 1.182 Prec@(1,5) (71.1%, 90.6%)
06/02 06:36:33 PM | Valid: [30/200] Step 078/078 Loss 1.294 Prec@(1,5) (69.9%, 90.1%)
06/02 06:36:33 PM | Valid: [30/200] Final Prec@1 69.8800%
06/02 06:36:33 PM | Current mask training best Prec@1 = 70.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37768.0, 0.5762939453125]
['model.relu.alpha_mask_1_0', 16384, 10017.0, 0.61138916015625]
['model.relu.alpha_mask_2_0', 16384, 9818.0, 0.5992431640625]
['model.relu.alpha_mask_3_0', 16384, 10047.0, 0.61322021484375]
['model.relu.alpha_mask_4_0', 16384, 9746.0, 0.5948486328125]
['model.relu.alpha_mask_5_0', 8192, 5679.0, 0.6932373046875]
['model.relu.alpha_mask_6_0', 8192, 5355.0, 0.6536865234375]
['model.relu.alpha_mask_7_0', 8192, 5486.0, 0.669677734375]
['model.relu.alpha_mask_8_0', 8192, 5367.0, 0.6551513671875]
['model.relu.alpha_mask_9_0', 4096, 3161.0, 0.771728515625]
['model.relu.alpha_mask_10_0', 4096, 3034.0, 0.74072265625]
['model.relu.alpha_mask_11_0', 4096, 3261.0, 0.796142578125]
['model.relu.alpha_mask_12_0', 4096, 2985.0, 0.728759765625]
['model.relu.alpha_mask_13_0', 2048, 2018.0, 0.9853515625]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1967.0, 0.96044921875]
['model.relu.alpha_mask_16_0', 2048, 1922.0, 0.9384765625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119674.0, 0.6351583729619565]
########## End ###########
06/02 06:36:34 PM | Train: [31/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:36:34 PM | layerwise density: [37768.0, 10017.0, 9818.0, 10047.0, 9746.0, 5679.0, 5355.0, 5486.0, 5367.0, 3161.0, 3034.0, 3261.0, 2985.0, 2018.0, 2043.0, 1967.0, 1922.0]
layerwise density percentage: ['0.576', '0.611', '0.599', '0.613', '0.595', '0.693', '0.654', '0.670', '0.655', '0.772', '0.741', '0.796', '0.729', '0.985', '0.998', '0.960', '0.938']
Global density: 0.635158360004425
06/02 06:36:44 PM | Train: [31/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:36:44 PM | layerwise density: [37771.0, 10034.0, 9817.0, 10049.0, 9761.0, 5700.0, 5372.0, 5489.0, 5364.0, 3151.0, 3037.0, 3270.0, 2983.0, 2016.0, 2041.0, 1960.0, 1923.0]
layerwise density percentage: ['0.576', '0.612', '0.599', '0.613', '0.596', '0.696', '0.656', '0.670', '0.655', '0.769', '0.741', '0.798', '0.728', '0.984', '0.997', '0.957', '0.939']
Global density: 0.635498046875
06/02 06:36:55 PM | Train: [31/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:36:55 PM | layerwise density: [37766.0, 10052.0, 9814.0, 10050.0, 9771.0, 5703.0, 5370.0, 5513.0, 5359.0, 3152.0, 3041.0, 3276.0, 2991.0, 2018.0, 2045.0, 1966.0, 1925.0]
layerwise density percentage: ['0.576', '0.614', '0.599', '0.613', '0.596', '0.696', '0.656', '0.673', '0.654', '0.770', '0.742', '0.800', '0.730', '0.985', '0.999', '0.960', '0.940']
Global density: 0.635890781879425
06/02 06:37:05 PM | Train: [31/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:37:05 PM | layerwise density: [37754.0, 10063.0, 9815.0, 10052.0, 9777.0, 5700.0, 5381.0, 5517.0, 5362.0, 3162.0, 3036.0, 3281.0, 2980.0, 2021.0, 2047.0, 1971.0, 1924.0]
layerwise density percentage: ['0.576', '0.614', '0.599', '0.614', '0.597', '0.696', '0.657', '0.673', '0.655', '0.772', '0.741', '0.801', '0.728', '0.987', '1.000', '0.962', '0.939']
Global density: 0.6360553503036499
06/02 06:37:14 PM | Train: [31/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:37:14 PM | layerwise density: [37753.0, 10073.0, 9824.0, 10063.0, 9782.0, 5699.0, 5377.0, 5509.0, 5377.0, 3162.0, 3060.0, 3288.0, 2998.0, 2023.0, 2045.0, 1967.0, 1924.0]
layerwise density percentage: ['0.576', '0.615', '0.600', '0.614', '0.597', '0.696', '0.656', '0.672', '0.656', '0.772', '0.747', '0.803', '0.732', '0.988', '0.999', '0.960', '0.939']
Global density: 0.63648521900177
06/02 06:37:15 PM | Train: [31/200] Final Prec@1 99.9220%
06/02 06:37:15 PM | Valid: [31/200] Step 000/078 Loss 1.163 Prec@(1,5) (75.0%, 91.4%)
06/02 06:37:17 PM | Valid: [31/200] Step 078/078 Loss 1.303 Prec@(1,5) (69.9%, 89.9%)
06/02 06:37:17 PM | Valid: [31/200] Final Prec@1 69.9300%
06/02 06:37:17 PM | Current mask training best Prec@1 = 70.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37754.0, 0.576080322265625]
['model.relu.alpha_mask_1_0', 16384, 10073.0, 0.61480712890625]
['model.relu.alpha_mask_2_0', 16384, 9824.0, 0.599609375]
['model.relu.alpha_mask_3_0', 16384, 10063.0, 0.61419677734375]
['model.relu.alpha_mask_4_0', 16384, 9782.0, 0.5970458984375]
['model.relu.alpha_mask_5_0', 8192, 5701.0, 0.6959228515625]
['model.relu.alpha_mask_6_0', 8192, 5377.0, 0.6563720703125]
['model.relu.alpha_mask_7_0', 8192, 5509.0, 0.6724853515625]
['model.relu.alpha_mask_8_0', 8192, 5376.0, 0.65625]
['model.relu.alpha_mask_9_0', 4096, 3162.0, 0.77197265625]
['model.relu.alpha_mask_10_0', 4096, 3060.0, 0.7470703125]
['model.relu.alpha_mask_11_0', 4096, 3288.0, 0.802734375]
['model.relu.alpha_mask_12_0', 4096, 2998.0, 0.73193359375]
['model.relu.alpha_mask_13_0', 2048, 2023.0, 0.98779296875]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1967.0, 0.96044921875]
['model.relu.alpha_mask_16_0', 2048, 1924.0, 0.939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119926.0, 0.6364958389945652]
########## End ###########
06/02 06:37:18 PM | Train: [32/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:37:18 PM | layerwise density: [37754.0, 10073.0, 9824.0, 10063.0, 9782.0, 5701.0, 5377.0, 5509.0, 5376.0, 3162.0, 3060.0, 3288.0, 2998.0, 2023.0, 2045.0, 1967.0, 1924.0]
layerwise density percentage: ['0.576', '0.615', '0.600', '0.614', '0.597', '0.696', '0.656', '0.672', '0.656', '0.772', '0.747', '0.803', '0.732', '0.988', '0.999', '0.960', '0.939']
Global density: 0.63649582862854
06/02 06:37:29 PM | Train: [32/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:37:29 PM | layerwise density: [37756.0, 10066.0, 9834.0, 10069.0, 9792.0, 5687.0, 5391.0, 5511.0, 5395.0, 3175.0, 3055.0, 3292.0, 3010.0, 2026.0, 2044.0, 1965.0, 1928.0]
layerwise density percentage: ['0.576', '0.614', '0.600', '0.615', '0.598', '0.694', '0.658', '0.673', '0.659', '0.775', '0.746', '0.804', '0.735', '0.989', '0.998', '0.959', '0.941']
Global density: 0.636867344379425
06/02 06:37:39 PM | Train: [32/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:37:39 PM | layerwise density: [37326.0, 9874.0, 9676.0, 9889.0, 9579.0, 5553.0, 5247.0, 5399.0, 5244.0, 3093.0, 2957.0, 3260.0, 2920.0, 2009.0, 2040.0, 1926.0, 1924.0]
layerwise density percentage: ['0.570', '0.603', '0.591', '0.604', '0.585', '0.678', '0.641', '0.659', '0.640', '0.755', '0.722', '0.796', '0.713', '0.981', '0.996', '0.940', '0.939']
Global density: 0.625827968120575
06/02 06:37:49 PM | Train: [32/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:37:49 PM | layerwise density: [37288.0, 9846.0, 9659.0, 9869.0, 9558.0, 5547.0, 5226.0, 5394.0, 5243.0, 3090.0, 2952.0, 3253.0, 2914.0, 2017.0, 2042.0, 1948.0, 1923.0]
layerwise density percentage: ['0.569', '0.601', '0.590', '0.602', '0.583', '0.677', '0.638', '0.658', '0.640', '0.754', '0.721', '0.794', '0.711', '0.985', '0.997', '0.951', '0.939']
Global density: 0.6250478029251099
06/02 06:37:59 PM | Train: [32/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:37:59 PM | layerwise density: [37270.0, 9837.0, 9648.0, 9866.0, 9554.0, 5575.0, 5233.0, 5401.0, 5239.0, 3093.0, 2985.0, 3255.0, 2926.0, 2009.0, 2045.0, 1962.0, 1924.0]
layerwise density percentage: ['0.569', '0.600', '0.589', '0.602', '0.583', '0.681', '0.639', '0.659', '0.640', '0.755', '0.729', '0.795', '0.714', '0.981', '0.999', '0.958', '0.939']
Global density: 0.6253290772438049
06/02 06:37:59 PM | Train: [32/200] Final Prec@1 99.9140%
06/02 06:37:59 PM | Valid: [32/200] Step 000/078 Loss 1.070 Prec@(1,5) (73.4%, 92.2%)
06/02 06:38:01 PM | Valid: [32/200] Step 078/078 Loss 1.297 Prec@(1,5) (69.8%, 89.8%)
06/02 06:38:01 PM | Valid: [32/200] Final Prec@1 69.8400%
06/02 06:38:01 PM | Current mask training best Prec@1 = 70.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37270.0, 0.568695068359375]
['model.relu.alpha_mask_1_0', 16384, 9836.0, 0.600341796875]
['model.relu.alpha_mask_2_0', 16384, 9648.0, 0.5888671875]
['model.relu.alpha_mask_3_0', 16384, 9866.0, 0.6021728515625]
['model.relu.alpha_mask_4_0', 16384, 9553.0, 0.58306884765625]
['model.relu.alpha_mask_5_0', 8192, 5577.0, 0.6807861328125]
['model.relu.alpha_mask_6_0', 8192, 5234.0, 0.638916015625]
['model.relu.alpha_mask_7_0', 8192, 5401.0, 0.6593017578125]
['model.relu.alpha_mask_8_0', 8192, 5238.0, 0.639404296875]
['model.relu.alpha_mask_9_0', 4096, 3093.0, 0.755126953125]
['model.relu.alpha_mask_10_0', 4096, 2985.0, 0.728759765625]
['model.relu.alpha_mask_11_0', 4096, 3255.0, 0.794677734375]
['model.relu.alpha_mask_12_0', 4096, 2926.0, 0.71435546875]
['model.relu.alpha_mask_13_0', 2048, 2009.0, 0.98095703125]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1962.0, 0.9580078125]
['model.relu.alpha_mask_16_0', 2048, 1924.0, 0.939453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 117822.0, 0.6253290591032609]
########## End ###########
06/02 06:38:02 PM | Train: [33/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:38:02 PM | layerwise density: [37270.0, 9836.0, 9648.0, 9866.0, 9553.0, 5577.0, 5234.0, 5401.0, 5238.0, 3093.0, 2985.0, 3255.0, 2926.0, 2009.0, 2045.0, 1962.0, 1924.0]
layerwise density percentage: ['0.569', '0.600', '0.589', '0.602', '0.583', '0.681', '0.639', '0.659', '0.639', '0.755', '0.729', '0.795', '0.714', '0.981', '0.999', '0.958', '0.939']
Global density: 0.6253290772438049
06/02 06:38:13 PM | Train: [33/80] Step 100/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 06:38:13 PM | layerwise density: [37265.0, 9847.0, 9644.0, 9870.0, 9553.0, 5578.0, 5272.0, 5410.0, 5248.0, 3129.0, 2992.0, 3263.0, 2939.0, 2015.0, 2046.0, 1960.0, 1926.0]
layerwise density percentage: ['0.569', '0.601', '0.589', '0.602', '0.583', '0.681', '0.644', '0.660', '0.641', '0.764', '0.730', '0.797', '0.718', '0.984', '0.999', '0.957', '0.940']
Global density: 0.6260455846786499
06/02 06:38:23 PM | Train: [33/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:38:23 PM | layerwise density: [37267.0, 9857.0, 9638.0, 9862.0, 9559.0, 5589.0, 5290.0, 5411.0, 5265.0, 3158.0, 3010.0, 3267.0, 2958.0, 2019.0, 2046.0, 1965.0, 1927.0]
layerwise density percentage: ['0.569', '0.602', '0.588', '0.602', '0.583', '0.682', '0.646', '0.661', '0.643', '0.771', '0.735', '0.798', '0.722', '0.986', '0.999', '0.959', '0.941']
Global density: 0.6267408132553101
06/02 06:38:34 PM | Train: [33/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:38:34 PM | layerwise density: [37256.0, 9868.0, 9652.0, 9874.0, 9564.0, 5597.0, 5308.0, 5420.0, 5283.0, 3159.0, 3016.0, 3279.0, 2949.0, 2014.0, 2045.0, 1970.0, 1927.0]
layerwise density percentage: ['0.568', '0.602', '0.589', '0.603', '0.584', '0.683', '0.648', '0.662', '0.645', '0.771', '0.736', '0.801', '0.720', '0.983', '0.999', '0.962', '0.941']
Global density: 0.6272344589233398
06/02 06:38:43 PM | Train: [33/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:38:43 PM | layerwise density: [37249.0, 9874.0, 9665.0, 9886.0, 9583.0, 5636.0, 5326.0, 5435.0, 5299.0, 3160.0, 3022.0, 3292.0, 2970.0, 2021.0, 2045.0, 1966.0, 1928.0]
layerwise density percentage: ['0.568', '0.603', '0.590', '0.603', '0.585', '0.688', '0.650', '0.663', '0.647', '0.771', '0.738', '0.804', '0.725', '0.987', '0.999', '0.960', '0.941']
Global density: 0.628168523311615
06/02 06:38:43 PM | Train: [33/200] Final Prec@1 99.9260%
06/02 06:38:43 PM | Valid: [33/200] Step 000/078 Loss 1.113 Prec@(1,5) (72.7%, 90.6%)
06/02 06:38:46 PM | Valid: [33/200] Step 078/078 Loss 1.297 Prec@(1,5) (70.0%, 90.0%)
06/02 06:38:46 PM | Valid: [33/200] Final Prec@1 70.0400%
06/02 06:38:46 PM | Current mask training best Prec@1 = 70.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37251.0, 0.5684051513671875]
['model.relu.alpha_mask_1_0', 16384, 9874.0, 0.6026611328125]
['model.relu.alpha_mask_2_0', 16384, 9665.0, 0.58990478515625]
['model.relu.alpha_mask_3_0', 16384, 9887.0, 0.60345458984375]
['model.relu.alpha_mask_4_0', 16384, 9583.0, 0.58489990234375]
['model.relu.alpha_mask_5_0', 8192, 5636.0, 0.68798828125]
['model.relu.alpha_mask_6_0', 8192, 5325.0, 0.6500244140625]
['model.relu.alpha_mask_7_0', 8192, 5435.0, 0.6634521484375]
['model.relu.alpha_mask_8_0', 8192, 5302.0, 0.647216796875]
['model.relu.alpha_mask_9_0', 4096, 3161.0, 0.771728515625]
['model.relu.alpha_mask_10_0', 4096, 3021.0, 0.737548828125]
['model.relu.alpha_mask_11_0', 4096, 3292.0, 0.8037109375]
['model.relu.alpha_mask_12_0', 4096, 2970.0, 0.72509765625]
['model.relu.alpha_mask_13_0', 2048, 2021.0, 0.98681640625]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1965.0, 0.95947265625]
['model.relu.alpha_mask_16_0', 2048, 1928.0, 0.94140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118361.0, 0.628189750339674]
########## End ###########
06/02 06:38:47 PM | Train: [34/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:38:47 PM | layerwise density: [37251.0, 9874.0, 9665.0, 9887.0, 9583.0, 5636.0, 5325.0, 5435.0, 5302.0, 3161.0, 3021.0, 3292.0, 2970.0, 2021.0, 2045.0, 1965.0, 1928.0]
layerwise density percentage: ['0.568', '0.603', '0.590', '0.603', '0.585', '0.688', '0.650', '0.663', '0.647', '0.772', '0.738', '0.804', '0.725', '0.987', '0.999', '0.959', '0.941']
Global density: 0.628189742565155
06/02 06:38:57 PM | Train: [34/80] Step 100/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 06:38:57 PM | layerwise density: [37254.0, 9877.0, 9676.0, 9901.0, 9610.0, 5653.0, 5314.0, 5443.0, 5303.0, 3145.0, 3023.0, 3302.0, 2977.0, 2022.0, 2045.0, 1963.0, 1930.0]
layerwise density percentage: ['0.568', '0.603', '0.591', '0.604', '0.587', '0.690', '0.649', '0.664', '0.647', '0.768', '0.738', '0.806', '0.727', '0.987', '0.999', '0.958', '0.942']
Global density: 0.6285984516143799
06/02 06:39:08 PM | Train: [34/80] Step 200/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 06:39:08 PM | layerwise density: [37266.0, 9885.0, 9675.0, 9905.0, 9609.0, 5674.0, 5321.0, 5465.0, 5312.0, 3157.0, 3024.0, 3305.0, 2981.0, 2023.0, 2046.0, 1972.0, 1930.0]
layerwise density percentage: ['0.569', '0.603', '0.591', '0.605', '0.586', '0.693', '0.650', '0.667', '0.648', '0.771', '0.738', '0.807', '0.728', '0.988', '0.999', '0.963', '0.942']
Global density: 0.6291928887367249
06/02 06:39:18 PM | Train: [34/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:39:18 PM | layerwise density: [37269.0, 9905.0, 9675.0, 9909.0, 9611.0, 5689.0, 5335.0, 5488.0, 5323.0, 3171.0, 3029.0, 3310.0, 3000.0, 2023.0, 2043.0, 1968.0, 1931.0]
layerwise density percentage: ['0.569', '0.605', '0.591', '0.605', '0.587', '0.694', '0.651', '0.670', '0.650', '0.774', '0.740', '0.808', '0.732', '0.988', '0.998', '0.961', '0.943']
Global density: 0.629877507686615
06/02 06:39:28 PM | Train: [34/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:39:28 PM | layerwise density: [37254.0, 9919.0, 9684.0, 9926.0, 9624.0, 5695.0, 5341.0, 5490.0, 5339.0, 3187.0, 3036.0, 3311.0, 3016.0, 2022.0, 2041.0, 1969.0, 1931.0]
layerwise density percentage: ['0.568', '0.605', '0.591', '0.606', '0.587', '0.695', '0.652', '0.670', '0.652', '0.778', '0.741', '0.808', '0.736', '0.987', '0.997', '0.961', '0.943']
Global density: 0.6304401159286499
06/02 06:39:28 PM | Train: [34/200] Final Prec@1 99.9300%
06/02 06:39:28 PM | Valid: [34/200] Step 000/078 Loss 1.098 Prec@(1,5) (72.7%, 92.2%)
06/02 06:39:30 PM | Valid: [34/200] Step 078/078 Loss 1.297 Prec@(1,5) (69.9%, 90.0%)
06/02 06:39:30 PM | Valid: [34/200] Final Prec@1 69.8900%
06/02 06:39:30 PM | Current mask training best Prec@1 = 70.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37254.0, 0.568450927734375]
['model.relu.alpha_mask_1_0', 16384, 9917.0, 0.60528564453125]
['model.relu.alpha_mask_2_0', 16384, 9684.0, 0.591064453125]
['model.relu.alpha_mask_3_0', 16384, 9926.0, 0.6058349609375]
['model.relu.alpha_mask_4_0', 16384, 9624.0, 0.58740234375]
['model.relu.alpha_mask_5_0', 8192, 5695.0, 0.6951904296875]
['model.relu.alpha_mask_6_0', 8192, 5340.0, 0.65185546875]
['model.relu.alpha_mask_7_0', 8192, 5491.0, 0.6702880859375]
['model.relu.alpha_mask_8_0', 8192, 5339.0, 0.6517333984375]
['model.relu.alpha_mask_9_0', 4096, 3187.0, 0.778076171875]
['model.relu.alpha_mask_10_0', 4096, 3036.0, 0.7412109375]
['model.relu.alpha_mask_11_0', 4096, 3310.0, 0.80810546875]
['model.relu.alpha_mask_12_0', 4096, 3016.0, 0.736328125]
['model.relu.alpha_mask_13_0', 2048, 2021.0, 0.98681640625]
['model.relu.alpha_mask_14_0', 2048, 2042.0, 0.9970703125]
['model.relu.alpha_mask_15_0', 2048, 1969.0, 0.96142578125]
['model.relu.alpha_mask_16_0', 2048, 1931.0, 0.94287109375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118782.0, 0.6304241677989131]
########## End ###########
06/02 06:39:31 PM | Train: [35/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 06:39:31 PM | layerwise density: [37254.0, 9917.0, 9684.0, 9926.0, 9624.0, 5695.0, 5340.0, 5491.0, 5339.0, 3187.0, 3036.0, 3310.0, 3016.0, 2021.0, 2042.0, 1969.0, 1931.0]
layerwise density percentage: ['0.568', '0.605', '0.591', '0.606', '0.587', '0.695', '0.652', '0.670', '0.652', '0.778', '0.741', '0.808', '0.736', '0.987', '0.997', '0.961', '0.943']
Global density: 0.6304242014884949
06/02 06:39:42 PM | Train: [35/80] Step 100/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:39:42 PM | layerwise density: [37247.0, 9930.0, 9691.0, 9937.0, 9628.0, 5709.0, 5351.0, 5503.0, 5349.0, 3174.0, 3041.0, 3310.0, 3003.0, 2029.0, 2046.0, 1972.0, 1932.0]
layerwise density percentage: ['0.568', '0.606', '0.591', '0.607', '0.588', '0.697', '0.653', '0.672', '0.653', '0.775', '0.742', '0.808', '0.733', '0.991', '0.999', '0.963', '0.943']
Global density: 0.6307957172393799
06/02 06:39:52 PM | Train: [35/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:39:52 PM | layerwise density: [37255.0, 9935.0, 9682.0, 9940.0, 9623.0, 5709.0, 5356.0, 5501.0, 5364.0, 3174.0, 3054.0, 3313.0, 3001.0, 2029.0, 2045.0, 1968.0, 1935.0]
layerwise density percentage: ['0.568', '0.606', '0.591', '0.607', '0.587', '0.697', '0.654', '0.672', '0.655', '0.775', '0.746', '0.809', '0.733', '0.991', '0.999', '0.961', '0.945']
Global density: 0.630965530872345
06/02 06:40:03 PM | Train: [35/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:40:03 PM | layerwise density: [37258.0, 9953.0, 9690.0, 9948.0, 9630.0, 5721.0, 5355.0, 5514.0, 5380.0, 3176.0, 3071.0, 3317.0, 3018.0, 2027.0, 2040.0, 1977.0, 1935.0]
layerwise density percentage: ['0.569', '0.607', '0.591', '0.607', '0.588', '0.698', '0.654', '0.673', '0.657', '0.775', '0.750', '0.810', '0.737', '0.990', '0.996', '0.965', '0.945']
Global density: 0.6316342949867249
06/02 06:40:12 PM | Train: [35/80] Step 390/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:40:12 PM | layerwise density: [37272.0, 9966.0, 9710.0, 9945.0, 9636.0, 5724.0, 5365.0, 5511.0, 5385.0, 3178.0, 3077.0, 3324.0, 3034.0, 2021.0, 2043.0, 1968.0, 1937.0]
layerwise density percentage: ['0.569', '0.608', '0.593', '0.607', '0.588', '0.699', '0.655', '0.673', '0.657', '0.776', '0.751', '0.812', '0.741', '0.987', '0.998', '0.961', '0.946']
Global density: 0.63209068775177
06/02 06:40:12 PM | Train: [35/200] Final Prec@1 99.9220%
06/02 06:40:12 PM | Valid: [35/200] Step 000/078 Loss 1.152 Prec@(1,5) (71.9%, 93.0%)
06/02 06:40:14 PM | Valid: [35/200] Step 078/078 Loss 1.309 Prec@(1,5) (69.7%, 89.6%)
06/02 06:40:14 PM | Valid: [35/200] Final Prec@1 69.7100%
06/02 06:40:14 PM | Current mask training best Prec@1 = 70.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37272.0, 0.5687255859375]
['model.relu.alpha_mask_1_0', 16384, 9968.0, 0.6083984375]
['model.relu.alpha_mask_2_0', 16384, 9711.0, 0.59271240234375]
['model.relu.alpha_mask_3_0', 16384, 9945.0, 0.60699462890625]
['model.relu.alpha_mask_4_0', 16384, 9636.0, 0.588134765625]
['model.relu.alpha_mask_5_0', 8192, 5724.0, 0.69873046875]
['model.relu.alpha_mask_6_0', 8192, 5367.0, 0.6551513671875]
['model.relu.alpha_mask_7_0', 8192, 5510.0, 0.672607421875]
['model.relu.alpha_mask_8_0', 8192, 5385.0, 0.6573486328125]
['model.relu.alpha_mask_9_0', 4096, 3177.0, 0.775634765625]
['model.relu.alpha_mask_10_0', 4096, 3078.0, 0.75146484375]
['model.relu.alpha_mask_11_0', 4096, 3324.0, 0.8115234375]
['model.relu.alpha_mask_12_0', 4096, 3033.0, 0.740478515625]
['model.relu.alpha_mask_13_0', 2048, 2020.0, 0.986328125]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1968.0, 0.9609375]
['model.relu.alpha_mask_16_0', 2048, 1937.0, 0.94580078125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119098.0, 0.6321013077445652]
########## End ###########
06/02 06:40:15 PM | Train: [36/80] Step 000/390 Loss 0.021 Prec@(1,5) (99.2%, 100.0%)
06/02 06:40:15 PM | layerwise density: [37272.0, 9968.0, 9711.0, 9945.0, 9636.0, 5724.0, 5367.0, 5510.0, 5385.0, 3177.0, 3078.0, 3324.0, 3033.0, 2020.0, 2043.0, 1968.0, 1937.0]
layerwise density percentage: ['0.569', '0.608', '0.593', '0.607', '0.588', '0.699', '0.655', '0.673', '0.657', '0.776', '0.751', '0.812', '0.740', '0.986', '0.998', '0.961', '0.946']
Global density: 0.63210129737854
06/02 06:40:26 PM | Train: [36/80] Step 100/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:40:26 PM | layerwise density: [37279.0, 9980.0, 9724.0, 9950.0, 9649.0, 5724.0, 5376.0, 5520.0, 5378.0, 3175.0, 3066.0, 3322.0, 3034.0, 2018.0, 2043.0, 1971.0, 1939.0]
layerwise density percentage: ['0.569', '0.609', '0.594', '0.607', '0.589', '0.699', '0.656', '0.674', '0.656', '0.775', '0.749', '0.811', '0.741', '0.985', '0.998', '0.962', '0.947']
Global density: 0.6323667168617249
06/02 06:40:36 PM | Train: [36/80] Step 200/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:40:36 PM | layerwise density: [37262.0, 9986.0, 9744.0, 9946.0, 9660.0, 5750.0, 5395.0, 5527.0, 5378.0, 3199.0, 3076.0, 3329.0, 3034.0, 2022.0, 2047.0, 1971.0, 1940.0]
layerwise density percentage: ['0.569', '0.609', '0.595', '0.607', '0.590', '0.702', '0.659', '0.675', '0.656', '0.781', '0.751', '0.813', '0.741', '0.987', '1.000', '0.962', '0.947']
Global density: 0.6329929828643799
06/02 06:40:47 PM | Train: [36/80] Step 300/390 Loss 0.008 Prec@(1,5) (99.9%, 100.0%)
06/02 06:40:47 PM | layerwise density: [37263.0, 9991.0, 9746.0, 9959.0, 9681.0, 5762.0, 5402.0, 5547.0, 5384.0, 3214.0, 3083.0, 3329.0, 3024.0, 2024.0, 2043.0, 1975.0, 1941.0]
layerwise density percentage: ['0.569', '0.610', '0.595', '0.608', '0.591', '0.703', '0.659', '0.677', '0.657', '0.785', '0.753', '0.813', '0.738', '0.988', '0.998', '0.964', '0.948']
Global density: 0.63353431224823
06/02 06:40:56 PM | Train: [36/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:40:56 PM | layerwise density: [37269.0, 9991.0, 9752.0, 9969.0, 9676.0, 5752.0, 5399.0, 5545.0, 5399.0, 3214.0, 3079.0, 3334.0, 3031.0, 2021.0, 2042.0, 1976.0, 1941.0]
layerwise density percentage: ['0.569', '0.610', '0.595', '0.608', '0.591', '0.702', '0.659', '0.677', '0.659', '0.785', '0.752', '0.814', '0.740', '0.987', '0.997', '0.965', '0.948']
Global density: 0.633651077747345
06/02 06:40:56 PM | Train: [36/200] Final Prec@1 99.9120%
06/02 06:40:56 PM | Valid: [36/200] Step 000/078 Loss 1.055 Prec@(1,5) (74.2%, 93.8%)
06/02 06:40:59 PM | Valid: [36/200] Step 078/078 Loss 1.280 Prec@(1,5) (70.5%, 90.0%)
06/02 06:40:59 PM | Valid: [36/200] Final Prec@1 70.5000%
06/02 06:40:59 PM | Current mask training best Prec@1 = 70.5000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37269.0, 0.5686798095703125]
['model.relu.alpha_mask_1_0', 16384, 9991.0, 0.60980224609375]
['model.relu.alpha_mask_2_0', 16384, 9752.0, 0.59521484375]
['model.relu.alpha_mask_3_0', 16384, 9969.0, 0.60845947265625]
['model.relu.alpha_mask_4_0', 16384, 9676.0, 0.590576171875]
['model.relu.alpha_mask_5_0', 8192, 5752.0, 0.7021484375]
['model.relu.alpha_mask_6_0', 8192, 5399.0, 0.6590576171875]
['model.relu.alpha_mask_7_0', 8192, 5545.0, 0.6768798828125]
['model.relu.alpha_mask_8_0', 8192, 5399.0, 0.6590576171875]
['model.relu.alpha_mask_9_0', 4096, 3214.0, 0.78466796875]
['model.relu.alpha_mask_10_0', 4096, 3079.0, 0.751708984375]
['model.relu.alpha_mask_11_0', 4096, 3334.0, 0.81396484375]
['model.relu.alpha_mask_12_0', 4096, 3031.0, 0.739990234375]
['model.relu.alpha_mask_13_0', 2048, 2021.0, 0.98681640625]
['model.relu.alpha_mask_14_0', 2048, 2042.0, 0.9970703125]
['model.relu.alpha_mask_15_0', 2048, 1976.0, 0.96484375]
['model.relu.alpha_mask_16_0', 2048, 1941.0, 0.94775390625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119390.0, 0.633651069972826]
########## End ###########
06/02 06:41:00 PM | Train: [37/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:41:00 PM | layerwise density: [37269.0, 9991.0, 9752.0, 9969.0, 9676.0, 5752.0, 5399.0, 5545.0, 5399.0, 3214.0, 3079.0, 3334.0, 3031.0, 2021.0, 2042.0, 1976.0, 1941.0]
layerwise density percentage: ['0.569', '0.610', '0.595', '0.608', '0.591', '0.702', '0.659', '0.677', '0.659', '0.785', '0.752', '0.814', '0.740', '0.987', '0.997', '0.965', '0.948']
Global density: 0.633651077747345
06/02 06:41:11 PM | Train: [37/80] Step 100/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 06:41:11 PM | layerwise density: [37262.0, 9988.0, 9754.0, 9988.0, 9678.0, 5763.0, 5408.0, 5562.0, 5409.0, 3208.0, 3080.0, 3347.0, 3042.0, 2020.0, 2042.0, 1981.0, 1943.0]
layerwise density percentage: ['0.569', '0.610', '0.595', '0.610', '0.591', '0.703', '0.660', '0.679', '0.660', '0.783', '0.752', '0.817', '0.743', '0.986', '0.997', '0.967', '0.949']
Global density: 0.6341022253036499
06/02 06:41:21 PM | Train: [37/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:41:21 PM | layerwise density: [37265.0, 10001.0, 9758.0, 10004.0, 9675.0, 5781.0, 5420.0, 5575.0, 5406.0, 3213.0, 3089.0, 3351.0, 3042.0, 2029.0, 2043.0, 1965.0, 1944.0]
layerwise density percentage: ['0.569', '0.610', '0.596', '0.611', '0.591', '0.706', '0.662', '0.681', '0.660', '0.784', '0.754', '0.818', '0.743', '0.991', '0.998', '0.959', '0.949']
Global density: 0.6345586776733398
06/02 06:41:32 PM | Train: [37/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:41:32 PM | layerwise density: [37263.0, 10021.0, 9764.0, 10002.0, 9674.0, 5790.0, 5430.0, 5584.0, 5404.0, 3223.0, 3077.0, 3352.0, 3019.0, 2034.0, 2047.0, 1969.0, 1945.0]
layerwise density percentage: ['0.569', '0.612', '0.596', '0.610', '0.590', '0.707', '0.663', '0.682', '0.660', '0.787', '0.751', '0.818', '0.737', '0.993', '1.000', '0.961', '0.950']
Global density: 0.63475501537323
06/02 06:41:41 PM | Train: [37/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:41:41 PM | layerwise density: [37265.0, 10031.0, 9767.0, 10013.0, 9678.0, 5786.0, 5434.0, 5588.0, 5405.0, 3218.0, 3072.0, 3354.0, 3041.0, 2026.0, 2046.0, 1982.0, 1946.0]
layerwise density percentage: ['0.569', '0.612', '0.596', '0.611', '0.591', '0.706', '0.663', '0.682', '0.660', '0.786', '0.750', '0.819', '0.742', '0.989', '0.999', '0.968', '0.950']
Global density: 0.6350415945053101
06/02 06:41:41 PM | Train: [37/200] Final Prec@1 99.9300%
06/02 06:41:42 PM | Valid: [37/200] Step 000/078 Loss 1.041 Prec@(1,5) (74.2%, 94.5%)
06/02 06:41:44 PM | Valid: [37/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.2%, 90.1%)
06/02 06:41:44 PM | Valid: [37/200] Final Prec@1 70.1600%
06/02 06:41:44 PM | Current mask training best Prec@1 = 70.5000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37265.0, 0.5686187744140625]
['model.relu.alpha_mask_1_0', 16384, 10031.0, 0.61224365234375]
['model.relu.alpha_mask_2_0', 16384, 9767.0, 0.59613037109375]
['model.relu.alpha_mask_3_0', 16384, 10013.0, 0.61114501953125]
['model.relu.alpha_mask_4_0', 16384, 9678.0, 0.5906982421875]
['model.relu.alpha_mask_5_0', 8192, 5786.0, 0.706298828125]
['model.relu.alpha_mask_6_0', 8192, 5434.0, 0.663330078125]
['model.relu.alpha_mask_7_0', 8192, 5588.0, 0.68212890625]
['model.relu.alpha_mask_8_0', 8192, 5405.0, 0.6597900390625]
['model.relu.alpha_mask_9_0', 4096, 3217.0, 0.785400390625]
['model.relu.alpha_mask_10_0', 4096, 3072.0, 0.75]
['model.relu.alpha_mask_11_0', 4096, 3354.0, 0.81884765625]
['model.relu.alpha_mask_12_0', 4096, 3041.0, 0.742431640625]
['model.relu.alpha_mask_13_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_mask_14_0', 2048, 2046.0, 0.9990234375]
['model.relu.alpha_mask_15_0', 2048, 1982.0, 0.9677734375]
['model.relu.alpha_mask_16_0', 2048, 1946.0, 0.9501953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119651.0, 0.6350363026494565]
########## End ###########
06/02 06:41:45 PM | Train: [38/80] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 06:41:45 PM | layerwise density: [37265.0, 10031.0, 9767.0, 10013.0, 9678.0, 5786.0, 5434.0, 5588.0, 5405.0, 3217.0, 3072.0, 3354.0, 3041.0, 2026.0, 2046.0, 1982.0, 1946.0]
layerwise density percentage: ['0.569', '0.612', '0.596', '0.611', '0.591', '0.706', '0.663', '0.682', '0.660', '0.785', '0.750', '0.819', '0.742', '0.989', '0.999', '0.968', '0.950']
Global density: 0.635036289691925
06/02 06:41:55 PM | Train: [38/80] Step 100/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:41:55 PM | layerwise density: [37263.0, 10036.0, 9760.0, 10019.0, 9682.0, 5781.0, 5438.0, 5590.0, 5412.0, 3224.0, 3074.0, 3357.0, 3052.0, 2025.0, 2045.0, 1968.0, 1945.0]
layerwise density percentage: ['0.569', '0.613', '0.596', '0.612', '0.591', '0.706', '0.664', '0.682', '0.661', '0.787', '0.750', '0.820', '0.745', '0.989', '0.999', '0.961', '0.950']
Global density: 0.63514244556427
06/02 06:42:06 PM | Train: [38/80] Step 200/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:42:06 PM | layerwise density: [37275.0, 10037.0, 9770.0, 10018.0, 9694.0, 5790.0, 5440.0, 5592.0, 5418.0, 3217.0, 3080.0, 3354.0, 3051.0, 2029.0, 2044.0, 1968.0, 1946.0]
layerwise density percentage: ['0.569', '0.613', '0.596', '0.611', '0.592', '0.707', '0.664', '0.683', '0.661', '0.785', '0.752', '0.819', '0.745', '0.991', '0.998', '0.961', '0.950']
Global density: 0.6354184746742249
06/02 06:42:16 PM | Train: [38/80] Step 300/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:42:16 PM | layerwise density: [37275.0, 10035.0, 9794.0, 10017.0, 9712.0, 5798.0, 5435.0, 5595.0, 5426.0, 3225.0, 3092.0, 3357.0, 3055.0, 2026.0, 2046.0, 1962.0, 1947.0]
layerwise density percentage: ['0.569', '0.612', '0.598', '0.611', '0.593', '0.708', '0.663', '0.683', '0.662', '0.787', '0.755', '0.820', '0.746', '0.989', '0.999', '0.958', '0.951']
Global density: 0.6358112096786499
06/02 06:42:25 PM | Train: [38/80] Step 390/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:42:25 PM | layerwise density: [37271.0, 10056.0, 9790.0, 10025.0, 9715.0, 5827.0, 5445.0, 5601.0, 5442.0, 3243.0, 3098.0, 3355.0, 3046.0, 2029.0, 2045.0, 1976.0, 1948.0]
layerwise density percentage: ['0.569', '0.614', '0.598', '0.612', '0.593', '0.711', '0.665', '0.684', '0.664', '0.792', '0.756', '0.819', '0.744', '0.991', '0.999', '0.965', '0.951']
Global density: 0.6364215612411499
06/02 06:42:25 PM | Train: [38/200] Final Prec@1 99.9340%
06/02 06:42:25 PM | Valid: [38/200] Step 000/078 Loss 1.137 Prec@(1,5) (72.7%, 92.2%)
06/02 06:42:28 PM | Valid: [38/200] Step 078/078 Loss 1.288 Prec@(1,5) (70.2%, 90.0%)
06/02 06:42:28 PM | Valid: [38/200] Final Prec@1 70.1800%
06/02 06:42:28 PM | Current mask training best Prec@1 = 70.5000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 37272.0, 0.5687255859375]
['model.relu.alpha_mask_1_0', 16384, 10057.0, 0.61383056640625]
['model.relu.alpha_mask_2_0', 16384, 9790.0, 0.5975341796875]
['model.relu.alpha_mask_3_0', 16384, 10025.0, 0.61187744140625]
['model.relu.alpha_mask_4_0', 16384, 9715.0, 0.59295654296875]
['model.relu.alpha_mask_5_0', 8192, 5827.0, 0.7113037109375]
['model.relu.alpha_mask_6_0', 8192, 5444.0, 0.66455078125]
['model.relu.alpha_mask_7_0', 8192, 5601.0, 0.6837158203125]
['model.relu.alpha_mask_8_0', 8192, 5442.0, 0.664306640625]
['model.relu.alpha_mask_9_0', 4096, 3243.0, 0.791748046875]
['model.relu.alpha_mask_10_0', 4096, 3098.0, 0.75634765625]
['model.relu.alpha_mask_11_0', 4096, 3355.0, 0.819091796875]
['model.relu.alpha_mask_12_0', 4096, 3045.0, 0.743408203125]
['model.relu.alpha_mask_13_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1976.0, 0.96484375]
['model.relu.alpha_mask_16_0', 2048, 1948.0, 0.951171875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119912.0, 0.6364215353260869]
########## End ###########
06/02 06:42:29 PM | Train: [39/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:42:29 PM | layerwise density: [37272.0, 10057.0, 9790.0, 10025.0, 9715.0, 5827.0, 5444.0, 5601.0, 5442.0, 3243.0, 3098.0, 3355.0, 3045.0, 2029.0, 2045.0, 1976.0, 1948.0]
layerwise density percentage: ['0.569', '0.614', '0.598', '0.612', '0.593', '0.711', '0.665', '0.684', '0.664', '0.792', '0.756', '0.819', '0.743', '0.991', '0.999', '0.965', '0.951']
Global density: 0.6364215612411499
06/02 06:42:39 PM | Train: [39/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:42:39 PM | layerwise density: [37265.0, 10079.0, 9792.0, 10021.0, 9719.0, 5844.0, 5450.0, 5608.0, 5446.0, 3255.0, 3101.0, 3356.0, 3051.0, 2027.0, 2043.0, 1968.0, 1947.0]
layerwise density percentage: ['0.569', '0.615', '0.598', '0.612', '0.593', '0.713', '0.665', '0.685', '0.665', '0.795', '0.757', '0.819', '0.745', '0.990', '0.998', '0.961', '0.951']
Global density: 0.63673996925354
06/02 06:42:49 PM | Train: [39/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:42:49 PM | layerwise density: [36997.0, 9938.0, 9680.0, 9933.0, 9580.0, 5749.0, 5372.0, 5542.0, 5350.0, 3189.0, 3061.0, 3334.0, 2997.0, 2013.0, 2038.0, 1938.0, 1947.0]
layerwise density percentage: ['0.565', '0.607', '0.591', '0.606', '0.585', '0.702', '0.656', '0.677', '0.653', '0.779', '0.747', '0.814', '0.732', '0.983', '0.995', '0.946', '0.951']
Global density: 0.629766047000885
06/02 06:43:00 PM | Train: [39/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:43:00 PM | layerwise density: [36891.0, 9864.0, 9608.0, 9871.0, 9511.0, 5659.0, 5291.0, 5488.0, 5278.0, 3128.0, 3015.0, 3322.0, 2935.0, 2003.0, 2039.0, 1925.0, 1945.0]
layerwise density percentage: ['0.563', '0.602', '0.586', '0.602', '0.581', '0.691', '0.646', '0.670', '0.644', '0.764', '0.736', '0.811', '0.717', '0.978', '0.996', '0.940', '0.950']
Global density: 0.6250690221786499
06/02 06:43:09 PM | Train: [39/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:43:09 PM | layerwise density: [36860.0, 9838.0, 9592.0, 9853.0, 9491.0, 5645.0, 5270.0, 5481.0, 5251.0, 3141.0, 3011.0, 3324.0, 2947.0, 2018.0, 2041.0, 1940.0, 1945.0]
layerwise density percentage: ['0.562', '0.600', '0.585', '0.601', '0.579', '0.689', '0.643', '0.669', '0.641', '0.767', '0.735', '0.812', '0.719', '0.985', '0.997', '0.947', '0.950']
Global density: 0.624405562877655
06/02 06:43:09 PM | Train: [39/200] Final Prec@1 99.9300%
06/02 06:43:09 PM | Valid: [39/200] Step 000/078 Loss 1.050 Prec@(1,5) (73.4%, 93.0%)
06/02 06:43:12 PM | Valid: [39/200] Step 078/078 Loss 1.287 Prec@(1,5) (70.4%, 89.8%)
06/02 06:43:12 PM | Valid: [39/200] Final Prec@1 70.4200%
06/02 06:43:12 PM | Current mask training best Prec@1 = 70.5000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36860.0, 0.56243896484375]
['model.relu.alpha_mask_1_0', 16384, 9838.0, 0.6004638671875]
['model.relu.alpha_mask_2_0', 16384, 9590.0, 0.5853271484375]
['model.relu.alpha_mask_3_0', 16384, 9853.0, 0.60137939453125]
['model.relu.alpha_mask_4_0', 16384, 9491.0, 0.57928466796875]
['model.relu.alpha_mask_5_0', 8192, 5647.0, 0.6893310546875]
['model.relu.alpha_mask_6_0', 8192, 5271.0, 0.6434326171875]
['model.relu.alpha_mask_7_0', 8192, 5480.0, 0.6689453125]
['model.relu.alpha_mask_8_0', 8192, 5252.0, 0.64111328125]
['model.relu.alpha_mask_9_0', 4096, 3142.0, 0.76708984375]
['model.relu.alpha_mask_10_0', 4096, 3010.0, 0.73486328125]
['model.relu.alpha_mask_11_0', 4096, 3323.0, 0.811279296875]
['model.relu.alpha_mask_12_0', 4096, 2947.0, 0.719482421875]
['model.relu.alpha_mask_13_0', 2048, 2018.0, 0.9853515625]
['model.relu.alpha_mask_14_0', 2048, 2041.0, 0.99658203125]
['model.relu.alpha_mask_15_0', 2048, 1940.0, 0.947265625]
['model.relu.alpha_mask_16_0', 2048, 1945.0, 0.94970703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 117648.0, 0.624405570652174]
########## End ###########
06/02 06:43:13 PM | Train: [40/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:43:13 PM | layerwise density: [36860.0, 9838.0, 9590.0, 9853.0, 9491.0, 5647.0, 5271.0, 5480.0, 5252.0, 3142.0, 3010.0, 3323.0, 2947.0, 2018.0, 2041.0, 1940.0, 1945.0]
layerwise density percentage: ['0.562', '0.600', '0.585', '0.601', '0.579', '0.689', '0.643', '0.669', '0.641', '0.767', '0.735', '0.811', '0.719', '0.985', '0.997', '0.947', '0.950']
Global density: 0.624405562877655
06/02 06:43:23 PM | Train: [40/80] Step 100/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:43:23 PM | layerwise density: [36834.0, 9822.0, 9575.0, 9845.0, 9476.0, 5649.0, 5262.0, 5480.0, 5248.0, 3162.0, 3010.0, 3323.0, 2958.0, 2022.0, 2044.0, 1954.0, 1946.0]
layerwise density percentage: ['0.562', '0.599', '0.584', '0.601', '0.578', '0.690', '0.642', '0.669', '0.641', '0.772', '0.735', '0.811', '0.722', '0.987', '0.998', '0.954', '0.950']
Global density: 0.6242039203643799
06/02 06:43:34 PM | Train: [40/80] Step 200/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:43:34 PM | layerwise density: [36821.0, 9820.0, 9566.0, 9838.0, 9472.0, 5653.0, 5284.0, 5488.0, 5273.0, 3184.0, 3018.0, 3325.0, 2979.0, 2022.0, 2042.0, 1966.0, 1947.0]
layerwise density percentage: ['0.562', '0.599', '0.584', '0.600', '0.578', '0.690', '0.645', '0.670', '0.644', '0.777', '0.737', '0.812', '0.727', '0.987', '0.997', '0.960', '0.951']
Global density: 0.6246709227561951
06/02 06:43:45 PM | Train: [40/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:43:45 PM | layerwise density: [36814.0, 9822.0, 9567.0, 9834.0, 9484.0, 5689.0, 5290.0, 5501.0, 5317.0, 3210.0, 3041.0, 3330.0, 3005.0, 2029.0, 2045.0, 1972.0, 1948.0]
layerwise density percentage: ['0.562', '0.599', '0.584', '0.600', '0.579', '0.694', '0.646', '0.672', '0.649', '0.784', '0.742', '0.813', '0.734', '0.991', '0.999', '0.963', '0.951']
Global density: 0.625732421875
06/02 06:43:54 PM | Train: [40/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:43:54 PM | layerwise density: [36814.0, 9829.0, 9572.0, 9836.0, 9485.0, 5699.0, 5311.0, 5514.0, 5336.0, 3215.0, 3054.0, 3329.0, 3004.0, 2028.0, 2046.0, 1975.0, 1948.0]
layerwise density percentage: ['0.562', '0.600', '0.584', '0.600', '0.579', '0.696', '0.648', '0.673', '0.651', '0.785', '0.746', '0.813', '0.733', '0.990', '0.999', '0.964', '0.951']
Global density: 0.626247227191925
06/02 06:43:54 PM | Train: [40/200] Final Prec@1 99.9360%
06/02 06:43:55 PM | Valid: [40/200] Step 000/078 Loss 1.106 Prec@(1,5) (73.4%, 92.2%)
06/02 06:43:57 PM | Valid: [40/200] Step 078/078 Loss 1.294 Prec@(1,5) (70.3%, 89.8%)
06/02 06:43:57 PM | Valid: [40/200] Final Prec@1 70.3000%
06/02 06:43:57 PM | Current mask training best Prec@1 = 70.5000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36814.0, 0.561737060546875]
['model.relu.alpha_mask_1_0', 16384, 9829.0, 0.59991455078125]
['model.relu.alpha_mask_2_0', 16384, 9571.0, 0.58416748046875]
['model.relu.alpha_mask_3_0', 16384, 9836.0, 0.600341796875]
['model.relu.alpha_mask_4_0', 16384, 9485.0, 0.57891845703125]
['model.relu.alpha_mask_5_0', 8192, 5700.0, 0.69580078125]
['model.relu.alpha_mask_6_0', 8192, 5311.0, 0.6483154296875]
['model.relu.alpha_mask_7_0', 8192, 5513.0, 0.6729736328125]
['model.relu.alpha_mask_8_0', 8192, 5336.0, 0.6513671875]
['model.relu.alpha_mask_9_0', 4096, 3215.0, 0.784912109375]
['model.relu.alpha_mask_10_0', 4096, 3054.0, 0.74560546875]
['model.relu.alpha_mask_11_0', 4096, 3329.0, 0.812744140625]
['model.relu.alpha_mask_12_0', 4096, 3004.0, 0.7333984375]
['model.relu.alpha_mask_13_0', 2048, 2027.0, 0.98974609375]
['model.relu.alpha_mask_14_0', 2048, 2046.0, 0.9990234375]
['model.relu.alpha_mask_15_0', 2048, 1975.0, 0.96435546875]
['model.relu.alpha_mask_16_0', 2048, 1948.0, 0.951171875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 117993.0, 0.626236625339674]
########## End ###########
06/02 06:43:58 PM | Train: [41/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:43:58 PM | layerwise density: [36814.0, 9829.0, 9571.0, 9836.0, 9485.0, 5700.0, 5311.0, 5513.0, 5336.0, 3215.0, 3054.0, 3329.0, 3004.0, 2027.0, 2046.0, 1975.0, 1948.0]
layerwise density percentage: ['0.562', '0.600', '0.584', '0.600', '0.579', '0.696', '0.648', '0.673', '0.651', '0.785', '0.746', '0.813', '0.733', '0.990', '0.999', '0.964', '0.951']
Global density: 0.626236617565155
06/02 06:44:08 PM | Train: [41/80] Step 100/390 Loss 0.007 Prec@(1,5) (99.9%, 100.0%)
06/02 06:44:08 PM | layerwise density: [36809.0, 9842.0, 9582.0, 9853.0, 9495.0, 5705.0, 5327.0, 5523.0, 5329.0, 3224.0, 3054.0, 3334.0, 3007.0, 2023.0, 2045.0, 1972.0, 1948.0]
layerwise density percentage: ['0.562', '0.601', '0.585', '0.601', '0.580', '0.696', '0.650', '0.674', '0.651', '0.787', '0.746', '0.814', '0.734', '0.988', '0.999', '0.963', '0.951']
Global density: 0.6266559362411499
06/02 06:44:19 PM | Train: [41/80] Step 200/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:44:19 PM | layerwise density: [36811.0, 9851.0, 9584.0, 9861.0, 9502.0, 5718.0, 5340.0, 5527.0, 5336.0, 3222.0, 3065.0, 3342.0, 3006.0, 2029.0, 2044.0, 1975.0, 1948.0]
layerwise density percentage: ['0.562', '0.601', '0.585', '0.602', '0.580', '0.698', '0.652', '0.675', '0.651', '0.787', '0.748', '0.816', '0.734', '0.991', '0.998', '0.964', '0.951']
Global density: 0.6271283030509949
06/02 06:44:29 PM | Train: [41/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:44:29 PM | layerwise density: [36820.0, 9860.0, 9591.0, 9872.0, 9515.0, 5728.0, 5352.0, 5532.0, 5341.0, 3223.0, 3075.0, 3345.0, 3017.0, 2031.0, 2043.0, 1976.0, 1948.0]
layerwise density percentage: ['0.562', '0.602', '0.585', '0.603', '0.581', '0.699', '0.653', '0.675', '0.652', '0.787', '0.751', '0.817', '0.737', '0.992', '0.998', '0.965', '0.951']
Global density: 0.627701461315155
06/02 06:44:39 PM | Train: [41/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:44:39 PM | layerwise density: [36815.0, 9873.0, 9599.0, 9871.0, 9527.0, 5740.0, 5365.0, 5545.0, 5345.0, 3227.0, 3084.0, 3352.0, 3024.0, 2032.0, 2045.0, 1973.0, 1948.0]
layerwise density percentage: ['0.562', '0.603', '0.586', '0.602', '0.581', '0.701', '0.655', '0.677', '0.652', '0.788', '0.753', '0.818', '0.738', '0.992', '0.999', '0.963', '0.951']
Global density: 0.6282110214233398
06/02 06:44:39 PM | Train: [41/200] Final Prec@1 99.9240%
06/02 06:44:39 PM | Valid: [41/200] Step 000/078 Loss 1.015 Prec@(1,5) (74.2%, 94.5%)
06/02 06:44:41 PM | Valid: [41/200] Step 078/078 Loss 1.293 Prec@(1,5) (70.2%, 89.6%)
06/02 06:44:41 PM | Valid: [41/200] Final Prec@1 70.2300%
06/02 06:44:41 PM | Current mask training best Prec@1 = 70.5000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36816.0, 0.561767578125]
['model.relu.alpha_mask_1_0', 16384, 9876.0, 0.602783203125]
['model.relu.alpha_mask_2_0', 16384, 9599.0, 0.58587646484375]
['model.relu.alpha_mask_3_0', 16384, 9871.0, 0.60247802734375]
['model.relu.alpha_mask_4_0', 16384, 9527.0, 0.58148193359375]
['model.relu.alpha_mask_5_0', 8192, 5739.0, 0.7005615234375]
['model.relu.alpha_mask_6_0', 8192, 5366.0, 0.655029296875]
['model.relu.alpha_mask_7_0', 8192, 5546.0, 0.677001953125]
['model.relu.alpha_mask_8_0', 8192, 5344.0, 0.65234375]
['model.relu.alpha_mask_9_0', 4096, 3227.0, 0.787841796875]
['model.relu.alpha_mask_10_0', 4096, 3084.0, 0.7529296875]
['model.relu.alpha_mask_11_0', 4096, 3352.0, 0.818359375]
['model.relu.alpha_mask_12_0', 4096, 3023.0, 0.738037109375]
['model.relu.alpha_mask_13_0', 2048, 2032.0, 0.9921875]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1972.0, 0.962890625]
['model.relu.alpha_mask_16_0', 2048, 1948.0, 0.951171875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118367.0, 0.6282215947690217]
########## End ###########
06/02 06:44:42 PM | Train: [42/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:44:42 PM | layerwise density: [36816.0, 9876.0, 9599.0, 9871.0, 9527.0, 5739.0, 5366.0, 5546.0, 5344.0, 3227.0, 3084.0, 3352.0, 3023.0, 2032.0, 2045.0, 1972.0, 1948.0]
layerwise density percentage: ['0.562', '0.603', '0.586', '0.602', '0.581', '0.701', '0.655', '0.677', '0.652', '0.788', '0.753', '0.818', '0.738', '0.992', '0.999', '0.963', '0.951']
Global density: 0.6282216310501099
06/02 06:44:53 PM | Train: [42/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:44:53 PM | layerwise density: [36827.0, 9888.0, 9598.0, 9874.0, 9528.0, 5742.0, 5366.0, 5554.0, 5348.0, 3224.0, 3092.0, 3351.0, 3028.0, 2034.0, 2046.0, 1973.0, 1948.0]
layerwise density percentage: ['0.562', '0.604', '0.586', '0.603', '0.582', '0.701', '0.655', '0.678', '0.653', '0.787', '0.755', '0.818', '0.739', '0.993', '0.999', '0.963', '0.951']
Global density: 0.6285082101821899
06/02 06:45:03 PM | Train: [42/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:45:03 PM | layerwise density: [36831.0, 9896.0, 9604.0, 9881.0, 9527.0, 5751.0, 5373.0, 5556.0, 5356.0, 3232.0, 3081.0, 3355.0, 3036.0, 2031.0, 2043.0, 1968.0, 1948.0]
layerwise density percentage: ['0.562', '0.604', '0.586', '0.603', '0.581', '0.702', '0.656', '0.678', '0.654', '0.789', '0.752', '0.819', '0.741', '0.992', '0.998', '0.961', '0.951']
Global density: 0.62876296043396
06/02 06:45:14 PM | Train: [42/80] Step 300/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:45:14 PM | layerwise density: [36822.0, 9914.0, 9610.0, 9888.0, 9537.0, 5758.0, 5380.0, 5561.0, 5366.0, 3245.0, 3079.0, 3356.0, 3037.0, 2029.0, 2045.0, 1972.0, 1949.0]
layerwise density percentage: ['0.562', '0.605', '0.587', '0.604', '0.582', '0.703', '0.657', '0.679', '0.655', '0.792', '0.752', '0.819', '0.741', '0.991', '0.999', '0.963', '0.952']
Global density: 0.6291822195053101
06/02 06:45:23 PM | Train: [42/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:45:23 PM | layerwise density: [36819.0, 9919.0, 9620.0, 9892.0, 9545.0, 5773.0, 5386.0, 5574.0, 5379.0, 3263.0, 3088.0, 3363.0, 3036.0, 2027.0, 2044.0, 1972.0, 1950.0]
layerwise density percentage: ['0.562', '0.605', '0.587', '0.604', '0.583', '0.705', '0.657', '0.680', '0.657', '0.797', '0.754', '0.821', '0.741', '0.990', '0.998', '0.963', '0.952']
Global density: 0.6297236084938049
06/02 06:45:23 PM | Train: [42/200] Final Prec@1 99.9420%
06/02 06:45:24 PM | Valid: [42/200] Step 000/078 Loss 1.094 Prec@(1,5) (75.8%, 96.1%)
06/02 06:45:26 PM | Valid: [42/200] Step 078/078 Loss 1.284 Prec@(1,5) (70.4%, 90.1%)
06/02 06:45:26 PM | Valid: [42/200] Final Prec@1 70.4400%
06/02 06:45:26 PM | Current mask training best Prec@1 = 70.5000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36819.0, 0.5618133544921875]
['model.relu.alpha_mask_1_0', 16384, 9919.0, 0.60540771484375]
['model.relu.alpha_mask_2_0', 16384, 9620.0, 0.587158203125]
['model.relu.alpha_mask_3_0', 16384, 9892.0, 0.603759765625]
['model.relu.alpha_mask_4_0', 16384, 9545.0, 0.58258056640625]
['model.relu.alpha_mask_5_0', 8192, 5773.0, 0.7047119140625]
['model.relu.alpha_mask_6_0', 8192, 5386.0, 0.657470703125]
['model.relu.alpha_mask_7_0', 8192, 5574.0, 0.680419921875]
['model.relu.alpha_mask_8_0', 8192, 5379.0, 0.6566162109375]
['model.relu.alpha_mask_9_0', 4096, 3263.0, 0.796630859375]
['model.relu.alpha_mask_10_0', 4096, 3088.0, 0.75390625]
['model.relu.alpha_mask_11_0', 4096, 3363.0, 0.821044921875]
['model.relu.alpha_mask_12_0', 4096, 3036.0, 0.7412109375]
['model.relu.alpha_mask_13_0', 2048, 2027.0, 0.98974609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1972.0, 0.962890625]
['model.relu.alpha_mask_16_0', 2048, 1950.0, 0.9521484375]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118650.0, 0.6297235903532609]
########## End ###########
06/02 06:45:27 PM | Train: [43/80] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:45:27 PM | layerwise density: [36819.0, 9919.0, 9620.0, 9892.0, 9545.0, 5773.0, 5386.0, 5574.0, 5379.0, 3263.0, 3088.0, 3363.0, 3036.0, 2027.0, 2044.0, 1972.0, 1950.0]
layerwise density percentage: ['0.562', '0.605', '0.587', '0.604', '0.583', '0.705', '0.657', '0.680', '0.657', '0.797', '0.754', '0.821', '0.741', '0.990', '0.998', '0.963', '0.952']
Global density: 0.6297236084938049
06/02 06:45:37 PM | Train: [43/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:45:37 PM | layerwise density: [36812.0, 9924.0, 9629.0, 9898.0, 9554.0, 5788.0, 5384.0, 5588.0, 5384.0, 3277.0, 3090.0, 3370.0, 3039.0, 2028.0, 2047.0, 1972.0, 1951.0]
layerwise density percentage: ['0.562', '0.606', '0.588', '0.604', '0.583', '0.707', '0.657', '0.682', '0.657', '0.800', '0.754', '0.823', '0.742', '0.990', '1.000', '0.963', '0.953']
Global density: 0.6301747560501099
06/02 06:45:48 PM | Train: [43/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:45:48 PM | layerwise density: [36815.0, 9936.0, 9634.0, 9903.0, 9555.0, 5792.0, 5381.0, 5600.0, 5402.0, 3283.0, 3096.0, 3373.0, 3042.0, 2029.0, 2045.0, 1969.0, 1951.0]
layerwise density percentage: ['0.562', '0.606', '0.588', '0.604', '0.583', '0.707', '0.657', '0.684', '0.659', '0.802', '0.756', '0.823', '0.743', '0.991', '0.999', '0.961', '0.953']
Global density: 0.6305515766143799
06/02 06:45:58 PM | Train: [43/80] Step 300/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 06:45:58 PM | layerwise density: [36826.0, 9941.0, 9640.0, 9910.0, 9568.0, 5794.0, 5378.0, 5609.0, 5406.0, 3277.0, 3093.0, 3380.0, 3056.0, 2029.0, 2043.0, 1973.0, 1952.0]
layerwise density percentage: ['0.562', '0.607', '0.588', '0.605', '0.584', '0.707', '0.656', '0.685', '0.660', '0.800', '0.755', '0.825', '0.746', '0.991', '0.998', '0.963', '0.953']
Global density: 0.6309177875518799
06/02 06:46:08 PM | Train: [43/80] Step 390/390 Loss 0.006 Prec@(1,5) (99.9%, 100.0%)
06/02 06:46:08 PM | layerwise density: [36840.0, 9952.0, 9639.0, 9916.0, 9575.0, 5808.0, 5387.0, 5614.0, 5420.0, 3283.0, 3084.0, 3379.0, 3051.0, 2031.0, 2045.0, 1975.0, 1952.0]
layerwise density percentage: ['0.562', '0.607', '0.588', '0.605', '0.584', '0.709', '0.658', '0.685', '0.662', '0.802', '0.753', '0.825', '0.745', '0.992', '0.999', '0.964', '0.953']
Global density: 0.631321132183075
06/02 06:46:08 PM | Train: [43/200] Final Prec@1 99.9420%
06/02 06:46:08 PM | Valid: [43/200] Step 000/078 Loss 1.111 Prec@(1,5) (72.7%, 93.0%)
06/02 06:46:11 PM | Valid: [43/200] Step 078/078 Loss 1.270 Prec@(1,5) (70.9%, 90.0%)
06/02 06:46:11 PM | Valid: [43/200] Final Prec@1 70.8500%
06/02 06:46:11 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36840.0, 0.5621337890625]
['model.relu.alpha_mask_1_0', 16384, 9952.0, 0.607421875]
['model.relu.alpha_mask_2_0', 16384, 9639.0, 0.58831787109375]
['model.relu.alpha_mask_3_0', 16384, 9916.0, 0.605224609375]
['model.relu.alpha_mask_4_0', 16384, 9575.0, 0.58441162109375]
['model.relu.alpha_mask_5_0', 8192, 5808.0, 0.708984375]
['model.relu.alpha_mask_6_0', 8192, 5387.0, 0.6575927734375]
['model.relu.alpha_mask_7_0', 8192, 5614.0, 0.685302734375]
['model.relu.alpha_mask_8_0', 8192, 5420.0, 0.66162109375]
['model.relu.alpha_mask_9_0', 4096, 3283.0, 0.801513671875]
['model.relu.alpha_mask_10_0', 4096, 3084.0, 0.7529296875]
['model.relu.alpha_mask_11_0', 4096, 3379.0, 0.824951171875]
['model.relu.alpha_mask_12_0', 4096, 3051.0, 0.744873046875]
['model.relu.alpha_mask_13_0', 2048, 2031.0, 0.99169921875]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1975.0, 0.96435546875]
['model.relu.alpha_mask_16_0', 2048, 1952.0, 0.953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118951.0, 0.6313211192255435]
########## End ###########
06/02 06:46:12 PM | Train: [44/80] Step 000/390 Loss 0.014 Prec@(1,5) (100.0%, 100.0%)
06/02 06:46:12 PM | layerwise density: [36840.0, 9952.0, 9639.0, 9916.0, 9575.0, 5808.0, 5387.0, 5614.0, 5420.0, 3283.0, 3084.0, 3379.0, 3051.0, 2031.0, 2045.0, 1975.0, 1952.0]
layerwise density percentage: ['0.562', '0.607', '0.588', '0.605', '0.584', '0.709', '0.658', '0.685', '0.662', '0.802', '0.753', '0.825', '0.745', '0.992', '0.999', '0.964', '0.953']
Global density: 0.631321132183075
06/02 06:46:23 PM | Train: [44/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:46:23 PM | layerwise density: [36842.0, 9961.0, 9644.0, 9932.0, 9585.0, 5821.0, 5388.0, 5618.0, 5412.0, 3276.0, 3087.0, 3382.0, 3046.0, 2032.0, 2042.0, 1970.0, 1953.0]
layerwise density percentage: ['0.562', '0.608', '0.589', '0.606', '0.585', '0.711', '0.658', '0.686', '0.661', '0.800', '0.754', '0.826', '0.744', '0.992', '0.997', '0.962', '0.954']
Global density: 0.6315334439277649
06/02 06:46:33 PM | Train: [44/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:46:33 PM | layerwise density: [36829.0, 9961.0, 9643.0, 9935.0, 9579.0, 5841.0, 5390.0, 5614.0, 5420.0, 3274.0, 3093.0, 3377.0, 3051.0, 2034.0, 2045.0, 1969.0, 1954.0]
layerwise density percentage: ['0.562', '0.608', '0.589', '0.606', '0.585', '0.713', '0.658', '0.685', '0.662', '0.799', '0.755', '0.824', '0.745', '0.993', '0.999', '0.961', '0.954']
Global density: 0.6316289901733398
06/02 06:46:44 PM | Train: [44/80] Step 300/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:46:44 PM | layerwise density: [36838.0, 9965.0, 9650.0, 9948.0, 9589.0, 5859.0, 5395.0, 5626.0, 5420.0, 3280.0, 3110.0, 3377.0, 3053.0, 2036.0, 2045.0, 1960.0, 1955.0]
layerwise density percentage: ['0.562', '0.608', '0.589', '0.607', '0.585', '0.715', '0.659', '0.687', '0.662', '0.801', '0.759', '0.824', '0.745', '0.994', '0.999', '0.957', '0.955']
Global density: 0.6321437954902649
06/02 06:46:53 PM | Train: [44/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:46:53 PM | layerwise density: [36833.0, 9975.0, 9655.0, 9957.0, 9607.0, 5861.0, 5404.0, 5630.0, 5425.0, 3278.0, 3116.0, 3386.0, 3054.0, 2034.0, 2044.0, 1964.0, 1956.0]
layerwise density percentage: ['0.562', '0.609', '0.589', '0.608', '0.586', '0.715', '0.660', '0.687', '0.662', '0.800', '0.761', '0.827', '0.746', '0.993', '0.998', '0.959', '0.955']
Global density: 0.6325312256813049
06/02 06:46:53 PM | Train: [44/200] Final Prec@1 99.9540%
06/02 06:46:53 PM | Valid: [44/200] Step 000/078 Loss 1.140 Prec@(1,5) (71.9%, 92.2%)
06/02 06:46:56 PM | Valid: [44/200] Step 078/078 Loss 1.285 Prec@(1,5) (70.7%, 90.0%)
06/02 06:46:56 PM | Valid: [44/200] Final Prec@1 70.7000%
06/02 06:46:56 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36834.0, 0.562042236328125]
['model.relu.alpha_mask_1_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_2_0', 16384, 9656.0, 0.58935546875]
['model.relu.alpha_mask_3_0', 16384, 9957.0, 0.60772705078125]
['model.relu.alpha_mask_4_0', 16384, 9607.0, 0.58636474609375]
['model.relu.alpha_mask_5_0', 8192, 5861.0, 0.7154541015625]
['model.relu.alpha_mask_6_0', 8192, 5405.0, 0.6597900390625]
['model.relu.alpha_mask_7_0', 8192, 5630.0, 0.687255859375]
['model.relu.alpha_mask_8_0', 8192, 5425.0, 0.6622314453125]
['model.relu.alpha_mask_9_0', 4096, 3277.0, 0.800048828125]
['model.relu.alpha_mask_10_0', 4096, 3115.0, 0.760498046875]
['model.relu.alpha_mask_11_0', 4096, 3386.0, 0.82666015625]
['model.relu.alpha_mask_12_0', 4096, 3054.0, 0.74560546875]
['model.relu.alpha_mask_13_0', 2048, 2034.0, 0.9931640625]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1964.0, 0.958984375]
['model.relu.alpha_mask_16_0', 2048, 1956.0, 0.955078125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119180.0, 0.6325365149456522]
########## End ###########
06/02 06:46:57 PM | Train: [45/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:46:57 PM | layerwise density: [36834.0, 9975.0, 9656.0, 9957.0, 9607.0, 5861.0, 5405.0, 5630.0, 5425.0, 3277.0, 3115.0, 3386.0, 3054.0, 2034.0, 2044.0, 1964.0, 1956.0]
layerwise density percentage: ['0.562', '0.609', '0.589', '0.608', '0.586', '0.715', '0.660', '0.687', '0.662', '0.800', '0.760', '0.827', '0.746', '0.993', '0.998', '0.959', '0.955']
Global density: 0.6325365304946899
06/02 06:47:07 PM | Train: [45/80] Step 100/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:47:07 PM | layerwise density: [36848.0, 9979.0, 9664.0, 9952.0, 9602.0, 5873.0, 5412.0, 5639.0, 5426.0, 3287.0, 3125.0, 3384.0, 3056.0, 2029.0, 2045.0, 1960.0, 1956.0]
layerwise density percentage: ['0.562', '0.609', '0.590', '0.607', '0.586', '0.717', '0.661', '0.688', '0.662', '0.802', '0.763', '0.826', '0.746', '0.991', '0.999', '0.957', '0.955']
Global density: 0.632839024066925
06/02 06:47:18 PM | Train: [45/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:47:18 PM | layerwise density: [36848.0, 9983.0, 9670.0, 9974.0, 9613.0, 5870.0, 5419.0, 5651.0, 5435.0, 3284.0, 3124.0, 3387.0, 3060.0, 2028.0, 2045.0, 1965.0, 1957.0]
layerwise density percentage: ['0.562', '0.609', '0.590', '0.609', '0.587', '0.717', '0.661', '0.690', '0.663', '0.802', '0.763', '0.827', '0.747', '0.990', '0.999', '0.959', '0.956']
Global density: 0.6332424283027649
06/02 06:47:29 PM | Train: [45/80] Step 300/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:47:29 PM | layerwise density: [36850.0, 9986.0, 9670.0, 9980.0, 9614.0, 5880.0, 5425.0, 5661.0, 5444.0, 3290.0, 3111.0, 3387.0, 3058.0, 2026.0, 2046.0, 1969.0, 1957.0]
layerwise density percentage: ['0.562', '0.609', '0.590', '0.609', '0.587', '0.718', '0.662', '0.691', '0.665', '0.803', '0.760', '0.827', '0.747', '0.989', '0.999', '0.961', '0.956']
Global density: 0.6334600448608398
06/02 06:47:38 PM | Train: [45/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:47:38 PM | layerwise density: [36866.0, 9992.0, 9669.0, 9983.0, 9624.0, 5868.0, 5444.0, 5665.0, 5443.0, 3302.0, 3127.0, 3388.0, 3059.0, 2029.0, 2044.0, 1969.0, 1957.0]
layerwise density percentage: ['0.563', '0.610', '0.590', '0.609', '0.587', '0.716', '0.665', '0.692', '0.664', '0.806', '0.763', '0.827', '0.747', '0.991', '0.998', '0.961', '0.956']
Global density: 0.6338580846786499
06/02 06:47:38 PM | Train: [45/200] Final Prec@1 99.9400%
06/02 06:47:38 PM | Valid: [45/200] Step 000/078 Loss 1.177 Prec@(1,5) (70.3%, 93.0%)
06/02 06:47:41 PM | Valid: [45/200] Step 078/078 Loss 1.283 Prec@(1,5) (70.4%, 90.0%)
06/02 06:47:41 PM | Valid: [45/200] Final Prec@1 70.4300%
06/02 06:47:41 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36866.0, 0.562530517578125]
['model.relu.alpha_mask_1_0', 16384, 9992.0, 0.60986328125]
['model.relu.alpha_mask_2_0', 16384, 9668.0, 0.590087890625]
['model.relu.alpha_mask_3_0', 16384, 9985.0, 0.60943603515625]
['model.relu.alpha_mask_4_0', 16384, 9624.0, 0.58740234375]
['model.relu.alpha_mask_5_0', 8192, 5868.0, 0.71630859375]
['model.relu.alpha_mask_6_0', 8192, 5443.0, 0.6644287109375]
['model.relu.alpha_mask_7_0', 8192, 5665.0, 0.6915283203125]
['model.relu.alpha_mask_8_0', 8192, 5444.0, 0.66455078125]
['model.relu.alpha_mask_9_0', 4096, 3303.0, 0.806396484375]
['model.relu.alpha_mask_10_0', 4096, 3127.0, 0.763427734375]
['model.relu.alpha_mask_11_0', 4096, 3388.0, 0.8271484375]
['model.relu.alpha_mask_12_0', 4096, 3060.0, 0.7470703125]
['model.relu.alpha_mask_13_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1968.0, 0.9609375]
['model.relu.alpha_mask_16_0', 2048, 1957.0, 0.95556640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119431.0, 0.6338686735733695]
########## End ###########
06/02 06:47:42 PM | Train: [46/80] Step 000/390 Loss 0.011 Prec@(1,5) (99.2%, 100.0%)
06/02 06:47:42 PM | layerwise density: [36866.0, 9992.0, 9668.0, 9985.0, 9624.0, 5868.0, 5443.0, 5665.0, 5444.0, 3303.0, 3127.0, 3388.0, 3060.0, 2029.0, 2044.0, 1968.0, 1957.0]
layerwise density percentage: ['0.563', '0.610', '0.590', '0.609', '0.587', '0.716', '0.664', '0.692', '0.665', '0.806', '0.763', '0.827', '0.747', '0.991', '0.998', '0.961', '0.956']
Global density: 0.6338686943054199
06/02 06:47:52 PM | Train: [46/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:47:52 PM | layerwise density: [36849.0, 9992.0, 9671.0, 9993.0, 9628.0, 5869.0, 5456.0, 5661.0, 5444.0, 3307.0, 3126.0, 3390.0, 3055.0, 2035.0, 2044.0, 1967.0, 1957.0]
layerwise density percentage: ['0.562', '0.610', '0.590', '0.610', '0.588', '0.716', '0.666', '0.691', '0.665', '0.807', '0.763', '0.828', '0.746', '0.994', '0.998', '0.960', '0.956']
Global density: 0.633937656879425
06/02 06:48:03 PM | Train: [46/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:48:03 PM | layerwise density: [36858.0, 10006.0, 9674.0, 10003.0, 9628.0, 5873.0, 5462.0, 5670.0, 5452.0, 3301.0, 3119.0, 3392.0, 3066.0, 2036.0, 2042.0, 1966.0, 1957.0]
layerwise density percentage: ['0.562', '0.611', '0.590', '0.611', '0.588', '0.717', '0.667', '0.692', '0.666', '0.806', '0.761', '0.828', '0.749', '0.994', '0.997', '0.960', '0.956']
Global density: 0.634261429309845
06/02 06:48:14 PM | Train: [46/80] Step 300/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:48:14 PM | layerwise density: [36872.0, 10024.0, 9691.0, 10002.0, 9631.0, 5891.0, 5467.0, 5663.0, 5451.0, 3311.0, 3121.0, 3395.0, 3061.0, 2033.0, 2045.0, 1968.0, 1957.0]
layerwise density percentage: ['0.563', '0.612', '0.591', '0.610', '0.588', '0.719', '0.667', '0.691', '0.665', '0.808', '0.762', '0.829', '0.747', '0.993', '0.999', '0.961', '0.956']
Global density: 0.6346753835678101
06/02 06:48:23 PM | Train: [46/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:48:23 PM | layerwise density: [36881.0, 10025.0, 9701.0, 10015.0, 9633.0, 5886.0, 5464.0, 5662.0, 5453.0, 3310.0, 3119.0, 3400.0, 3080.0, 2029.0, 2043.0, 1972.0, 1958.0]
layerwise density percentage: ['0.563', '0.612', '0.592', '0.611', '0.588', '0.719', '0.667', '0.691', '0.666', '0.808', '0.761', '0.830', '0.752', '0.991', '0.998', '0.963', '0.956']
Global density: 0.6349301934242249
06/02 06:48:23 PM | Train: [46/200] Final Prec@1 99.9460%
06/02 06:48:23 PM | Valid: [46/200] Step 000/078 Loss 1.128 Prec@(1,5) (74.2%, 93.0%)
06/02 06:48:25 PM | Valid: [46/200] Step 078/078 Loss 1.273 Prec@(1,5) (70.7%, 90.2%)
06/02 06:48:26 PM | Valid: [46/200] Final Prec@1 70.7200%
06/02 06:48:26 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36884.0, 0.56280517578125]
['model.relu.alpha_mask_1_0', 16384, 10024.0, 0.61181640625]
['model.relu.alpha_mask_2_0', 16384, 9701.0, 0.59210205078125]
['model.relu.alpha_mask_3_0', 16384, 10015.0, 0.61126708984375]
['model.relu.alpha_mask_4_0', 16384, 9633.0, 0.58795166015625]
['model.relu.alpha_mask_5_0', 8192, 5886.0, 0.718505859375]
['model.relu.alpha_mask_6_0', 8192, 5464.0, 0.6669921875]
['model.relu.alpha_mask_7_0', 8192, 5661.0, 0.6910400390625]
['model.relu.alpha_mask_8_0', 8192, 5452.0, 0.66552734375]
['model.relu.alpha_mask_9_0', 4096, 3310.0, 0.80810546875]
['model.relu.alpha_mask_10_0', 4096, 3120.0, 0.76171875]
['model.relu.alpha_mask_11_0', 4096, 3400.0, 0.830078125]
['model.relu.alpha_mask_12_0', 4096, 3083.0, 0.752685546875]
['model.relu.alpha_mask_13_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1972.0, 0.962890625]
['model.relu.alpha_mask_16_0', 2048, 1958.0, 0.9560546875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119635.0, 0.6349513841711957]
########## End ###########
06/02 06:48:26 PM | Train: [47/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:48:26 PM | layerwise density: [36884.0, 10024.0, 9701.0, 10015.0, 9633.0, 5886.0, 5464.0, 5661.0, 5452.0, 3310.0, 3120.0, 3400.0, 3083.0, 2029.0, 2043.0, 1972.0, 1958.0]
layerwise density percentage: ['0.563', '0.612', '0.592', '0.611', '0.588', '0.719', '0.667', '0.691', '0.666', '0.808', '0.762', '0.830', '0.753', '0.991', '0.998', '0.963', '0.956']
Global density: 0.6349514126777649
06/02 06:48:37 PM | Train: [47/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:48:37 PM | layerwise density: [36886.0, 10039.0, 9703.0, 10015.0, 9638.0, 5892.0, 5457.0, 5655.0, 5461.0, 3313.0, 3117.0, 3396.0, 3090.0, 2034.0, 2044.0, 1971.0, 1958.0]
layerwise density percentage: ['0.563', '0.613', '0.592', '0.611', '0.588', '0.719', '0.666', '0.690', '0.667', '0.809', '0.761', '0.829', '0.754', '0.993', '0.998', '0.962', '0.956']
Global density: 0.6351318359375
06/02 06:48:48 PM | Train: [47/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:48:48 PM | layerwise density: [36894.0, 10050.0, 9712.0, 10017.0, 9644.0, 5894.0, 5460.0, 5657.0, 5459.0, 3321.0, 3117.0, 3397.0, 3082.0, 2035.0, 2043.0, 1970.0, 1958.0]
layerwise density percentage: ['0.563', '0.613', '0.593', '0.611', '0.589', '0.719', '0.667', '0.691', '0.666', '0.811', '0.761', '0.829', '0.752', '0.994', '0.998', '0.962', '0.956']
Global density: 0.635349452495575
06/02 06:48:58 PM | Train: [47/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:48:58 PM | layerwise density: [36899.0, 10057.0, 9721.0, 10017.0, 9655.0, 5896.0, 5467.0, 5666.0, 5460.0, 3315.0, 3132.0, 3396.0, 3084.0, 2036.0, 2045.0, 1968.0, 1958.0]
layerwise density percentage: ['0.563', '0.614', '0.593', '0.611', '0.589', '0.720', '0.667', '0.692', '0.667', '0.809', '0.765', '0.829', '0.753', '0.994', '0.999', '0.961', '0.956']
Global density: 0.6356785297393799
06/02 06:49:08 PM | Train: [47/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:49:08 PM | layerwise density: [36910.0, 10059.0, 9729.0, 10019.0, 9654.0, 5903.0, 5460.0, 5672.0, 5463.0, 3319.0, 3143.0, 3397.0, 3087.0, 2035.0, 2045.0, 1976.0, 1958.0]
layerwise density percentage: ['0.563', '0.614', '0.594', '0.612', '0.589', '0.721', '0.667', '0.692', '0.667', '0.810', '0.767', '0.829', '0.754', '0.994', '0.999', '0.965', '0.956']
Global density: 0.635981023311615
06/02 06:49:08 PM | Train: [47/200] Final Prec@1 99.9500%
06/02 06:49:08 PM | Valid: [47/200] Step 000/078 Loss 1.140 Prec@(1,5) (75.0%, 91.4%)
06/02 06:49:10 PM | Valid: [47/200] Step 078/078 Loss 1.275 Prec@(1,5) (70.6%, 89.9%)
06/02 06:49:11 PM | Valid: [47/200] Final Prec@1 70.6000%
06/02 06:49:11 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36910.0, 0.563201904296875]
['model.relu.alpha_mask_1_0', 16384, 10061.0, 0.61407470703125]
['model.relu.alpha_mask_2_0', 16384, 9728.0, 0.59375]
['model.relu.alpha_mask_3_0', 16384, 10019.0, 0.61151123046875]
['model.relu.alpha_mask_4_0', 16384, 9653.0, 0.58917236328125]
['model.relu.alpha_mask_5_0', 8192, 5902.0, 0.720458984375]
['model.relu.alpha_mask_6_0', 8192, 5460.0, 0.66650390625]
['model.relu.alpha_mask_7_0', 8192, 5673.0, 0.6925048828125]
['model.relu.alpha_mask_8_0', 8192, 5464.0, 0.6669921875]
['model.relu.alpha_mask_9_0', 4096, 3319.0, 0.810302734375]
['model.relu.alpha_mask_10_0', 4096, 3143.0, 0.767333984375]
['model.relu.alpha_mask_11_0', 4096, 3397.0, 0.829345703125]
['model.relu.alpha_mask_12_0', 4096, 3086.0, 0.75341796875]
['model.relu.alpha_mask_13_0', 2048, 2035.0, 0.99365234375]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1976.0, 0.96484375]
['model.relu.alpha_mask_16_0', 2048, 1958.0, 0.9560546875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119829.0, 0.6359810207201086]
########## End ###########
06/02 06:49:11 PM | Train: [48/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:49:11 PM | layerwise density: [36910.0, 10061.0, 9728.0, 10019.0, 9653.0, 5902.0, 5460.0, 5673.0, 5464.0, 3319.0, 3143.0, 3397.0, 3086.0, 2035.0, 2045.0, 1976.0, 1958.0]
layerwise density percentage: ['0.563', '0.614', '0.594', '0.612', '0.589', '0.720', '0.667', '0.693', '0.667', '0.810', '0.767', '0.829', '0.753', '0.994', '0.999', '0.965', '0.956']
Global density: 0.635981023311615
06/02 06:49:22 PM | Train: [48/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:49:22 PM | layerwise density: [36912.0, 10064.0, 9732.0, 10026.0, 9665.0, 5917.0, 5454.0, 5677.0, 5466.0, 3313.0, 3143.0, 3394.0, 3082.0, 2034.0, 2047.0, 1962.0, 1958.0]
layerwise density percentage: ['0.563', '0.614', '0.594', '0.612', '0.590', '0.722', '0.666', '0.693', '0.667', '0.809', '0.767', '0.829', '0.752', '0.993', '1.000', '0.958', '0.956']
Global density: 0.6360712647438049
06/02 06:49:33 PM | Train: [48/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:49:33 PM | layerwise density: [36914.0, 10053.0, 9735.0, 10030.0, 9672.0, 5926.0, 5455.0, 5680.0, 5470.0, 3309.0, 3141.0, 3397.0, 3084.0, 2030.0, 2045.0, 1966.0, 1957.0]
layerwise density percentage: ['0.563', '0.614', '0.594', '0.612', '0.590', '0.723', '0.666', '0.693', '0.668', '0.808', '0.767', '0.829', '0.753', '0.991', '0.999', '0.960', '0.956']
Global density: 0.6361668109893799
06/02 06:49:43 PM | Train: [48/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:49:43 PM | layerwise density: [36910.0, 10057.0, 9738.0, 10029.0, 9677.0, 5922.0, 5465.0, 5686.0, 5474.0, 3319.0, 3161.0, 3401.0, 3087.0, 2030.0, 2045.0, 1970.0, 1957.0]
layerwise density percentage: ['0.563', '0.614', '0.594', '0.612', '0.591', '0.723', '0.667', '0.694', '0.668', '0.810', '0.772', '0.830', '0.754', '0.991', '0.999', '0.962', '0.956']
Global density: 0.6365064382553101
06/02 06:49:53 PM | Train: [48/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:49:53 PM | layerwise density: [36915.0, 10061.0, 9738.0, 10029.0, 9682.0, 5927.0, 5478.0, 5693.0, 5475.0, 3323.0, 3162.0, 3405.0, 3094.0, 2030.0, 2047.0, 1973.0, 1957.0]
layerwise density percentage: ['0.563', '0.614', '0.594', '0.612', '0.591', '0.724', '0.669', '0.695', '0.668', '0.811', '0.772', '0.831', '0.755', '0.991', '1.000', '0.963', '0.956']
Global density: 0.63683021068573
06/02 06:49:53 PM | Train: [48/200] Final Prec@1 99.9400%
06/02 06:49:53 PM | Valid: [48/200] Step 000/078 Loss 1.139 Prec@(1,5) (71.9%, 93.0%)
06/02 06:49:55 PM | Valid: [48/200] Step 078/078 Loss 1.270 Prec@(1,5) (70.7%, 90.2%)
06/02 06:49:55 PM | Valid: [48/200] Final Prec@1 70.6900%
06/02 06:49:55 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36915.0, 0.5632781982421875]
['model.relu.alpha_mask_1_0', 16384, 10061.0, 0.61407470703125]
['model.relu.alpha_mask_2_0', 16384, 9738.0, 0.5943603515625]
['model.relu.alpha_mask_3_0', 16384, 10029.0, 0.61212158203125]
['model.relu.alpha_mask_4_0', 16384, 9682.0, 0.5909423828125]
['model.relu.alpha_mask_5_0', 8192, 5927.0, 0.7235107421875]
['model.relu.alpha_mask_6_0', 8192, 5478.0, 0.668701171875]
['model.relu.alpha_mask_7_0', 8192, 5693.0, 0.6949462890625]
['model.relu.alpha_mask_8_0', 8192, 5475.0, 0.6683349609375]
['model.relu.alpha_mask_9_0', 4096, 3323.0, 0.811279296875]
['model.relu.alpha_mask_10_0', 4096, 3162.0, 0.77197265625]
['model.relu.alpha_mask_11_0', 4096, 3405.0, 0.831298828125]
['model.relu.alpha_mask_12_0', 4096, 3094.0, 0.75537109375]
['model.relu.alpha_mask_13_0', 2048, 2030.0, 0.9912109375]
['model.relu.alpha_mask_14_0', 2048, 2047.0, 0.99951171875]
['model.relu.alpha_mask_15_0', 2048, 1973.0, 0.96337890625]
['model.relu.alpha_mask_16_0', 2048, 1957.0, 0.95556640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119989.0, 0.6368302055027174]
########## End ###########
06/02 06:49:56 PM | Train: [49/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:49:56 PM | layerwise density: [36915.0, 10061.0, 9738.0, 10029.0, 9682.0, 5927.0, 5478.0, 5693.0, 5475.0, 3323.0, 3162.0, 3405.0, 3094.0, 2030.0, 2047.0, 1973.0, 1957.0]
layerwise density percentage: ['0.563', '0.614', '0.594', '0.612', '0.591', '0.724', '0.669', '0.695', '0.668', '0.811', '0.772', '0.831', '0.755', '0.991', '1.000', '0.963', '0.956']
Global density: 0.63683021068573
06/02 06:50:07 PM | Train: [49/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:50:07 PM | layerwise density: [36648.0, 9923.0, 9602.0, 9917.0, 9550.0, 5803.0, 5379.0, 5609.0, 5354.0, 3262.0, 3087.0, 3383.0, 2998.0, 2014.0, 2043.0, 1924.0, 1956.0]
layerwise density percentage: ['0.559', '0.606', '0.586', '0.605', '0.583', '0.708', '0.657', '0.685', '0.654', '0.796', '0.754', '0.826', '0.732', '0.983', '0.998', '0.939', '0.955']
Global density: 0.62867271900177
06/02 06:50:18 PM | Train: [49/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:50:18 PM | layerwise density: [36619.0, 9894.0, 9584.0, 9900.0, 9521.0, 5767.0, 5350.0, 5586.0, 5338.0, 3223.0, 3059.0, 3378.0, 2984.0, 2010.0, 2044.0, 1915.0, 1956.0]
layerwise density percentage: ['0.559', '0.604', '0.585', '0.604', '0.581', '0.704', '0.653', '0.682', '0.652', '0.787', '0.747', '0.825', '0.729', '0.981', '0.998', '0.935', '0.955']
Global density: 0.626953125
06/02 06:50:28 PM | Train: [49/80] Step 300/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:50:28 PM | layerwise density: [36598.0, 9873.0, 9570.0, 9882.0, 9501.0, 5759.0, 5335.0, 5569.0, 5319.0, 3232.0, 3048.0, 3379.0, 2989.0, 2015.0, 2042.0, 1930.0, 1956.0]
layerwise density percentage: ['0.558', '0.603', '0.584', '0.603', '0.580', '0.703', '0.651', '0.680', '0.649', '0.789', '0.744', '0.825', '0.730', '0.984', '0.997', '0.942', '0.955']
Global density: 0.6262578964233398
06/02 06:50:38 PM | Train: [49/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:50:38 PM | layerwise density: [36584.0, 9856.0, 9560.0, 9874.0, 9491.0, 5760.0, 5328.0, 5556.0, 5322.0, 3229.0, 3038.0, 3380.0, 2990.0, 2020.0, 2042.0, 1951.0, 1956.0]
layerwise density percentage: ['0.558', '0.602', '0.583', '0.603', '0.579', '0.703', '0.650', '0.678', '0.650', '0.788', '0.742', '0.825', '0.730', '0.986', '0.997', '0.953', '0.955']
Global density: 0.6259394288063049
06/02 06:50:38 PM | Train: [49/200] Final Prec@1 99.9380%
06/02 06:50:38 PM | Valid: [49/200] Step 000/078 Loss 1.146 Prec@(1,5) (75.0%, 93.8%)
06/02 06:50:40 PM | Valid: [49/200] Step 078/078 Loss 1.273 Prec@(1,5) (70.7%, 90.4%)
06/02 06:50:40 PM | Valid: [49/200] Final Prec@1 70.7300%
06/02 06:50:40 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36584.0, 0.5582275390625]
['model.relu.alpha_mask_1_0', 16384, 9856.0, 0.6015625]
['model.relu.alpha_mask_2_0', 16384, 9560.0, 0.58349609375]
['model.relu.alpha_mask_3_0', 16384, 9874.0, 0.6026611328125]
['model.relu.alpha_mask_4_0', 16384, 9491.0, 0.57928466796875]
['model.relu.alpha_mask_5_0', 8192, 5760.0, 0.703125]
['model.relu.alpha_mask_6_0', 8192, 5328.0, 0.650390625]
['model.relu.alpha_mask_7_0', 8192, 5556.0, 0.67822265625]
['model.relu.alpha_mask_8_0', 8192, 5322.0, 0.649658203125]
['model.relu.alpha_mask_9_0', 4096, 3228.0, 0.7880859375]
['model.relu.alpha_mask_10_0', 4096, 3038.0, 0.74169921875]
['model.relu.alpha_mask_11_0', 4096, 3380.0, 0.8251953125]
['model.relu.alpha_mask_12_0', 4096, 2990.0, 0.72998046875]
['model.relu.alpha_mask_13_0', 2048, 2020.0, 0.986328125]
['model.relu.alpha_mask_14_0', 2048, 2042.0, 0.9970703125]
['model.relu.alpha_mask_15_0', 2048, 1951.0, 0.95263671875]
['model.relu.alpha_mask_16_0', 2048, 1956.0, 0.955078125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 117936.0, 0.6259341032608695]
########## End ###########
06/02 06:50:41 PM | Train: [50/80] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:50:41 PM | layerwise density: [36584.0, 9856.0, 9560.0, 9874.0, 9491.0, 5760.0, 5328.0, 5556.0, 5322.0, 3228.0, 3038.0, 3380.0, 2990.0, 2020.0, 2042.0, 1951.0, 1956.0]
layerwise density percentage: ['0.558', '0.602', '0.583', '0.603', '0.579', '0.703', '0.650', '0.678', '0.650', '0.788', '0.742', '0.825', '0.730', '0.986', '0.997', '0.953', '0.955']
Global density: 0.6259341239929199
06/02 06:50:51 PM | Train: [50/80] Step 100/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:50:51 PM | layerwise density: [36575.0, 9852.0, 9557.0, 9865.0, 9491.0, 5760.0, 5332.0, 5552.0, 5323.0, 3226.0, 3043.0, 3375.0, 3007.0, 2023.0, 2043.0, 1961.0, 1956.0]
layerwise density percentage: ['0.558', '0.601', '0.583', '0.602', '0.579', '0.703', '0.651', '0.678', '0.650', '0.788', '0.743', '0.824', '0.734', '0.988', '0.998', '0.958', '0.955']
Global density: 0.625960648059845
06/02 06:51:02 PM | Train: [50/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:51:02 PM | layerwise density: [36565.0, 9847.0, 9556.0, 9863.0, 9486.0, 5762.0, 5336.0, 5555.0, 5328.0, 3241.0, 3056.0, 3372.0, 3020.0, 2026.0, 2042.0, 1971.0, 1956.0]
layerwise density percentage: ['0.558', '0.601', '0.583', '0.602', '0.579', '0.703', '0.651', '0.678', '0.650', '0.791', '0.746', '0.823', '0.737', '0.989', '0.997', '0.962', '0.955']
Global density: 0.6261782646179199
06/02 06:51:12 PM | Train: [50/80] Step 300/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:51:12 PM | layerwise density: [36551.0, 9842.0, 9546.0, 9860.0, 9484.0, 5773.0, 5344.0, 5561.0, 5346.0, 3260.0, 3063.0, 3377.0, 3018.0, 2027.0, 2040.0, 1973.0, 1956.0]
layerwise density percentage: ['0.558', '0.601', '0.583', '0.602', '0.579', '0.705', '0.652', '0.679', '0.653', '0.796', '0.748', '0.824', '0.737', '0.990', '0.996', '0.963', '0.955']
Global density: 0.6263852715492249
06/02 06:51:22 PM | Train: [50/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:51:22 PM | layerwise density: [36538.0, 9842.0, 9545.0, 9857.0, 9485.0, 5782.0, 5346.0, 5572.0, 5344.0, 3259.0, 3071.0, 3380.0, 3022.0, 2035.0, 2041.0, 1966.0, 1956.0]
layerwise density percentage: ['0.558', '0.601', '0.583', '0.602', '0.579', '0.706', '0.653', '0.680', '0.652', '0.796', '0.750', '0.825', '0.738', '0.994', '0.997', '0.960', '0.955']
Global density: 0.626491367816925
06/02 06:51:22 PM | Train: [50/200] Final Prec@1 99.9480%
06/02 06:51:22 PM | Valid: [50/200] Step 000/078 Loss 1.100 Prec@(1,5) (74.2%, 93.0%)
06/02 06:51:25 PM | Valid: [50/200] Step 078/078 Loss 1.279 Prec@(1,5) (70.9%, 90.3%)
06/02 06:51:25 PM | Valid: [50/200] Final Prec@1 70.8500%
06/02 06:51:25 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36538.0, 0.557525634765625]
['model.relu.alpha_mask_1_0', 16384, 9842.0, 0.6007080078125]
['model.relu.alpha_mask_2_0', 16384, 9545.0, 0.58258056640625]
['model.relu.alpha_mask_3_0', 16384, 9856.0, 0.6015625]
['model.relu.alpha_mask_4_0', 16384, 9485.0, 0.57891845703125]
['model.relu.alpha_mask_5_0', 8192, 5784.0, 0.7060546875]
['model.relu.alpha_mask_6_0', 8192, 5345.0, 0.6524658203125]
['model.relu.alpha_mask_7_0', 8192, 5572.0, 0.68017578125]
['model.relu.alpha_mask_8_0', 8192, 5345.0, 0.6524658203125]
['model.relu.alpha_mask_9_0', 4096, 3259.0, 0.795654296875]
['model.relu.alpha_mask_10_0', 4096, 3070.0, 0.74951171875]
['model.relu.alpha_mask_11_0', 4096, 3380.0, 0.8251953125]
['model.relu.alpha_mask_12_0', 4096, 3022.0, 0.73779296875]
['model.relu.alpha_mask_13_0', 2048, 2036.0, 0.994140625]
['model.relu.alpha_mask_14_0', 2048, 2041.0, 0.99658203125]
['model.relu.alpha_mask_15_0', 2048, 1967.0, 0.96044921875]
['model.relu.alpha_mask_16_0', 2048, 1956.0, 0.955078125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118043.0, 0.6265019955842391]
########## End ###########
06/02 06:51:26 PM | Train: [51/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:51:26 PM | layerwise density: [36538.0, 9842.0, 9545.0, 9856.0, 9485.0, 5784.0, 5345.0, 5572.0, 5345.0, 3259.0, 3070.0, 3380.0, 3022.0, 2036.0, 2041.0, 1967.0, 1956.0]
layerwise density percentage: ['0.558', '0.601', '0.583', '0.602', '0.579', '0.706', '0.652', '0.680', '0.652', '0.796', '0.750', '0.825', '0.738', '0.994', '0.997', '0.960', '0.955']
Global density: 0.6265020370483398
06/02 06:51:36 PM | Train: [51/80] Step 100/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:51:36 PM | layerwise density: [36540.0, 9842.0, 9544.0, 9855.0, 9488.0, 5787.0, 5355.0, 5574.0, 5346.0, 3261.0, 3080.0, 3390.0, 3036.0, 2031.0, 2046.0, 1955.0, 1956.0]
layerwise density percentage: ['0.558', '0.601', '0.583', '0.602', '0.579', '0.706', '0.654', '0.680', '0.653', '0.796', '0.752', '0.828', '0.741', '0.992', '0.999', '0.955', '0.955']
Global density: 0.62673020362854
06/02 06:51:47 PM | Train: [51/80] Step 200/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:51:47 PM | layerwise density: [36547.0, 9846.0, 9547.0, 9855.0, 9489.0, 5804.0, 5369.0, 5583.0, 5357.0, 3277.0, 3082.0, 3392.0, 3030.0, 2033.0, 2044.0, 1960.0, 1957.0]
layerwise density percentage: ['0.558', '0.601', '0.583', '0.602', '0.579', '0.708', '0.655', '0.682', '0.654', '0.800', '0.752', '0.828', '0.740', '0.993', '0.998', '0.957', '0.956']
Global density: 0.62718665599823
06/02 06:51:57 PM | Train: [51/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:51:57 PM | layerwise density: [36542.0, 9856.0, 9554.0, 9851.0, 9487.0, 5807.0, 5374.0, 5588.0, 5362.0, 3287.0, 3088.0, 3389.0, 3035.0, 2038.0, 2044.0, 1959.0, 1957.0]
layerwise density percentage: ['0.558', '0.602', '0.583', '0.601', '0.579', '0.709', '0.656', '0.682', '0.655', '0.802', '0.754', '0.827', '0.741', '0.995', '0.998', '0.957', '0.956']
Global density: 0.62743079662323
06/02 06:52:07 PM | Train: [51/80] Step 390/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:52:07 PM | layerwise density: [36539.0, 9864.0, 9554.0, 9855.0, 9492.0, 5819.0, 5375.0, 5591.0, 5370.0, 3282.0, 3094.0, 3387.0, 3048.0, 2036.0, 2046.0, 1966.0, 1957.0]
layerwise density percentage: ['0.558', '0.602', '0.583', '0.602', '0.579', '0.710', '0.656', '0.682', '0.656', '0.801', '0.755', '0.827', '0.744', '0.994', '0.999', '0.960', '0.956']
Global density: 0.6277333498001099
06/02 06:52:07 PM | Train: [51/200] Final Prec@1 99.9520%
06/02 06:52:07 PM | Valid: [51/200] Step 000/078 Loss 1.122 Prec@(1,5) (74.2%, 94.5%)
06/02 06:52:10 PM | Valid: [51/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.5%, 90.2%)
06/02 06:52:10 PM | Valid: [51/200] Final Prec@1 70.5200%
06/02 06:52:10 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36539.0, 0.5575408935546875]
['model.relu.alpha_mask_1_0', 16384, 9865.0, 0.60211181640625]
['model.relu.alpha_mask_2_0', 16384, 9554.0, 0.5831298828125]
['model.relu.alpha_mask_3_0', 16384, 9855.0, 0.60150146484375]
['model.relu.alpha_mask_4_0', 16384, 9493.0, 0.57940673828125]
['model.relu.alpha_mask_5_0', 8192, 5819.0, 0.7103271484375]
['model.relu.alpha_mask_6_0', 8192, 5375.0, 0.6561279296875]
['model.relu.alpha_mask_7_0', 8192, 5592.0, 0.6826171875]
['model.relu.alpha_mask_8_0', 8192, 5370.0, 0.655517578125]
['model.relu.alpha_mask_9_0', 4096, 3282.0, 0.80126953125]
['model.relu.alpha_mask_10_0', 4096, 3094.0, 0.75537109375]
['model.relu.alpha_mask_11_0', 4096, 3387.0, 0.826904296875]
['model.relu.alpha_mask_12_0', 4096, 3049.0, 0.744384765625]
['model.relu.alpha_mask_13_0', 2048, 2036.0, 0.994140625]
['model.relu.alpha_mask_14_0', 2048, 2046.0, 0.9990234375]
['model.relu.alpha_mask_15_0', 2048, 1966.0, 0.9599609375]
['model.relu.alpha_mask_16_0', 2048, 1957.0, 0.95556640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118279.0, 0.6277545431385869]
########## End ###########
06/02 06:52:11 PM | Train: [52/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 06:52:11 PM | layerwise density: [36539.0, 9865.0, 9554.0, 9855.0, 9493.0, 5819.0, 5375.0, 5592.0, 5370.0, 3282.0, 3094.0, 3387.0, 3049.0, 2036.0, 2046.0, 1966.0, 1957.0]
layerwise density percentage: ['0.558', '0.602', '0.583', '0.602', '0.579', '0.710', '0.656', '0.683', '0.656', '0.801', '0.755', '0.827', '0.744', '0.994', '0.999', '0.960', '0.956']
Global density: 0.6277545690536499
06/02 06:52:21 PM | Train: [52/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:52:21 PM | layerwise density: [36539.0, 9875.0, 9555.0, 9861.0, 9495.0, 5834.0, 5381.0, 5605.0, 5373.0, 3287.0, 3101.0, 3386.0, 3053.0, 2036.0, 2045.0, 1963.0, 1957.0]
layerwise density percentage: ['0.558', '0.603', '0.583', '0.602', '0.580', '0.712', '0.657', '0.684', '0.656', '0.802', '0.757', '0.827', '0.745', '0.994', '0.999', '0.958', '0.956']
Global density: 0.6281101703643799
06/02 06:52:32 PM | Train: [52/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:52:32 PM | layerwise density: [36538.0, 9873.0, 9559.0, 9866.0, 9500.0, 5841.0, 5387.0, 5606.0, 5376.0, 3302.0, 3109.0, 3386.0, 3053.0, 2037.0, 2045.0, 1961.0, 1957.0]
layerwise density percentage: ['0.558', '0.603', '0.583', '0.602', '0.580', '0.713', '0.658', '0.684', '0.656', '0.806', '0.759', '0.827', '0.745', '0.995', '0.999', '0.958', '0.956']
Global density: 0.6283755302429199
06/02 06:52:43 PM | Train: [52/80] Step 300/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 06:52:43 PM | layerwise density: [36543.0, 9883.0, 9564.0, 9866.0, 9512.0, 5844.0, 5385.0, 5613.0, 5381.0, 3316.0, 3121.0, 3392.0, 3061.0, 2031.0, 2046.0, 1960.0, 1957.0]
layerwise density percentage: ['0.558', '0.603', '0.584', '0.602', '0.581', '0.713', '0.657', '0.685', '0.657', '0.810', '0.762', '0.828', '0.747', '0.992', '0.999', '0.957', '0.956']
Global density: 0.62879478931427
06/02 06:52:52 PM | Train: [52/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:52:52 PM | layerwise density: [36542.0, 9894.0, 9564.0, 9869.0, 9518.0, 5859.0, 5387.0, 5619.0, 5383.0, 3315.0, 3133.0, 3397.0, 3078.0, 2026.0, 2043.0, 1958.0, 1957.0]
layerwise density percentage: ['0.558', '0.604', '0.584', '0.602', '0.581', '0.715', '0.658', '0.686', '0.657', '0.809', '0.765', '0.829', '0.751', '0.989', '0.998', '0.956', '0.956']
Global density: 0.629150390625
06/02 06:52:53 PM | Train: [52/200] Final Prec@1 99.9440%
06/02 06:52:53 PM | Valid: [52/200] Step 000/078 Loss 1.187 Prec@(1,5) (71.9%, 94.5%)
06/02 06:52:55 PM | Valid: [52/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.8%, 89.9%)
06/02 06:52:55 PM | Valid: [52/200] Final Prec@1 70.8200%
06/02 06:52:55 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36542.0, 0.557586669921875]
['model.relu.alpha_mask_1_0', 16384, 9894.0, 0.6038818359375]
['model.relu.alpha_mask_2_0', 16384, 9565.0, 0.58380126953125]
['model.relu.alpha_mask_3_0', 16384, 9870.0, 0.6024169921875]
['model.relu.alpha_mask_4_0', 16384, 9517.0, 0.58087158203125]
['model.relu.alpha_mask_5_0', 8192, 5859.0, 0.7152099609375]
['model.relu.alpha_mask_6_0', 8192, 5386.0, 0.657470703125]
['model.relu.alpha_mask_7_0', 8192, 5619.0, 0.6859130859375]
['model.relu.alpha_mask_8_0', 8192, 5384.0, 0.6572265625]
['model.relu.alpha_mask_9_0', 4096, 3315.0, 0.809326171875]
['model.relu.alpha_mask_10_0', 4096, 3133.0, 0.764892578125]
['model.relu.alpha_mask_11_0', 4096, 3397.0, 0.829345703125]
['model.relu.alpha_mask_12_0', 4096, 3077.0, 0.751220703125]
['model.relu.alpha_mask_13_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1958.0, 0.9560546875]
['model.relu.alpha_mask_16_0', 2048, 1957.0, 0.95556640625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118542.0, 0.629150390625]
########## End ###########
06/02 06:52:56 PM | Train: [53/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:52:56 PM | layerwise density: [36542.0, 9894.0, 9565.0, 9870.0, 9517.0, 5859.0, 5386.0, 5619.0, 5384.0, 3315.0, 3133.0, 3397.0, 3077.0, 2026.0, 2043.0, 1958.0, 1957.0]
layerwise density percentage: ['0.558', '0.604', '0.584', '0.602', '0.581', '0.715', '0.657', '0.686', '0.657', '0.809', '0.765', '0.829', '0.751', '0.989', '0.998', '0.956', '0.956']
Global density: 0.629150390625
06/02 06:53:07 PM | Train: [53/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:53:07 PM | layerwise density: [36545.0, 9896.0, 9567.0, 9878.0, 9525.0, 5872.0, 5385.0, 5624.0, 5401.0, 3318.0, 3138.0, 3401.0, 3090.0, 2025.0, 2045.0, 1959.0, 1958.0]
layerwise density percentage: ['0.558', '0.604', '0.584', '0.603', '0.581', '0.717', '0.657', '0.687', '0.659', '0.810', '0.766', '0.830', '0.754', '0.989', '0.999', '0.957', '0.956']
Global density: 0.6296015381813049
06/02 06:53:17 PM | Train: [53/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:53:17 PM | layerwise density: [36544.0, 9909.0, 9573.0, 9884.0, 9526.0, 5889.0, 5394.0, 5635.0, 5403.0, 3318.0, 3136.0, 3405.0, 3093.0, 2023.0, 2043.0, 1967.0, 1958.0]
layerwise density percentage: ['0.558', '0.605', '0.584', '0.603', '0.581', '0.719', '0.658', '0.688', '0.660', '0.810', '0.766', '0.831', '0.755', '0.988', '0.998', '0.960', '0.956']
Global density: 0.629988968372345
06/02 06:53:28 PM | Train: [53/80] Step 300/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:53:28 PM | layerwise density: [36540.0, 9913.0, 9569.0, 9884.0, 9528.0, 5891.0, 5399.0, 5638.0, 5411.0, 3315.0, 3136.0, 3410.0, 3097.0, 2028.0, 2044.0, 1962.0, 1959.0]
layerwise density percentage: ['0.558', '0.605', '0.584', '0.603', '0.582', '0.719', '0.659', '0.688', '0.661', '0.809', '0.766', '0.833', '0.756', '0.990', '0.998', '0.958', '0.957']
Global density: 0.63011634349823
06/02 06:53:36 PM | Train: [53/80] Step 390/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:53:36 PM | layerwise density: [36534.0, 9917.0, 9576.0, 9884.0, 9538.0, 5898.0, 5406.0, 5647.0, 5414.0, 3323.0, 3140.0, 3412.0, 3101.0, 2029.0, 2043.0, 1959.0, 1958.0]
layerwise density percentage: ['0.557', '0.605', '0.584', '0.603', '0.582', '0.720', '0.660', '0.689', '0.661', '0.811', '0.767', '0.833', '0.757', '0.991', '0.998', '0.957', '0.956']
Global density: 0.6304082870483398
06/02 06:53:37 PM | Train: [53/200] Final Prec@1 99.9460%
06/02 06:53:37 PM | Valid: [53/200] Step 000/078 Loss 1.150 Prec@(1,5) (73.4%, 92.2%)
06/02 06:53:39 PM | Valid: [53/200] Step 078/078 Loss 1.274 Prec@(1,5) (70.6%, 90.0%)
06/02 06:53:39 PM | Valid: [53/200] Final Prec@1 70.6400%
06/02 06:53:39 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36534.0, 0.557464599609375]
['model.relu.alpha_mask_1_0', 16384, 9917.0, 0.60528564453125]
['model.relu.alpha_mask_2_0', 16384, 9575.0, 0.58441162109375]
['model.relu.alpha_mask_3_0', 16384, 9883.0, 0.60321044921875]
['model.relu.alpha_mask_4_0', 16384, 9539.0, 0.58221435546875]
['model.relu.alpha_mask_5_0', 8192, 5899.0, 0.7200927734375]
['model.relu.alpha_mask_6_0', 8192, 5404.0, 0.65966796875]
['model.relu.alpha_mask_7_0', 8192, 5647.0, 0.6893310546875]
['model.relu.alpha_mask_8_0', 8192, 5413.0, 0.6607666015625]
['model.relu.alpha_mask_9_0', 4096, 3322.0, 0.81103515625]
['model.relu.alpha_mask_10_0', 4096, 3139.0, 0.766357421875]
['model.relu.alpha_mask_11_0', 4096, 3412.0, 0.8330078125]
['model.relu.alpha_mask_12_0', 4096, 3102.0, 0.75732421875]
['model.relu.alpha_mask_13_0', 2048, 2028.0, 0.990234375]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1959.0, 0.95654296875]
['model.relu.alpha_mask_16_0', 2048, 1958.0, 0.9560546875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118774.0, 0.6303817085597826]
########## End ###########
06/02 06:53:40 PM | Train: [54/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 06:53:40 PM | layerwise density: [36534.0, 9917.0, 9575.0, 9883.0, 9539.0, 5899.0, 5404.0, 5647.0, 5413.0, 3322.0, 3139.0, 3412.0, 3102.0, 2028.0, 2043.0, 1959.0, 1958.0]
layerwise density percentage: ['0.557', '0.605', '0.584', '0.603', '0.582', '0.720', '0.660', '0.689', '0.661', '0.811', '0.766', '0.833', '0.757', '0.990', '0.998', '0.957', '0.956']
Global density: 0.63038170337677
06/02 06:53:51 PM | Train: [54/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:53:51 PM | layerwise density: [36536.0, 9920.0, 9582.0, 9890.0, 9541.0, 5898.0, 5414.0, 5652.0, 5420.0, 3326.0, 3146.0, 3413.0, 3108.0, 2029.0, 2043.0, 1958.0, 1958.0]
layerwise density percentage: ['0.557', '0.605', '0.585', '0.604', '0.582', '0.720', '0.661', '0.690', '0.662', '0.812', '0.768', '0.833', '0.759', '0.991', '0.998', '0.956', '0.956']
Global density: 0.6307001709938049
06/02 06:54:02 PM | Train: [54/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:54:02 PM | layerwise density: [36539.0, 9925.0, 9584.0, 9895.0, 9540.0, 5894.0, 5427.0, 5657.0, 5417.0, 3331.0, 3153.0, 3412.0, 3111.0, 2027.0, 2046.0, 1967.0, 1958.0]
layerwise density percentage: ['0.558', '0.606', '0.585', '0.604', '0.582', '0.719', '0.662', '0.691', '0.661', '0.813', '0.770', '0.833', '0.760', '0.990', '0.999', '0.960', '0.956']
Global density: 0.63096022605896
06/02 06:54:12 PM | Train: [54/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:54:12 PM | layerwise density: [36541.0, 9923.0, 9587.0, 9899.0, 9539.0, 5897.0, 5431.0, 5662.0, 5425.0, 3342.0, 3161.0, 3414.0, 3105.0, 2033.0, 2044.0, 1968.0, 1958.0]
layerwise density percentage: ['0.558', '0.606', '0.585', '0.604', '0.582', '0.720', '0.663', '0.691', '0.662', '0.816', '0.772', '0.833', '0.758', '0.993', '0.998', '0.961', '0.956']
Global density: 0.63120436668396
06/02 06:54:22 PM | Train: [54/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:54:22 PM | layerwise density: [36542.0, 9927.0, 9591.0, 9906.0, 9542.0, 5918.0, 5432.0, 5669.0, 5431.0, 3335.0, 3161.0, 3417.0, 3093.0, 2036.0, 2045.0, 1966.0, 1958.0]
layerwise density percentage: ['0.558', '0.606', '0.585', '0.605', '0.582', '0.722', '0.663', '0.692', '0.663', '0.814', '0.772', '0.834', '0.755', '0.994', '0.999', '0.960', '0.956']
Global density: 0.6314166784286499
06/02 06:54:22 PM | Train: [54/200] Final Prec@1 99.9520%
06/02 06:54:22 PM | Valid: [54/200] Step 000/078 Loss 1.138 Prec@(1,5) (74.2%, 90.6%)
06/02 06:54:24 PM | Valid: [54/200] Step 078/078 Loss 1.265 Prec@(1,5) (70.8%, 90.3%)
06/02 06:54:24 PM | Valid: [54/200] Final Prec@1 70.7600%
06/02 06:54:24 PM | Current mask training best Prec@1 = 70.8500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36542.0, 0.557586669921875]
['model.relu.alpha_mask_1_0', 16384, 9927.0, 0.60589599609375]
['model.relu.alpha_mask_2_0', 16384, 9591.0, 0.58538818359375]
['model.relu.alpha_mask_3_0', 16384, 9906.0, 0.6046142578125]
['model.relu.alpha_mask_4_0', 16384, 9542.0, 0.5823974609375]
['model.relu.alpha_mask_5_0', 8192, 5918.0, 0.722412109375]
['model.relu.alpha_mask_6_0', 8192, 5432.0, 0.6630859375]
['model.relu.alpha_mask_7_0', 8192, 5669.0, 0.6920166015625]
['model.relu.alpha_mask_8_0', 8192, 5431.0, 0.6629638671875]
['model.relu.alpha_mask_9_0', 4096, 3335.0, 0.814208984375]
['model.relu.alpha_mask_10_0', 4096, 3161.0, 0.771728515625]
['model.relu.alpha_mask_11_0', 4096, 3417.0, 0.834228515625]
['model.relu.alpha_mask_12_0', 4096, 3093.0, 0.755126953125]
['model.relu.alpha_mask_13_0', 2048, 2036.0, 0.994140625]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1966.0, 0.9599609375]
['model.relu.alpha_mask_16_0', 2048, 1958.0, 0.9560546875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 118969.0, 0.6314166525135869]
########## End ###########
06/02 06:54:25 PM | Train: [55/80] Step 000/390 Loss 0.013 Prec@(1,5) (99.2%, 100.0%)
06/02 06:54:25 PM | layerwise density: [36542.0, 9927.0, 9591.0, 9906.0, 9542.0, 5918.0, 5432.0, 5669.0, 5431.0, 3335.0, 3161.0, 3417.0, 3093.0, 2036.0, 2045.0, 1966.0, 1958.0]
layerwise density percentage: ['0.558', '0.606', '0.585', '0.605', '0.582', '0.722', '0.663', '0.692', '0.663', '0.814', '0.772', '0.834', '0.755', '0.994', '0.999', '0.960', '0.956']
Global density: 0.6314166784286499
06/02 06:54:36 PM | Train: [55/80] Step 100/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:54:36 PM | layerwise density: [36549.0, 9937.0, 9596.0, 9913.0, 9547.0, 5930.0, 5442.0, 5682.0, 5434.0, 3339.0, 3160.0, 3417.0, 3099.0, 2036.0, 2046.0, 1962.0, 1958.0]
layerwise density percentage: ['0.558', '0.607', '0.586', '0.605', '0.583', '0.724', '0.664', '0.694', '0.663', '0.815', '0.771', '0.834', '0.757', '0.994', '0.999', '0.958', '0.956']
Global density: 0.631830632686615
06/02 06:54:46 PM | Train: [55/80] Step 200/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:54:46 PM | layerwise density: [36550.0, 9948.0, 9601.0, 9922.0, 9555.0, 5930.0, 5443.0, 5690.0, 5437.0, 3345.0, 3169.0, 3417.0, 3111.0, 2031.0, 2043.0, 1964.0, 1959.0]
layerwise density percentage: ['0.558', '0.607', '0.586', '0.606', '0.583', '0.724', '0.664', '0.695', '0.664', '0.817', '0.774', '0.834', '0.760', '0.992', '0.998', '0.959', '0.957']
Global density: 0.63219153881073
06/02 06:54:57 PM | Train: [55/80] Step 300/390 Loss 0.005 Prec@(1,5) (99.9%, 100.0%)
06/02 06:54:57 PM | layerwise density: [36552.0, 9948.0, 9612.0, 9932.0, 9560.0, 5946.0, 5448.0, 5687.0, 5440.0, 3348.0, 3173.0, 3417.0, 3117.0, 2030.0, 2042.0, 1964.0, 1960.0]
layerwise density percentage: ['0.558', '0.607', '0.587', '0.606', '0.583', '0.726', '0.665', '0.694', '0.664', '0.817', '0.775', '0.834', '0.761', '0.991', '0.997', '0.959', '0.957']
Global density: 0.6325153112411499
06/02 06:55:06 PM | Train: [55/80] Step 390/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 06:55:06 PM | layerwise density: [36554.0, 9956.0, 9618.0, 9938.0, 9562.0, 5958.0, 5447.0, 5684.0, 5440.0, 3361.0, 3168.0, 3415.0, 3117.0, 2034.0, 2045.0, 1959.0, 1960.0]
layerwise density percentage: ['0.558', '0.608', '0.587', '0.607', '0.584', '0.727', '0.665', '0.694', '0.664', '0.821', '0.773', '0.834', '0.761', '0.993', '0.999', '0.957', '0.957']
Global density: 0.6327276229858398
06/02 06:55:06 PM | Train: [55/200] Final Prec@1 99.9480%
06/02 06:55:07 PM | Valid: [55/200] Step 000/078 Loss 1.141 Prec@(1,5) (73.4%, 92.2%)
06/02 06:55:09 PM | Valid: [55/200] Step 078/078 Loss 1.260 Prec@(1,5) (71.0%, 90.5%)
06/02 06:55:09 PM | Valid: [55/200] Final Prec@1 70.9700%
06/02 06:55:10 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36554.0, 0.557769775390625]
['model.relu.alpha_mask_1_0', 16384, 9956.0, 0.607666015625]
['model.relu.alpha_mask_2_0', 16384, 9618.0, 0.5870361328125]
['model.relu.alpha_mask_3_0', 16384, 9938.0, 0.6065673828125]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 5958.0, 0.727294921875]
['model.relu.alpha_mask_6_0', 8192, 5447.0, 0.6649169921875]
['model.relu.alpha_mask_7_0', 8192, 5684.0, 0.69384765625]
['model.relu.alpha_mask_8_0', 8192, 5440.0, 0.6640625]
['model.relu.alpha_mask_9_0', 4096, 3361.0, 0.820556640625]
['model.relu.alpha_mask_10_0', 4096, 3168.0, 0.7734375]
['model.relu.alpha_mask_11_0', 4096, 3415.0, 0.833740234375]
['model.relu.alpha_mask_12_0', 4096, 3118.0, 0.76123046875]
['model.relu.alpha_mask_13_0', 2048, 2034.0, 0.9931640625]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1959.0, 0.95654296875]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119217.0, 0.6327328889266305]
########## End ###########
06/02 06:55:11 PM | Train: [56/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 06:55:11 PM | layerwise density: [36554.0, 9956.0, 9618.0, 9938.0, 9562.0, 5958.0, 5447.0, 5684.0, 5440.0, 3361.0, 3168.0, 3415.0, 3118.0, 2034.0, 2045.0, 1959.0, 1960.0]
layerwise density percentage: ['0.558', '0.608', '0.587', '0.607', '0.584', '0.727', '0.665', '0.694', '0.664', '0.821', '0.773', '0.834', '0.761', '0.993', '0.999', '0.957', '0.957']
Global density: 0.6327329277992249
06/02 06:55:21 PM | Train: [56/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:55:21 PM | layerwise density: [36556.0, 9961.0, 9624.0, 9944.0, 9567.0, 5960.0, 5451.0, 5682.0, 5441.0, 3358.0, 3169.0, 3416.0, 3117.0, 2034.0, 2046.0, 1964.0, 1960.0]
layerwise density percentage: ['0.558', '0.608', '0.587', '0.607', '0.584', '0.728', '0.665', '0.694', '0.664', '0.820', '0.774', '0.834', '0.761', '0.993', '0.999', '0.959', '0.957']
Global density: 0.632908046245575
06/02 06:55:32 PM | Train: [56/80] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 06:55:32 PM | layerwise density: [36558.0, 9969.0, 9631.0, 9945.0, 9573.0, 5969.0, 5455.0, 5684.0, 5439.0, 3360.0, 3167.0, 3412.0, 3106.0, 2037.0, 2043.0, 1954.0, 1960.0]
layerwise density percentage: ['0.558', '0.608', '0.588', '0.607', '0.584', '0.729', '0.666', '0.694', '0.664', '0.820', '0.773', '0.833', '0.758', '0.995', '0.998', '0.954', '0.957']
Global density: 0.6329717636108398
06/02 06:55:43 PM | Train: [56/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:55:43 PM | layerwise density: [36562.0, 9979.0, 9636.0, 9954.0, 9574.0, 5966.0, 5453.0, 5691.0, 5447.0, 3357.0, 3164.0, 3412.0, 3096.0, 2035.0, 2043.0, 1945.0, 1960.0]
layerwise density percentage: ['0.558', '0.609', '0.588', '0.608', '0.584', '0.728', '0.666', '0.695', '0.665', '0.820', '0.772', '0.833', '0.756', '0.994', '0.998', '0.950', '0.957']
Global density: 0.63303542137146
06/02 06:55:52 PM | Train: [56/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:55:52 PM | layerwise density: [36566.0, 9987.0, 9640.0, 9954.0, 9570.0, 5964.0, 5463.0, 5692.0, 5445.0, 3358.0, 3165.0, 3412.0, 3100.0, 2033.0, 2043.0, 1946.0, 1960.0]
layerwise density percentage: ['0.558', '0.610', '0.588', '0.608', '0.584', '0.728', '0.667', '0.695', '0.665', '0.820', '0.773', '0.833', '0.757', '0.993', '0.998', '0.950', '0.957']
Global density: 0.633162796497345
06/02 06:55:52 PM | Train: [56/200] Final Prec@1 99.9500%
06/02 06:55:53 PM | Valid: [56/200] Step 000/078 Loss 1.177 Prec@(1,5) (71.9%, 91.4%)
06/02 06:55:55 PM | Valid: [56/200] Step 078/078 Loss 1.278 Prec@(1,5) (70.6%, 90.5%)
06/02 06:55:55 PM | Valid: [56/200] Final Prec@1 70.5700%
06/02 06:55:55 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36566.0, 0.557952880859375]
['model.relu.alpha_mask_1_0', 16384, 9988.0, 0.609619140625]
['model.relu.alpha_mask_2_0', 16384, 9640.0, 0.58837890625]
['model.relu.alpha_mask_3_0', 16384, 9954.0, 0.6075439453125]
['model.relu.alpha_mask_4_0', 16384, 9570.0, 0.5841064453125]
['model.relu.alpha_mask_5_0', 8192, 5964.0, 0.72802734375]
['model.relu.alpha_mask_6_0', 8192, 5463.0, 0.6668701171875]
['model.relu.alpha_mask_7_0', 8192, 5692.0, 0.69482421875]
['model.relu.alpha_mask_8_0', 8192, 5445.0, 0.6646728515625]
['model.relu.alpha_mask_9_0', 4096, 3358.0, 0.81982421875]
['model.relu.alpha_mask_10_0', 4096, 3166.0, 0.77294921875]
['model.relu.alpha_mask_11_0', 4096, 3412.0, 0.8330078125]
['model.relu.alpha_mask_12_0', 4096, 3100.0, 0.7568359375]
['model.relu.alpha_mask_13_0', 2048, 2033.0, 0.99267578125]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1946.0, 0.9501953125]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119300.0, 0.6331734035326086]
########## End ###########
06/02 06:55:56 PM | Train: [57/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 06:55:56 PM | layerwise density: [36566.0, 9988.0, 9640.0, 9954.0, 9570.0, 5964.0, 5463.0, 5692.0, 5445.0, 3358.0, 3166.0, 3412.0, 3100.0, 2033.0, 2043.0, 1946.0, 1960.0]
layerwise density percentage: ['0.558', '0.610', '0.588', '0.608', '0.584', '0.728', '0.667', '0.695', '0.665', '0.820', '0.773', '0.833', '0.757', '0.993', '0.998', '0.950', '0.957']
Global density: 0.633173406124115
06/02 06:56:06 PM | Train: [57/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:56:06 PM | layerwise density: [36567.0, 9993.0, 9638.0, 9959.0, 9573.0, 5976.0, 5461.0, 5691.0, 5456.0, 3360.0, 3160.0, 3413.0, 3106.0, 2032.0, 2043.0, 1958.0, 1960.0]
layerwise density percentage: ['0.558', '0.610', '0.588', '0.608', '0.584', '0.729', '0.667', '0.695', '0.666', '0.820', '0.771', '0.833', '0.758', '0.992', '0.998', '0.956', '0.957']
Global density: 0.633417546749115
06/02 06:56:17 PM | Train: [57/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:56:17 PM | layerwise density: [36570.0, 10000.0, 9642.0, 9965.0, 9576.0, 5977.0, 5467.0, 5689.0, 5454.0, 3357.0, 3159.0, 3413.0, 3113.0, 2033.0, 2044.0, 1956.0, 1960.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.608', '0.584', '0.730', '0.667', '0.694', '0.666', '0.820', '0.771', '0.833', '0.760', '0.993', '0.998', '0.955', '0.957']
Global density: 0.633571445941925
06/02 06:56:28 PM | Train: [57/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:56:28 PM | layerwise density: [36572.0, 9998.0, 9646.0, 9968.0, 9574.0, 5981.0, 5465.0, 5691.0, 5462.0, 3357.0, 3163.0, 3414.0, 3122.0, 2035.0, 2045.0, 1960.0, 1960.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.608', '0.584', '0.730', '0.667', '0.695', '0.667', '0.820', '0.772', '0.833', '0.762', '0.994', '0.999', '0.957', '0.957']
Global density: 0.633773148059845
06/02 06:56:38 PM | Train: [57/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:56:38 PM | layerwise density: [36581.0, 9994.0, 9649.0, 9966.0, 9574.0, 5986.0, 5465.0, 5698.0, 5466.0, 3359.0, 3158.0, 3416.0, 3127.0, 2037.0, 2045.0, 1961.0, 1960.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.608', '0.584', '0.731', '0.667', '0.696', '0.667', '0.820', '0.771', '0.834', '0.763', '0.995', '0.999', '0.958', '0.957']
Global density: 0.633927047252655
06/02 06:56:38 PM | Train: [57/200] Final Prec@1 99.9540%
06/02 06:56:38 PM | Valid: [57/200] Step 000/078 Loss 1.190 Prec@(1,5) (72.7%, 91.4%)
06/02 06:56:40 PM | Valid: [57/200] Step 078/078 Loss 1.267 Prec@(1,5) (70.5%, 90.1%)
06/02 06:56:40 PM | Valid: [57/200] Final Prec@1 70.4600%
06/02 06:56:40 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36581.0, 0.5581817626953125]
['model.relu.alpha_mask_1_0', 16384, 9994.0, 0.6099853515625]
['model.relu.alpha_mask_2_0', 16384, 9649.0, 0.58892822265625]
['model.relu.alpha_mask_3_0', 16384, 9966.0, 0.6082763671875]
['model.relu.alpha_mask_4_0', 16384, 9575.0, 0.58441162109375]
['model.relu.alpha_mask_5_0', 8192, 5985.0, 0.7305908203125]
['model.relu.alpha_mask_6_0', 8192, 5465.0, 0.6671142578125]
['model.relu.alpha_mask_7_0', 8192, 5697.0, 0.6954345703125]
['model.relu.alpha_mask_8_0', 8192, 5466.0, 0.667236328125]
['model.relu.alpha_mask_9_0', 4096, 3359.0, 0.820068359375]
['model.relu.alpha_mask_10_0', 4096, 3157.0, 0.770751953125]
['model.relu.alpha_mask_11_0', 4096, 3416.0, 0.833984375]
['model.relu.alpha_mask_12_0', 4096, 3128.0, 0.763671875]
['model.relu.alpha_mask_13_0', 2048, 2037.0, 0.99462890625]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1961.0, 0.95751953125]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119441.0, 0.6339217476222826]
########## End ###########
06/02 06:56:41 PM | Train: [58/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 06:56:41 PM | layerwise density: [36581.0, 9994.0, 9649.0, 9966.0, 9575.0, 5985.0, 5465.0, 5697.0, 5466.0, 3359.0, 3157.0, 3416.0, 3128.0, 2037.0, 2045.0, 1961.0, 1960.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.608', '0.584', '0.731', '0.667', '0.695', '0.667', '0.820', '0.771', '0.834', '0.764', '0.995', '0.999', '0.958', '0.957']
Global density: 0.63392174243927
06/02 06:56:52 PM | Train: [58/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:56:52 PM | layerwise density: [36586.0, 9993.0, 9647.0, 9968.0, 9584.0, 5986.0, 5471.0, 5700.0, 5466.0, 3360.0, 3163.0, 3416.0, 3124.0, 2030.0, 2044.0, 1960.0, 1960.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.608', '0.585', '0.731', '0.668', '0.696', '0.667', '0.820', '0.772', '0.834', '0.763', '0.991', '0.998', '0.957', '0.957']
Global density: 0.63401198387146
06/02 06:57:03 PM | Train: [58/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:57:03 PM | layerwise density: [36593.0, 10000.0, 9646.0, 9967.0, 9587.0, 5990.0, 5473.0, 5698.0, 5469.0, 3365.0, 3156.0, 3417.0, 3120.0, 2026.0, 2041.0, 1963.0, 1961.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.608', '0.585', '0.731', '0.668', '0.696', '0.668', '0.822', '0.771', '0.834', '0.762', '0.989', '0.997', '0.958', '0.958']
Global density: 0.6340863108634949
06/02 06:57:13 PM | Train: [58/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:57:13 PM | layerwise density: [36592.0, 10003.0, 9647.0, 9968.0, 9588.0, 5993.0, 5485.0, 5701.0, 5473.0, 3371.0, 3160.0, 3418.0, 3122.0, 2024.0, 2043.0, 1965.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.589', '0.608', '0.585', '0.732', '0.670', '0.696', '0.668', '0.823', '0.771', '0.834', '0.762', '0.988', '0.998', '0.959', '0.958']
Global density: 0.6343091726303101
06/02 06:57:23 PM | Train: [58/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:57:23 PM | layerwise density: [36596.0, 10009.0, 9653.0, 9973.0, 9594.0, 5996.0, 5487.0, 5706.0, 5473.0, 3378.0, 3156.0, 3421.0, 3118.0, 2023.0, 2044.0, 1967.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.589', '0.609', '0.586', '0.732', '0.670', '0.697', '0.668', '0.825', '0.771', '0.835', '0.761', '0.988', '0.998', '0.960', '0.958']
Global density: 0.634526789188385
06/02 06:57:23 PM | Train: [58/200] Final Prec@1 99.9640%
06/02 06:57:23 PM | Valid: [58/200] Step 000/078 Loss 1.143 Prec@(1,5) (72.7%, 93.8%)
06/02 06:57:25 PM | Valid: [58/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.8%, 90.4%)
06/02 06:57:26 PM | Valid: [58/200] Final Prec@1 70.8300%
06/02 06:57:26 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36597.0, 0.5584259033203125]
['model.relu.alpha_mask_1_0', 16384, 10009.0, 0.61090087890625]
['model.relu.alpha_mask_2_0', 16384, 9653.0, 0.58917236328125]
['model.relu.alpha_mask_3_0', 16384, 9973.0, 0.60870361328125]
['model.relu.alpha_mask_4_0', 16384, 9594.0, 0.5855712890625]
['model.relu.alpha_mask_5_0', 8192, 5996.0, 0.73193359375]
['model.relu.alpha_mask_6_0', 8192, 5487.0, 0.6697998046875]
['model.relu.alpha_mask_7_0', 8192, 5706.0, 0.696533203125]
['model.relu.alpha_mask_8_0', 8192, 5474.0, 0.668212890625]
['model.relu.alpha_mask_9_0', 4096, 3377.0, 0.824462890625]
['model.relu.alpha_mask_10_0', 4096, 3156.0, 0.7705078125]
['model.relu.alpha_mask_11_0', 4096, 3421.0, 0.835205078125]
['model.relu.alpha_mask_12_0', 4096, 3118.0, 0.76123046875]
['model.relu.alpha_mask_13_0', 2048, 2023.0, 0.98779296875]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1967.0, 0.96044921875]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119556.0, 0.6345320991847826]
########## End ###########
06/02 06:57:26 PM | Train: [59/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:57:26 PM | layerwise density: [36597.0, 10009.0, 9653.0, 9973.0, 9594.0, 5996.0, 5487.0, 5706.0, 5474.0, 3377.0, 3156.0, 3421.0, 3118.0, 2023.0, 2044.0, 1967.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.589', '0.609', '0.586', '0.732', '0.670', '0.697', '0.668', '0.824', '0.771', '0.835', '0.761', '0.988', '0.998', '0.960', '0.958']
Global density: 0.63453209400177
06/02 06:57:37 PM | Train: [59/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:57:37 PM | layerwise density: [36592.0, 10007.0, 9652.0, 9975.0, 9596.0, 5993.0, 5496.0, 5710.0, 5470.0, 3377.0, 3152.0, 3426.0, 3116.0, 2025.0, 2043.0, 1966.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.589', '0.609', '0.586', '0.732', '0.671', '0.697', '0.668', '0.824', '0.770', '0.836', '0.761', '0.989', '0.998', '0.960', '0.958']
Global density: 0.634537398815155
06/02 06:57:47 PM | Train: [59/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:57:47 PM | layerwise density: [36599.0, 10012.0, 9658.0, 9978.0, 9599.0, 6001.0, 5500.0, 5709.0, 5471.0, 3383.0, 3154.0, 3425.0, 3118.0, 2028.0, 2043.0, 1960.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.589', '0.609', '0.586', '0.733', '0.671', '0.697', '0.668', '0.826', '0.770', '0.836', '0.761', '0.990', '0.998', '0.957', '0.958']
Global density: 0.634760320186615
06/02 06:57:58 PM | Train: [59/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:57:58 PM | layerwise density: [36602.0, 10010.0, 9663.0, 9981.0, 9599.0, 6005.0, 5499.0, 5712.0, 5472.0, 3386.0, 3154.0, 3428.0, 3121.0, 2027.0, 2044.0, 1960.0, 1961.0]
layerwise density percentage: ['0.559', '0.611', '0.590', '0.609', '0.586', '0.733', '0.671', '0.697', '0.668', '0.827', '0.770', '0.837', '0.762', '0.990', '0.998', '0.957', '0.958']
Global density: 0.634893000125885
06/02 06:58:07 PM | Train: [59/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:58:07 PM | layerwise density: [36597.0, 10014.0, 9667.0, 9984.0, 9599.0, 6002.0, 5499.0, 5714.0, 5469.0, 3386.0, 3166.0, 3433.0, 3136.0, 2033.0, 2041.0, 1958.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.590', '0.609', '0.586', '0.733', '0.671', '0.698', '0.668', '0.827', '0.773', '0.838', '0.766', '0.993', '0.997', '0.956', '0.958']
Global density: 0.6350787878036499
06/02 06:58:07 PM | Train: [59/200] Final Prec@1 99.9600%
06/02 06:58:08 PM | Valid: [59/200] Step 000/078 Loss 1.169 Prec@(1,5) (72.7%, 94.5%)
06/02 06:58:10 PM | Valid: [59/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.4%)
06/02 06:58:10 PM | Valid: [59/200] Final Prec@1 70.9400%
06/02 06:58:10 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36598.0, 0.558441162109375]
['model.relu.alpha_mask_1_0', 16384, 10014.0, 0.6112060546875]
['model.relu.alpha_mask_2_0', 16384, 9667.0, 0.59002685546875]
['model.relu.alpha_mask_3_0', 16384, 9984.0, 0.609375]
['model.relu.alpha_mask_4_0', 16384, 9599.0, 0.58587646484375]
['model.relu.alpha_mask_5_0', 8192, 6002.0, 0.732666015625]
['model.relu.alpha_mask_6_0', 8192, 5499.0, 0.6712646484375]
['model.relu.alpha_mask_7_0', 8192, 5713.0, 0.6973876953125]
['model.relu.alpha_mask_8_0', 8192, 5468.0, 0.66748046875]
['model.relu.alpha_mask_9_0', 4096, 3386.0, 0.82666015625]
['model.relu.alpha_mask_10_0', 4096, 3166.0, 0.77294921875]
['model.relu.alpha_mask_11_0', 4096, 3433.0, 0.838134765625]
['model.relu.alpha_mask_12_0', 4096, 3136.0, 0.765625]
['model.relu.alpha_mask_13_0', 2048, 2033.0, 0.99267578125]
['model.relu.alpha_mask_14_0', 2048, 2042.0, 0.9970703125]
['model.relu.alpha_mask_15_0', 2048, 1958.0, 0.9560546875]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119659.0, 0.6350787618885869]
########## End ###########
06/02 06:58:11 PM | Train: [60/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:58:11 PM | layerwise density: [36598.0, 10014.0, 9667.0, 9984.0, 9599.0, 6002.0, 5499.0, 5713.0, 5468.0, 3386.0, 3166.0, 3433.0, 3136.0, 2033.0, 2042.0, 1958.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.590', '0.609', '0.586', '0.733', '0.671', '0.697', '0.667', '0.827', '0.773', '0.838', '0.766', '0.993', '0.997', '0.956', '0.958']
Global density: 0.6350787878036499
06/02 06:58:22 PM | Train: [60/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:58:22 PM | layerwise density: [36602.0, 10015.0, 9668.0, 9986.0, 9602.0, 6001.0, 5496.0, 5715.0, 5466.0, 3396.0, 3173.0, 3433.0, 3133.0, 2038.0, 2044.0, 1958.0, 1961.0]
layerwise density percentage: ['0.559', '0.611', '0.590', '0.609', '0.586', '0.733', '0.671', '0.698', '0.667', '0.829', '0.775', '0.838', '0.765', '0.995', '0.998', '0.956', '0.958']
Global density: 0.635227382183075
06/02 06:58:32 PM | Train: [60/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:58:32 PM | layerwise density: [36607.0, 10013.0, 9662.0, 9989.0, 9604.0, 5993.0, 5502.0, 5715.0, 5472.0, 3394.0, 3178.0, 3433.0, 3132.0, 2036.0, 2043.0, 1962.0, 1961.0]
layerwise density percentage: ['0.559', '0.611', '0.590', '0.610', '0.586', '0.732', '0.672', '0.698', '0.668', '0.829', '0.776', '0.838', '0.765', '0.994', '0.998', '0.958', '0.958']
Global density: 0.63527512550354
06/02 06:58:43 PM | Train: [60/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:58:43 PM | layerwise density: [36611.0, 10015.0, 9665.0, 9993.0, 9603.0, 6004.0, 5512.0, 5715.0, 5479.0, 3392.0, 3187.0, 3433.0, 3130.0, 2036.0, 2041.0, 1959.0, 1961.0]
layerwise density percentage: ['0.559', '0.611', '0.590', '0.610', '0.586', '0.733', '0.673', '0.698', '0.669', '0.828', '0.778', '0.838', '0.764', '0.994', '0.997', '0.957', '0.958']
Global density: 0.63548743724823
06/02 06:58:53 PM | Train: [60/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:58:53 PM | layerwise density: [36609.0, 10017.0, 9672.0, 9999.0, 9602.0, 6013.0, 5508.0, 5725.0, 5484.0, 3396.0, 3199.0, 3431.0, 3129.0, 2032.0, 2042.0, 1960.0, 1961.0]
layerwise density percentage: ['0.559', '0.611', '0.590', '0.610', '0.586', '0.734', '0.672', '0.699', '0.669', '0.829', '0.781', '0.838', '0.764', '0.992', '0.997', '0.957', '0.958']
Global density: 0.635715663433075
06/02 06:58:53 PM | Train: [60/200] Final Prec@1 99.9700%
06/02 06:58:53 PM | Valid: [60/200] Step 000/078 Loss 1.174 Prec@(1,5) (72.7%, 93.8%)
06/02 06:58:55 PM | Valid: [60/200] Step 078/078 Loss 1.264 Prec@(1,5) (70.9%, 90.3%)
06/02 06:58:55 PM | Valid: [60/200] Final Prec@1 70.9400%
06/02 06:58:55 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36609.0, 0.5586090087890625]
['model.relu.alpha_mask_1_0', 16384, 10017.0, 0.61138916015625]
['model.relu.alpha_mask_2_0', 16384, 9672.0, 0.59033203125]
['model.relu.alpha_mask_3_0', 16384, 9999.0, 0.61029052734375]
['model.relu.alpha_mask_4_0', 16384, 9602.0, 0.5860595703125]
['model.relu.alpha_mask_5_0', 8192, 6013.0, 0.7340087890625]
['model.relu.alpha_mask_6_0', 8192, 5508.0, 0.67236328125]
['model.relu.alpha_mask_7_0', 8192, 5725.0, 0.6988525390625]
['model.relu.alpha_mask_8_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_9_0', 4096, 3396.0, 0.8291015625]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3431.0, 0.837646484375]
['model.relu.alpha_mask_12_0', 4096, 3129.0, 0.763916015625]
['model.relu.alpha_mask_13_0', 2048, 2032.0, 0.9921875]
['model.relu.alpha_mask_14_0', 2048, 2042.0, 0.9970703125]
['model.relu.alpha_mask_15_0', 2048, 1960.0, 0.95703125]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119779.0, 0.6357156504755435]
########## End ###########
06/02 06:58:56 PM | Train: [61/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:58:56 PM | layerwise density: [36609.0, 10017.0, 9672.0, 9999.0, 9602.0, 6013.0, 5508.0, 5725.0, 5484.0, 3396.0, 3199.0, 3431.0, 3129.0, 2032.0, 2042.0, 1960.0, 1961.0]
layerwise density percentage: ['0.559', '0.611', '0.590', '0.610', '0.586', '0.734', '0.672', '0.699', '0.669', '0.829', '0.781', '0.838', '0.764', '0.992', '0.997', '0.957', '0.958']
Global density: 0.635715663433075
06/02 06:59:07 PM | Train: [61/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:59:07 PM | layerwise density: [36618.0, 10023.0, 9676.0, 10003.0, 9604.0, 6016.0, 5515.0, 5725.0, 5488.0, 3395.0, 3199.0, 3431.0, 3120.0, 2033.0, 2042.0, 1965.0, 1961.0]
layerwise density percentage: ['0.559', '0.612', '0.591', '0.611', '0.586', '0.734', '0.673', '0.699', '0.670', '0.829', '0.781', '0.838', '0.762', '0.993', '0.997', '0.959', '0.958']
Global density: 0.6359014511108398
06/02 06:59:17 PM | Train: [61/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:59:17 PM | layerwise density: [36628.0, 10028.0, 9682.0, 10007.0, 9606.0, 6031.0, 5517.0, 5735.0, 5491.0, 3386.0, 3195.0, 3432.0, 3118.0, 2033.0, 2040.0, 1961.0, 1961.0]
layerwise density percentage: ['0.559', '0.612', '0.591', '0.611', '0.586', '0.736', '0.673', '0.700', '0.670', '0.827', '0.780', '0.838', '0.761', '0.993', '0.996', '0.958', '0.958']
Global density: 0.63609778881073
06/02 06:59:28 PM | Train: [61/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:59:28 PM | layerwise density: [36630.0, 10024.0, 9685.0, 10008.0, 9611.0, 6025.0, 5519.0, 5736.0, 5491.0, 3379.0, 3196.0, 3431.0, 3119.0, 2034.0, 2042.0, 1955.0, 1961.0]
layerwise density percentage: ['0.559', '0.612', '0.591', '0.611', '0.587', '0.735', '0.674', '0.700', '0.670', '0.825', '0.780', '0.838', '0.761', '0.993', '0.997', '0.955', '0.958']
Global density: 0.6360712647438049
06/02 06:59:38 PM | Train: [61/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:59:38 PM | layerwise density: [36634.0, 10028.0, 9688.0, 10009.0, 9617.0, 6026.0, 5520.0, 5741.0, 5495.0, 3382.0, 3194.0, 3431.0, 3120.0, 2032.0, 2043.0, 1953.0, 1961.0]
layerwise density percentage: ['0.559', '0.612', '0.591', '0.611', '0.587', '0.736', '0.674', '0.701', '0.671', '0.826', '0.780', '0.838', '0.762', '0.992', '0.998', '0.954', '0.958']
Global density: 0.63621985912323
06/02 06:59:38 PM | Train: [61/200] Final Prec@1 99.9540%
06/02 06:59:38 PM | Valid: [61/200] Step 000/078 Loss 1.190 Prec@(1,5) (71.1%, 93.0%)
06/02 06:59:40 PM | Valid: [61/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.7%, 90.2%)
06/02 06:59:40 PM | Valid: [61/200] Final Prec@1 70.7000%
06/02 06:59:40 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36634.0, 0.558990478515625]
['model.relu.alpha_mask_1_0', 16384, 10028.0, 0.612060546875]
['model.relu.alpha_mask_2_0', 16384, 9688.0, 0.59130859375]
['model.relu.alpha_mask_3_0', 16384, 10009.0, 0.61090087890625]
['model.relu.alpha_mask_4_0', 16384, 9617.0, 0.58697509765625]
['model.relu.alpha_mask_5_0', 8192, 6026.0, 0.735595703125]
['model.relu.alpha_mask_6_0', 8192, 5520.0, 0.673828125]
['model.relu.alpha_mask_7_0', 8192, 5741.0, 0.7008056640625]
['model.relu.alpha_mask_8_0', 8192, 5495.0, 0.6707763671875]
['model.relu.alpha_mask_9_0', 4096, 3382.0, 0.82568359375]
['model.relu.alpha_mask_10_0', 4096, 3194.0, 0.77978515625]
['model.relu.alpha_mask_11_0', 4096, 3431.0, 0.837646484375]
['model.relu.alpha_mask_12_0', 4096, 3120.0, 0.76171875]
['model.relu.alpha_mask_13_0', 2048, 2032.0, 0.9921875]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1953.0, 0.95361328125]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119874.0, 0.6362198539402174]
########## End ###########
06/02 06:59:41 PM | Train: [62/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 06:59:41 PM | layerwise density: [36634.0, 10028.0, 9688.0, 10009.0, 9617.0, 6026.0, 5520.0, 5741.0, 5495.0, 3382.0, 3194.0, 3431.0, 3120.0, 2032.0, 2043.0, 1953.0, 1961.0]
layerwise density percentage: ['0.559', '0.612', '0.591', '0.611', '0.587', '0.736', '0.674', '0.701', '0.671', '0.826', '0.780', '0.838', '0.762', '0.992', '0.998', '0.954', '0.958']
Global density: 0.63621985912323
06/02 06:59:52 PM | Train: [62/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 06:59:52 PM | layerwise density: [36636.0, 10027.0, 9690.0, 10013.0, 9616.0, 6036.0, 5516.0, 5744.0, 5492.0, 3375.0, 3195.0, 3430.0, 3124.0, 2033.0, 2042.0, 1956.0, 1961.0]
layerwise density percentage: ['0.559', '0.612', '0.591', '0.611', '0.587', '0.737', '0.673', '0.701', '0.670', '0.824', '0.780', '0.837', '0.763', '0.993', '0.997', '0.955', '0.958']
Global density: 0.6362835764884949
06/02 07:00:03 PM | Train: [62/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:00:03 PM | layerwise density: [36635.0, 10037.0, 9690.0, 10016.0, 9620.0, 6043.0, 5511.0, 5743.0, 5496.0, 3372.0, 3189.0, 3431.0, 3130.0, 2033.0, 2042.0, 1952.0, 1961.0]
layerwise density percentage: ['0.559', '0.613', '0.591', '0.611', '0.587', '0.738', '0.673', '0.701', '0.671', '0.823', '0.779', '0.838', '0.764', '0.993', '0.997', '0.953', '0.958']
Global density: 0.63636314868927
06/02 07:00:13 PM | Train: [62/80] Step 300/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:00:13 PM | layerwise density: [36630.0, 10041.0, 9690.0, 10018.0, 9623.0, 6044.0, 5518.0, 5746.0, 5493.0, 3376.0, 3185.0, 3430.0, 3125.0, 2033.0, 2041.0, 1947.0, 1961.0]
layerwise density percentage: ['0.559', '0.613', '0.591', '0.611', '0.587', '0.738', '0.674', '0.701', '0.671', '0.824', '0.778', '0.837', '0.763', '0.993', '0.997', '0.951', '0.958']
Global density: 0.63636314868927
06/02 07:00:23 PM | Train: [62/80] Step 390/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:00:23 PM | layerwise density: [36630.0, 10042.0, 9694.0, 10020.0, 9620.0, 6050.0, 5516.0, 5748.0, 5492.0, 3377.0, 3188.0, 3430.0, 3133.0, 2035.0, 2042.0, 1948.0, 1961.0]
layerwise density percentage: ['0.559', '0.613', '0.592', '0.612', '0.587', '0.739', '0.673', '0.702', '0.670', '0.824', '0.778', '0.837', '0.765', '0.994', '0.997', '0.951', '0.958']
Global density: 0.63649582862854
06/02 07:00:23 PM | Train: [62/200] Final Prec@1 99.9460%
06/02 07:00:23 PM | Valid: [62/200] Step 000/078 Loss 1.141 Prec@(1,5) (72.7%, 93.0%)
06/02 07:00:25 PM | Valid: [62/200] Step 078/078 Loss 1.262 Prec@(1,5) (70.9%, 90.5%)
06/02 07:00:25 PM | Valid: [62/200] Final Prec@1 70.8800%
06/02 07:00:25 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36630.0, 0.558929443359375]
['model.relu.alpha_mask_1_0', 16384, 10042.0, 0.6129150390625]
['model.relu.alpha_mask_2_0', 16384, 9694.0, 0.5916748046875]
['model.relu.alpha_mask_3_0', 16384, 10020.0, 0.611572265625]
['model.relu.alpha_mask_4_0', 16384, 9620.0, 0.587158203125]
['model.relu.alpha_mask_5_0', 8192, 6050.0, 0.738525390625]
['model.relu.alpha_mask_6_0', 8192, 5516.0, 0.67333984375]
['model.relu.alpha_mask_7_0', 8192, 5748.0, 0.70166015625]
['model.relu.alpha_mask_8_0', 8192, 5492.0, 0.67041015625]
['model.relu.alpha_mask_9_0', 4096, 3377.0, 0.824462890625]
['model.relu.alpha_mask_10_0', 4096, 3188.0, 0.7783203125]
['model.relu.alpha_mask_11_0', 4096, 3430.0, 0.83740234375]
['model.relu.alpha_mask_12_0', 4096, 3133.0, 0.764892578125]
['model.relu.alpha_mask_13_0', 2048, 2035.0, 0.99365234375]
['model.relu.alpha_mask_14_0', 2048, 2042.0, 0.9970703125]
['model.relu.alpha_mask_15_0', 2048, 1949.0, 0.95166015625]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119927.0, 0.6365011463994565]
########## End ###########
06/02 07:00:26 PM | Train: [63/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:00:26 PM | layerwise density: [36630.0, 10042.0, 9694.0, 10020.0, 9620.0, 6050.0, 5516.0, 5748.0, 5492.0, 3377.0, 3188.0, 3430.0, 3133.0, 2035.0, 2042.0, 1949.0, 1961.0]
layerwise density percentage: ['0.559', '0.613', '0.592', '0.612', '0.587', '0.739', '0.673', '0.702', '0.670', '0.824', '0.778', '0.837', '0.765', '0.994', '0.997', '0.952', '0.958']
Global density: 0.636501133441925
06/02 07:00:37 PM | Train: [63/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:00:37 PM | layerwise density: [36633.0, 10044.0, 9705.0, 10022.0, 9621.0, 6053.0, 5524.0, 5752.0, 5495.0, 3378.0, 3192.0, 3432.0, 3134.0, 2035.0, 2042.0, 1948.0, 1961.0]
layerwise density percentage: ['0.559', '0.613', '0.592', '0.612', '0.587', '0.739', '0.674', '0.702', '0.671', '0.825', '0.779', '0.838', '0.765', '0.994', '0.997', '0.951', '0.958']
Global density: 0.636734664440155
06/02 07:00:47 PM | Train: [63/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:00:47 PM | layerwise density: [36582.0, 10028.0, 9689.0, 10011.0, 9602.0, 6032.0, 5501.0, 5735.0, 5472.0, 3360.0, 3172.0, 3429.0, 3110.0, 2028.0, 2047.0, 1937.0, 1961.0]
layerwise density percentage: ['0.558', '0.612', '0.591', '0.611', '0.586', '0.736', '0.672', '0.700', '0.668', '0.820', '0.774', '0.837', '0.759', '0.990', '1.000', '0.946', '0.958']
Global density: 0.63527512550354
06/02 07:00:58 PM | Train: [63/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:00:58 PM | layerwise density: [36556.0, 10012.0, 9674.0, 9999.0, 9587.0, 6014.0, 5483.0, 5724.0, 5451.0, 3344.0, 3160.0, 3426.0, 3090.0, 2025.0, 2046.0, 1932.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.590', '0.610', '0.585', '0.734', '0.669', '0.699', '0.665', '0.816', '0.771', '0.836', '0.754', '0.989', '0.999', '0.943', '0.958']
Global density: 0.634149968624115
06/02 07:01:07 PM | Train: [63/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:01:07 PM | layerwise density: [36551.0, 10005.0, 9669.0, 9997.0, 9581.0, 6006.0, 5479.0, 5720.0, 5444.0, 3337.0, 3151.0, 3423.0, 3080.0, 2023.0, 2046.0, 1928.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.590', '0.610', '0.585', '0.733', '0.669', '0.698', '0.665', '0.815', '0.769', '0.836', '0.752', '0.988', '0.999', '0.941', '0.958']
Global density: 0.6337094902992249
06/02 07:01:07 PM | Train: [63/200] Final Prec@1 99.9600%
06/02 07:01:08 PM | Valid: [63/200] Step 000/078 Loss 1.149 Prec@(1,5) (71.1%, 92.2%)
06/02 07:01:10 PM | Valid: [63/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.8%, 90.4%)
06/02 07:01:10 PM | Valid: [63/200] Final Prec@1 70.7600%
06/02 07:01:10 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36551.0, 0.5577239990234375]
['model.relu.alpha_mask_1_0', 16384, 10004.0, 0.610595703125]
['model.relu.alpha_mask_2_0', 16384, 9669.0, 0.59014892578125]
['model.relu.alpha_mask_3_0', 16384, 9997.0, 0.61016845703125]
['model.relu.alpha_mask_4_0', 16384, 9581.0, 0.58477783203125]
['model.relu.alpha_mask_5_0', 8192, 6005.0, 0.7330322265625]
['model.relu.alpha_mask_6_0', 8192, 5478.0, 0.668701171875]
['model.relu.alpha_mask_7_0', 8192, 5720.0, 0.6982421875]
['model.relu.alpha_mask_8_0', 8192, 5444.0, 0.66455078125]
['model.relu.alpha_mask_9_0', 4096, 3339.0, 0.815185546875]
['model.relu.alpha_mask_10_0', 4096, 3151.0, 0.769287109375]
['model.relu.alpha_mask_11_0', 4096, 3423.0, 0.835693359375]
['model.relu.alpha_mask_12_0', 4096, 3079.0, 0.751708984375]
['model.relu.alpha_mask_13_0', 2048, 2023.0, 0.98779296875]
['model.relu.alpha_mask_14_0', 2048, 2046.0, 0.9990234375]
['model.relu.alpha_mask_15_0', 2048, 1928.0, 0.94140625]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119399.0, 0.6336988366168478]
########## End ###########
06/02 07:01:11 PM | Train: [64/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:01:11 PM | layerwise density: [36551.0, 10004.0, 9669.0, 9997.0, 9581.0, 6005.0, 5478.0, 5720.0, 5444.0, 3339.0, 3151.0, 3423.0, 3079.0, 2023.0, 2046.0, 1928.0, 1961.0]
layerwise density percentage: ['0.558', '0.611', '0.590', '0.610', '0.585', '0.733', '0.669', '0.698', '0.665', '0.815', '0.769', '0.836', '0.752', '0.988', '0.999', '0.941', '0.958']
Global density: 0.6336988210678101
06/02 07:01:21 PM | Train: [64/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:01:21 PM | layerwise density: [36550.0, 9999.0, 9665.0, 9995.0, 9576.0, 6001.0, 5472.0, 5718.0, 5440.0, 3342.0, 3141.0, 3422.0, 3073.0, 2020.0, 2046.0, 1929.0, 1961.0]
layerwise density percentage: ['0.558', '0.610', '0.590', '0.610', '0.584', '0.733', '0.668', '0.698', '0.664', '0.816', '0.767', '0.835', '0.750', '0.986', '0.999', '0.942', '0.958']
Global density: 0.633438766002655
06/02 07:01:32 PM | Train: [64/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:01:32 PM | layerwise density: [36545.0, 9997.0, 9664.0, 9990.0, 9574.0, 6000.0, 5468.0, 5716.0, 5438.0, 3337.0, 3137.0, 3421.0, 3069.0, 2021.0, 2046.0, 1927.0, 1961.0]
layerwise density percentage: ['0.558', '0.610', '0.590', '0.610', '0.584', '0.732', '0.667', '0.698', '0.664', '0.815', '0.766', '0.835', '0.749', '0.987', '0.999', '0.941', '0.958']
Global density: 0.6332318186759949
06/02 07:01:42 PM | Train: [64/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:01:42 PM | layerwise density: [36542.0, 9996.0, 9660.0, 9989.0, 9572.0, 5999.0, 5464.0, 5712.0, 5434.0, 3340.0, 3141.0, 3418.0, 3066.0, 2019.0, 2046.0, 1930.0, 1961.0]
layerwise density percentage: ['0.558', '0.610', '0.590', '0.610', '0.584', '0.732', '0.667', '0.697', '0.663', '0.815', '0.767', '0.834', '0.749', '0.986', '0.999', '0.942', '0.958']
Global density: 0.6331150531768799
06/02 07:01:52 PM | Train: [64/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:01:52 PM | layerwise density: [36538.0, 9994.0, 9658.0, 9987.0, 9569.0, 5994.0, 5462.0, 5709.0, 5436.0, 3338.0, 3135.0, 3417.0, 3067.0, 2016.0, 2047.0, 1931.0, 1961.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.610', '0.584', '0.732', '0.667', '0.697', '0.664', '0.815', '0.765', '0.834', '0.749', '0.984', '1.000', '0.943', '0.958']
Global density: 0.63295578956604
06/02 07:01:52 PM | Train: [64/200] Final Prec@1 99.9500%
06/02 07:01:52 PM | Valid: [64/200] Step 000/078 Loss 1.163 Prec@(1,5) (74.2%, 93.0%)
06/02 07:01:54 PM | Valid: [64/200] Step 078/078 Loss 1.266 Prec@(1,5) (70.7%, 90.4%)
06/02 07:01:55 PM | Valid: [64/200] Final Prec@1 70.7400%
06/02 07:01:55 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36538.0, 0.557525634765625]
['model.relu.alpha_mask_1_0', 16384, 9994.0, 0.6099853515625]
['model.relu.alpha_mask_2_0', 16384, 9658.0, 0.5894775390625]
['model.relu.alpha_mask_3_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_4_0', 16384, 9569.0, 0.58404541015625]
['model.relu.alpha_mask_5_0', 8192, 5995.0, 0.7318115234375]
['model.relu.alpha_mask_6_0', 8192, 5462.0, 0.666748046875]
['model.relu.alpha_mask_7_0', 8192, 5709.0, 0.6968994140625]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_mask_10_0', 4096, 3135.0, 0.765380859375]
['model.relu.alpha_mask_11_0', 4096, 3417.0, 0.834228515625]
['model.relu.alpha_mask_12_0', 4096, 3067.0, 0.748779296875]
['model.relu.alpha_mask_13_0', 2048, 2016.0, 0.984375]
['model.relu.alpha_mask_14_0', 2048, 2047.0, 0.99951171875]
['model.relu.alpha_mask_15_0', 2048, 1931.0, 0.94287109375]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119260.0, 0.6329611073369565]
########## End ###########
06/02 07:01:55 PM | Train: [65/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:01:55 PM | layerwise density: [36538.0, 9994.0, 9658.0, 9987.0, 9569.0, 5995.0, 5462.0, 5709.0, 5436.0, 3338.0, 3135.0, 3417.0, 3067.0, 2016.0, 2047.0, 1931.0, 1961.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.610', '0.584', '0.732', '0.667', '0.697', '0.664', '0.815', '0.765', '0.834', '0.749', '0.984', '1.000', '0.943', '0.958']
Global density: 0.632961094379425
06/02 07:02:06 PM | Train: [65/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:02:06 PM | layerwise density: [36537.0, 9991.0, 9657.0, 9986.0, 9568.0, 5992.0, 5459.0, 5704.0, 5434.0, 3340.0, 3132.0, 3418.0, 3063.0, 2017.0, 2047.0, 1932.0, 1961.0]
layerwise density percentage: ['0.558', '0.610', '0.589', '0.609', '0.584', '0.731', '0.666', '0.696', '0.663', '0.815', '0.765', '0.834', '0.748', '0.985', '1.000', '0.943', '0.958']
Global density: 0.6328443288803101
06/02 07:02:16 PM | Train: [65/80] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:02:16 PM | layerwise density: [36535.0, 9991.0, 9656.0, 9984.0, 9566.0, 5992.0, 5454.0, 5704.0, 5433.0, 3339.0, 3127.0, 3417.0, 3061.0, 2019.0, 2048.0, 1934.0, 1961.0]
layerwise density percentage: ['0.557', '0.610', '0.589', '0.609', '0.584', '0.731', '0.666', '0.696', '0.663', '0.815', '0.763', '0.834', '0.747', '0.986', '1.000', '0.944', '0.958']
Global density: 0.6327541470527649
06/02 07:02:27 PM | Train: [65/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:02:27 PM | layerwise density: [36532.0, 9988.0, 9655.0, 9983.0, 9565.0, 5988.0, 5452.0, 5698.0, 5430.0, 3335.0, 3126.0, 3417.0, 3066.0, 2019.0, 2047.0, 1935.0, 1961.0]
layerwise density percentage: ['0.557', '0.610', '0.589', '0.609', '0.584', '0.731', '0.666', '0.696', '0.663', '0.814', '0.763', '0.834', '0.749', '0.986', '1.000', '0.945', '0.958']
Global density: 0.6326267719268799
06/02 07:02:37 PM | Train: [65/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:02:37 PM | layerwise density: [36532.0, 9987.0, 9657.0, 9977.0, 9560.0, 5985.0, 5450.0, 5699.0, 5426.0, 3338.0, 3125.0, 3417.0, 3068.0, 2019.0, 2046.0, 1940.0, 1961.0]
layerwise density percentage: ['0.557', '0.610', '0.589', '0.609', '0.583', '0.731', '0.665', '0.696', '0.662', '0.815', '0.763', '0.834', '0.749', '0.986', '0.999', '0.947', '0.958']
Global density: 0.632573664188385
06/02 07:02:37 PM | Train: [65/200] Final Prec@1 99.9540%
06/02 07:02:37 PM | Valid: [65/200] Step 000/078 Loss 1.152 Prec@(1,5) (74.2%, 92.2%)
06/02 07:02:39 PM | Valid: [65/200] Step 078/078 Loss 1.263 Prec@(1,5) (70.9%, 90.3%)
06/02 07:02:39 PM | Valid: [65/200] Final Prec@1 70.9000%
06/02 07:02:39 PM | Current mask training best Prec@1 = 70.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36532.0, 0.55743408203125]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9657.0, 0.58941650390625]
['model.relu.alpha_mask_3_0', 16384, 9977.0, 0.60894775390625]
['model.relu.alpha_mask_4_0', 16384, 9560.0, 0.58349609375]
['model.relu.alpha_mask_5_0', 8192, 5985.0, 0.7305908203125]
['model.relu.alpha_mask_6_0', 8192, 5450.0, 0.665283203125]
['model.relu.alpha_mask_7_0', 8192, 5699.0, 0.6956787109375]
['model.relu.alpha_mask_8_0', 8192, 5426.0, 0.662353515625]
['model.relu.alpha_mask_9_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_mask_10_0', 4096, 3125.0, 0.762939453125]
['model.relu.alpha_mask_11_0', 4096, 3416.0, 0.833984375]
['model.relu.alpha_mask_12_0', 4096, 3068.0, 0.7490234375]
['model.relu.alpha_mask_13_0', 2048, 2019.0, 0.98583984375]
['model.relu.alpha_mask_14_0', 2048, 2046.0, 0.9990234375]
['model.relu.alpha_mask_15_0', 2048, 1939.0, 0.94677734375]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119185.0, 0.6325630519701086]
########## End ###########
06/02 07:02:40 PM | Train: [66/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:02:40 PM | layerwise density: [36532.0, 9987.0, 9657.0, 9977.0, 9560.0, 5985.0, 5450.0, 5699.0, 5426.0, 3338.0, 3125.0, 3416.0, 3068.0, 2019.0, 2046.0, 1939.0, 1961.0]
layerwise density percentage: ['0.557', '0.610', '0.589', '0.609', '0.583', '0.731', '0.665', '0.696', '0.662', '0.815', '0.763', '0.834', '0.749', '0.986', '0.999', '0.947', '0.958']
Global density: 0.632563054561615
06/02 07:02:51 PM | Train: [66/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:02:51 PM | layerwise density: [36525.0, 9986.0, 9655.0, 9976.0, 9562.0, 5985.0, 5450.0, 5698.0, 5429.0, 3341.0, 3122.0, 3416.0, 3075.0, 2021.0, 2044.0, 1940.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.589', '0.609', '0.584', '0.731', '0.665', '0.696', '0.663', '0.816', '0.762', '0.834', '0.751', '0.987', '0.998', '0.947', '0.958']
Global density: 0.632568359375
06/02 07:03:01 PM | Train: [66/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:03:01 PM | layerwise density: [36525.0, 9986.0, 9658.0, 9974.0, 9563.0, 5985.0, 5449.0, 5697.0, 5428.0, 3345.0, 3119.0, 3416.0, 3077.0, 2022.0, 2044.0, 1944.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.589', '0.609', '0.584', '0.731', '0.665', '0.695', '0.663', '0.817', '0.761', '0.834', '0.751', '0.987', '0.998', '0.949', '0.958']
Global density: 0.6326055526733398
06/02 07:03:12 PM | Train: [66/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:03:12 PM | layerwise density: [36526.0, 9984.0, 9653.0, 9972.0, 9562.0, 5986.0, 5448.0, 5695.0, 5426.0, 3345.0, 3121.0, 3416.0, 3077.0, 2024.0, 2043.0, 1950.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.589', '0.609', '0.584', '0.731', '0.665', '0.695', '0.662', '0.817', '0.762', '0.834', '0.751', '0.988', '0.998', '0.952', '0.958']
Global density: 0.632584273815155
06/02 07:03:22 PM | Train: [66/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:03:22 PM | layerwise density: [36526.0, 9983.0, 9651.0, 9970.0, 9562.0, 5986.0, 5447.0, 5696.0, 5425.0, 3350.0, 3123.0, 3416.0, 3077.0, 2026.0, 2043.0, 1954.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.589', '0.609', '0.584', '0.731', '0.665', '0.695', '0.662', '0.818', '0.762', '0.834', '0.751', '0.989', '0.998', '0.954', '0.958']
Global density: 0.6326214671134949
06/02 07:03:22 PM | Train: [66/200] Final Prec@1 99.9660%
06/02 07:03:22 PM | Valid: [66/200] Step 000/078 Loss 1.223 Prec@(1,5) (73.4%, 92.2%)
06/02 07:03:24 PM | Valid: [66/200] Step 078/078 Loss 1.258 Prec@(1,5) (71.0%, 90.5%)
06/02 07:03:24 PM | Valid: [66/200] Final Prec@1 71.0200%
06/02 07:03:25 PM | Current mask training best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36526.0, 0.557342529296875]
['model.relu.alpha_mask_1_0', 16384, 9983.0, 0.60931396484375]
['model.relu.alpha_mask_2_0', 16384, 9651.0, 0.58905029296875]
['model.relu.alpha_mask_3_0', 16384, 9970.0, 0.6085205078125]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 5986.0, 0.730712890625]
['model.relu.alpha_mask_6_0', 8192, 5447.0, 0.6649169921875]
['model.relu.alpha_mask_7_0', 8192, 5696.0, 0.6953125]
['model.relu.alpha_mask_8_0', 8192, 5425.0, 0.6622314453125]
['model.relu.alpha_mask_9_0', 4096, 3350.0, 0.81787109375]
['model.relu.alpha_mask_10_0', 4096, 3123.0, 0.762451171875]
['model.relu.alpha_mask_11_0', 4096, 3416.0, 0.833984375]
['model.relu.alpha_mask_12_0', 4096, 3077.0, 0.751220703125]
['model.relu.alpha_mask_13_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1954.0, 0.9541015625]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119196.0, 0.6326214334239131]
########## End ###########
06/02 07:03:26 PM | Train: [67/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:03:26 PM | layerwise density: [36526.0, 9983.0, 9651.0, 9970.0, 9562.0, 5986.0, 5447.0, 5696.0, 5425.0, 3350.0, 3123.0, 3416.0, 3077.0, 2026.0, 2043.0, 1954.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.589', '0.609', '0.584', '0.731', '0.665', '0.695', '0.662', '0.818', '0.762', '0.834', '0.751', '0.989', '0.998', '0.954', '0.958']
Global density: 0.6326214671134949
06/02 07:03:36 PM | Train: [67/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:03:36 PM | layerwise density: [36523.0, 9983.0, 9650.0, 9970.0, 9561.0, 5984.0, 5447.0, 5695.0, 5420.0, 3351.0, 3125.0, 3416.0, 3077.0, 2025.0, 2043.0, 1958.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.589', '0.609', '0.584', '0.730', '0.665', '0.695', '0.662', '0.818', '0.763', '0.834', '0.751', '0.989', '0.998', '0.956', '0.958']
Global density: 0.632584273815155
06/02 07:03:47 PM | Train: [67/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:03:47 PM | layerwise density: [36523.0, 9984.0, 9645.0, 9969.0, 9558.0, 5985.0, 5444.0, 5696.0, 5421.0, 3351.0, 3133.0, 3416.0, 3081.0, 2027.0, 2043.0, 1960.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.589', '0.608', '0.583', '0.731', '0.665', '0.695', '0.662', '0.818', '0.765', '0.834', '0.752', '0.990', '0.998', '0.957', '0.958']
Global density: 0.6326267719268799
06/02 07:03:58 PM | Train: [67/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:03:58 PM | layerwise density: [36522.0, 9983.0, 9640.0, 9965.0, 9556.0, 5985.0, 5448.0, 5694.0, 5422.0, 3350.0, 3141.0, 3415.0, 3080.0, 2030.0, 2044.0, 1960.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.731', '0.665', '0.695', '0.662', '0.818', '0.767', '0.834', '0.752', '0.991', '0.998', '0.957', '0.958']
Global density: 0.6326214671134949
06/02 07:04:07 PM | Train: [67/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:04:07 PM | layerwise density: [36521.0, 9983.0, 9638.0, 9966.0, 9555.0, 5986.0, 5449.0, 5702.0, 5424.0, 3348.0, 3145.0, 3413.0, 3083.0, 2031.0, 2044.0, 1957.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.731', '0.665', '0.696', '0.662', '0.817', '0.768', '0.833', '0.753', '0.992', '0.998', '0.956', '0.958']
Global density: 0.632674515247345
06/02 07:04:07 PM | Train: [67/200] Final Prec@1 99.9600%
06/02 07:04:07 PM | Valid: [67/200] Step 000/078 Loss 1.209 Prec@(1,5) (72.7%, 92.2%)
06/02 07:04:10 PM | Valid: [67/200] Step 078/078 Loss 1.261 Prec@(1,5) (70.9%, 90.5%)
06/02 07:04:10 PM | Valid: [67/200] Final Prec@1 70.8600%
06/02 07:04:10 PM | Current mask training best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36521.0, 0.5572662353515625]
['model.relu.alpha_mask_1_0', 16384, 9983.0, 0.60931396484375]
['model.relu.alpha_mask_2_0', 16384, 9638.0, 0.5882568359375]
['model.relu.alpha_mask_3_0', 16384, 9966.0, 0.6082763671875]
['model.relu.alpha_mask_4_0', 16384, 9555.0, 0.58319091796875]
['model.relu.alpha_mask_5_0', 8192, 5986.0, 0.730712890625]
['model.relu.alpha_mask_6_0', 8192, 5449.0, 0.6651611328125]
['model.relu.alpha_mask_7_0', 8192, 5702.0, 0.696044921875]
['model.relu.alpha_mask_8_0', 8192, 5424.0, 0.662109375]
['model.relu.alpha_mask_9_0', 4096, 3348.0, 0.8173828125]
['model.relu.alpha_mask_10_0', 4096, 3145.0, 0.767822265625]
['model.relu.alpha_mask_11_0', 4096, 3413.0, 0.833251953125]
['model.relu.alpha_mask_12_0', 4096, 3083.0, 0.752685546875]
['model.relu.alpha_mask_13_0', 2048, 2031.0, 0.99169921875]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1957.0, 0.95556640625]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119206.0, 0.632674507472826]
########## End ###########
06/02 07:04:11 PM | Train: [68/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:04:11 PM | layerwise density: [36521.0, 9983.0, 9638.0, 9966.0, 9555.0, 5986.0, 5449.0, 5702.0, 5424.0, 3348.0, 3145.0, 3413.0, 3083.0, 2031.0, 2044.0, 1957.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.731', '0.665', '0.696', '0.662', '0.817', '0.768', '0.833', '0.753', '0.992', '0.998', '0.956', '0.958']
Global density: 0.632674515247345
06/02 07:04:21 PM | Train: [68/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:04:21 PM | layerwise density: [36519.0, 9982.0, 9639.0, 9966.0, 9554.0, 5987.0, 5454.0, 5701.0, 5423.0, 3350.0, 3146.0, 3413.0, 3086.0, 2031.0, 2045.0, 1960.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.731', '0.666', '0.696', '0.662', '0.818', '0.768', '0.833', '0.753', '0.992', '0.999', '0.957', '0.958']
Global density: 0.6327329277992249
06/02 07:04:32 PM | Train: [68/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:04:32 PM | layerwise density: [36518.0, 9981.0, 9639.0, 9966.0, 9554.0, 5986.0, 5458.0, 5702.0, 5427.0, 3351.0, 3149.0, 3411.0, 3089.0, 2029.0, 2043.0, 1959.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.731', '0.666', '0.696', '0.662', '0.818', '0.769', '0.833', '0.754', '0.991', '0.998', '0.957', '0.958']
Global density: 0.6327647566795349
06/02 07:04:42 PM | Train: [68/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:04:42 PM | layerwise density: [36518.0, 9983.0, 9636.0, 9967.0, 9552.0, 5987.0, 5459.0, 5704.0, 5428.0, 3348.0, 3150.0, 3411.0, 3088.0, 2029.0, 2043.0, 1959.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.731', '0.666', '0.696', '0.663', '0.817', '0.769', '0.833', '0.754', '0.991', '0.998', '0.957', '0.958']
Global density: 0.6327647566795349
06/02 07:04:52 PM | Train: [68/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:04:52 PM | layerwise density: [36513.0, 9983.0, 9634.0, 9967.0, 9550.0, 5985.0, 5457.0, 5703.0, 5428.0, 3354.0, 3154.0, 3410.0, 3091.0, 2029.0, 2043.0, 1958.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.731', '0.666', '0.696', '0.663', '0.819', '0.770', '0.833', '0.755', '0.991', '0.998', '0.956', '0.958']
Global density: 0.6327488422393799
06/02 07:04:52 PM | Train: [68/200] Final Prec@1 99.9540%
06/02 07:04:52 PM | Valid: [68/200] Step 000/078 Loss 1.229 Prec@(1,5) (72.7%, 92.2%)
06/02 07:04:55 PM | Valid: [68/200] Step 078/078 Loss 1.260 Prec@(1,5) (70.8%, 90.4%)
06/02 07:04:55 PM | Valid: [68/200] Final Prec@1 70.7900%
06/02 07:04:55 PM | Current mask training best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36513.0, 0.5571441650390625]
['model.relu.alpha_mask_1_0', 16384, 9983.0, 0.60931396484375]
['model.relu.alpha_mask_2_0', 16384, 9634.0, 0.5880126953125]
['model.relu.alpha_mask_3_0', 16384, 9967.0, 0.60833740234375]
['model.relu.alpha_mask_4_0', 16384, 9550.0, 0.5828857421875]
['model.relu.alpha_mask_5_0', 8192, 5985.0, 0.7305908203125]
['model.relu.alpha_mask_6_0', 8192, 5457.0, 0.6661376953125]
['model.relu.alpha_mask_7_0', 8192, 5703.0, 0.6961669921875]
['model.relu.alpha_mask_8_0', 8192, 5428.0, 0.66259765625]
['model.relu.alpha_mask_9_0', 4096, 3354.0, 0.81884765625]
['model.relu.alpha_mask_10_0', 4096, 3153.0, 0.769775390625]
['model.relu.alpha_mask_11_0', 4096, 3410.0, 0.83251953125]
['model.relu.alpha_mask_12_0', 4096, 3091.0, 0.754638671875]
['model.relu.alpha_mask_13_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1958.0, 0.9560546875]
['model.relu.alpha_mask_16_0', 2048, 1961.0, 0.95751953125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119219.0, 0.6327435037364131]
########## End ###########
06/02 07:04:56 PM | Train: [69/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:04:56 PM | layerwise density: [36513.0, 9983.0, 9634.0, 9967.0, 9550.0, 5985.0, 5457.0, 5703.0, 5428.0, 3354.0, 3153.0, 3410.0, 3091.0, 2029.0, 2043.0, 1958.0, 1961.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.731', '0.666', '0.696', '0.663', '0.819', '0.770', '0.833', '0.755', '0.991', '0.998', '0.956', '0.958']
Global density: 0.6327435374259949
06/02 07:05:06 PM | Train: [69/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:05:06 PM | layerwise density: [36515.0, 9983.0, 9633.0, 9968.0, 9549.0, 5981.0, 5460.0, 5704.0, 5430.0, 3358.0, 3154.0, 3410.0, 3096.0, 2029.0, 2044.0, 1960.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.608', '0.583', '0.730', '0.667', '0.696', '0.663', '0.820', '0.770', '0.833', '0.756', '0.991', '0.998', '0.957', '0.957']
Global density: 0.63282310962677
06/02 07:05:17 PM | Train: [69/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:05:17 PM | layerwise density: [36512.0, 9986.0, 9633.0, 9970.0, 9547.0, 5978.0, 5459.0, 5706.0, 5430.0, 3364.0, 3157.0, 3411.0, 3098.0, 2029.0, 2044.0, 1963.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.730', '0.666', '0.697', '0.663', '0.821', '0.771', '0.833', '0.756', '0.991', '0.998', '0.958', '0.957']
Global density: 0.6328921318054199
06/02 07:05:27 PM | Train: [69/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:05:27 PM | layerwise density: [36510.0, 9987.0, 9634.0, 9972.0, 9542.0, 5981.0, 5459.0, 5708.0, 5432.0, 3367.0, 3155.0, 3412.0, 3103.0, 2029.0, 2044.0, 1963.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.582', '0.730', '0.666', '0.697', '0.663', '0.822', '0.770', '0.833', '0.758', '0.991', '0.998', '0.958', '0.957']
Global density: 0.632950484752655
06/02 07:05:37 PM | Train: [69/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:05:37 PM | layerwise density: [36511.0, 9990.0, 9634.0, 9972.0, 9545.0, 5984.0, 5459.0, 5710.0, 5435.0, 3368.0, 3157.0, 3412.0, 3105.0, 2030.0, 2045.0, 1964.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.730', '0.666', '0.697', '0.663', '0.822', '0.771', '0.833', '0.758', '0.991', '0.999', '0.959', '0.957']
Global density: 0.633072555065155
06/02 07:05:37 PM | Train: [69/200] Final Prec@1 99.9600%
06/02 07:05:37 PM | Valid: [69/200] Step 000/078 Loss 1.190 Prec@(1,5) (74.2%, 93.0%)
06/02 07:05:39 PM | Valid: [69/200] Step 078/078 Loss 1.252 Prec@(1,5) (71.0%, 90.3%)
06/02 07:05:40 PM | Valid: [69/200] Final Prec@1 70.9900%
06/02 07:05:40 PM | Current mask training best Prec@1 = 71.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36511.0, 0.5571136474609375]
['model.relu.alpha_mask_1_0', 16384, 9990.0, 0.6097412109375]
['model.relu.alpha_mask_2_0', 16384, 9634.0, 0.5880126953125]
['model.relu.alpha_mask_3_0', 16384, 9972.0, 0.608642578125]
['model.relu.alpha_mask_4_0', 16384, 9545.0, 0.58258056640625]
['model.relu.alpha_mask_5_0', 8192, 5984.0, 0.73046875]
['model.relu.alpha_mask_6_0', 8192, 5459.0, 0.6663818359375]
['model.relu.alpha_mask_7_0', 8192, 5711.0, 0.6971435546875]
['model.relu.alpha_mask_8_0', 8192, 5435.0, 0.6634521484375]
['model.relu.alpha_mask_9_0', 4096, 3368.0, 0.822265625]
['model.relu.alpha_mask_10_0', 4096, 3158.0, 0.77099609375]
['model.relu.alpha_mask_11_0', 4096, 3412.0, 0.8330078125]
['model.relu.alpha_mask_12_0', 4096, 3105.0, 0.758056640625]
['model.relu.alpha_mask_13_0', 2048, 2030.0, 0.9912109375]
['model.relu.alpha_mask_14_0', 2048, 2045.0, 0.99853515625]
['model.relu.alpha_mask_15_0', 2048, 1963.0, 0.95849609375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119282.0, 0.6330778702445652]
########## End ###########
06/02 07:05:40 PM | Train: [70/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:05:40 PM | layerwise density: [36511.0, 9990.0, 9634.0, 9972.0, 9545.0, 5984.0, 5459.0, 5711.0, 5435.0, 3368.0, 3158.0, 3412.0, 3105.0, 2030.0, 2045.0, 1963.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.730', '0.666', '0.697', '0.663', '0.822', '0.771', '0.833', '0.758', '0.991', '0.999', '0.958', '0.957']
Global density: 0.63307785987854
06/02 07:05:51 PM | Train: [70/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:05:51 PM | layerwise density: [36509.0, 9989.0, 9634.0, 9972.0, 9544.0, 5988.0, 5456.0, 5711.0, 5433.0, 3376.0, 3161.0, 3411.0, 3106.0, 2030.0, 2045.0, 1966.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.731', '0.666', '0.697', '0.663', '0.824', '0.772', '0.833', '0.758', '0.991', '0.999', '0.960', '0.957']
Global density: 0.6331256628036499
06/02 07:06:02 PM | Train: [70/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:06:02 PM | layerwise density: [36508.0, 9990.0, 9636.0, 9971.0, 9547.0, 5987.0, 5459.0, 5711.0, 5436.0, 3379.0, 3165.0, 3411.0, 3106.0, 2030.0, 2045.0, 1966.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.731', '0.666', '0.697', '0.664', '0.825', '0.773', '0.833', '0.758', '0.991', '0.999', '0.960', '0.957']
Global density: 0.6332105398178101
06/02 07:06:12 PM | Train: [70/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:06:12 PM | layerwise density: [36507.0, 9986.0, 9636.0, 9971.0, 9546.0, 5983.0, 5461.0, 5711.0, 5435.0, 3378.0, 3166.0, 3411.0, 3108.0, 2030.0, 2045.0, 1963.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.730', '0.667', '0.697', '0.663', '0.825', '0.773', '0.833', '0.759', '0.991', '0.999', '0.958', '0.957']
Global density: 0.63315749168396
06/02 07:06:21 PM | Train: [70/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:06:21 PM | layerwise density: [36506.0, 9987.0, 9636.0, 9973.0, 9547.0, 5985.0, 5461.0, 5712.0, 5432.0, 3377.0, 3166.0, 3411.0, 3107.0, 2031.0, 2046.0, 1961.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.731', '0.667', '0.697', '0.663', '0.824', '0.773', '0.833', '0.759', '0.992', '0.999', '0.958', '0.957']
Global density: 0.633162796497345
06/02 07:06:21 PM | Train: [70/200] Final Prec@1 99.9600%
06/02 07:06:21 PM | Valid: [70/200] Step 000/078 Loss 1.200 Prec@(1,5) (75.0%, 92.2%)
06/02 07:06:24 PM | Valid: [70/200] Step 078/078 Loss 1.253 Prec@(1,5) (71.2%, 90.3%)
06/02 07:06:24 PM | Valid: [70/200] Final Prec@1 71.1500%
06/02 07:06:24 PM | Current mask training best Prec@1 = 71.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36506.0, 0.557037353515625]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9636.0, 0.588134765625]
['model.relu.alpha_mask_3_0', 16384, 9973.0, 0.60870361328125]
['model.relu.alpha_mask_4_0', 16384, 9547.0, 0.58270263671875]
['model.relu.alpha_mask_5_0', 8192, 5985.0, 0.7305908203125]
['model.relu.alpha_mask_6_0', 8192, 5459.0, 0.6663818359375]
['model.relu.alpha_mask_7_0', 8192, 5712.0, 0.697265625]
['model.relu.alpha_mask_8_0', 8192, 5432.0, 0.6630859375]
['model.relu.alpha_mask_9_0', 4096, 3377.0, 0.824462890625]
['model.relu.alpha_mask_10_0', 4096, 3166.0, 0.77294921875]
['model.relu.alpha_mask_11_0', 4096, 3411.0, 0.832763671875]
['model.relu.alpha_mask_12_0', 4096, 3107.0, 0.758544921875]
['model.relu.alpha_mask_13_0', 2048, 2031.0, 0.99169921875]
['model.relu.alpha_mask_14_0', 2048, 2046.0, 0.9990234375]
['model.relu.alpha_mask_15_0', 2048, 1961.0, 0.95751953125]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119296.0, 0.6331521739130435]
########## End ###########
06/02 07:06:25 PM | Train: [71/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:06:25 PM | layerwise density: [36506.0, 9987.0, 9636.0, 9973.0, 9547.0, 5985.0, 5459.0, 5712.0, 5432.0, 3377.0, 3166.0, 3411.0, 3107.0, 2031.0, 2046.0, 1961.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.731', '0.666', '0.697', '0.663', '0.824', '0.773', '0.833', '0.759', '0.992', '0.999', '0.958', '0.957']
Global density: 0.633152186870575
06/02 07:06:36 PM | Train: [71/80] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:06:36 PM | layerwise density: [36503.0, 9986.0, 9637.0, 9972.0, 9547.0, 5985.0, 5459.0, 5714.0, 5430.0, 3378.0, 3169.0, 3411.0, 3104.0, 2033.0, 2046.0, 1961.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.731', '0.666', '0.698', '0.663', '0.825', '0.774', '0.833', '0.758', '0.993', '0.999', '0.958', '0.957']
Global density: 0.6331468820571899
06/02 07:06:46 PM | Train: [71/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:06:46 PM | layerwise density: [36503.0, 9986.0, 9635.0, 9972.0, 9545.0, 5985.0, 5457.0, 5714.0, 5434.0, 3381.0, 3167.0, 3411.0, 3107.0, 2034.0, 2047.0, 1960.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.731', '0.666', '0.698', '0.663', '0.825', '0.773', '0.833', '0.759', '0.993', '1.000', '0.957', '0.957']
Global density: 0.633162796497345
06/02 07:06:57 PM | Train: [71/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:06:57 PM | layerwise density: [36500.0, 9986.0, 9636.0, 9973.0, 9543.0, 5986.0, 5457.0, 5712.0, 5431.0, 3381.0, 3169.0, 3410.0, 3108.0, 2034.0, 2047.0, 1961.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.582', '0.731', '0.666', '0.697', '0.663', '0.825', '0.774', '0.833', '0.759', '0.993', '1.000', '0.958', '0.957']
Global density: 0.6331415772438049
06/02 07:07:06 PM | Train: [71/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:07:06 PM | layerwise density: [36501.0, 9986.0, 9637.0, 9973.0, 9543.0, 5989.0, 5461.0, 5715.0, 5432.0, 3378.0, 3170.0, 3410.0, 3109.0, 2034.0, 2046.0, 1961.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.582', '0.731', '0.667', '0.698', '0.663', '0.825', '0.774', '0.833', '0.759', '0.993', '0.999', '0.958', '0.957']
Global density: 0.63319993019104
06/02 07:07:06 PM | Train: [71/200] Final Prec@1 99.9600%
06/02 07:07:06 PM | Valid: [71/200] Step 000/078 Loss 1.198 Prec@(1,5) (75.0%, 93.0%)
06/02 07:07:09 PM | Valid: [71/200] Step 078/078 Loss 1.253 Prec@(1,5) (71.0%, 90.4%)
06/02 07:07:09 PM | Valid: [71/200] Final Prec@1 71.0400%
06/02 07:07:09 PM | Current mask training best Prec@1 = 71.1500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36501.0, 0.5569610595703125]
['model.relu.alpha_mask_1_0', 16384, 9986.0, 0.6094970703125]
['model.relu.alpha_mask_2_0', 16384, 9637.0, 0.58819580078125]
['model.relu.alpha_mask_3_0', 16384, 9973.0, 0.60870361328125]
['model.relu.alpha_mask_4_0', 16384, 9543.0, 0.58245849609375]
['model.relu.alpha_mask_5_0', 8192, 5990.0, 0.731201171875]
['model.relu.alpha_mask_6_0', 8192, 5461.0, 0.6666259765625]
['model.relu.alpha_mask_7_0', 8192, 5715.0, 0.6976318359375]
['model.relu.alpha_mask_8_0', 8192, 5432.0, 0.6630859375]
['model.relu.alpha_mask_9_0', 4096, 3378.0, 0.82470703125]
['model.relu.alpha_mask_10_0', 4096, 3170.0, 0.77392578125]
['model.relu.alpha_mask_11_0', 4096, 3410.0, 0.83251953125]
['model.relu.alpha_mask_12_0', 4096, 3109.0, 0.759033203125]
['model.relu.alpha_mask_13_0', 2048, 2034.0, 0.9931640625]
['model.relu.alpha_mask_14_0', 2048, 2046.0, 0.9990234375]
['model.relu.alpha_mask_15_0', 2048, 1961.0, 0.95751953125]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119306.0, 0.6332052479619565]
########## End ###########
06/02 07:07:10 PM | Train: [72/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:07:10 PM | layerwise density: [36501.0, 9986.0, 9637.0, 9973.0, 9543.0, 5990.0, 5461.0, 5715.0, 5432.0, 3378.0, 3170.0, 3410.0, 3109.0, 2034.0, 2046.0, 1961.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.582', '0.731', '0.667', '0.698', '0.663', '0.825', '0.774', '0.833', '0.759', '0.993', '0.999', '0.958', '0.957']
Global density: 0.633205235004425
06/02 07:07:21 PM | Train: [72/80] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:07:21 PM | layerwise density: [36501.0, 9985.0, 9636.0, 9974.0, 9544.0, 5993.0, 5464.0, 5715.0, 5429.0, 3379.0, 3170.0, 3410.0, 3116.0, 2037.0, 2046.0, 1961.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.698', '0.663', '0.825', '0.774', '0.833', '0.761', '0.995', '0.999', '0.958', '0.957']
Global density: 0.63327956199646
06/02 07:07:31 PM | Train: [72/80] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:07:31 PM | layerwise density: [36500.0, 9985.0, 9636.0, 9975.0, 9544.0, 5993.0, 5464.0, 5718.0, 5427.0, 3384.0, 3169.0, 3411.0, 3119.0, 2035.0, 2046.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.698', '0.662', '0.826', '0.774', '0.833', '0.761', '0.994', '0.999', '0.956', '0.957']
Global density: 0.633295476436615
06/02 07:07:42 PM | Train: [72/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:07:42 PM | layerwise density: [36498.0, 9986.0, 9636.0, 9976.0, 9547.0, 5994.0, 5464.0, 5721.0, 5426.0, 3386.0, 3170.0, 3411.0, 3122.0, 2036.0, 2045.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.698', '0.662', '0.827', '0.774', '0.833', '0.762', '0.994', '0.999', '0.956', '0.957']
Global density: 0.6333591938018799
06/02 07:07:51 PM | Train: [72/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:07:51 PM | layerwise density: [36498.0, 9986.0, 9636.0, 9978.0, 9547.0, 5994.0, 5464.0, 5721.0, 5426.0, 3387.0, 3172.0, 3411.0, 3123.0, 2036.0, 2044.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.698', '0.662', '0.827', '0.774', '0.833', '0.762', '0.994', '0.998', '0.956', '0.957']
Global density: 0.6333857178688049
06/02 07:07:51 PM | Train: [72/200] Final Prec@1 99.9720%
06/02 07:07:51 PM | Valid: [72/200] Step 000/078 Loss 1.179 Prec@(1,5) (74.2%, 94.5%)
06/02 07:07:54 PM | Valid: [72/200] Step 078/078 Loss 1.253 Prec@(1,5) (71.2%, 90.6%)
06/02 07:07:54 PM | Valid: [72/200] Final Prec@1 71.1700%
06/02 07:07:54 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36498.0, 0.556915283203125]
['model.relu.alpha_mask_1_0', 16384, 9986.0, 0.6094970703125]
['model.relu.alpha_mask_2_0', 16384, 9636.0, 0.588134765625]
['model.relu.alpha_mask_3_0', 16384, 9978.0, 0.6090087890625]
['model.relu.alpha_mask_4_0', 16384, 9547.0, 0.58270263671875]
['model.relu.alpha_mask_5_0', 8192, 5994.0, 0.731689453125]
['model.relu.alpha_mask_6_0', 8192, 5464.0, 0.6669921875]
['model.relu.alpha_mask_7_0', 8192, 5721.0, 0.6983642578125]
['model.relu.alpha_mask_8_0', 8192, 5426.0, 0.662353515625]
['model.relu.alpha_mask_9_0', 4096, 3387.0, 0.826904296875]
['model.relu.alpha_mask_10_0', 4096, 3172.0, 0.7744140625]
['model.relu.alpha_mask_11_0', 4096, 3411.0, 0.832763671875]
['model.relu.alpha_mask_12_0', 4096, 3123.0, 0.762451171875]
['model.relu.alpha_mask_13_0', 2048, 2036.0, 0.994140625]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1957.0, 0.95556640625]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119340.0, 0.6333856997282609]
########## End ###########
06/02 07:07:55 PM | Train: [73/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:07:55 PM | layerwise density: [36498.0, 9986.0, 9636.0, 9978.0, 9547.0, 5994.0, 5464.0, 5721.0, 5426.0, 3387.0, 3172.0, 3411.0, 3123.0, 2036.0, 2044.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.698', '0.662', '0.827', '0.774', '0.833', '0.762', '0.994', '0.998', '0.956', '0.957']
Global density: 0.6333857178688049
06/02 07:08:06 PM | Train: [73/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:08:06 PM | layerwise density: [36498.0, 9985.0, 9636.0, 9978.0, 9547.0, 5995.0, 5463.0, 5723.0, 5423.0, 3389.0, 3173.0, 3411.0, 3123.0, 2038.0, 2043.0, 1959.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.699', '0.662', '0.827', '0.775', '0.833', '0.762', '0.995', '0.998', '0.957', '0.957']
Global density: 0.633406937122345
06/02 07:08:17 PM | Train: [73/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:08:17 PM | layerwise density: [36499.0, 9984.0, 9636.0, 9978.0, 9548.0, 5996.0, 5462.0, 5723.0, 5427.0, 3390.0, 3176.0, 3411.0, 3124.0, 2039.0, 2043.0, 1961.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.699', '0.662', '0.828', '0.775', '0.833', '0.763', '0.996', '0.998', '0.958', '0.957']
Global density: 0.6334759593009949
06/02 07:08:28 PM | Train: [73/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:08:28 PM | layerwise density: [36500.0, 9985.0, 9636.0, 9977.0, 9550.0, 5998.0, 5465.0, 5725.0, 5426.0, 3393.0, 3178.0, 3411.0, 3123.0, 2039.0, 2043.0, 1962.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.699', '0.662', '0.828', '0.776', '0.833', '0.762', '0.996', '0.998', '0.958', '0.957']
Global density: 0.633550226688385
06/02 07:08:37 PM | Train: [73/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:08:37 PM | layerwise density: [36500.0, 9986.0, 9635.0, 9978.0, 9548.0, 6000.0, 5465.0, 5724.0, 5426.0, 3392.0, 3181.0, 3411.0, 3124.0, 2039.0, 2042.0, 1962.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.699', '0.662', '0.828', '0.777', '0.833', '0.763', '0.996', '0.997', '0.958', '0.957']
Global density: 0.633560836315155
06/02 07:08:38 PM | Train: [73/200] Final Prec@1 99.9640%
06/02 07:08:38 PM | Valid: [73/200] Step 000/078 Loss 1.165 Prec@(1,5) (73.4%, 93.0%)
06/02 07:08:40 PM | Valid: [73/200] Step 078/078 Loss 1.252 Prec@(1,5) (71.1%, 90.4%)
06/02 07:08:40 PM | Valid: [73/200] Final Prec@1 71.1300%
06/02 07:08:40 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36500.0, 0.55694580078125]
['model.relu.alpha_mask_1_0', 16384, 9986.0, 0.6094970703125]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9978.0, 0.6090087890625]
['model.relu.alpha_mask_4_0', 16384, 9548.0, 0.582763671875]
['model.relu.alpha_mask_5_0', 8192, 6000.0, 0.732421875]
['model.relu.alpha_mask_6_0', 8192, 5465.0, 0.6671142578125]
['model.relu.alpha_mask_7_0', 8192, 5724.0, 0.69873046875]
['model.relu.alpha_mask_8_0', 8192, 5426.0, 0.662353515625]
['model.relu.alpha_mask_9_0', 4096, 3392.0, 0.828125]
['model.relu.alpha_mask_10_0', 4096, 3181.0, 0.776611328125]
['model.relu.alpha_mask_11_0', 4096, 3411.0, 0.832763671875]
['model.relu.alpha_mask_12_0', 4096, 3124.0, 0.7626953125]
['model.relu.alpha_mask_13_0', 2048, 2039.0, 0.99560546875]
['model.relu.alpha_mask_14_0', 2048, 2042.0, 0.9970703125]
['model.relu.alpha_mask_15_0', 2048, 1962.0, 0.9580078125]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119373.0, 0.633560844089674]
########## End ###########
06/02 07:08:41 PM | Train: [74/80] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:08:41 PM | layerwise density: [36500.0, 9986.0, 9635.0, 9978.0, 9548.0, 6000.0, 5465.0, 5724.0, 5426.0, 3392.0, 3181.0, 3411.0, 3124.0, 2039.0, 2042.0, 1962.0, 1960.0]
layerwise density percentage: ['0.557', '0.609', '0.588', '0.609', '0.583', '0.732', '0.667', '0.699', '0.662', '0.828', '0.777', '0.833', '0.763', '0.996', '0.997', '0.958', '0.957']
Global density: 0.633560836315155
06/02 07:08:52 PM | Train: [74/80] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:08:52 PM | layerwise density: [36501.0, 9987.0, 9636.0, 9978.0, 9547.0, 6001.0, 5466.0, 5724.0, 5428.0, 3391.0, 3181.0, 3411.0, 3125.0, 2039.0, 2042.0, 1963.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.667', '0.699', '0.663', '0.828', '0.777', '0.833', '0.763', '0.996', '0.997', '0.958', '0.957']
Global density: 0.6335980296134949
06/02 07:09:02 PM | Train: [74/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:09:02 PM | layerwise density: [36501.0, 9987.0, 9637.0, 9979.0, 9547.0, 6004.0, 5463.0, 5724.0, 5431.0, 3392.0, 3183.0, 3412.0, 3125.0, 2039.0, 2042.0, 1960.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.667', '0.699', '0.663', '0.828', '0.777', '0.833', '0.763', '0.996', '0.997', '0.957', '0.957']
Global density: 0.6336298584938049
06/02 07:09:13 PM | Train: [74/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:09:13 PM | layerwise density: [36502.0, 9989.0, 9640.0, 9980.0, 9548.0, 6005.0, 5465.0, 5722.0, 5432.0, 3394.0, 3183.0, 3412.0, 3124.0, 2038.0, 2042.0, 1959.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.667', '0.698', '0.663', '0.829', '0.777', '0.833', '0.763', '0.995', '0.997', '0.957', '0.957']
Global density: 0.63367760181427
06/02 07:09:22 PM | Train: [74/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:09:22 PM | layerwise density: [36500.0, 9990.0, 9640.0, 9980.0, 9549.0, 6005.0, 5464.0, 5721.0, 5432.0, 3393.0, 3184.0, 3412.0, 3127.0, 2037.0, 2042.0, 1958.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.667', '0.698', '0.663', '0.828', '0.777', '0.833', '0.763', '0.995', '0.997', '0.956', '0.957']
Global density: 0.633672297000885
06/02 07:09:22 PM | Train: [74/200] Final Prec@1 99.9600%
06/02 07:09:23 PM | Valid: [74/200] Step 000/078 Loss 1.168 Prec@(1,5) (74.2%, 93.8%)
06/02 07:09:25 PM | Valid: [74/200] Step 078/078 Loss 1.251 Prec@(1,5) (71.0%, 90.5%)
06/02 07:09:25 PM | Valid: [74/200] Final Prec@1 71.0100%
06/02 07:09:25 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36500.0, 0.55694580078125]
['model.relu.alpha_mask_1_0', 16384, 9990.0, 0.6097412109375]
['model.relu.alpha_mask_2_0', 16384, 9640.0, 0.58837890625]
['model.relu.alpha_mask_3_0', 16384, 9980.0, 0.609130859375]
['model.relu.alpha_mask_4_0', 16384, 9549.0, 0.58282470703125]
['model.relu.alpha_mask_5_0', 8192, 6005.0, 0.7330322265625]
['model.relu.alpha_mask_6_0', 8192, 5464.0, 0.6669921875]
['model.relu.alpha_mask_7_0', 8192, 5721.0, 0.6983642578125]
['model.relu.alpha_mask_8_0', 8192, 5432.0, 0.6630859375]
['model.relu.alpha_mask_9_0', 4096, 3393.0, 0.828369140625]
['model.relu.alpha_mask_10_0', 4096, 3184.0, 0.77734375]
['model.relu.alpha_mask_11_0', 4096, 3412.0, 0.8330078125]
['model.relu.alpha_mask_12_0', 4096, 3127.0, 0.763427734375]
['model.relu.alpha_mask_13_0', 2048, 2037.0, 0.99462890625]
['model.relu.alpha_mask_14_0', 2048, 2042.0, 0.9970703125]
['model.relu.alpha_mask_15_0', 2048, 1958.0, 0.9560546875]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119394.0, 0.6336722995923914]
########## End ###########
06/02 07:09:26 PM | Train: [75/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:09:26 PM | layerwise density: [36500.0, 9990.0, 9640.0, 9980.0, 9549.0, 6005.0, 5464.0, 5721.0, 5432.0, 3393.0, 3184.0, 3412.0, 3127.0, 2037.0, 2042.0, 1958.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.667', '0.698', '0.663', '0.828', '0.777', '0.833', '0.763', '0.995', '0.997', '0.956', '0.957']
Global density: 0.633672297000885
06/02 07:09:37 PM | Train: [75/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:09:37 PM | layerwise density: [36500.0, 9989.0, 9640.0, 9980.0, 9551.0, 6005.0, 5465.0, 5722.0, 5431.0, 3393.0, 3187.0, 3412.0, 3127.0, 2038.0, 2042.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.667', '0.698', '0.663', '0.828', '0.778', '0.833', '0.763', '0.995', '0.997', '0.956', '0.957']
Global density: 0.6336988210678101
06/02 07:09:47 PM | Train: [75/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:09:47 PM | layerwise density: [36500.0, 9990.0, 9640.0, 9979.0, 9552.0, 6007.0, 5466.0, 5722.0, 5431.0, 3393.0, 3186.0, 3412.0, 3128.0, 2038.0, 2043.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.667', '0.698', '0.663', '0.828', '0.778', '0.833', '0.764', '0.995', '0.998', '0.956', '0.957']
Global density: 0.6337254047393799
06/02 07:09:58 PM | Train: [75/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:09:58 PM | layerwise density: [36500.0, 9989.0, 9640.0, 9979.0, 9553.0, 6009.0, 5468.0, 5723.0, 5431.0, 3393.0, 3186.0, 3412.0, 3128.0, 2038.0, 2043.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.667', '0.699', '0.663', '0.828', '0.778', '0.833', '0.764', '0.995', '0.998', '0.956', '0.957']
Global density: 0.6337519288063049
06/02 07:10:07 PM | Train: [75/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:10:07 PM | layerwise density: [36499.0, 9988.0, 9640.0, 9979.0, 9554.0, 6009.0, 5468.0, 5724.0, 5431.0, 3394.0, 3190.0, 3413.0, 3129.0, 2038.0, 2043.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.667', '0.699', '0.663', '0.829', '0.779', '0.833', '0.764', '0.995', '0.998', '0.956', '0.957']
Global density: 0.6337890625
06/02 07:10:07 PM | Train: [75/200] Final Prec@1 99.9620%
06/02 07:10:08 PM | Valid: [75/200] Step 000/078 Loss 1.209 Prec@(1,5) (71.1%, 93.0%)
06/02 07:10:10 PM | Valid: [75/200] Step 078/078 Loss 1.254 Prec@(1,5) (71.1%, 90.5%)
06/02 07:10:10 PM | Valid: [75/200] Final Prec@1 71.1000%
06/02 07:10:10 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9988.0, 0.609619140625]
['model.relu.alpha_mask_2_0', 16384, 9640.0, 0.58837890625]
['model.relu.alpha_mask_3_0', 16384, 9979.0, 0.60906982421875]
['model.relu.alpha_mask_4_0', 16384, 9554.0, 0.5831298828125]
['model.relu.alpha_mask_5_0', 8192, 6009.0, 0.7335205078125]
['model.relu.alpha_mask_6_0', 8192, 5468.0, 0.66748046875]
['model.relu.alpha_mask_7_0', 8192, 5724.0, 0.69873046875]
['model.relu.alpha_mask_8_0', 8192, 5431.0, 0.6629638671875]
['model.relu.alpha_mask_9_0', 4096, 3394.0, 0.82861328125]
['model.relu.alpha_mask_10_0', 4096, 3190.0, 0.77880859375]
['model.relu.alpha_mask_11_0', 4096, 3413.0, 0.833251953125]
['model.relu.alpha_mask_12_0', 4096, 3129.0, 0.763916015625]
['model.relu.alpha_mask_13_0', 2048, 2038.0, 0.9951171875]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1957.0, 0.95556640625]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119416.0, 0.6337890625]
########## End ###########
06/02 07:10:11 PM | Train: [76/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:10:11 PM | layerwise density: [36499.0, 9988.0, 9640.0, 9979.0, 9554.0, 6009.0, 5468.0, 5724.0, 5431.0, 3394.0, 3190.0, 3413.0, 3129.0, 2038.0, 2043.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.667', '0.699', '0.663', '0.829', '0.779', '0.833', '0.764', '0.995', '0.998', '0.956', '0.957']
Global density: 0.6337890625
06/02 07:10:22 PM | Train: [76/80] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:10:22 PM | layerwise density: [36497.0, 9988.0, 9637.0, 9978.0, 9553.0, 6009.0, 5469.0, 5727.0, 5432.0, 3394.0, 3190.0, 3413.0, 3128.0, 2039.0, 2043.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.699', '0.663', '0.829', '0.779', '0.833', '0.764', '0.996', '0.998', '0.956', '0.957']
Global density: 0.63377845287323
06/02 07:10:32 PM | Train: [76/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:10:32 PM | layerwise density: [36496.0, 9987.0, 9637.0, 9978.0, 9553.0, 6008.0, 5471.0, 5727.0, 5434.0, 3394.0, 3191.0, 3413.0, 3131.0, 2039.0, 2043.0, 1956.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.668', '0.699', '0.663', '0.829', '0.779', '0.833', '0.764', '0.996', '0.998', '0.955', '0.957']
Global density: 0.63379967212677
06/02 07:10:42 PM | Train: [76/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:10:42 PM | layerwise density: [36496.0, 9988.0, 9637.0, 9978.0, 9555.0, 6008.0, 5471.0, 5727.0, 5433.0, 3397.0, 3190.0, 3413.0, 3131.0, 2040.0, 2043.0, 1955.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.733', '0.668', '0.699', '0.663', '0.829', '0.779', '0.833', '0.764', '0.996', '0.998', '0.955', '0.957']
Global density: 0.6338208913803101
06/02 07:10:52 PM | Train: [76/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:10:52 PM | layerwise density: [36496.0, 9988.0, 9637.0, 9978.0, 9556.0, 6010.0, 5471.0, 5727.0, 5432.0, 3397.0, 3190.0, 3414.0, 3133.0, 2040.0, 2043.0, 1953.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.699', '0.663', '0.829', '0.779', '0.833', '0.765', '0.996', '0.998', '0.954', '0.957']
Global density: 0.6338368654251099
06/02 07:10:52 PM | Train: [76/200] Final Prec@1 99.9680%
06/02 07:10:52 PM | Valid: [76/200] Step 000/078 Loss 1.182 Prec@(1,5) (74.2%, 93.0%)
06/02 07:10:54 PM | Valid: [76/200] Step 078/078 Loss 1.253 Prec@(1,5) (71.1%, 90.4%)
06/02 07:10:55 PM | Valid: [76/200] Final Prec@1 71.1200%
06/02 07:10:55 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36496.0, 0.556884765625]
['model.relu.alpha_mask_1_0', 16384, 9988.0, 0.609619140625]
['model.relu.alpha_mask_2_0', 16384, 9637.0, 0.58819580078125]
['model.relu.alpha_mask_3_0', 16384, 9978.0, 0.6090087890625]
['model.relu.alpha_mask_4_0', 16384, 9556.0, 0.583251953125]
['model.relu.alpha_mask_5_0', 8192, 6010.0, 0.733642578125]
['model.relu.alpha_mask_6_0', 8192, 5471.0, 0.6678466796875]
['model.relu.alpha_mask_7_0', 8192, 5727.0, 0.6990966796875]
['model.relu.alpha_mask_8_0', 8192, 5432.0, 0.6630859375]
['model.relu.alpha_mask_9_0', 4096, 3397.0, 0.829345703125]
['model.relu.alpha_mask_10_0', 4096, 3190.0, 0.77880859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3133.0, 0.764892578125]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2043.0, 0.99755859375]
['model.relu.alpha_mask_15_0', 2048, 1953.0, 0.95361328125]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119425.0, 0.6338368291440217]
########## End ###########
06/02 07:10:56 PM | Train: [77/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:10:56 PM | layerwise density: [36496.0, 9988.0, 9637.0, 9978.0, 9556.0, 6010.0, 5471.0, 5727.0, 5432.0, 3397.0, 3190.0, 3414.0, 3133.0, 2040.0, 2043.0, 1953.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.699', '0.663', '0.829', '0.779', '0.833', '0.765', '0.996', '0.998', '0.954', '0.957']
Global density: 0.6338368654251099
06/02 07:11:06 PM | Train: [77/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:11:06 PM | layerwise density: [36496.0, 9988.0, 9637.0, 9978.0, 9556.0, 6011.0, 5471.0, 5727.0, 5432.0, 3398.0, 3190.0, 3414.0, 3133.0, 2040.0, 2043.0, 1956.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.699', '0.663', '0.830', '0.779', '0.833', '0.765', '0.996', '0.998', '0.955', '0.957']
Global density: 0.6338633894920349
06/02 07:11:17 PM | Train: [77/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:11:17 PM | layerwise density: [36496.0, 9988.0, 9637.0, 9978.0, 9557.0, 6012.0, 5472.0, 5728.0, 5431.0, 3398.0, 3193.0, 3414.0, 3134.0, 2040.0, 2043.0, 1956.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.699', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.955', '0.957']
Global density: 0.63390052318573
06/02 07:11:27 PM | Train: [77/80] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:11:27 PM | layerwise density: [36496.0, 9988.0, 9637.0, 9979.0, 9557.0, 6012.0, 5474.0, 5728.0, 5432.0, 3398.0, 3193.0, 3414.0, 3133.0, 2040.0, 2043.0, 1956.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.699', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.955', '0.957']
Global density: 0.633916437625885
06/02 07:11:37 PM | Train: [77/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:11:37 PM | layerwise density: [36496.0, 9988.0, 9637.0, 9979.0, 9559.0, 6012.0, 5474.0, 5729.0, 5433.0, 3399.0, 3193.0, 3414.0, 3134.0, 2040.0, 2044.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.699', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.956', '0.957']
Global density: 0.6339589357376099
06/02 07:11:37 PM | Train: [77/200] Final Prec@1 99.9540%
06/02 07:11:37 PM | Valid: [77/200] Step 000/078 Loss 1.202 Prec@(1,5) (73.4%, 93.0%)
06/02 07:11:40 PM | Valid: [77/200] Step 078/078 Loss 1.253 Prec@(1,5) (71.0%, 90.6%)
06/02 07:11:40 PM | Valid: [77/200] Final Prec@1 71.0000%
06/02 07:11:40 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36496.0, 0.556884765625]
['model.relu.alpha_mask_1_0', 16384, 9988.0, 0.609619140625]
['model.relu.alpha_mask_2_0', 16384, 9637.0, 0.58819580078125]
['model.relu.alpha_mask_3_0', 16384, 9979.0, 0.60906982421875]
['model.relu.alpha_mask_4_0', 16384, 9559.0, 0.58343505859375]
['model.relu.alpha_mask_5_0', 8192, 6012.0, 0.73388671875]
['model.relu.alpha_mask_6_0', 8192, 5474.0, 0.668212890625]
['model.relu.alpha_mask_7_0', 8192, 5729.0, 0.6993408203125]
['model.relu.alpha_mask_8_0', 8192, 5433.0, 0.6632080078125]
['model.relu.alpha_mask_9_0', 4096, 3399.0, 0.829833984375]
['model.relu.alpha_mask_10_0', 4096, 3192.0, 0.779296875]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3134.0, 0.76513671875]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1957.0, 0.95556640625]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119447.0, 0.6339535920516305]
########## End ###########
06/02 07:11:41 PM | Train: [78/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:11:41 PM | layerwise density: [36496.0, 9988.0, 9637.0, 9979.0, 9559.0, 6012.0, 5474.0, 5729.0, 5433.0, 3399.0, 3192.0, 3414.0, 3134.0, 2040.0, 2044.0, 1957.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.699', '0.663', '0.830', '0.779', '0.833', '0.765', '0.996', '0.998', '0.956', '0.957']
Global density: 0.6339536309242249
06/02 07:11:51 PM | Train: [78/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:11:51 PM | layerwise density: [36497.0, 9988.0, 9637.0, 9979.0, 9559.0, 6012.0, 5474.0, 5731.0, 5433.0, 3399.0, 3193.0, 3414.0, 3134.0, 2040.0, 2044.0, 1958.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.700', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.956', '0.957']
Global density: 0.6339801549911499
06/02 07:12:02 PM | Train: [78/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:12:02 PM | layerwise density: [36496.0, 9987.0, 9637.0, 9979.0, 9559.0, 6013.0, 5475.0, 5731.0, 5433.0, 3399.0, 3193.0, 3414.0, 3133.0, 2040.0, 2044.0, 1959.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.700', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.957', '0.957']
Global density: 0.6339801549911499
06/02 07:12:12 PM | Train: [78/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:12:12 PM | layerwise density: [36496.0, 9987.0, 9636.0, 9979.0, 9559.0, 6014.0, 5474.0, 5731.0, 5435.0, 3399.0, 3195.0, 3414.0, 3134.0, 2040.0, 2044.0, 1959.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.700', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.957', '0.957']
Global density: 0.6340013742446899
06/02 07:12:22 PM | Train: [78/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:12:22 PM | layerwise density: [36497.0, 9988.0, 9636.0, 9979.0, 9560.0, 6015.0, 5475.0, 5731.0, 5435.0, 3399.0, 3196.0, 3414.0, 3133.0, 2039.0, 2044.0, 1959.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.700', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.957', '0.957']
Global density: 0.63402259349823
06/02 07:12:22 PM | Train: [78/200] Final Prec@1 99.9580%
06/02 07:12:22 PM | Valid: [78/200] Step 000/078 Loss 1.160 Prec@(1,5) (75.0%, 93.8%)
06/02 07:12:25 PM | Valid: [78/200] Step 078/078 Loss 1.250 Prec@(1,5) (71.1%, 90.5%)
06/02 07:12:25 PM | Valid: [78/200] Final Prec@1 71.1300%
06/02 07:12:25 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36497.0, 0.5569000244140625]
['model.relu.alpha_mask_1_0', 16384, 9988.0, 0.609619140625]
['model.relu.alpha_mask_2_0', 16384, 9636.0, 0.588134765625]
['model.relu.alpha_mask_3_0', 16384, 9979.0, 0.60906982421875]
['model.relu.alpha_mask_4_0', 16384, 9560.0, 0.58349609375]
['model.relu.alpha_mask_5_0', 8192, 6015.0, 0.7342529296875]
['model.relu.alpha_mask_6_0', 8192, 5475.0, 0.6683349609375]
['model.relu.alpha_mask_7_0', 8192, 5731.0, 0.6995849609375]
['model.relu.alpha_mask_8_0', 8192, 5435.0, 0.6634521484375]
['model.relu.alpha_mask_9_0', 4096, 3399.0, 0.829833984375]
['model.relu.alpha_mask_10_0', 4096, 3196.0, 0.7802734375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3133.0, 0.764892578125]
['model.relu.alpha_mask_13_0', 2048, 2039.0, 0.99560546875]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1959.0, 0.95654296875]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119460.0, 0.6340225883152174]
########## End ###########
06/02 07:12:26 PM | Train: [79/80] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:12:26 PM | layerwise density: [36497.0, 9988.0, 9636.0, 9979.0, 9560.0, 6015.0, 5475.0, 5731.0, 5435.0, 3399.0, 3196.0, 3414.0, 3133.0, 2039.0, 2044.0, 1959.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.700', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.957', '0.957']
Global density: 0.63402259349823
06/02 07:12:36 PM | Train: [79/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:12:36 PM | layerwise density: [36498.0, 9988.0, 9636.0, 9979.0, 9560.0, 6015.0, 5476.0, 5732.0, 5436.0, 3400.0, 3195.0, 3414.0, 3134.0, 2039.0, 2044.0, 1956.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.668', '0.700', '0.664', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.955', '0.957']
Global density: 0.634033203125
06/02 07:12:46 PM | Train: [79/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:12:46 PM | layerwise density: [36498.0, 9988.0, 9636.0, 9978.0, 9560.0, 6017.0, 5477.0, 5732.0, 5435.0, 3400.0, 3196.0, 3414.0, 3135.0, 2039.0, 2044.0, 1956.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.669', '0.700', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.955', '0.957']
Global density: 0.634049117565155
06/02 07:12:57 PM | Train: [79/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:12:57 PM | layerwise density: [36498.0, 9987.0, 9636.0, 9978.0, 9561.0, 6017.0, 5477.0, 5733.0, 5435.0, 3400.0, 3196.0, 3414.0, 3135.0, 2040.0, 2044.0, 1956.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.584', '0.734', '0.669', '0.700', '0.663', '0.830', '0.780', '0.833', '0.765', '0.996', '0.998', '0.955', '0.957']
Global density: 0.634059727191925
06/02 07:13:07 PM | Train: [79/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:13:07 PM | layerwise density: [36498.0, 9987.0, 9636.0, 9977.0, 9561.0, 6017.0, 5479.0, 5733.0, 5436.0, 3400.0, 3197.0, 3414.0, 3137.0, 2040.0, 2044.0, 1955.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.584', '0.734', '0.669', '0.700', '0.664', '0.830', '0.781', '0.833', '0.766', '0.996', '0.998', '0.955', '0.957']
Global density: 0.6340810060501099
06/02 07:13:07 PM | Train: [79/200] Final Prec@1 99.9660%
06/02 07:13:07 PM | Valid: [79/200] Step 000/078 Loss 1.188 Prec@(1,5) (74.2%, 93.0%)
06/02 07:13:09 PM | Valid: [79/200] Step 078/078 Loss 1.253 Prec@(1,5) (71.1%, 90.4%)
06/02 07:13:09 PM | Valid: [79/200] Final Prec@1 71.1100%
06/02 07:13:09 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36498.0, 0.556915283203125]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9636.0, 0.588134765625]
['model.relu.alpha_mask_3_0', 16384, 9977.0, 0.60894775390625]
['model.relu.alpha_mask_4_0', 16384, 9561.0, 0.58355712890625]
['model.relu.alpha_mask_5_0', 8192, 6017.0, 0.7344970703125]
['model.relu.alpha_mask_6_0', 8192, 5479.0, 0.6688232421875]
['model.relu.alpha_mask_7_0', 8192, 5733.0, 0.6998291015625]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3400.0, 0.830078125]
['model.relu.alpha_mask_10_0', 4096, 3197.0, 0.780517578125]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119471.0, 0.6340809697690217]
########## End ###########
06/02 07:13:10 PM | Train: [80/80] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 07:13:10 PM | layerwise density: [36498.0, 9987.0, 9636.0, 9977.0, 9561.0, 6017.0, 5479.0, 5733.0, 5436.0, 3400.0, 3197.0, 3414.0, 3137.0, 2040.0, 2044.0, 1955.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.584', '0.734', '0.669', '0.700', '0.664', '0.830', '0.781', '0.833', '0.766', '0.996', '0.998', '0.955', '0.957']
Global density: 0.6340810060501099
06/02 07:13:21 PM | Train: [80/80] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:13:21 PM | layerwise density: [36499.0, 9987.0, 9636.0, 9976.0, 9560.0, 6017.0, 5480.0, 5733.0, 5438.0, 3401.0, 3197.0, 3414.0, 3137.0, 2040.0, 2044.0, 1955.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.583', '0.734', '0.669', '0.700', '0.664', '0.830', '0.781', '0.833', '0.766', '0.996', '0.998', '0.955', '0.957']
Global density: 0.6340969204902649
06/02 07:13:31 PM | Train: [80/80] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:13:31 PM | layerwise density: [36499.0, 9987.0, 9636.0, 9975.0, 9561.0, 6018.0, 5481.0, 5733.0, 5437.0, 3402.0, 3198.0, 3414.0, 3137.0, 2040.0, 2044.0, 1956.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.584', '0.735', '0.669', '0.700', '0.664', '0.831', '0.781', '0.833', '0.766', '0.996', '0.998', '0.955', '0.957']
Global density: 0.6341181397438049
06/02 07:13:42 PM | Train: [80/80] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:13:42 PM | layerwise density: [36499.0, 9987.0, 9636.0, 9975.0, 9561.0, 6018.0, 5483.0, 5734.0, 5436.0, 3402.0, 3198.0, 3414.0, 3138.0, 2040.0, 2044.0, 1955.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.584', '0.735', '0.669', '0.700', '0.664', '0.831', '0.781', '0.833', '0.766', '0.996', '0.998', '0.955', '0.957']
Global density: 0.634128749370575
06/02 07:13:52 PM | Train: [80/80] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:13:52 PM | layerwise density: [36499.0, 9987.0, 9635.0, 9975.0, 9562.0, 6019.0, 5484.0, 5734.0, 5436.0, 3403.0, 3199.0, 3414.0, 3137.0, 2040.0, 2044.0, 1955.0, 1960.0]
layerwise density percentage: ['0.557', '0.610', '0.588', '0.609', '0.584', '0.735', '0.669', '0.700', '0.664', '0.831', '0.781', '0.833', '0.766', '0.996', '0.998', '0.955', '0.957']
Global density: 0.63414466381073
06/02 07:13:52 PM | Train: [80/200] Final Prec@1 99.9620%
06/02 07:13:52 PM | Valid: [80/200] Step 000/078 Loss 1.191 Prec@(1,5) (71.9%, 93.0%)
06/02 07:13:54 PM | Valid: [80/200] Step 078/078 Loss 1.252 Prec@(1,5) (71.2%, 90.4%)
06/02 07:13:54 PM | Valid: [80/200] Final Prec@1 71.1700%
06/02 07:13:54 PM | Current mask training best Prec@1 = 71.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:13:55 PM | Train: [ 1/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:14:06 PM | Train: [ 1/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:14:15 PM | Train: [ 1/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:14:26 PM | Train: [ 1/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:14:35 PM | Train: [ 1/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:14:35 PM | Train: [ 1/200] Final Prec@1 99.9760%
06/02 07:14:35 PM | Valid: [ 1/200] Step 000/078 Loss 1.183 Prec@(1,5) (72.7%, 93.8%)
06/02 07:14:37 PM | Valid: [ 1/200] Step 078/078 Loss 1.251 Prec@(1,5) (71.2%, 90.5%)
06/02 07:14:37 PM | Valid: [ 1/200] Final Prec@1 71.2300%
06/02 07:14:38 PM | Current best Prec@1 = 71.2300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:14:38 PM | Train: [ 2/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:14:49 PM | Train: [ 2/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:14:59 PM | Train: [ 2/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:15:09 PM | Train: [ 2/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:15:18 PM | Train: [ 2/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:15:18 PM | Train: [ 2/200] Final Prec@1 99.9700%
06/02 07:15:18 PM | Valid: [ 2/200] Step 000/078 Loss 1.187 Prec@(1,5) (73.4%, 93.0%)
06/02 07:15:21 PM | Valid: [ 2/200] Step 078/078 Loss 1.246 Prec@(1,5) (71.2%, 90.4%)
06/02 07:15:21 PM | Valid: [ 2/200] Final Prec@1 71.2200%
06/02 07:15:21 PM | Current best Prec@1 = 71.2300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:15:22 PM | Train: [ 3/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:15:32 PM | Train: [ 3/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:15:42 PM | Train: [ 3/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:15:52 PM | Train: [ 3/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:16:01 PM | Train: [ 3/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:16:01 PM | Train: [ 3/200] Final Prec@1 99.9680%
06/02 07:16:01 PM | Valid: [ 3/200] Step 000/078 Loss 1.190 Prec@(1,5) (71.9%, 93.0%)
06/02 07:16:03 PM | Valid: [ 3/200] Step 078/078 Loss 1.249 Prec@(1,5) (71.2%, 90.5%)
06/02 07:16:03 PM | Valid: [ 3/200] Final Prec@1 71.1800%
06/02 07:16:04 PM | Current best Prec@1 = 71.2300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:16:04 PM | Train: [ 4/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:16:14 PM | Train: [ 4/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:16:25 PM | Train: [ 4/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:16:35 PM | Train: [ 4/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:16:44 PM | Train: [ 4/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:16:44 PM | Train: [ 4/200] Final Prec@1 99.9680%
06/02 07:16:44 PM | Valid: [ 4/200] Step 000/078 Loss 1.187 Prec@(1,5) (75.0%, 93.0%)
06/02 07:16:46 PM | Valid: [ 4/200] Step 078/078 Loss 1.248 Prec@(1,5) (71.2%, 90.4%)
06/02 07:16:46 PM | Valid: [ 4/200] Final Prec@1 71.2200%
06/02 07:16:47 PM | Current best Prec@1 = 71.2300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:16:47 PM | Train: [ 5/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:16:58 PM | Train: [ 5/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:17:08 PM | Train: [ 5/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:17:18 PM | Train: [ 5/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:17:28 PM | Train: [ 5/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:17:28 PM | Train: [ 5/200] Final Prec@1 99.9780%
06/02 07:17:28 PM | Valid: [ 5/200] Step 000/078 Loss 1.206 Prec@(1,5) (72.7%, 93.0%)
06/02 07:17:30 PM | Valid: [ 5/200] Step 078/078 Loss 1.246 Prec@(1,5) (71.2%, 90.4%)
06/02 07:17:30 PM | Valid: [ 5/200] Final Prec@1 71.2400%
06/02 07:17:31 PM | Current best Prec@1 = 71.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:17:31 PM | Train: [ 6/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:17:41 PM | Train: [ 6/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:17:51 PM | Train: [ 6/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:18:01 PM | Train: [ 6/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:18:11 PM | Train: [ 6/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:18:11 PM | Train: [ 6/200] Final Prec@1 99.9720%
06/02 07:18:11 PM | Valid: [ 6/200] Step 000/078 Loss 1.172 Prec@(1,5) (73.4%, 93.0%)
06/02 07:18:13 PM | Valid: [ 6/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.2%, 90.4%)
06/02 07:18:13 PM | Valid: [ 6/200] Final Prec@1 71.1900%
06/02 07:18:14 PM | Current best Prec@1 = 71.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:18:14 PM | Train: [ 7/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:18:25 PM | Train: [ 7/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:18:35 PM | Train: [ 7/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:18:45 PM | Train: [ 7/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:18:54 PM | Train: [ 7/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:18:54 PM | Train: [ 7/200] Final Prec@1 99.9680%
06/02 07:18:54 PM | Valid: [ 7/200] Step 000/078 Loss 1.179 Prec@(1,5) (72.7%, 93.0%)
06/02 07:18:57 PM | Valid: [ 7/200] Step 078/078 Loss 1.247 Prec@(1,5) (71.1%, 90.4%)
06/02 07:18:57 PM | Valid: [ 7/200] Final Prec@1 71.1200%
06/02 07:18:57 PM | Current best Prec@1 = 71.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:18:58 PM | Train: [ 8/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 07:19:08 PM | Train: [ 8/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:19:18 PM | Train: [ 8/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:19:28 PM | Train: [ 8/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:19:37 PM | Train: [ 8/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:19:37 PM | Train: [ 8/200] Final Prec@1 99.9640%
06/02 07:19:38 PM | Valid: [ 8/200] Step 000/078 Loss 1.180 Prec@(1,5) (72.7%, 93.8%)
06/02 07:19:40 PM | Valid: [ 8/200] Step 078/078 Loss 1.250 Prec@(1,5) (70.9%, 90.4%)
06/02 07:19:40 PM | Valid: [ 8/200] Final Prec@1 70.9200%
06/02 07:19:41 PM | Current best Prec@1 = 71.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:19:41 PM | Train: [ 9/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:19:51 PM | Train: [ 9/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:20:01 PM | Train: [ 9/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:20:12 PM | Train: [ 9/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:20:21 PM | Train: [ 9/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:20:21 PM | Train: [ 9/200] Final Prec@1 99.9620%
06/02 07:20:21 PM | Valid: [ 9/200] Step 000/078 Loss 1.175 Prec@(1,5) (72.7%, 92.2%)
06/02 07:20:23 PM | Valid: [ 9/200] Step 078/078 Loss 1.249 Prec@(1,5) (71.2%, 90.3%)
06/02 07:20:23 PM | Valid: [ 9/200] Final Prec@1 71.1500%
06/02 07:20:24 PM | Current best Prec@1 = 71.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:20:24 PM | Train: [10/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:20:34 PM | Train: [10/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:20:45 PM | Train: [10/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:20:55 PM | Train: [10/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:21:04 PM | Train: [10/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:21:04 PM | Train: [10/200] Final Prec@1 99.9720%
06/02 07:21:04 PM | Valid: [10/200] Step 000/078 Loss 1.171 Prec@(1,5) (73.4%, 93.8%)
06/02 07:21:06 PM | Valid: [10/200] Step 078/078 Loss 1.248 Prec@(1,5) (71.1%, 90.5%)
06/02 07:21:06 PM | Valid: [10/200] Final Prec@1 71.1300%
06/02 07:21:07 PM | Current best Prec@1 = 71.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:21:07 PM | Train: [11/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:21:18 PM | Train: [11/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:21:28 PM | Train: [11/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:21:38 PM | Train: [11/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:21:47 PM | Train: [11/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:21:47 PM | Train: [11/200] Final Prec@1 99.9720%
06/02 07:21:47 PM | Valid: [11/200] Step 000/078 Loss 1.155 Prec@(1,5) (75.0%, 93.0%)
06/02 07:21:50 PM | Valid: [11/200] Step 078/078 Loss 1.246 Prec@(1,5) (71.2%, 90.4%)
06/02 07:21:50 PM | Valid: [11/200] Final Prec@1 71.2000%
06/02 07:21:50 PM | Current best Prec@1 = 71.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:21:51 PM | Train: [12/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 07:22:01 PM | Train: [12/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:22:11 PM | Train: [12/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:22:22 PM | Train: [12/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:22:31 PM | Train: [12/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:22:31 PM | Train: [12/200] Final Prec@1 99.9640%
06/02 07:22:31 PM | Valid: [12/200] Step 000/078 Loss 1.166 Prec@(1,5) (72.7%, 93.0%)
06/02 07:22:34 PM | Valid: [12/200] Step 078/078 Loss 1.247 Prec@(1,5) (71.1%, 90.5%)
06/02 07:22:34 PM | Valid: [12/200] Final Prec@1 71.1300%
06/02 07:22:34 PM | Current best Prec@1 = 71.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:22:35 PM | Train: [13/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:22:45 PM | Train: [13/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:22:55 PM | Train: [13/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:23:06 PM | Train: [13/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:23:15 PM | Train: [13/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:23:15 PM | Train: [13/200] Final Prec@1 99.9600%
06/02 07:23:15 PM | Valid: [13/200] Step 000/078 Loss 1.186 Prec@(1,5) (75.0%, 93.0%)
06/02 07:23:17 PM | Valid: [13/200] Step 078/078 Loss 1.246 Prec@(1,5) (71.5%, 90.3%)
06/02 07:23:18 PM | Valid: [13/200] Final Prec@1 71.4500%
06/02 07:23:18 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:23:18 PM | Train: [14/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:23:29 PM | Train: [14/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:23:39 PM | Train: [14/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:23:49 PM | Train: [14/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:23:58 PM | Train: [14/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:23:58 PM | Train: [14/200] Final Prec@1 99.9740%
06/02 07:23:59 PM | Valid: [14/200] Step 000/078 Loss 1.195 Prec@(1,5) (72.7%, 93.0%)
06/02 07:24:01 PM | Valid: [14/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.2%, 90.4%)
06/02 07:24:01 PM | Valid: [14/200] Final Prec@1 71.2300%
06/02 07:24:02 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:24:02 PM | Train: [15/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:24:12 PM | Train: [15/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:24:22 PM | Train: [15/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:24:33 PM | Train: [15/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:24:42 PM | Train: [15/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:24:42 PM | Train: [15/200] Final Prec@1 99.9680%
06/02 07:24:42 PM | Valid: [15/200] Step 000/078 Loss 1.163 Prec@(1,5) (72.7%, 93.0%)
06/02 07:24:45 PM | Valid: [15/200] Step 078/078 Loss 1.248 Prec@(1,5) (71.0%, 90.2%)
06/02 07:24:45 PM | Valid: [15/200] Final Prec@1 70.9900%
06/02 07:24:45 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:24:46 PM | Train: [16/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:24:56 PM | Train: [16/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:25:06 PM | Train: [16/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:25:17 PM | Train: [16/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:25:26 PM | Train: [16/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:25:26 PM | Train: [16/200] Final Prec@1 99.9740%
06/02 07:25:26 PM | Valid: [16/200] Step 000/078 Loss 1.191 Prec@(1,5) (72.7%, 92.2%)
06/02 07:25:29 PM | Valid: [16/200] Step 078/078 Loss 1.247 Prec@(1,5) (71.1%, 90.5%)
06/02 07:25:29 PM | Valid: [16/200] Final Prec@1 71.0900%
06/02 07:25:30 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:25:30 PM | Train: [17/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:25:40 PM | Train: [17/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:25:50 PM | Train: [17/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:26:01 PM | Train: [17/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:26:11 PM | Train: [17/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:26:11 PM | Train: [17/200] Final Prec@1 99.9820%
06/02 07:26:11 PM | Valid: [17/200] Step 000/078 Loss 1.192 Prec@(1,5) (73.4%, 93.0%)
06/02 07:26:13 PM | Valid: [17/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.2%, 90.5%)
06/02 07:26:13 PM | Valid: [17/200] Final Prec@1 71.1700%
06/02 07:26:14 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:26:14 PM | Train: [18/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:26:25 PM | Train: [18/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:26:35 PM | Train: [18/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:26:46 PM | Train: [18/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:26:56 PM | Train: [18/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:26:56 PM | Train: [18/200] Final Prec@1 99.9720%
06/02 07:26:56 PM | Valid: [18/200] Step 000/078 Loss 1.175 Prec@(1,5) (74.2%, 93.0%)
06/02 07:26:58 PM | Valid: [18/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.2%, 90.4%)
06/02 07:26:58 PM | Valid: [18/200] Final Prec@1 71.2100%
06/02 07:26:59 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:26:59 PM | Train: [19/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 07:27:10 PM | Train: [19/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:27:20 PM | Train: [19/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:27:31 PM | Train: [19/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:27:40 PM | Train: [19/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:27:40 PM | Train: [19/200] Final Prec@1 99.9780%
06/02 07:27:40 PM | Valid: [19/200] Step 000/078 Loss 1.170 Prec@(1,5) (73.4%, 93.0%)
06/02 07:27:43 PM | Valid: [19/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.2%, 90.4%)
06/02 07:27:43 PM | Valid: [19/200] Final Prec@1 71.2200%
06/02 07:27:43 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:27:44 PM | Train: [20/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:27:54 PM | Train: [20/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:28:05 PM | Train: [20/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:28:15 PM | Train: [20/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:28:25 PM | Train: [20/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:28:25 PM | Train: [20/200] Final Prec@1 99.9760%
06/02 07:28:25 PM | Valid: [20/200] Step 000/078 Loss 1.174 Prec@(1,5) (73.4%, 93.0%)
06/02 07:28:27 PM | Valid: [20/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.4%, 90.5%)
06/02 07:28:27 PM | Valid: [20/200] Final Prec@1 71.4000%
06/02 07:28:28 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:28:28 PM | Train: [21/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:28:39 PM | Train: [21/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:28:50 PM | Train: [21/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:29:01 PM | Train: [21/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:29:10 PM | Train: [21/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:29:10 PM | Train: [21/200] Final Prec@1 99.9760%
06/02 07:29:11 PM | Valid: [21/200] Step 000/078 Loss 1.164 Prec@(1,5) (73.4%, 92.2%)
06/02 07:29:13 PM | Valid: [21/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.6%)
06/02 07:29:13 PM | Valid: [21/200] Final Prec@1 71.2300%
06/02 07:29:14 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:29:14 PM | Train: [22/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:29:24 PM | Train: [22/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:29:35 PM | Train: [22/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:29:46 PM | Train: [22/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:29:55 PM | Train: [22/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:29:55 PM | Train: [22/200] Final Prec@1 99.9740%
06/02 07:29:55 PM | Valid: [22/200] Step 000/078 Loss 1.175 Prec@(1,5) (74.2%, 93.0%)
06/02 07:29:58 PM | Valid: [22/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.2%, 90.5%)
06/02 07:29:58 PM | Valid: [22/200] Final Prec@1 71.2100%
06/02 07:29:58 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:29:59 PM | Train: [23/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:30:09 PM | Train: [23/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:30:19 PM | Train: [23/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:30:30 PM | Train: [23/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:30:39 PM | Train: [23/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:30:39 PM | Train: [23/200] Final Prec@1 99.9740%
06/02 07:30:39 PM | Valid: [23/200] Step 000/078 Loss 1.175 Prec@(1,5) (73.4%, 93.0%)
06/02 07:30:42 PM | Valid: [23/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.1%, 90.4%)
06/02 07:30:42 PM | Valid: [23/200] Final Prec@1 71.1000%
06/02 07:30:42 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:30:43 PM | Train: [24/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:30:53 PM | Train: [24/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:31:04 PM | Train: [24/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:31:14 PM | Train: [24/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:31:23 PM | Train: [24/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:31:23 PM | Train: [24/200] Final Prec@1 99.9680%
06/02 07:31:23 PM | Valid: [24/200] Step 000/078 Loss 1.186 Prec@(1,5) (72.7%, 92.2%)
06/02 07:31:26 PM | Valid: [24/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.5%)
06/02 07:31:26 PM | Valid: [24/200] Final Prec@1 71.2400%
06/02 07:31:26 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:31:27 PM | Train: [25/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:31:37 PM | Train: [25/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:31:48 PM | Train: [25/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:31:58 PM | Train: [25/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:32:08 PM | Train: [25/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:32:08 PM | Train: [25/200] Final Prec@1 99.9640%
06/02 07:32:08 PM | Valid: [25/200] Step 000/078 Loss 1.196 Prec@(1,5) (73.4%, 92.2%)
06/02 07:32:11 PM | Valid: [25/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.1%, 90.4%)
06/02 07:32:11 PM | Valid: [25/200] Final Prec@1 71.1400%
06/02 07:32:11 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:32:12 PM | Train: [26/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:32:22 PM | Train: [26/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:32:32 PM | Train: [26/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:32:43 PM | Train: [26/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:32:52 PM | Train: [26/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:32:52 PM | Train: [26/200] Final Prec@1 99.9800%
06/02 07:32:52 PM | Valid: [26/200] Step 000/078 Loss 1.189 Prec@(1,5) (72.7%, 93.8%)
06/02 07:32:55 PM | Valid: [26/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.2%, 90.3%)
06/02 07:32:55 PM | Valid: [26/200] Final Prec@1 71.1800%
06/02 07:32:55 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:32:56 PM | Train: [27/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:33:06 PM | Train: [27/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:33:17 PM | Train: [27/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:33:27 PM | Train: [27/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:33:37 PM | Train: [27/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:33:37 PM | Train: [27/200] Final Prec@1 99.9820%
06/02 07:33:37 PM | Valid: [27/200] Step 000/078 Loss 1.191 Prec@(1,5) (72.7%, 93.0%)
06/02 07:33:39 PM | Valid: [27/200] Step 078/078 Loss 1.247 Prec@(1,5) (71.2%, 90.3%)
06/02 07:33:40 PM | Valid: [27/200] Final Prec@1 71.1700%
06/02 07:33:40 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:33:40 PM | Train: [28/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:33:51 PM | Train: [28/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:34:01 PM | Train: [28/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:34:12 PM | Train: [28/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:34:21 PM | Train: [28/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:34:21 PM | Train: [28/200] Final Prec@1 99.9720%
06/02 07:34:21 PM | Valid: [28/200] Step 000/078 Loss 1.177 Prec@(1,5) (73.4%, 92.2%)
06/02 07:34:24 PM | Valid: [28/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.2%, 90.5%)
06/02 07:34:24 PM | Valid: [28/200] Final Prec@1 71.2400%
06/02 07:34:25 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:34:25 PM | Train: [29/200] Step 000/390 Loss 0.008 Prec@(1,5) (99.2%, 100.0%)
06/02 07:34:36 PM | Train: [29/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:34:46 PM | Train: [29/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:34:57 PM | Train: [29/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:35:06 PM | Train: [29/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:35:06 PM | Train: [29/200] Final Prec@1 99.9740%
06/02 07:35:06 PM | Valid: [29/200] Step 000/078 Loss 1.191 Prec@(1,5) (73.4%, 92.2%)
06/02 07:35:09 PM | Valid: [29/200] Step 078/078 Loss 1.246 Prec@(1,5) (71.3%, 90.5%)
06/02 07:35:09 PM | Valid: [29/200] Final Prec@1 71.2700%
06/02 07:35:09 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:35:10 PM | Train: [30/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:35:19 PM | Train: [30/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:35:30 PM | Train: [30/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:35:40 PM | Train: [30/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:35:49 PM | Train: [30/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:35:49 PM | Train: [30/200] Final Prec@1 99.9760%
06/02 07:35:50 PM | Valid: [30/200] Step 000/078 Loss 1.183 Prec@(1,5) (72.7%, 93.0%)
06/02 07:35:52 PM | Valid: [30/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.2%, 90.4%)
06/02 07:35:52 PM | Valid: [30/200] Final Prec@1 71.2500%
06/02 07:35:53 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:35:53 PM | Train: [31/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:36:03 PM | Train: [31/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:36:13 PM | Train: [31/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:36:23 PM | Train: [31/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:36:33 PM | Train: [31/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:36:33 PM | Train: [31/200] Final Prec@1 99.9660%
06/02 07:36:33 PM | Valid: [31/200] Step 000/078 Loss 1.182 Prec@(1,5) (71.9%, 92.2%)
06/02 07:36:35 PM | Valid: [31/200] Step 078/078 Loss 1.246 Prec@(1,5) (71.2%, 90.5%)
06/02 07:36:35 PM | Valid: [31/200] Final Prec@1 71.1500%
06/02 07:36:36 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:36:36 PM | Train: [32/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:36:46 PM | Train: [32/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:36:57 PM | Train: [32/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:37:07 PM | Train: [32/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:37:16 PM | Train: [32/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:37:17 PM | Train: [32/200] Final Prec@1 99.9720%
06/02 07:37:17 PM | Valid: [32/200] Step 000/078 Loss 1.186 Prec@(1,5) (72.7%, 93.8%)
06/02 07:37:19 PM | Valid: [32/200] Step 078/078 Loss 1.246 Prec@(1,5) (71.2%, 90.5%)
06/02 07:37:19 PM | Valid: [32/200] Final Prec@1 71.1600%
06/02 07:37:20 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:37:20 PM | Train: [33/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:37:30 PM | Train: [33/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:37:41 PM | Train: [33/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:37:51 PM | Train: [33/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:38:00 PM | Train: [33/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:38:00 PM | Train: [33/200] Final Prec@1 99.9800%
06/02 07:38:00 PM | Valid: [33/200] Step 000/078 Loss 1.170 Prec@(1,5) (72.7%, 93.0%)
06/02 07:38:03 PM | Valid: [33/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.5%)
06/02 07:38:03 PM | Valid: [33/200] Final Prec@1 71.2200%
06/02 07:38:03 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:38:04 PM | Train: [34/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:38:14 PM | Train: [34/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:38:24 PM | Train: [34/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:38:34 PM | Train: [34/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:38:44 PM | Train: [34/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:38:44 PM | Train: [34/200] Final Prec@1 99.9720%
06/02 07:38:44 PM | Valid: [34/200] Step 000/078 Loss 1.173 Prec@(1,5) (73.4%, 92.2%)
06/02 07:38:46 PM | Valid: [34/200] Step 078/078 Loss 1.246 Prec@(1,5) (71.2%, 90.4%)
06/02 07:38:46 PM | Valid: [34/200] Final Prec@1 71.1900%
06/02 07:38:47 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:38:47 PM | Train: [35/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:38:57 PM | Train: [35/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:39:08 PM | Train: [35/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:39:18 PM | Train: [35/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:39:26 PM | Train: [35/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:39:26 PM | Train: [35/200] Final Prec@1 99.9740%
06/02 07:39:27 PM | Valid: [35/200] Step 000/078 Loss 1.187 Prec@(1,5) (73.4%, 93.8%)
06/02 07:39:29 PM | Valid: [35/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.4%, 90.5%)
06/02 07:39:29 PM | Valid: [35/200] Final Prec@1 71.3800%
06/02 07:39:30 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:39:30 PM | Train: [36/200] Step 000/390 Loss 0.011 Prec@(1,5) (99.2%, 100.0%)
06/02 07:39:40 PM | Train: [36/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:39:50 PM | Train: [36/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:40:01 PM | Train: [36/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:40:10 PM | Train: [36/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:40:10 PM | Train: [36/200] Final Prec@1 99.9760%
06/02 07:40:10 PM | Valid: [36/200] Step 000/078 Loss 1.162 Prec@(1,5) (73.4%, 93.0%)
06/02 07:40:12 PM | Valid: [36/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.2%, 90.5%)
06/02 07:40:12 PM | Valid: [36/200] Final Prec@1 71.1600%
06/02 07:40:13 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:40:13 PM | Train: [37/200] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 07:40:23 PM | Train: [37/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:40:34 PM | Train: [37/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:40:44 PM | Train: [37/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:40:53 PM | Train: [37/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:40:53 PM | Train: [37/200] Final Prec@1 99.9760%
06/02 07:40:53 PM | Valid: [37/200] Step 000/078 Loss 1.160 Prec@(1,5) (72.7%, 93.8%)
06/02 07:40:56 PM | Valid: [37/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.3%, 90.3%)
06/02 07:40:56 PM | Valid: [37/200] Final Prec@1 71.2800%
06/02 07:40:56 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:40:57 PM | Train: [38/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:41:07 PM | Train: [38/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:41:17 PM | Train: [38/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:41:27 PM | Train: [38/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:41:37 PM | Train: [38/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:41:37 PM | Train: [38/200] Final Prec@1 99.9740%
06/02 07:41:37 PM | Valid: [38/200] Step 000/078 Loss 1.184 Prec@(1,5) (74.2%, 92.2%)
06/02 07:41:39 PM | Valid: [38/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.3%, 90.4%)
06/02 07:41:39 PM | Valid: [38/200] Final Prec@1 71.2900%
06/02 07:41:40 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:41:40 PM | Train: [39/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:41:50 PM | Train: [39/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:42:01 PM | Train: [39/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:42:11 PM | Train: [39/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:42:20 PM | Train: [39/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:42:20 PM | Train: [39/200] Final Prec@1 99.9760%
06/02 07:42:21 PM | Valid: [39/200] Step 000/078 Loss 1.189 Prec@(1,5) (73.4%, 93.0%)
06/02 07:42:23 PM | Valid: [39/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.5%)
06/02 07:42:23 PM | Valid: [39/200] Final Prec@1 71.1800%
06/02 07:42:24 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:42:24 PM | Train: [40/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:42:34 PM | Train: [40/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:42:44 PM | Train: [40/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:42:54 PM | Train: [40/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:43:03 PM | Train: [40/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:43:03 PM | Train: [40/200] Final Prec@1 99.9700%
06/02 07:43:03 PM | Valid: [40/200] Step 000/078 Loss 1.179 Prec@(1,5) (71.9%, 93.8%)
06/02 07:43:06 PM | Valid: [40/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.2%, 90.4%)
06/02 07:43:06 PM | Valid: [40/200] Final Prec@1 71.2300%
06/02 07:43:06 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:43:07 PM | Train: [41/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:43:17 PM | Train: [41/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:43:27 PM | Train: [41/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:43:38 PM | Train: [41/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:43:47 PM | Train: [41/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:43:47 PM | Train: [41/200] Final Prec@1 99.9780%
06/02 07:43:47 PM | Valid: [41/200] Step 000/078 Loss 1.170 Prec@(1,5) (74.2%, 91.4%)
06/02 07:43:49 PM | Valid: [41/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.3%, 90.3%)
06/02 07:43:49 PM | Valid: [41/200] Final Prec@1 71.3000%
06/02 07:43:50 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:43:50 PM | Train: [42/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:44:01 PM | Train: [42/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:44:11 PM | Train: [42/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:44:21 PM | Train: [42/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:44:30 PM | Train: [42/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:44:31 PM | Train: [42/200] Final Prec@1 99.9580%
06/02 07:44:31 PM | Valid: [42/200] Step 000/078 Loss 1.182 Prec@(1,5) (73.4%, 93.0%)
06/02 07:44:33 PM | Valid: [42/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.4%)
06/02 07:44:33 PM | Valid: [42/200] Final Prec@1 71.2300%
06/02 07:44:34 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:44:34 PM | Train: [43/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:44:45 PM | Train: [43/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:44:55 PM | Train: [43/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:45:05 PM | Train: [43/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:45:14 PM | Train: [43/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:45:14 PM | Train: [43/200] Final Prec@1 99.9700%
06/02 07:45:15 PM | Valid: [43/200] Step 000/078 Loss 1.175 Prec@(1,5) (72.7%, 93.0%)
06/02 07:45:17 PM | Valid: [43/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.3%, 90.4%)
06/02 07:45:17 PM | Valid: [43/200] Final Prec@1 71.3200%
06/02 07:45:18 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:45:18 PM | Train: [44/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:45:28 PM | Train: [44/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:45:38 PM | Train: [44/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:45:48 PM | Train: [44/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:45:57 PM | Train: [44/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:45:57 PM | Train: [44/200] Final Prec@1 99.9660%
06/02 07:45:57 PM | Valid: [44/200] Step 000/078 Loss 1.170 Prec@(1,5) (74.2%, 93.0%)
06/02 07:45:59 PM | Valid: [44/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 07:45:59 PM | Valid: [44/200] Final Prec@1 71.2400%
06/02 07:46:00 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:46:00 PM | Train: [45/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:46:10 PM | Train: [45/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:46:21 PM | Train: [45/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:46:31 PM | Train: [45/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:46:40 PM | Train: [45/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:46:40 PM | Train: [45/200] Final Prec@1 99.9700%
06/02 07:46:41 PM | Valid: [45/200] Step 000/078 Loss 1.171 Prec@(1,5) (74.2%, 93.0%)
06/02 07:46:43 PM | Valid: [45/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.2%, 90.5%)
06/02 07:46:43 PM | Valid: [45/200] Final Prec@1 71.2300%
06/02 07:46:43 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:46:44 PM | Train: [46/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:46:54 PM | Train: [46/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:47:04 PM | Train: [46/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:47:15 PM | Train: [46/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:47:24 PM | Train: [46/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:47:24 PM | Train: [46/200] Final Prec@1 99.9660%
06/02 07:47:24 PM | Valid: [46/200] Step 000/078 Loss 1.165 Prec@(1,5) (73.4%, 92.2%)
06/02 07:47:27 PM | Valid: [46/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.4%)
06/02 07:47:27 PM | Valid: [46/200] Final Prec@1 71.2700%
06/02 07:47:27 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:47:28 PM | Train: [47/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:47:38 PM | Train: [47/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:47:48 PM | Train: [47/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:47:59 PM | Train: [47/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:48:08 PM | Train: [47/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:48:08 PM | Train: [47/200] Final Prec@1 99.9720%
06/02 07:48:09 PM | Valid: [47/200] Step 000/078 Loss 1.178 Prec@(1,5) (71.9%, 92.2%)
06/02 07:48:11 PM | Valid: [47/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.4%)
06/02 07:48:11 PM | Valid: [47/200] Final Prec@1 71.2500%
06/02 07:48:11 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:48:12 PM | Train: [48/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:48:22 PM | Train: [48/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:48:32 PM | Train: [48/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:48:42 PM | Train: [48/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:48:51 PM | Train: [48/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:48:52 PM | Train: [48/200] Final Prec@1 99.9760%
06/02 07:48:52 PM | Valid: [48/200] Step 000/078 Loss 1.173 Prec@(1,5) (72.7%, 93.0%)
06/02 07:48:54 PM | Valid: [48/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.4%)
06/02 07:48:54 PM | Valid: [48/200] Final Prec@1 71.2500%
06/02 07:48:55 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:48:55 PM | Train: [49/200] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
06/02 07:49:05 PM | Train: [49/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:49:15 PM | Train: [49/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:49:26 PM | Train: [49/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:49:35 PM | Train: [49/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:49:35 PM | Train: [49/200] Final Prec@1 99.9720%
06/02 07:49:35 PM | Valid: [49/200] Step 000/078 Loss 1.170 Prec@(1,5) (72.7%, 92.2%)
06/02 07:49:37 PM | Valid: [49/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.3%, 90.5%)
06/02 07:49:38 PM | Valid: [49/200] Final Prec@1 71.2900%
06/02 07:49:38 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:49:39 PM | Train: [50/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:49:49 PM | Train: [50/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:49:59 PM | Train: [50/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:50:09 PM | Train: [50/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:50:18 PM | Train: [50/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:50:18 PM | Train: [50/200] Final Prec@1 99.9740%
06/02 07:50:18 PM | Valid: [50/200] Step 000/078 Loss 1.180 Prec@(1,5) (73.4%, 92.2%)
06/02 07:50:20 PM | Valid: [50/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.6%)
06/02 07:50:21 PM | Valid: [50/200] Final Prec@1 71.2600%
06/02 07:50:21 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:50:21 PM | Train: [51/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:50:31 PM | Train: [51/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:50:42 PM | Train: [51/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:50:52 PM | Train: [51/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:51:01 PM | Train: [51/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:51:01 PM | Train: [51/200] Final Prec@1 99.9640%
06/02 07:51:02 PM | Valid: [51/200] Step 000/078 Loss 1.171 Prec@(1,5) (73.4%, 93.0%)
06/02 07:51:04 PM | Valid: [51/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.1%, 90.5%)
06/02 07:51:04 PM | Valid: [51/200] Final Prec@1 71.0900%
06/02 07:51:05 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:51:05 PM | Train: [52/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:51:15 PM | Train: [52/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:51:25 PM | Train: [52/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:51:36 PM | Train: [52/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:51:45 PM | Train: [52/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:51:45 PM | Train: [52/200] Final Prec@1 99.9540%
06/02 07:51:45 PM | Valid: [52/200] Step 000/078 Loss 1.168 Prec@(1,5) (72.7%, 93.0%)
06/02 07:51:48 PM | Valid: [52/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.1%, 90.3%)
06/02 07:51:48 PM | Valid: [52/200] Final Prec@1 71.1200%
06/02 07:51:48 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:51:49 PM | Train: [53/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:51:59 PM | Train: [53/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 07:52:09 PM | Train: [53/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:52:19 PM | Train: [53/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:52:28 PM | Train: [53/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:52:28 PM | Train: [53/200] Final Prec@1 99.9720%
06/02 07:52:29 PM | Valid: [53/200] Step 000/078 Loss 1.182 Prec@(1,5) (73.4%, 93.0%)
06/02 07:52:31 PM | Valid: [53/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.2%, 90.4%)
06/02 07:52:31 PM | Valid: [53/200] Final Prec@1 71.1600%
06/02 07:52:31 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:52:32 PM | Train: [54/200] Step 000/390 Loss 0.009 Prec@(1,5) (99.2%, 100.0%)
06/02 07:52:42 PM | Train: [54/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:52:52 PM | Train: [54/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:53:02 PM | Train: [54/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:53:11 PM | Train: [54/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:53:11 PM | Train: [54/200] Final Prec@1 99.9620%
06/02 07:53:12 PM | Valid: [54/200] Step 000/078 Loss 1.181 Prec@(1,5) (74.2%, 92.2%)
06/02 07:53:14 PM | Valid: [54/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.5%)
06/02 07:53:14 PM | Valid: [54/200] Final Prec@1 71.2400%
06/02 07:53:15 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:53:15 PM | Train: [55/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:53:25 PM | Train: [55/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:53:35 PM | Train: [55/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:53:46 PM | Train: [55/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:53:55 PM | Train: [55/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:53:55 PM | Train: [55/200] Final Prec@1 99.9800%
06/02 07:53:55 PM | Valid: [55/200] Step 000/078 Loss 1.174 Prec@(1,5) (72.7%, 93.0%)
06/02 07:53:58 PM | Valid: [55/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.6%)
06/02 07:53:58 PM | Valid: [55/200] Final Prec@1 71.2300%
06/02 07:53:58 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:53:59 PM | Train: [56/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:54:09 PM | Train: [56/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:54:19 PM | Train: [56/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:54:29 PM | Train: [56/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:54:38 PM | Train: [56/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:54:38 PM | Train: [56/200] Final Prec@1 99.9820%
06/02 07:54:38 PM | Valid: [56/200] Step 000/078 Loss 1.182 Prec@(1,5) (72.7%, 93.0%)
06/02 07:54:41 PM | Valid: [56/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.2%, 90.5%)
06/02 07:54:41 PM | Valid: [56/200] Final Prec@1 71.2300%
06/02 07:54:41 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:54:42 PM | Train: [57/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:54:52 PM | Train: [57/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:55:03 PM | Train: [57/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:55:13 PM | Train: [57/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:55:22 PM | Train: [57/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:55:22 PM | Train: [57/200] Final Prec@1 99.9720%
06/02 07:55:22 PM | Valid: [57/200] Step 000/078 Loss 1.171 Prec@(1,5) (73.4%, 93.0%)
06/02 07:55:25 PM | Valid: [57/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.4%)
06/02 07:55:25 PM | Valid: [57/200] Final Prec@1 71.1700%
06/02 07:55:25 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:55:26 PM | Train: [58/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:55:36 PM | Train: [58/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:55:46 PM | Train: [58/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:55:55 PM | Train: [58/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:56:04 PM | Train: [58/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:56:04 PM | Train: [58/200] Final Prec@1 99.9640%
06/02 07:56:05 PM | Valid: [58/200] Step 000/078 Loss 1.173 Prec@(1,5) (73.4%, 92.2%)
06/02 07:56:07 PM | Valid: [58/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.3%, 90.4%)
06/02 07:56:07 PM | Valid: [58/200] Final Prec@1 71.3200%
06/02 07:56:08 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:56:08 PM | Train: [59/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:56:18 PM | Train: [59/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:56:28 PM | Train: [59/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:56:38 PM | Train: [59/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:56:47 PM | Train: [59/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:56:48 PM | Train: [59/200] Final Prec@1 99.9700%
06/02 07:56:48 PM | Valid: [59/200] Step 000/078 Loss 1.183 Prec@(1,5) (73.4%, 93.0%)
06/02 07:56:50 PM | Valid: [59/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.2%, 90.3%)
06/02 07:56:50 PM | Valid: [59/200] Final Prec@1 71.1500%
06/02 07:56:51 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:56:51 PM | Train: [60/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 07:57:01 PM | Train: [60/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:57:12 PM | Train: [60/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:57:22 PM | Train: [60/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:57:31 PM | Train: [60/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:57:31 PM | Train: [60/200] Final Prec@1 99.9780%
06/02 07:57:32 PM | Valid: [60/200] Step 000/078 Loss 1.183 Prec@(1,5) (73.4%, 92.2%)
06/02 07:57:34 PM | Valid: [60/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.3%, 90.4%)
06/02 07:57:34 PM | Valid: [60/200] Final Prec@1 71.2800%
06/02 07:57:35 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:57:35 PM | Train: [61/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:57:45 PM | Train: [61/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:57:55 PM | Train: [61/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:58:06 PM | Train: [61/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:58:15 PM | Train: [61/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:58:15 PM | Train: [61/200] Final Prec@1 99.9700%
06/02 07:58:15 PM | Valid: [61/200] Step 000/078 Loss 1.185 Prec@(1,5) (72.7%, 92.2%)
06/02 07:58:18 PM | Valid: [61/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.6%)
06/02 07:58:18 PM | Valid: [61/200] Final Prec@1 71.1600%
06/02 07:58:18 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:58:19 PM | Train: [62/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:58:29 PM | Train: [62/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:58:39 PM | Train: [62/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 07:58:50 PM | Train: [62/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:58:59 PM | Train: [62/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:58:59 PM | Train: [62/200] Final Prec@1 99.9760%
06/02 07:58:59 PM | Valid: [62/200] Step 000/078 Loss 1.189 Prec@(1,5) (71.9%, 93.8%)
06/02 07:59:01 PM | Valid: [62/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.0%, 90.4%)
06/02 07:59:01 PM | Valid: [62/200] Final Prec@1 71.0400%
06/02 07:59:02 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:59:02 PM | Train: [63/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 07:59:13 PM | Train: [63/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:59:23 PM | Train: [63/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:59:33 PM | Train: [63/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:59:42 PM | Train: [63/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:59:42 PM | Train: [63/200] Final Prec@1 99.9720%
06/02 07:59:43 PM | Valid: [63/200] Step 000/078 Loss 1.168 Prec@(1,5) (74.2%, 92.2%)
06/02 07:59:45 PM | Valid: [63/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.4%)
06/02 07:59:45 PM | Valid: [63/200] Final Prec@1 71.1700%
06/02 07:59:45 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 07:59:46 PM | Train: [64/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 07:59:56 PM | Train: [64/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:00:07 PM | Train: [64/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:00:17 PM | Train: [64/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:00:26 PM | Train: [64/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:00:26 PM | Train: [64/200] Final Prec@1 99.9780%
06/02 08:00:27 PM | Valid: [64/200] Step 000/078 Loss 1.179 Prec@(1,5) (73.4%, 92.2%)
06/02 08:00:29 PM | Valid: [64/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.0%, 90.2%)
06/02 08:00:29 PM | Valid: [64/200] Final Prec@1 70.9800%
06/02 08:00:30 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:00:30 PM | Train: [65/200] Step 000/390 Loss 0.010 Prec@(1,5) (99.2%, 100.0%)
06/02 08:00:40 PM | Train: [65/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:00:50 PM | Train: [65/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:01:01 PM | Train: [65/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:01:11 PM | Train: [65/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:01:11 PM | Train: [65/200] Final Prec@1 99.9680%
06/02 08:01:11 PM | Valid: [65/200] Step 000/078 Loss 1.192 Prec@(1,5) (73.4%, 93.0%)
06/02 08:01:13 PM | Valid: [65/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.1%, 90.5%)
06/02 08:01:13 PM | Valid: [65/200] Final Prec@1 71.0800%
06/02 08:01:14 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:01:14 PM | Train: [66/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:01:25 PM | Train: [66/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:01:35 PM | Train: [66/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:01:45 PM | Train: [66/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:01:54 PM | Train: [66/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:01:54 PM | Train: [66/200] Final Prec@1 99.9600%
06/02 08:01:55 PM | Valid: [66/200] Step 000/078 Loss 1.185 Prec@(1,5) (72.7%, 91.4%)
06/02 08:01:57 PM | Valid: [66/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.5%)
06/02 08:01:57 PM | Valid: [66/200] Final Prec@1 71.1800%
06/02 08:01:58 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:01:58 PM | Train: [67/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:02:08 PM | Train: [67/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:02:18 PM | Train: [67/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:02:28 PM | Train: [67/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:02:38 PM | Train: [67/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:02:38 PM | Train: [67/200] Final Prec@1 99.9740%
06/02 08:02:38 PM | Valid: [67/200] Step 000/078 Loss 1.173 Prec@(1,5) (71.9%, 93.0%)
06/02 08:02:40 PM | Valid: [67/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.1%, 90.5%)
06/02 08:02:41 PM | Valid: [67/200] Final Prec@1 71.1200%
06/02 08:02:41 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:02:41 PM | Train: [68/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:02:52 PM | Train: [68/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 08:03:02 PM | Train: [68/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:03:13 PM | Train: [68/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:03:22 PM | Train: [68/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:03:22 PM | Train: [68/200] Final Prec@1 99.9680%
06/02 08:03:22 PM | Valid: [68/200] Step 000/078 Loss 1.179 Prec@(1,5) (72.7%, 93.0%)
06/02 08:03:24 PM | Valid: [68/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.4%)
06/02 08:03:25 PM | Valid: [68/200] Final Prec@1 71.1900%
06/02 08:03:25 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:03:25 PM | Train: [69/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:03:36 PM | Train: [69/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:03:46 PM | Train: [69/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:03:56 PM | Train: [69/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:04:06 PM | Train: [69/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:04:06 PM | Train: [69/200] Final Prec@1 99.9760%
06/02 08:04:06 PM | Valid: [69/200] Step 000/078 Loss 1.178 Prec@(1,5) (74.2%, 92.2%)
06/02 08:04:08 PM | Valid: [69/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.4%)
06/02 08:04:08 PM | Valid: [69/200] Final Prec@1 71.2800%
06/02 08:04:09 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:04:09 PM | Train: [70/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:04:19 PM | Train: [70/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:04:29 PM | Train: [70/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:04:40 PM | Train: [70/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:04:49 PM | Train: [70/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:04:49 PM | Train: [70/200] Final Prec@1 99.9720%
06/02 08:04:49 PM | Valid: [70/200] Step 000/078 Loss 1.180 Prec@(1,5) (73.4%, 93.0%)
06/02 08:04:52 PM | Valid: [70/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.5%)
06/02 08:04:52 PM | Valid: [70/200] Final Prec@1 71.1700%
06/02 08:04:52 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:04:53 PM | Train: [71/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:05:03 PM | Train: [71/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 08:05:13 PM | Train: [71/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:05:23 PM | Train: [71/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:05:33 PM | Train: [71/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:05:33 PM | Train: [71/200] Final Prec@1 99.9660%
06/02 08:05:33 PM | Valid: [71/200] Step 000/078 Loss 1.169 Prec@(1,5) (72.7%, 92.2%)
06/02 08:05:35 PM | Valid: [71/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.5%)
06/02 08:05:35 PM | Valid: [71/200] Final Prec@1 71.1700%
06/02 08:05:36 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:05:36 PM | Train: [72/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:05:46 PM | Train: [72/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:05:56 PM | Train: [72/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:06:07 PM | Train: [72/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:06:17 PM | Train: [72/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:06:17 PM | Train: [72/200] Final Prec@1 99.9760%
06/02 08:06:17 PM | Valid: [72/200] Step 000/078 Loss 1.178 Prec@(1,5) (72.7%, 93.0%)
06/02 08:06:19 PM | Valid: [72/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.5%)
06/02 08:06:19 PM | Valid: [72/200] Final Prec@1 71.2300%
06/02 08:06:20 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:06:20 PM | Train: [73/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 08:06:30 PM | Train: [73/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:06:40 PM | Train: [73/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:06:50 PM | Train: [73/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:07:00 PM | Train: [73/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:07:00 PM | Train: [73/200] Final Prec@1 99.9820%
06/02 08:07:00 PM | Valid: [73/200] Step 000/078 Loss 1.173 Prec@(1,5) (73.4%, 93.0%)
06/02 08:07:02 PM | Valid: [73/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.1%, 90.4%)
06/02 08:07:02 PM | Valid: [73/200] Final Prec@1 71.0900%
06/02 08:07:03 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:07:03 PM | Train: [74/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:07:13 PM | Train: [74/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:07:23 PM | Train: [74/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:07:34 PM | Train: [74/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:07:43 PM | Train: [74/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:07:43 PM | Train: [74/200] Final Prec@1 99.9700%
06/02 08:07:43 PM | Valid: [74/200] Step 000/078 Loss 1.185 Prec@(1,5) (72.7%, 93.0%)
06/02 08:07:46 PM | Valid: [74/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 08:07:46 PM | Valid: [74/200] Final Prec@1 71.1600%
06/02 08:07:46 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:07:47 PM | Train: [75/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:07:56 PM | Train: [75/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:08:06 PM | Train: [75/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:08:16 PM | Train: [75/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:08:25 PM | Train: [75/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:08:25 PM | Train: [75/200] Final Prec@1 99.9640%
06/02 08:08:26 PM | Valid: [75/200] Step 000/078 Loss 1.187 Prec@(1,5) (73.4%, 91.4%)
06/02 08:08:28 PM | Valid: [75/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.3%)
06/02 08:08:28 PM | Valid: [75/200] Final Prec@1 71.2200%
06/02 08:08:29 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:08:29 PM | Train: [76/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 08:08:40 PM | Train: [76/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:08:50 PM | Train: [76/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:09:00 PM | Train: [76/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:09:10 PM | Train: [76/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:09:10 PM | Train: [76/200] Final Prec@1 99.9760%
06/02 08:09:10 PM | Valid: [76/200] Step 000/078 Loss 1.180 Prec@(1,5) (72.7%, 92.2%)
06/02 08:09:12 PM | Valid: [76/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.1%, 90.4%)
06/02 08:09:12 PM | Valid: [76/200] Final Prec@1 71.1100%
06/02 08:09:13 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:09:13 PM | Train: [77/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:09:24 PM | Train: [77/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:09:34 PM | Train: [77/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:09:44 PM | Train: [77/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:09:53 PM | Train: [77/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:09:53 PM | Train: [77/200] Final Prec@1 99.9740%
06/02 08:09:54 PM | Valid: [77/200] Step 000/078 Loss 1.175 Prec@(1,5) (72.7%, 93.0%)
06/02 08:09:56 PM | Valid: [77/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.3%, 90.5%)
06/02 08:09:56 PM | Valid: [77/200] Final Prec@1 71.2900%
06/02 08:09:57 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:09:57 PM | Train: [78/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:10:07 PM | Train: [78/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:10:18 PM | Train: [78/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:10:28 PM | Train: [78/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:10:37 PM | Train: [78/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:10:37 PM | Train: [78/200] Final Prec@1 99.9680%
06/02 08:10:38 PM | Valid: [78/200] Step 000/078 Loss 1.181 Prec@(1,5) (72.7%, 93.8%)
06/02 08:10:40 PM | Valid: [78/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.4%)
06/02 08:10:40 PM | Valid: [78/200] Final Prec@1 71.1700%
06/02 08:10:41 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:10:41 PM | Train: [79/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:10:51 PM | Train: [79/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:11:01 PM | Train: [79/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:11:12 PM | Train: [79/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:11:21 PM | Train: [79/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:11:21 PM | Train: [79/200] Final Prec@1 99.9620%
06/02 08:11:22 PM | Valid: [79/200] Step 000/078 Loss 1.175 Prec@(1,5) (72.7%, 93.8%)
06/02 08:11:24 PM | Valid: [79/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.5%)
06/02 08:11:24 PM | Valid: [79/200] Final Prec@1 71.1700%
06/02 08:11:25 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:11:25 PM | Train: [80/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:11:35 PM | Train: [80/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:11:45 PM | Train: [80/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:11:55 PM | Train: [80/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:12:04 PM | Train: [80/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:12:04 PM | Train: [80/200] Final Prec@1 99.9660%
06/02 08:12:05 PM | Valid: [80/200] Step 000/078 Loss 1.185 Prec@(1,5) (71.9%, 92.2%)
06/02 08:12:07 PM | Valid: [80/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.3%, 90.5%)
06/02 08:12:07 PM | Valid: [80/200] Final Prec@1 71.2800%
06/02 08:12:08 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:12:08 PM | Train: [81/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:12:18 PM | Train: [81/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:12:28 PM | Train: [81/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:12:38 PM | Train: [81/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:12:47 PM | Train: [81/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:12:47 PM | Train: [81/200] Final Prec@1 99.9680%
06/02 08:12:47 PM | Valid: [81/200] Step 000/078 Loss 1.170 Prec@(1,5) (72.7%, 93.8%)
06/02 08:12:49 PM | Valid: [81/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.3%, 90.3%)
06/02 08:12:49 PM | Valid: [81/200] Final Prec@1 71.2900%
06/02 08:12:50 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:12:50 PM | Train: [82/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:13:00 PM | Train: [82/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:13:10 PM | Train: [82/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:13:19 PM | Train: [82/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:13:28 PM | Train: [82/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:13:28 PM | Train: [82/200] Final Prec@1 99.9800%
06/02 08:13:29 PM | Valid: [82/200] Step 000/078 Loss 1.172 Prec@(1,5) (73.4%, 93.8%)
06/02 08:13:31 PM | Valid: [82/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.4%)
06/02 08:13:31 PM | Valid: [82/200] Final Prec@1 71.2200%
06/02 08:13:31 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:13:32 PM | Train: [83/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:13:41 PM | Train: [83/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 08:13:52 PM | Train: [83/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:14:02 PM | Train: [83/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:14:11 PM | Train: [83/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:14:11 PM | Train: [83/200] Final Prec@1 99.9720%
06/02 08:14:11 PM | Valid: [83/200] Step 000/078 Loss 1.155 Prec@(1,5) (72.7%, 92.2%)
06/02 08:14:14 PM | Valid: [83/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.1%, 90.5%)
06/02 08:14:14 PM | Valid: [83/200] Final Prec@1 71.1000%
06/02 08:14:14 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:14:15 PM | Train: [84/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:14:25 PM | Train: [84/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:14:35 PM | Train: [84/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:14:45 PM | Train: [84/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:14:54 PM | Train: [84/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:14:54 PM | Train: [84/200] Final Prec@1 99.9840%
06/02 08:14:54 PM | Valid: [84/200] Step 000/078 Loss 1.174 Prec@(1,5) (72.7%, 93.8%)
06/02 08:14:56 PM | Valid: [84/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.4%)
06/02 08:14:56 PM | Valid: [84/200] Final Prec@1 71.2400%
06/02 08:14:57 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:14:57 PM | Train: [85/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:15:08 PM | Train: [85/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:15:18 PM | Train: [85/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:15:28 PM | Train: [85/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:15:37 PM | Train: [85/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:15:37 PM | Train: [85/200] Final Prec@1 99.9800%
06/02 08:15:37 PM | Valid: [85/200] Step 000/078 Loss 1.178 Prec@(1,5) (73.4%, 93.0%)
06/02 08:15:40 PM | Valid: [85/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 08:15:40 PM | Valid: [85/200] Final Prec@1 71.2000%
06/02 08:15:40 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:15:41 PM | Train: [86/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:15:51 PM | Train: [86/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 08:16:01 PM | Train: [86/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:16:11 PM | Train: [86/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:16:21 PM | Train: [86/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:16:21 PM | Train: [86/200] Final Prec@1 99.9720%
06/02 08:16:21 PM | Valid: [86/200] Step 000/078 Loss 1.162 Prec@(1,5) (73.4%, 93.0%)
06/02 08:16:23 PM | Valid: [86/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.4%, 90.4%)
06/02 08:16:23 PM | Valid: [86/200] Final Prec@1 71.3600%
06/02 08:16:24 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:16:24 PM | Train: [87/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 08:16:34 PM | Train: [87/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:16:44 PM | Train: [87/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:16:54 PM | Train: [87/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:17:03 PM | Train: [87/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:17:03 PM | Train: [87/200] Final Prec@1 99.9800%
06/02 08:17:04 PM | Valid: [87/200] Step 000/078 Loss 1.183 Prec@(1,5) (73.4%, 93.0%)
06/02 08:17:06 PM | Valid: [87/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.5%)
06/02 08:17:06 PM | Valid: [87/200] Final Prec@1 71.1800%
06/02 08:17:07 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:17:07 PM | Train: [88/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:17:17 PM | Train: [88/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:17:27 PM | Train: [88/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:17:37 PM | Train: [88/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:17:46 PM | Train: [88/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:17:46 PM | Train: [88/200] Final Prec@1 99.9640%
06/02 08:17:46 PM | Valid: [88/200] Step 000/078 Loss 1.175 Prec@(1,5) (71.9%, 93.8%)
06/02 08:17:48 PM | Valid: [88/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.4%)
06/02 08:17:48 PM | Valid: [88/200] Final Prec@1 71.2300%
06/02 08:17:49 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:17:49 PM | Train: [89/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:18:00 PM | Train: [89/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:18:10 PM | Train: [89/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:18:20 PM | Train: [89/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:18:29 PM | Train: [89/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:18:29 PM | Train: [89/200] Final Prec@1 99.9800%
06/02 08:18:29 PM | Valid: [89/200] Step 000/078 Loss 1.173 Prec@(1,5) (72.7%, 93.0%)
06/02 08:18:32 PM | Valid: [89/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 08:18:32 PM | Valid: [89/200] Final Prec@1 71.1800%
06/02 08:18:32 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:18:33 PM | Train: [90/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:18:43 PM | Train: [90/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:18:53 PM | Train: [90/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:19:04 PM | Train: [90/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:19:13 PM | Train: [90/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:19:13 PM | Train: [90/200] Final Prec@1 99.9820%
06/02 08:19:13 PM | Valid: [90/200] Step 000/078 Loss 1.170 Prec@(1,5) (71.9%, 93.0%)
06/02 08:19:16 PM | Valid: [90/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.6%)
06/02 08:19:16 PM | Valid: [90/200] Final Prec@1 71.2200%
06/02 08:19:16 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:19:17 PM | Train: [91/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:19:27 PM | Train: [91/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:19:37 PM | Train: [91/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:19:48 PM | Train: [91/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:19:57 PM | Train: [91/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:19:57 PM | Train: [91/200] Final Prec@1 99.9620%
06/02 08:19:57 PM | Valid: [91/200] Step 000/078 Loss 1.166 Prec@(1,5) (72.7%, 93.0%)
06/02 08:19:59 PM | Valid: [91/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 08:20:00 PM | Valid: [91/200] Final Prec@1 71.1600%
06/02 08:20:00 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:20:00 PM | Train: [92/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:20:10 PM | Train: [92/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 08:20:21 PM | Train: [92/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:20:31 PM | Train: [92/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:20:40 PM | Train: [92/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:20:40 PM | Train: [92/200] Final Prec@1 99.9660%
06/02 08:20:40 PM | Valid: [92/200] Step 000/078 Loss 1.171 Prec@(1,5) (74.2%, 93.0%)
06/02 08:20:43 PM | Valid: [92/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.3%, 90.4%)
06/02 08:20:43 PM | Valid: [92/200] Final Prec@1 71.3400%
06/02 08:20:43 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:20:44 PM | Train: [93/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:20:54 PM | Train: [93/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:21:04 PM | Train: [93/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:21:15 PM | Train: [93/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:21:24 PM | Train: [93/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:21:24 PM | Train: [93/200] Final Prec@1 99.9740%
06/02 08:21:24 PM | Valid: [93/200] Step 000/078 Loss 1.182 Prec@(1,5) (72.7%, 93.0%)
06/02 08:21:26 PM | Valid: [93/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.4%)
06/02 08:21:26 PM | Valid: [93/200] Final Prec@1 71.1700%
06/02 08:21:27 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:21:27 PM | Train: [94/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:21:38 PM | Train: [94/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:21:48 PM | Train: [94/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:21:58 PM | Train: [94/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:22:07 PM | Train: [94/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:22:07 PM | Train: [94/200] Final Prec@1 99.9700%
06/02 08:22:07 PM | Valid: [94/200] Step 000/078 Loss 1.175 Prec@(1,5) (73.4%, 92.2%)
06/02 08:22:10 PM | Valid: [94/200] Step 078/078 Loss 1.236 Prec@(1,5) (71.3%, 90.5%)
06/02 08:22:10 PM | Valid: [94/200] Final Prec@1 71.3300%
06/02 08:22:10 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:22:11 PM | Train: [95/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:22:21 PM | Train: [95/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:22:31 PM | Train: [95/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:22:41 PM | Train: [95/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:22:50 PM | Train: [95/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:22:50 PM | Train: [95/200] Final Prec@1 99.9780%
06/02 08:22:51 PM | Valid: [95/200] Step 000/078 Loss 1.158 Prec@(1,5) (73.4%, 92.2%)
06/02 08:22:53 PM | Valid: [95/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.1%, 90.5%)
06/02 08:22:53 PM | Valid: [95/200] Final Prec@1 71.1400%
06/02 08:22:54 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:22:54 PM | Train: [96/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 08:23:04 PM | Train: [96/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:23:14 PM | Train: [96/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:23:25 PM | Train: [96/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:23:34 PM | Train: [96/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:23:35 PM | Train: [96/200] Final Prec@1 99.9680%
06/02 08:23:35 PM | Valid: [96/200] Step 000/078 Loss 1.152 Prec@(1,5) (71.9%, 93.8%)
06/02 08:23:37 PM | Valid: [96/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.4%, 90.4%)
06/02 08:23:37 PM | Valid: [96/200] Final Prec@1 71.3500%
06/02 08:23:38 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:23:38 PM | Train: [97/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:23:48 PM | Train: [97/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:23:59 PM | Train: [97/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:24:09 PM | Train: [97/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:24:18 PM | Train: [97/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:24:18 PM | Train: [97/200] Final Prec@1 99.9760%
06/02 08:24:19 PM | Valid: [97/200] Step 000/078 Loss 1.149 Prec@(1,5) (74.2%, 93.8%)
06/02 08:24:21 PM | Valid: [97/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.4%)
06/02 08:24:21 PM | Valid: [97/200] Final Prec@1 71.2400%
06/02 08:24:22 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:24:22 PM | Train: [98/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:24:32 PM | Train: [98/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:24:42 PM | Train: [98/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:24:52 PM | Train: [98/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:25:01 PM | Train: [98/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:25:01 PM | Train: [98/200] Final Prec@1 99.9740%
06/02 08:25:01 PM | Valid: [98/200] Step 000/078 Loss 1.160 Prec@(1,5) (72.7%, 93.0%)
06/02 08:25:03 PM | Valid: [98/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.2%, 90.3%)
06/02 08:25:03 PM | Valid: [98/200] Final Prec@1 71.1600%
06/02 08:25:04 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:25:04 PM | Train: [99/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:25:14 PM | Train: [99/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:25:25 PM | Train: [99/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:25:35 PM | Train: [99/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:25:44 PM | Train: [99/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:25:44 PM | Train: [99/200] Final Prec@1 99.9660%
06/02 08:25:44 PM | Valid: [99/200] Step 000/078 Loss 1.161 Prec@(1,5) (75.0%, 93.0%)
06/02 08:25:47 PM | Valid: [99/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.6%)
06/02 08:25:47 PM | Valid: [99/200] Final Prec@1 71.2100%
06/02 08:25:47 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:25:48 PM | Train: [100/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:25:57 PM | Train: [100/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:26:06 PM | Train: [100/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:26:17 PM | Train: [100/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:26:26 PM | Train: [100/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:26:26 PM | Train: [100/200] Final Prec@1 99.9740%
06/02 08:26:26 PM | Valid: [100/200] Step 000/078 Loss 1.172 Prec@(1,5) (72.7%, 93.8%)
06/02 08:26:29 PM | Valid: [100/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.1%, 90.4%)
06/02 08:26:29 PM | Valid: [100/200] Final Prec@1 71.1100%
06/02 08:26:29 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:26:30 PM | Train: [101/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:26:40 PM | Train: [101/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:26:50 PM | Train: [101/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:27:01 PM | Train: [101/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:27:10 PM | Train: [101/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:27:10 PM | Train: [101/200] Final Prec@1 99.9740%
06/02 08:27:11 PM | Valid: [101/200] Step 000/078 Loss 1.159 Prec@(1,5) (71.9%, 93.0%)
06/02 08:27:13 PM | Valid: [101/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.2%, 90.5%)
06/02 08:27:13 PM | Valid: [101/200] Final Prec@1 71.1700%
06/02 08:27:14 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:27:14 PM | Train: [102/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:27:24 PM | Train: [102/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:27:34 PM | Train: [102/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:27:44 PM | Train: [102/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:27:53 PM | Train: [102/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:27:53 PM | Train: [102/200] Final Prec@1 99.9720%
06/02 08:27:53 PM | Valid: [102/200] Step 000/078 Loss 1.164 Prec@(1,5) (72.7%, 93.0%)
06/02 08:27:56 PM | Valid: [102/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.2%, 90.4%)
06/02 08:27:56 PM | Valid: [102/200] Final Prec@1 71.1900%
06/02 08:27:56 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:27:57 PM | Train: [103/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:28:07 PM | Train: [103/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:28:17 PM | Train: [103/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:28:28 PM | Train: [103/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:28:37 PM | Train: [103/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:28:38 PM | Train: [103/200] Final Prec@1 99.9780%
06/02 08:28:38 PM | Valid: [103/200] Step 000/078 Loss 1.164 Prec@(1,5) (73.4%, 93.8%)
06/02 08:28:40 PM | Valid: [103/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.4%, 90.6%)
06/02 08:28:40 PM | Valid: [103/200] Final Prec@1 71.4200%
06/02 08:28:41 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:28:41 PM | Train: [104/200] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
06/02 08:28:52 PM | Train: [104/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:29:02 PM | Train: [104/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:29:13 PM | Train: [104/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:29:22 PM | Train: [104/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:29:22 PM | Train: [104/200] Final Prec@1 99.9620%
06/02 08:29:22 PM | Valid: [104/200] Step 000/078 Loss 1.158 Prec@(1,5) (73.4%, 93.0%)
06/02 08:29:25 PM | Valid: [104/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.3%, 90.5%)
06/02 08:29:25 PM | Valid: [104/200] Final Prec@1 71.3100%
06/02 08:29:25 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:29:26 PM | Train: [105/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 08:29:36 PM | Train: [105/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:29:46 PM | Train: [105/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:29:57 PM | Train: [105/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:30:07 PM | Train: [105/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:30:07 PM | Train: [105/200] Final Prec@1 99.9780%
06/02 08:30:07 PM | Valid: [105/200] Step 000/078 Loss 1.183 Prec@(1,5) (72.7%, 93.8%)
06/02 08:30:09 PM | Valid: [105/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.4%, 90.5%)
06/02 08:30:09 PM | Valid: [105/200] Final Prec@1 71.4000%
06/02 08:30:10 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:30:10 PM | Train: [106/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:30:20 PM | Train: [106/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:30:31 PM | Train: [106/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:30:41 PM | Train: [106/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:30:50 PM | Train: [106/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:30:50 PM | Train: [106/200] Final Prec@1 99.9800%
06/02 08:30:50 PM | Valid: [106/200] Step 000/078 Loss 1.169 Prec@(1,5) (73.4%, 93.0%)
06/02 08:30:53 PM | Valid: [106/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.4%, 90.5%)
06/02 08:30:53 PM | Valid: [106/200] Final Prec@1 71.3800%
06/02 08:30:53 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:30:54 PM | Train: [107/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:31:04 PM | Train: [107/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:31:14 PM | Train: [107/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:31:23 PM | Train: [107/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:31:33 PM | Train: [107/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:31:33 PM | Train: [107/200] Final Prec@1 99.9660%
06/02 08:31:33 PM | Valid: [107/200] Step 000/078 Loss 1.167 Prec@(1,5) (72.7%, 93.0%)
06/02 08:31:35 PM | Valid: [107/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.3%, 90.5%)
06/02 08:31:35 PM | Valid: [107/200] Final Prec@1 71.2600%
06/02 08:31:36 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:31:36 PM | Train: [108/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:31:47 PM | Train: [108/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:31:57 PM | Train: [108/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:32:07 PM | Train: [108/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:32:17 PM | Train: [108/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:32:17 PM | Train: [108/200] Final Prec@1 99.9640%
06/02 08:32:17 PM | Valid: [108/200] Step 000/078 Loss 1.190 Prec@(1,5) (71.9%, 93.0%)
06/02 08:32:19 PM | Valid: [108/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.4%)
06/02 08:32:19 PM | Valid: [108/200] Final Prec@1 71.3200%
06/02 08:32:20 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:32:20 PM | Train: [109/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:32:30 PM | Train: [109/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:32:41 PM | Train: [109/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:32:50 PM | Train: [109/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:33:00 PM | Train: [109/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:33:00 PM | Train: [109/200] Final Prec@1 99.9720%
06/02 08:33:00 PM | Valid: [109/200] Step 000/078 Loss 1.143 Prec@(1,5) (74.2%, 93.0%)
06/02 08:33:02 PM | Valid: [109/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.1%, 90.6%)
06/02 08:33:02 PM | Valid: [109/200] Final Prec@1 71.0600%
06/02 08:33:03 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:33:03 PM | Train: [110/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:33:13 PM | Train: [110/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:33:23 PM | Train: [110/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:33:33 PM | Train: [110/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:33:43 PM | Train: [110/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:33:43 PM | Train: [110/200] Final Prec@1 99.9680%
06/02 08:33:43 PM | Valid: [110/200] Step 000/078 Loss 1.174 Prec@(1,5) (73.4%, 92.2%)
06/02 08:33:45 PM | Valid: [110/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.6%)
06/02 08:33:45 PM | Valid: [110/200] Final Prec@1 71.2100%
06/02 08:33:46 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:33:46 PM | Train: [111/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:33:56 PM | Train: [111/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:34:06 PM | Train: [111/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:34:17 PM | Train: [111/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:34:26 PM | Train: [111/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:34:26 PM | Train: [111/200] Final Prec@1 99.9760%
06/02 08:34:26 PM | Valid: [111/200] Step 000/078 Loss 1.189 Prec@(1,5) (72.7%, 93.0%)
06/02 08:34:28 PM | Valid: [111/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.1%, 90.5%)
06/02 08:34:28 PM | Valid: [111/200] Final Prec@1 71.1200%
06/02 08:34:29 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:34:29 PM | Train: [112/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 08:34:40 PM | Train: [112/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:34:50 PM | Train: [112/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:35:00 PM | Train: [112/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:35:10 PM | Train: [112/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:35:10 PM | Train: [112/200] Final Prec@1 99.9700%
06/02 08:35:10 PM | Valid: [112/200] Step 000/078 Loss 1.173 Prec@(1,5) (74.2%, 92.2%)
06/02 08:35:12 PM | Valid: [112/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.4%)
06/02 08:35:12 PM | Valid: [112/200] Final Prec@1 71.2800%
06/02 08:35:13 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:35:13 PM | Train: [113/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:35:23 PM | Train: [113/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:35:34 PM | Train: [113/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:35:44 PM | Train: [113/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:35:53 PM | Train: [113/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:35:53 PM | Train: [113/200] Final Prec@1 99.9660%
06/02 08:35:54 PM | Valid: [113/200] Step 000/078 Loss 1.166 Prec@(1,5) (72.7%, 92.2%)
06/02 08:35:56 PM | Valid: [113/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.4%, 90.4%)
06/02 08:35:56 PM | Valid: [113/200] Final Prec@1 71.3700%
06/02 08:35:57 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:35:57 PM | Train: [114/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:36:07 PM | Train: [114/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:36:17 PM | Train: [114/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:36:27 PM | Train: [114/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:36:35 PM | Train: [114/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:36:35 PM | Train: [114/200] Final Prec@1 99.9740%
06/02 08:36:36 PM | Valid: [114/200] Step 000/078 Loss 1.176 Prec@(1,5) (73.4%, 93.0%)
06/02 08:36:38 PM | Valid: [114/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.5%)
06/02 08:36:38 PM | Valid: [114/200] Final Prec@1 71.2600%
06/02 08:36:39 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:36:39 PM | Train: [115/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:36:49 PM | Train: [115/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:36:59 PM | Train: [115/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:37:09 PM | Train: [115/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:37:18 PM | Train: [115/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:37:19 PM | Train: [115/200] Final Prec@1 99.9720%
06/02 08:37:19 PM | Valid: [115/200] Step 000/078 Loss 1.178 Prec@(1,5) (72.7%, 93.0%)
06/02 08:37:21 PM | Valid: [115/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.1%, 90.5%)
06/02 08:37:21 PM | Valid: [115/200] Final Prec@1 71.0900%
06/02 08:37:22 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:37:22 PM | Train: [116/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:37:32 PM | Train: [116/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:37:43 PM | Train: [116/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:37:53 PM | Train: [116/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:38:02 PM | Train: [116/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:38:02 PM | Train: [116/200] Final Prec@1 99.9720%
06/02 08:38:02 PM | Valid: [116/200] Step 000/078 Loss 1.183 Prec@(1,5) (73.4%, 93.8%)
06/02 08:38:04 PM | Valid: [116/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.5%)
06/02 08:38:04 PM | Valid: [116/200] Final Prec@1 71.2100%
06/02 08:38:05 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:38:05 PM | Train: [117/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:38:16 PM | Train: [117/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:38:26 PM | Train: [117/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:38:36 PM | Train: [117/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:38:46 PM | Train: [117/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:38:46 PM | Train: [117/200] Final Prec@1 99.9780%
06/02 08:38:46 PM | Valid: [117/200] Step 000/078 Loss 1.164 Prec@(1,5) (73.4%, 92.2%)
06/02 08:38:48 PM | Valid: [117/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.3%, 90.5%)
06/02 08:38:48 PM | Valid: [117/200] Final Prec@1 71.2900%
06/02 08:38:49 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:38:49 PM | Train: [118/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:39:00 PM | Train: [118/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 08:39:10 PM | Train: [118/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 08:39:20 PM | Train: [118/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:39:29 PM | Train: [118/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:39:29 PM | Train: [118/200] Final Prec@1 99.9600%
06/02 08:39:29 PM | Valid: [118/200] Step 000/078 Loss 1.171 Prec@(1,5) (72.7%, 93.0%)
06/02 08:39:31 PM | Valid: [118/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.5%)
06/02 08:39:32 PM | Valid: [118/200] Final Prec@1 71.2500%
06/02 08:39:32 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:39:33 PM | Train: [119/200] Step 000/390 Loss 0.008 Prec@(1,5) (99.2%, 100.0%)
06/02 08:39:43 PM | Train: [119/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:39:53 PM | Train: [119/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:40:03 PM | Train: [119/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:40:13 PM | Train: [119/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:40:13 PM | Train: [119/200] Final Prec@1 99.9800%
06/02 08:40:13 PM | Valid: [119/200] Step 000/078 Loss 1.170 Prec@(1,5) (73.4%, 92.2%)
06/02 08:40:15 PM | Valid: [119/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.1%, 90.5%)
06/02 08:40:15 PM | Valid: [119/200] Final Prec@1 71.0800%
06/02 08:40:16 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:40:16 PM | Train: [120/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:40:26 PM | Train: [120/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:40:36 PM | Train: [120/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:40:46 PM | Train: [120/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:40:55 PM | Train: [120/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:40:55 PM | Train: [120/200] Final Prec@1 99.9760%
06/02 08:40:55 PM | Valid: [120/200] Step 000/078 Loss 1.181 Prec@(1,5) (73.4%, 93.0%)
06/02 08:40:58 PM | Valid: [120/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.3%, 90.5%)
06/02 08:40:58 PM | Valid: [120/200] Final Prec@1 71.2700%
06/02 08:40:58 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:40:59 PM | Train: [121/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:41:08 PM | Train: [121/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:41:19 PM | Train: [121/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:41:29 PM | Train: [121/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:41:38 PM | Train: [121/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:41:38 PM | Train: [121/200] Final Prec@1 99.9600%
06/02 08:41:38 PM | Valid: [121/200] Step 000/078 Loss 1.171 Prec@(1,5) (73.4%, 92.2%)
06/02 08:41:41 PM | Valid: [121/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.3%, 90.5%)
06/02 08:41:41 PM | Valid: [121/200] Final Prec@1 71.3000%
06/02 08:41:42 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:41:42 PM | Train: [122/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:41:52 PM | Train: [122/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:42:02 PM | Train: [122/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:42:12 PM | Train: [122/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:42:22 PM | Train: [122/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:42:22 PM | Train: [122/200] Final Prec@1 99.9720%
06/02 08:42:22 PM | Valid: [122/200] Step 000/078 Loss 1.179 Prec@(1,5) (72.7%, 93.8%)
06/02 08:42:24 PM | Valid: [122/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.5%)
06/02 08:42:24 PM | Valid: [122/200] Final Prec@1 71.2200%
06/02 08:42:25 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:42:25 PM | Train: [123/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:42:35 PM | Train: [123/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:42:45 PM | Train: [123/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:42:55 PM | Train: [123/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:43:04 PM | Train: [123/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:43:04 PM | Train: [123/200] Final Prec@1 99.9700%
06/02 08:43:05 PM | Valid: [123/200] Step 000/078 Loss 1.167 Prec@(1,5) (72.7%, 93.0%)
06/02 08:43:07 PM | Valid: [123/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.4%)
06/02 08:43:07 PM | Valid: [123/200] Final Prec@1 71.2800%
06/02 08:43:08 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:43:08 PM | Train: [124/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:43:18 PM | Train: [124/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:43:29 PM | Train: [124/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:43:39 PM | Train: [124/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:43:48 PM | Train: [124/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:43:48 PM | Train: [124/200] Final Prec@1 99.9780%
06/02 08:43:49 PM | Valid: [124/200] Step 000/078 Loss 1.171 Prec@(1,5) (74.2%, 92.2%)
06/02 08:43:51 PM | Valid: [124/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.3%, 90.4%)
06/02 08:43:51 PM | Valid: [124/200] Final Prec@1 71.3000%
06/02 08:43:51 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:43:52 PM | Train: [125/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:44:02 PM | Train: [125/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:44:12 PM | Train: [125/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:44:22 PM | Train: [125/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:44:31 PM | Train: [125/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:44:31 PM | Train: [125/200] Final Prec@1 99.9740%
06/02 08:44:31 PM | Valid: [125/200] Step 000/078 Loss 1.166 Prec@(1,5) (73.4%, 92.2%)
06/02 08:44:34 PM | Valid: [125/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.4%)
06/02 08:44:34 PM | Valid: [125/200] Final Prec@1 71.1800%
06/02 08:44:34 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:44:35 PM | Train: [126/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:44:45 PM | Train: [126/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:44:55 PM | Train: [126/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:45:05 PM | Train: [126/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:45:14 PM | Train: [126/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:45:14 PM | Train: [126/200] Final Prec@1 99.9600%
06/02 08:45:14 PM | Valid: [126/200] Step 000/078 Loss 1.163 Prec@(1,5) (73.4%, 92.2%)
06/02 08:45:17 PM | Valid: [126/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.3%)
06/02 08:45:17 PM | Valid: [126/200] Final Prec@1 71.3000%
06/02 08:45:17 PM | Current best Prec@1 = 71.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:45:18 PM | Train: [127/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:45:28 PM | Train: [127/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:45:38 PM | Train: [127/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:45:48 PM | Train: [127/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:45:57 PM | Train: [127/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:45:57 PM | Train: [127/200] Final Prec@1 99.9740%
06/02 08:45:57 PM | Valid: [127/200] Step 000/078 Loss 1.172 Prec@(1,5) (73.4%, 93.0%)
06/02 08:45:59 PM | Valid: [127/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.5%, 90.5%)
06/02 08:46:00 PM | Valid: [127/200] Final Prec@1 71.4800%
06/02 08:46:00 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:46:00 PM | Train: [128/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 08:46:11 PM | Train: [128/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:46:22 PM | Train: [128/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:46:32 PM | Train: [128/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:46:42 PM | Train: [128/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:46:42 PM | Train: [128/200] Final Prec@1 99.9720%
06/02 08:46:42 PM | Valid: [128/200] Step 000/078 Loss 1.159 Prec@(1,5) (73.4%, 93.0%)
06/02 08:46:44 PM | Valid: [128/200] Step 078/078 Loss 1.236 Prec@(1,5) (71.4%, 90.5%)
06/02 08:46:44 PM | Valid: [128/200] Final Prec@1 71.4000%
06/02 08:46:45 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:46:45 PM | Train: [129/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:46:55 PM | Train: [129/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:47:05 PM | Train: [129/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:47:15 PM | Train: [129/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:47:24 PM | Train: [129/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:47:24 PM | Train: [129/200] Final Prec@1 99.9820%
06/02 08:47:25 PM | Valid: [129/200] Step 000/078 Loss 1.178 Prec@(1,5) (74.2%, 92.2%)
06/02 08:47:27 PM | Valid: [129/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.4%)
06/02 08:47:27 PM | Valid: [129/200] Final Prec@1 71.2700%
06/02 08:47:28 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:47:28 PM | Train: [130/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:47:38 PM | Train: [130/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:47:48 PM | Train: [130/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:47:58 PM | Train: [130/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:48:07 PM | Train: [130/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:48:08 PM | Train: [130/200] Final Prec@1 99.9680%
06/02 08:48:08 PM | Valid: [130/200] Step 000/078 Loss 1.158 Prec@(1,5) (73.4%, 93.0%)
06/02 08:48:10 PM | Valid: [130/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.5%, 90.4%)
06/02 08:48:10 PM | Valid: [130/200] Final Prec@1 71.4600%
06/02 08:48:11 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:48:11 PM | Train: [131/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:48:21 PM | Train: [131/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:48:32 PM | Train: [131/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:48:42 PM | Train: [131/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:48:51 PM | Train: [131/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:48:51 PM | Train: [131/200] Final Prec@1 99.9720%
06/02 08:48:52 PM | Valid: [131/200] Step 000/078 Loss 1.161 Prec@(1,5) (74.2%, 92.2%)
06/02 08:48:54 PM | Valid: [131/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.3%, 90.4%)
06/02 08:48:54 PM | Valid: [131/200] Final Prec@1 71.3200%
06/02 08:48:55 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:48:55 PM | Train: [132/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:49:05 PM | Train: [132/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:49:16 PM | Train: [132/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:49:26 PM | Train: [132/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:49:35 PM | Train: [132/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:49:35 PM | Train: [132/200] Final Prec@1 99.9800%
06/02 08:49:35 PM | Valid: [132/200] Step 000/078 Loss 1.169 Prec@(1,5) (73.4%, 93.8%)
06/02 08:49:38 PM | Valid: [132/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.5%)
06/02 08:49:38 PM | Valid: [132/200] Final Prec@1 71.2200%
06/02 08:49:38 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:49:39 PM | Train: [133/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 08:49:49 PM | Train: [133/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:49:59 PM | Train: [133/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:50:09 PM | Train: [133/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:50:18 PM | Train: [133/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:50:19 PM | Train: [133/200] Final Prec@1 99.9720%
06/02 08:50:19 PM | Valid: [133/200] Step 000/078 Loss 1.162 Prec@(1,5) (72.7%, 92.2%)
06/02 08:50:21 PM | Valid: [133/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.1%, 90.4%)
06/02 08:50:21 PM | Valid: [133/200] Final Prec@1 71.1400%
06/02 08:50:22 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:50:22 PM | Train: [134/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:50:32 PM | Train: [134/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:50:43 PM | Train: [134/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:50:53 PM | Train: [134/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:51:02 PM | Train: [134/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:51:02 PM | Train: [134/200] Final Prec@1 99.9740%
06/02 08:51:02 PM | Valid: [134/200] Step 000/078 Loss 1.177 Prec@(1,5) (72.7%, 92.2%)
06/02 08:51:05 PM | Valid: [134/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.0%, 90.4%)
06/02 08:51:05 PM | Valid: [134/200] Final Prec@1 71.0500%
06/02 08:51:05 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:51:06 PM | Train: [135/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:51:16 PM | Train: [135/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:51:26 PM | Train: [135/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:51:36 PM | Train: [135/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:51:45 PM | Train: [135/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:51:45 PM | Train: [135/200] Final Prec@1 99.9780%
06/02 08:51:45 PM | Valid: [135/200] Step 000/078 Loss 1.162 Prec@(1,5) (73.4%, 92.2%)
06/02 08:51:47 PM | Valid: [135/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.4%)
06/02 08:51:47 PM | Valid: [135/200] Final Prec@1 71.2500%
06/02 08:51:48 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:51:48 PM | Train: [136/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:51:58 PM | Train: [136/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:52:09 PM | Train: [136/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:52:19 PM | Train: [136/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:52:28 PM | Train: [136/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:52:28 PM | Train: [136/200] Final Prec@1 99.9760%
06/02 08:52:28 PM | Valid: [136/200] Step 000/078 Loss 1.158 Prec@(1,5) (73.4%, 93.0%)
06/02 08:52:31 PM | Valid: [136/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.3%, 90.5%)
06/02 08:52:31 PM | Valid: [136/200] Final Prec@1 71.2600%
06/02 08:52:31 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:52:32 PM | Train: [137/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:52:42 PM | Train: [137/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:52:52 PM | Train: [137/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:53:02 PM | Train: [137/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:53:11 PM | Train: [137/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:53:11 PM | Train: [137/200] Final Prec@1 99.9800%
06/02 08:53:12 PM | Valid: [137/200] Step 000/078 Loss 1.169 Prec@(1,5) (72.7%, 93.0%)
06/02 08:53:14 PM | Valid: [137/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.5%)
06/02 08:53:14 PM | Valid: [137/200] Final Prec@1 71.1600%
06/02 08:53:15 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:53:15 PM | Train: [138/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:53:25 PM | Train: [138/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:53:35 PM | Train: [138/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:53:46 PM | Train: [138/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:53:55 PM | Train: [138/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:53:55 PM | Train: [138/200] Final Prec@1 99.9700%
06/02 08:53:55 PM | Valid: [138/200] Step 000/078 Loss 1.156 Prec@(1,5) (72.7%, 92.2%)
06/02 08:53:58 PM | Valid: [138/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.3%, 90.5%)
06/02 08:53:58 PM | Valid: [138/200] Final Prec@1 71.3100%
06/02 08:53:58 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:53:59 PM | Train: [139/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:54:08 PM | Train: [139/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:54:18 PM | Train: [139/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:54:29 PM | Train: [139/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:54:38 PM | Train: [139/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:54:38 PM | Train: [139/200] Final Prec@1 99.9700%
06/02 08:54:38 PM | Valid: [139/200] Step 000/078 Loss 1.158 Prec@(1,5) (73.4%, 92.2%)
06/02 08:54:40 PM | Valid: [139/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.3%, 90.3%)
06/02 08:54:40 PM | Valid: [139/200] Final Prec@1 71.3000%
06/02 08:54:41 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:54:41 PM | Train: [140/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:54:51 PM | Train: [140/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:55:02 PM | Train: [140/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:55:12 PM | Train: [140/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:55:21 PM | Train: [140/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:55:21 PM | Train: [140/200] Final Prec@1 99.9660%
06/02 08:55:21 PM | Valid: [140/200] Step 000/078 Loss 1.167 Prec@(1,5) (72.7%, 93.8%)
06/02 08:55:24 PM | Valid: [140/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.5%, 90.4%)
06/02 08:55:24 PM | Valid: [140/200] Final Prec@1 71.4600%
06/02 08:55:24 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:55:25 PM | Train: [141/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 08:55:35 PM | Train: [141/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:55:45 PM | Train: [141/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:55:56 PM | Train: [141/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:56:05 PM | Train: [141/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:56:05 PM | Train: [141/200] Final Prec@1 99.9720%
06/02 08:56:05 PM | Valid: [141/200] Step 000/078 Loss 1.168 Prec@(1,5) (74.2%, 93.0%)
06/02 08:56:08 PM | Valid: [141/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.3%, 90.5%)
06/02 08:56:08 PM | Valid: [141/200] Final Prec@1 71.3000%
06/02 08:56:08 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:56:09 PM | Train: [142/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:56:18 PM | Train: [142/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:56:27 PM | Train: [142/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:56:37 PM | Train: [142/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:56:46 PM | Train: [142/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:56:46 PM | Train: [142/200] Final Prec@1 99.9740%
06/02 08:56:46 PM | Valid: [142/200] Step 000/078 Loss 1.170 Prec@(1,5) (72.7%, 93.0%)
06/02 08:56:49 PM | Valid: [142/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 08:56:49 PM | Valid: [142/200] Final Prec@1 71.1700%
06/02 08:56:49 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:56:50 PM | Train: [143/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:57:00 PM | Train: [143/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:57:10 PM | Train: [143/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:57:20 PM | Train: [143/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:57:29 PM | Train: [143/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:57:29 PM | Train: [143/200] Final Prec@1 99.9740%
06/02 08:57:30 PM | Valid: [143/200] Step 000/078 Loss 1.165 Prec@(1,5) (73.4%, 93.0%)
06/02 08:57:32 PM | Valid: [143/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.5%)
06/02 08:57:32 PM | Valid: [143/200] Final Prec@1 71.1800%
06/02 08:57:33 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:57:33 PM | Train: [144/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 08:57:43 PM | Train: [144/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:57:54 PM | Train: [144/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:58:04 PM | Train: [144/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:58:13 PM | Train: [144/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:58:14 PM | Train: [144/200] Final Prec@1 99.9760%
06/02 08:58:14 PM | Valid: [144/200] Step 000/078 Loss 1.186 Prec@(1,5) (72.7%, 93.0%)
06/02 08:58:16 PM | Valid: [144/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.4%)
06/02 08:58:16 PM | Valid: [144/200] Final Prec@1 71.1500%
06/02 08:58:17 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:58:17 PM | Train: [145/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:58:27 PM | Train: [145/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:58:37 PM | Train: [145/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:58:48 PM | Train: [145/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:58:57 PM | Train: [145/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:58:57 PM | Train: [145/200] Final Prec@1 99.9840%
06/02 08:58:57 PM | Valid: [145/200] Step 000/078 Loss 1.179 Prec@(1,5) (73.4%, 93.8%)
06/02 08:58:59 PM | Valid: [145/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.1%, 90.5%)
06/02 08:58:59 PM | Valid: [145/200] Final Prec@1 71.1100%
06/02 08:59:00 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:59:00 PM | Train: [146/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:59:10 PM | Train: [146/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 08:59:20 PM | Train: [146/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:59:30 PM | Train: [146/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:59:38 PM | Train: [146/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:59:38 PM | Train: [146/200] Final Prec@1 99.9780%
06/02 08:59:39 PM | Valid: [146/200] Step 000/078 Loss 1.185 Prec@(1,5) (73.4%, 92.2%)
06/02 08:59:41 PM | Valid: [146/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.4%)
06/02 08:59:41 PM | Valid: [146/200] Final Prec@1 71.1900%
06/02 08:59:42 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 08:59:42 PM | Train: [147/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 08:59:52 PM | Train: [147/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:00:02 PM | Train: [147/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:00:13 PM | Train: [147/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:00:22 PM | Train: [147/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:00:22 PM | Train: [147/200] Final Prec@1 99.9660%
06/02 09:00:22 PM | Valid: [147/200] Step 000/078 Loss 1.170 Prec@(1,5) (73.4%, 92.2%)
06/02 09:00:24 PM | Valid: [147/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.3%, 90.5%)
06/02 09:00:24 PM | Valid: [147/200] Final Prec@1 71.3400%
06/02 09:00:25 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:00:25 PM | Train: [148/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:00:35 PM | Train: [148/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:00:46 PM | Train: [148/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:00:56 PM | Train: [148/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:01:05 PM | Train: [148/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:01:05 PM | Train: [148/200] Final Prec@1 99.9780%
06/02 09:01:06 PM | Valid: [148/200] Step 000/078 Loss 1.194 Prec@(1,5) (73.4%, 93.8%)
06/02 09:01:08 PM | Valid: [148/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.2%, 90.5%)
06/02 09:01:08 PM | Valid: [148/200] Final Prec@1 71.2500%
06/02 09:01:09 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:01:09 PM | Train: [149/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:01:19 PM | Train: [149/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:01:30 PM | Train: [149/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:01:40 PM | Train: [149/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:01:49 PM | Train: [149/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:01:49 PM | Train: [149/200] Final Prec@1 99.9800%
06/02 09:01:50 PM | Valid: [149/200] Step 000/078 Loss 1.175 Prec@(1,5) (73.4%, 92.2%)
06/02 09:01:52 PM | Valid: [149/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.2%, 90.6%)
06/02 09:01:52 PM | Valid: [149/200] Final Prec@1 71.2300%
06/02 09:01:53 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:01:53 PM | Train: [150/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:02:03 PM | Train: [150/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:02:13 PM | Train: [150/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:02:24 PM | Train: [150/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:02:33 PM | Train: [150/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:02:33 PM | Train: [150/200] Final Prec@1 99.9740%
06/02 09:02:33 PM | Valid: [150/200] Step 000/078 Loss 1.173 Prec@(1,5) (74.2%, 93.8%)
06/02 09:02:35 PM | Valid: [150/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.5%)
06/02 09:02:36 PM | Valid: [150/200] Final Prec@1 71.1500%
06/02 09:02:36 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:02:37 PM | Train: [151/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:02:47 PM | Train: [151/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:02:57 PM | Train: [151/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:03:08 PM | Train: [151/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:03:17 PM | Train: [151/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:03:17 PM | Train: [151/200] Final Prec@1 99.9700%
06/02 09:03:17 PM | Valid: [151/200] Step 000/078 Loss 1.160 Prec@(1,5) (73.4%, 93.8%)
06/02 09:03:20 PM | Valid: [151/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 09:03:20 PM | Valid: [151/200] Final Prec@1 71.1500%
06/02 09:03:20 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:03:21 PM | Train: [152/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:03:31 PM | Train: [152/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:03:41 PM | Train: [152/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:03:51 PM | Train: [152/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:04:01 PM | Train: [152/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:04:01 PM | Train: [152/200] Final Prec@1 99.9620%
06/02 09:04:01 PM | Valid: [152/200] Step 000/078 Loss 1.156 Prec@(1,5) (73.4%, 93.0%)
06/02 09:04:03 PM | Valid: [152/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.2%, 90.4%)
06/02 09:04:03 PM | Valid: [152/200] Final Prec@1 71.1500%
06/02 09:04:04 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:04:04 PM | Train: [153/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:04:14 PM | Train: [153/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:04:25 PM | Train: [153/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:04:34 PM | Train: [153/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:04:44 PM | Train: [153/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:04:44 PM | Train: [153/200] Final Prec@1 99.9720%
06/02 09:04:44 PM | Valid: [153/200] Step 000/078 Loss 1.160 Prec@(1,5) (72.7%, 92.2%)
06/02 09:04:46 PM | Valid: [153/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.4%)
06/02 09:04:46 PM | Valid: [153/200] Final Prec@1 71.2400%
06/02 09:04:47 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:04:47 PM | Train: [154/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:04:57 PM | Train: [154/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:05:08 PM | Train: [154/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:05:17 PM | Train: [154/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:05:26 PM | Train: [154/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:05:26 PM | Train: [154/200] Final Prec@1 99.9740%
06/02 09:05:27 PM | Valid: [154/200] Step 000/078 Loss 1.167 Prec@(1,5) (72.7%, 92.2%)
06/02 09:05:29 PM | Valid: [154/200] Step 078/078 Loss 1.236 Prec@(1,5) (71.4%, 90.5%)
06/02 09:05:29 PM | Valid: [154/200] Final Prec@1 71.4100%
06/02 09:05:30 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:05:30 PM | Train: [155/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:05:41 PM | Train: [155/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:05:51 PM | Train: [155/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:06:01 PM | Train: [155/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:06:10 PM | Train: [155/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:06:10 PM | Train: [155/200] Final Prec@1 99.9740%
06/02 09:06:10 PM | Valid: [155/200] Step 000/078 Loss 1.165 Prec@(1,5) (73.4%, 92.2%)
06/02 09:06:13 PM | Valid: [155/200] Step 078/078 Loss 1.245 Prec@(1,5) (71.3%, 90.5%)
06/02 09:06:13 PM | Valid: [155/200] Final Prec@1 71.2600%
06/02 09:06:13 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:06:14 PM | Train: [156/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:06:24 PM | Train: [156/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:06:34 PM | Train: [156/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:06:44 PM | Train: [156/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:06:53 PM | Train: [156/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:06:53 PM | Train: [156/200] Final Prec@1 99.9720%
06/02 09:06:54 PM | Valid: [156/200] Step 000/078 Loss 1.164 Prec@(1,5) (71.9%, 93.0%)
06/02 09:06:56 PM | Valid: [156/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.2%, 90.5%)
06/02 09:06:56 PM | Valid: [156/200] Final Prec@1 71.2500%
06/02 09:06:57 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:06:57 PM | Train: [157/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:07:07 PM | Train: [157/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:07:18 PM | Train: [157/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:07:28 PM | Train: [157/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:07:38 PM | Train: [157/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:07:38 PM | Train: [157/200] Final Prec@1 99.9740%
06/02 09:07:38 PM | Valid: [157/200] Step 000/078 Loss 1.173 Prec@(1,5) (73.4%, 92.2%)
06/02 09:07:40 PM | Valid: [157/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.4%)
06/02 09:07:40 PM | Valid: [157/200] Final Prec@1 71.1900%
06/02 09:07:41 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:07:41 PM | Train: [158/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:07:51 PM | Train: [158/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:08:01 PM | Train: [158/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:08:11 PM | Train: [158/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:08:21 PM | Train: [158/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:08:21 PM | Train: [158/200] Final Prec@1 99.9700%
06/02 09:08:21 PM | Valid: [158/200] Step 000/078 Loss 1.176 Prec@(1,5) (72.7%, 93.8%)
06/02 09:08:23 PM | Valid: [158/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.2%, 90.5%)
06/02 09:08:23 PM | Valid: [158/200] Final Prec@1 71.2000%
06/02 09:08:24 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:08:24 PM | Train: [159/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:08:34 PM | Train: [159/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:08:44 PM | Train: [159/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:08:54 PM | Train: [159/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:09:04 PM | Train: [159/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:09:04 PM | Train: [159/200] Final Prec@1 99.9680%
06/02 09:09:04 PM | Valid: [159/200] Step 000/078 Loss 1.193 Prec@(1,5) (72.7%, 92.2%)
06/02 09:09:06 PM | Valid: [159/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.3%, 90.5%)
06/02 09:09:06 PM | Valid: [159/200] Final Prec@1 71.3100%
06/02 09:09:07 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:09:07 PM | Train: [160/200] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
06/02 09:09:17 PM | Train: [160/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:09:27 PM | Train: [160/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:09:38 PM | Train: [160/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:09:47 PM | Train: [160/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:09:47 PM | Train: [160/200] Final Prec@1 99.9700%
06/02 09:09:48 PM | Valid: [160/200] Step 000/078 Loss 1.166 Prec@(1,5) (73.4%, 93.0%)
06/02 09:09:50 PM | Valid: [160/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.4%, 90.5%)
06/02 09:09:50 PM | Valid: [160/200] Final Prec@1 71.4100%
06/02 09:09:51 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:09:51 PM | Train: [161/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:10:01 PM | Train: [161/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:10:11 PM | Train: [161/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:10:21 PM | Train: [161/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:10:30 PM | Train: [161/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:10:30 PM | Train: [161/200] Final Prec@1 99.9760%
06/02 09:10:30 PM | Valid: [161/200] Step 000/078 Loss 1.173 Prec@(1,5) (72.7%, 93.0%)
06/02 09:10:32 PM | Valid: [161/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.4%, 90.5%)
06/02 09:10:33 PM | Valid: [161/200] Final Prec@1 71.4200%
06/02 09:10:33 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:10:33 PM | Train: [162/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:10:44 PM | Train: [162/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:10:54 PM | Train: [162/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:11:04 PM | Train: [162/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:11:14 PM | Train: [162/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:11:14 PM | Train: [162/200] Final Prec@1 99.9740%
06/02 09:11:14 PM | Valid: [162/200] Step 000/078 Loss 1.173 Prec@(1,5) (74.2%, 93.0%)
06/02 09:11:16 PM | Valid: [162/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.2%, 90.5%)
06/02 09:11:16 PM | Valid: [162/200] Final Prec@1 71.1600%
06/02 09:11:17 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:11:17 PM | Train: [163/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:11:28 PM | Train: [163/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:11:38 PM | Train: [163/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:11:49 PM | Train: [163/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:11:58 PM | Train: [163/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:11:58 PM | Train: [163/200] Final Prec@1 99.9680%
06/02 09:11:58 PM | Valid: [163/200] Step 000/078 Loss 1.153 Prec@(1,5) (72.7%, 93.0%)
06/02 09:12:01 PM | Valid: [163/200] Step 078/078 Loss 1.236 Prec@(1,5) (71.4%, 90.5%)
06/02 09:12:01 PM | Valid: [163/200] Final Prec@1 71.3500%
06/02 09:12:01 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:12:02 PM | Train: [164/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:12:12 PM | Train: [164/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:12:22 PM | Train: [164/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:12:33 PM | Train: [164/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:12:42 PM | Train: [164/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:12:42 PM | Train: [164/200] Final Prec@1 99.9760%
06/02 09:12:42 PM | Valid: [164/200] Step 000/078 Loss 1.181 Prec@(1,5) (72.7%, 92.2%)
06/02 09:12:45 PM | Valid: [164/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.3%, 90.5%)
06/02 09:12:45 PM | Valid: [164/200] Final Prec@1 71.2800%
06/02 09:12:45 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:12:46 PM | Train: [165/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:12:56 PM | Train: [165/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:13:06 PM | Train: [165/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:13:17 PM | Train: [165/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:13:26 PM | Train: [165/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:13:26 PM | Train: [165/200] Final Prec@1 99.9740%
06/02 09:13:27 PM | Valid: [165/200] Step 000/078 Loss 1.175 Prec@(1,5) (72.7%, 93.0%)
06/02 09:13:29 PM | Valid: [165/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.4%)
06/02 09:13:29 PM | Valid: [165/200] Final Prec@1 71.2300%
06/02 09:13:29 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:13:30 PM | Train: [166/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:13:40 PM | Train: [166/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:13:51 PM | Train: [166/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:14:01 PM | Train: [166/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:14:10 PM | Train: [166/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:14:10 PM | Train: [166/200] Final Prec@1 99.9740%
06/02 09:14:11 PM | Valid: [166/200] Step 000/078 Loss 1.183 Prec@(1,5) (72.7%, 93.0%)
06/02 09:14:13 PM | Valid: [166/200] Step 078/078 Loss 1.244 Prec@(1,5) (71.3%, 90.4%)
06/02 09:14:13 PM | Valid: [166/200] Final Prec@1 71.2600%
06/02 09:14:14 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:14:14 PM | Train: [167/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:14:24 PM | Train: [167/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:14:35 PM | Train: [167/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:14:45 PM | Train: [167/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:14:54 PM | Train: [167/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:14:54 PM | Train: [167/200] Final Prec@1 99.9660%
06/02 09:14:55 PM | Valid: [167/200] Step 000/078 Loss 1.153 Prec@(1,5) (72.7%, 93.0%)
06/02 09:14:57 PM | Valid: [167/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.4%, 90.4%)
06/02 09:14:57 PM | Valid: [167/200] Final Prec@1 71.4400%
06/02 09:14:57 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:14:58 PM | Train: [168/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:15:08 PM | Train: [168/200] Step 100/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 09:15:18 PM | Train: [168/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:15:28 PM | Train: [168/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:15:38 PM | Train: [168/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:15:38 PM | Train: [168/200] Final Prec@1 99.9700%
06/02 09:15:38 PM | Valid: [168/200] Step 000/078 Loss 1.168 Prec@(1,5) (71.9%, 93.8%)
06/02 09:15:40 PM | Valid: [168/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.1%, 90.5%)
06/02 09:15:40 PM | Valid: [168/200] Final Prec@1 71.1400%
06/02 09:15:41 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:15:41 PM | Train: [169/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:15:51 PM | Train: [169/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:16:02 PM | Train: [169/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:16:12 PM | Train: [169/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:16:22 PM | Train: [169/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:16:22 PM | Train: [169/200] Final Prec@1 99.9860%
06/02 09:16:22 PM | Valid: [169/200] Step 000/078 Loss 1.175 Prec@(1,5) (72.7%, 93.0%)
06/02 09:16:25 PM | Valid: [169/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.3%, 90.5%)
06/02 09:16:25 PM | Valid: [169/200] Final Prec@1 71.2600%
06/02 09:16:25 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:16:25 PM | Train: [170/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:16:35 PM | Train: [170/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:16:46 PM | Train: [170/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:16:55 PM | Train: [170/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:17:05 PM | Train: [170/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:17:05 PM | Train: [170/200] Final Prec@1 99.9760%
06/02 09:17:05 PM | Valid: [170/200] Step 000/078 Loss 1.196 Prec@(1,5) (72.7%, 92.2%)
06/02 09:17:07 PM | Valid: [170/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.1%, 90.5%)
06/02 09:17:07 PM | Valid: [170/200] Final Prec@1 71.1400%
06/02 09:17:08 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:17:08 PM | Train: [171/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:17:19 PM | Train: [171/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:17:29 PM | Train: [171/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:17:39 PM | Train: [171/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:17:49 PM | Train: [171/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:17:49 PM | Train: [171/200] Final Prec@1 99.9760%
06/02 09:17:49 PM | Valid: [171/200] Step 000/078 Loss 1.175 Prec@(1,5) (73.4%, 92.2%)
06/02 09:17:51 PM | Valid: [171/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.2%, 90.5%)
06/02 09:17:51 PM | Valid: [171/200] Final Prec@1 71.1800%
06/02 09:17:52 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:17:52 PM | Train: [172/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:18:02 PM | Train: [172/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:18:13 PM | Train: [172/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:18:23 PM | Train: [172/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:18:32 PM | Train: [172/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:18:32 PM | Train: [172/200] Final Prec@1 99.9740%
06/02 09:18:32 PM | Valid: [172/200] Step 000/078 Loss 1.167 Prec@(1,5) (73.4%, 92.2%)
06/02 09:18:35 PM | Valid: [172/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.3%, 90.5%)
06/02 09:18:35 PM | Valid: [172/200] Final Prec@1 71.3300%
06/02 09:18:35 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:18:36 PM | Train: [173/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:18:46 PM | Train: [173/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:18:56 PM | Train: [173/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:19:06 PM | Train: [173/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:19:16 PM | Train: [173/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:19:16 PM | Train: [173/200] Final Prec@1 99.9680%
06/02 09:19:16 PM | Valid: [173/200] Step 000/078 Loss 1.162 Prec@(1,5) (72.7%, 93.0%)
06/02 09:19:18 PM | Valid: [173/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.6%)
06/02 09:19:18 PM | Valid: [173/200] Final Prec@1 71.1900%
06/02 09:19:19 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:19:19 PM | Train: [174/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:19:29 PM | Train: [174/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:19:39 PM | Train: [174/200] Step 200/390 Loss 0.004 Prec@(1,5) (99.9%, 100.0%)
06/02 09:19:49 PM | Train: [174/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:19:58 PM | Train: [174/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:19:58 PM | Train: [174/200] Final Prec@1 99.9600%
06/02 09:19:58 PM | Valid: [174/200] Step 000/078 Loss 1.174 Prec@(1,5) (73.4%, 93.0%)
06/02 09:20:00 PM | Valid: [174/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.1%, 90.4%)
06/02 09:20:00 PM | Valid: [174/200] Final Prec@1 71.1300%
06/02 09:20:01 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:20:01 PM | Train: [175/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 09:20:12 PM | Train: [175/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:20:22 PM | Train: [175/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:20:32 PM | Train: [175/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:20:42 PM | Train: [175/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:20:42 PM | Train: [175/200] Final Prec@1 99.9740%
06/02 09:20:42 PM | Valid: [175/200] Step 000/078 Loss 1.175 Prec@(1,5) (71.9%, 93.0%)
06/02 09:20:44 PM | Valid: [175/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.1%, 90.5%)
06/02 09:20:44 PM | Valid: [175/200] Final Prec@1 71.1000%
06/02 09:20:45 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:20:45 PM | Train: [176/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:20:55 PM | Train: [176/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:21:06 PM | Train: [176/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:21:16 PM | Train: [176/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:21:25 PM | Train: [176/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:21:25 PM | Train: [176/200] Final Prec@1 99.9760%
06/02 09:21:26 PM | Valid: [176/200] Step 000/078 Loss 1.182 Prec@(1,5) (72.7%, 93.8%)
06/02 09:21:28 PM | Valid: [176/200] Step 078/078 Loss 1.242 Prec@(1,5) (71.1%, 90.5%)
06/02 09:21:28 PM | Valid: [176/200] Final Prec@1 71.1400%
06/02 09:21:29 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:21:29 PM | Train: [177/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:21:39 PM | Train: [177/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:21:50 PM | Train: [177/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:22:00 PM | Train: [177/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:22:09 PM | Train: [177/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:22:09 PM | Train: [177/200] Final Prec@1 99.9720%
06/02 09:22:09 PM | Valid: [177/200] Step 000/078 Loss 1.182 Prec@(1,5) (72.7%, 91.4%)
06/02 09:22:12 PM | Valid: [177/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.1%, 90.3%)
06/02 09:22:12 PM | Valid: [177/200] Final Prec@1 71.1200%
06/02 09:22:12 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:22:13 PM | Train: [178/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:22:23 PM | Train: [178/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:22:33 PM | Train: [178/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:22:43 PM | Train: [178/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:22:53 PM | Train: [178/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:22:53 PM | Train: [178/200] Final Prec@1 99.9680%
06/02 09:22:53 PM | Valid: [178/200] Step 000/078 Loss 1.165 Prec@(1,5) (72.7%, 93.0%)
06/02 09:22:55 PM | Valid: [178/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.5%, 90.5%)
06/02 09:22:55 PM | Valid: [178/200] Final Prec@1 71.4800%
06/02 09:22:56 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:22:56 PM | Train: [179/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:23:07 PM | Train: [179/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:23:17 PM | Train: [179/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:23:27 PM | Train: [179/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:23:36 PM | Train: [179/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:23:37 PM | Train: [179/200] Final Prec@1 99.9740%
06/02 09:23:37 PM | Valid: [179/200] Step 000/078 Loss 1.172 Prec@(1,5) (72.7%, 93.8%)
06/02 09:23:39 PM | Valid: [179/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.4%)
06/02 09:23:39 PM | Valid: [179/200] Final Prec@1 71.2200%
06/02 09:23:40 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:23:40 PM | Train: [180/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:23:50 PM | Train: [180/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:24:00 PM | Train: [180/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:24:11 PM | Train: [180/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:24:20 PM | Train: [180/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:24:20 PM | Train: [180/200] Final Prec@1 99.9700%
06/02 09:24:20 PM | Valid: [180/200] Step 000/078 Loss 1.179 Prec@(1,5) (72.7%, 93.8%)
06/02 09:24:23 PM | Valid: [180/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.1%, 90.5%)
06/02 09:24:23 PM | Valid: [180/200] Final Prec@1 71.1000%
06/02 09:24:23 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:24:24 PM | Train: [181/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:24:34 PM | Train: [181/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:24:44 PM | Train: [181/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:24:54 PM | Train: [181/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:25:03 PM | Train: [181/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:25:03 PM | Train: [181/200] Final Prec@1 99.9780%
06/02 09:25:03 PM | Valid: [181/200] Step 000/078 Loss 1.181 Prec@(1,5) (73.4%, 92.2%)
06/02 09:25:06 PM | Valid: [181/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.3%, 90.4%)
06/02 09:25:06 PM | Valid: [181/200] Final Prec@1 71.2600%
06/02 09:25:06 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:25:07 PM | Train: [182/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:25:17 PM | Train: [182/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:25:28 PM | Train: [182/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:25:38 PM | Train: [182/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:25:47 PM | Train: [182/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:25:48 PM | Train: [182/200] Final Prec@1 99.9720%
06/02 09:25:48 PM | Valid: [182/200] Step 000/078 Loss 1.191 Prec@(1,5) (72.7%, 92.2%)
06/02 09:25:50 PM | Valid: [182/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.1%, 90.5%)
06/02 09:25:50 PM | Valid: [182/200] Final Prec@1 71.0800%
06/02 09:25:51 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:25:51 PM | Train: [183/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:26:01 PM | Train: [183/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:26:12 PM | Train: [183/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:26:22 PM | Train: [183/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:26:31 PM | Train: [183/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:26:31 PM | Train: [183/200] Final Prec@1 99.9800%
06/02 09:26:32 PM | Valid: [183/200] Step 000/078 Loss 1.182 Prec@(1,5) (72.7%, 93.0%)
06/02 09:26:34 PM | Valid: [183/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.2%, 90.6%)
06/02 09:26:34 PM | Valid: [183/200] Final Prec@1 71.2400%
06/02 09:26:35 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:26:35 PM | Train: [184/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:26:45 PM | Train: [184/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:26:56 PM | Train: [184/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:27:07 PM | Train: [184/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:27:16 PM | Train: [184/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:27:16 PM | Train: [184/200] Final Prec@1 99.9720%
06/02 09:27:16 PM | Valid: [184/200] Step 000/078 Loss 1.172 Prec@(1,5) (72.7%, 92.2%)
06/02 09:27:18 PM | Valid: [184/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.0%, 90.5%)
06/02 09:27:19 PM | Valid: [184/200] Final Prec@1 71.0400%
06/02 09:27:19 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:27:20 PM | Train: [185/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 09:27:30 PM | Train: [185/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:27:40 PM | Train: [185/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:27:50 PM | Train: [185/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:28:00 PM | Train: [185/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:28:00 PM | Train: [185/200] Final Prec@1 99.9740%
06/02 09:28:00 PM | Valid: [185/200] Step 000/078 Loss 1.166 Prec@(1,5) (72.7%, 93.8%)
06/02 09:28:02 PM | Valid: [185/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.4%, 90.4%)
06/02 09:28:03 PM | Valid: [185/200] Final Prec@1 71.4100%
06/02 09:28:03 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:28:04 PM | Train: [186/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:28:14 PM | Train: [186/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:28:24 PM | Train: [186/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:28:35 PM | Train: [186/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:28:44 PM | Train: [186/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:28:44 PM | Train: [186/200] Final Prec@1 99.9740%
06/02 09:28:45 PM | Valid: [186/200] Step 000/078 Loss 1.175 Prec@(1,5) (72.7%, 93.8%)
06/02 09:28:47 PM | Valid: [186/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.3%, 90.5%)
06/02 09:28:47 PM | Valid: [186/200] Final Prec@1 71.2600%
06/02 09:28:48 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:28:48 PM | Train: [187/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 09:28:58 PM | Train: [187/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:29:09 PM | Train: [187/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:29:20 PM | Train: [187/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:29:28 PM | Train: [187/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:29:28 PM | Train: [187/200] Final Prec@1 99.9840%
06/02 09:29:29 PM | Valid: [187/200] Step 000/078 Loss 1.179 Prec@(1,5) (73.4%, 93.0%)
06/02 09:29:31 PM | Valid: [187/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.4%, 90.5%)
06/02 09:29:31 PM | Valid: [187/200] Final Prec@1 71.3500%
06/02 09:29:32 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:29:32 PM | Train: [188/200] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
06/02 09:29:42 PM | Train: [188/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:29:53 PM | Train: [188/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:30:03 PM | Train: [188/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:30:13 PM | Train: [188/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:30:13 PM | Train: [188/200] Final Prec@1 99.9720%
06/02 09:30:13 PM | Valid: [188/200] Step 000/078 Loss 1.181 Prec@(1,5) (72.7%, 92.2%)
06/02 09:30:16 PM | Valid: [188/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 09:30:16 PM | Valid: [188/200] Final Prec@1 71.2500%
06/02 09:30:16 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:30:17 PM | Train: [189/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:30:27 PM | Train: [189/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:30:38 PM | Train: [189/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:30:48 PM | Train: [189/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:30:58 PM | Train: [189/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:30:58 PM | Train: [189/200] Final Prec@1 99.9740%
06/02 09:30:58 PM | Valid: [189/200] Step 000/078 Loss 1.179 Prec@(1,5) (72.7%, 92.2%)
06/02 09:31:00 PM | Valid: [189/200] Step 078/078 Loss 1.241 Prec@(1,5) (71.2%, 90.4%)
06/02 09:31:00 PM | Valid: [189/200] Final Prec@1 71.2200%
06/02 09:31:01 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:31:01 PM | Train: [190/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:31:12 PM | Train: [190/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:31:22 PM | Train: [190/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:31:33 PM | Train: [190/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:31:42 PM | Train: [190/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:31:42 PM | Train: [190/200] Final Prec@1 99.9800%
06/02 09:31:42 PM | Valid: [190/200] Step 000/078 Loss 1.176 Prec@(1,5) (73.4%, 92.2%)
06/02 09:31:45 PM | Valid: [190/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.2%, 90.5%)
06/02 09:31:45 PM | Valid: [190/200] Final Prec@1 71.1900%
06/02 09:31:45 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:31:46 PM | Train: [191/200] Step 000/390 Loss 0.009 Prec@(1,5) (99.2%, 100.0%)
06/02 09:31:56 PM | Train: [191/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:32:07 PM | Train: [191/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:32:17 PM | Train: [191/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:32:26 PM | Train: [191/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:32:26 PM | Train: [191/200] Final Prec@1 99.9740%
06/02 09:32:27 PM | Valid: [191/200] Step 000/078 Loss 1.181 Prec@(1,5) (72.7%, 93.0%)
06/02 09:32:29 PM | Valid: [191/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.3%, 90.5%)
06/02 09:32:29 PM | Valid: [191/200] Final Prec@1 71.3400%
06/02 09:32:29 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:32:30 PM | Train: [192/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:32:40 PM | Train: [192/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:32:51 PM | Train: [192/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:33:01 PM | Train: [192/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:33:11 PM | Train: [192/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:33:11 PM | Train: [192/200] Final Prec@1 99.9700%
06/02 09:33:11 PM | Valid: [192/200] Step 000/078 Loss 1.167 Prec@(1,5) (74.2%, 93.0%)
06/02 09:33:14 PM | Valid: [192/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.3%, 90.6%)
06/02 09:33:14 PM | Valid: [192/200] Final Prec@1 71.3100%
06/02 09:33:14 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:33:15 PM | Train: [193/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:33:25 PM | Train: [193/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:33:35 PM | Train: [193/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:33:46 PM | Train: [193/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:33:55 PM | Train: [193/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:33:55 PM | Train: [193/200] Final Prec@1 99.9740%
06/02 09:33:56 PM | Valid: [193/200] Step 000/078 Loss 1.172 Prec@(1,5) (73.4%, 93.0%)
06/02 09:33:58 PM | Valid: [193/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.4%, 90.5%)
06/02 09:33:58 PM | Valid: [193/200] Final Prec@1 71.4300%
06/02 09:33:59 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:33:59 PM | Train: [194/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:34:09 PM | Train: [194/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:34:20 PM | Train: [194/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:34:30 PM | Train: [194/200] Step 300/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:34:39 PM | Train: [194/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:34:39 PM | Train: [194/200] Final Prec@1 99.9720%
06/02 09:34:39 PM | Valid: [194/200] Step 000/078 Loss 1.169 Prec@(1,5) (73.4%, 93.0%)
06/02 09:34:41 PM | Valid: [194/200] Step 078/078 Loss 1.240 Prec@(1,5) (71.3%, 90.5%)
06/02 09:34:41 PM | Valid: [194/200] Final Prec@1 71.3000%
06/02 09:34:42 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:34:42 PM | Train: [195/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:34:53 PM | Train: [195/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:35:04 PM | Train: [195/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:35:14 PM | Train: [195/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:35:23 PM | Train: [195/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:35:23 PM | Train: [195/200] Final Prec@1 99.9820%
06/02 09:35:23 PM | Valid: [195/200] Step 000/078 Loss 1.167 Prec@(1,5) (73.4%, 92.2%)
06/02 09:35:26 PM | Valid: [195/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.4%, 90.5%)
06/02 09:35:26 PM | Valid: [195/200] Final Prec@1 71.3900%
06/02 09:35:26 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:35:27 PM | Train: [196/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:35:37 PM | Train: [196/200] Step 100/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:35:47 PM | Train: [196/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:35:58 PM | Train: [196/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:36:07 PM | Train: [196/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:36:07 PM | Train: [196/200] Final Prec@1 99.9820%
06/02 09:36:08 PM | Valid: [196/200] Step 000/078 Loss 1.178 Prec@(1,5) (72.7%, 93.0%)
06/02 09:36:10 PM | Valid: [196/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.3%, 90.4%)
06/02 09:36:10 PM | Valid: [196/200] Final Prec@1 71.3100%
06/02 09:36:11 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:36:11 PM | Train: [197/200] Step 000/390 Loss 0.002 Prec@(1,5) (100.0%, 100.0%)
06/02 09:36:22 PM | Train: [197/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:36:33 PM | Train: [197/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:36:43 PM | Train: [197/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:36:53 PM | Train: [197/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:36:53 PM | Train: [197/200] Final Prec@1 99.9820%
06/02 09:36:53 PM | Valid: [197/200] Step 000/078 Loss 1.167 Prec@(1,5) (72.7%, 93.0%)
06/02 09:36:55 PM | Valid: [197/200] Step 078/078 Loss 1.238 Prec@(1,5) (71.2%, 90.5%)
06/02 09:36:55 PM | Valid: [197/200] Final Prec@1 71.1800%
06/02 09:36:56 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:36:57 PM | Train: [198/200] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:37:07 PM | Train: [198/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:37:18 PM | Train: [198/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:37:29 PM | Train: [198/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:37:39 PM | Train: [198/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:37:39 PM | Train: [198/200] Final Prec@1 99.9740%
06/02 09:37:39 PM | Valid: [198/200] Step 000/078 Loss 1.183 Prec@(1,5) (72.7%, 93.0%)
06/02 09:37:41 PM | Valid: [198/200] Step 078/078 Loss 1.243 Prec@(1,5) (71.1%, 90.5%)
06/02 09:37:41 PM | Valid: [198/200] Final Prec@1 71.1100%
06/02 09:37:42 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:37:42 PM | Train: [199/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:37:53 PM | Train: [199/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:38:03 PM | Train: [199/200] Step 200/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:38:14 PM | Train: [199/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:38:23 PM | Train: [199/200] Step 390/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:38:23 PM | Train: [199/200] Final Prec@1 99.9760%
06/02 09:38:23 PM | Valid: [199/200] Step 000/078 Loss 1.184 Prec@(1,5) (72.7%, 92.2%)
06/02 09:38:25 PM | Valid: [199/200] Step 078/078 Loss 1.239 Prec@(1,5) (71.3%, 90.4%)
06/02 09:38:25 PM | Valid: [199/200] Final Prec@1 71.3000%
06/02 09:38:26 PM | Current best Prec@1 = 71.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, remained count, remained percentage]
['model.relu.alpha_mask_0_0', 65536, 36499.0, 0.5569305419921875]
['model.relu.alpha_mask_1_0', 16384, 9987.0, 0.60955810546875]
['model.relu.alpha_mask_2_0', 16384, 9635.0, 0.58807373046875]
['model.relu.alpha_mask_3_0', 16384, 9975.0, 0.60882568359375]
['model.relu.alpha_mask_4_0', 16384, 9562.0, 0.5836181640625]
['model.relu.alpha_mask_5_0', 8192, 6019.0, 0.7347412109375]
['model.relu.alpha_mask_6_0', 8192, 5484.0, 0.66943359375]
['model.relu.alpha_mask_7_0', 8192, 5734.0, 0.699951171875]
['model.relu.alpha_mask_8_0', 8192, 5436.0, 0.66357421875]
['model.relu.alpha_mask_9_0', 4096, 3403.0, 0.830810546875]
['model.relu.alpha_mask_10_0', 4096, 3199.0, 0.781005859375]
['model.relu.alpha_mask_11_0', 4096, 3414.0, 0.83349609375]
['model.relu.alpha_mask_12_0', 4096, 3137.0, 0.765869140625]
['model.relu.alpha_mask_13_0', 2048, 2040.0, 0.99609375]
['model.relu.alpha_mask_14_0', 2048, 2044.0, 0.998046875]
['model.relu.alpha_mask_15_0', 2048, 1955.0, 0.95458984375]
['model.relu.alpha_mask_16_0', 2048, 1960.0, 0.95703125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, remained count, remained percentage]
[188416, 119483.0, 0.6341446586277174]
########## End ###########
06/02 09:38:26 PM | Train: [200/200] Step 000/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:38:37 PM | Train: [200/200] Step 100/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:38:47 PM | Train: [200/200] Step 200/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:38:58 PM | Train: [200/200] Step 300/390 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)
06/02 09:39:07 PM | Train: [200/200] Step 390/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
06/02 09:39:08 PM | Train: [200/200] Final Prec@1 99.9700%
06/02 09:39:08 PM | Valid: [200/200] Step 000/078 Loss 1.174 Prec@(1,5) (74.2%, 92.2%)
06/02 09:39:10 PM | Valid: [200/200] Step 078/078 Loss 1.237 Prec@(1,5) (71.2%, 90.4%)
06/02 09:39:10 PM | Valid: [200/200] Final Prec@1 71.2300%
06/02 09:39:11 PM | Current best Prec@1 = 71.4800%
06/02 09:39:11 PM | Final best validation Prec@1 = 71.4800%
[HAMI-core Msg(622451:133346881969024:multiprocess_memory_limit.c:498)]: Calling exit handler 622451
